#!/usr/bin/env python3
"""
FACEIQ RACE&GENDER HAR File Analyzer + Minimizer
Analyzes the HAR file and creates a smaller version
"""

import sys
import os
import json
import re
from datetime import datetime
from urllib.parse import urlparse
from collections import defaultdict

# Add haralyzer to path
sys.path.insert(0, '/Users/imorgado/Desktop/HAR ANALYZER/haralyzer')

from haralyzer import HarParser

def sanitize_filename(name, max_length=50):
    """Create a safe filename from a string"""
    name = re.sub(r'[<>:"/\\|?*]', '_', name)
    name = re.sub(r'\s+', '_', name)
    name = re.sub(r'_+', '_', name)
    name = name.strip('_')
    if len(name) > max_length:
        name = name[:max_length]
    return name or 'unnamed'

def format_size(size_bytes):
    """Format bytes to human readable format"""
    if size_bytes < 0:
        return "N/A"
    for unit in ['B', 'KB', 'MB', 'GB']:
        if abs(size_bytes) < 1024.0:
            return f"{size_bytes:.2f} {unit}"
        size_bytes /= 1024.0
    return f"{size_bytes:.2f} TB"

def format_time(time_ms):
    """Format milliseconds to human readable format"""
    if time_ms is None or time_ms < 0:
        return "N/A"
    if time_ms < 1000:
        return f"{time_ms:.2f} ms"
    elif time_ms < 60000:
        return f"{time_ms/1000:.2f} s"
    else:
        return f"{time_ms/60000:.2f} min"

def get_domain(url):
    """Extract domain from URL"""
    try:
        parsed = urlparse(url)
        return parsed.netloc
    except:
        return "unknown"

def is_api_or_json(entry_raw):
    """Check if entry is an API call or JSON response"""
    try:
        url = entry_raw.get('request', {}).get('url', '')
        mime = entry_raw.get('response', {}).get('content', {}).get('mimeType', '')

        # Check for API endpoints
        if '/api/' in url or 'api.' in url:
            return True
        # Check for JSON responses
        if 'json' in mime.lower():
            return True
        # Check for specific FaceIQ endpoints
        if any(x in url for x in ['analyze', 'detect', 'predict', 'score', 'landmark']):
            return True
        return False
    except:
        return False

def minimize_har(input_file, output_file):
    """Create a minimized version of the HAR file"""
    print(f"\nMinimizing HAR file...")

    with open(input_file, 'r', encoding='utf-8') as f:
        har_data = json.load(f)

    original_size = os.path.getsize(input_file)
    print(f"Original size: {format_size(original_size)}")

    entries = har_data.get('log', {}).get('entries', [])
    print(f"Total entries: {len(entries)}")

    # Keep track of what we filter
    kept_count = 0
    removed_count = 0

    new_entries = []

    for entry in entries:
        url = entry.get('request', {}).get('url', '')
        mime = entry.get('response', {}).get('content', {}).get('mimeType', '')

        # Always strip large binary content
        content = entry.get('response', {}).get('content', {})

        # Check what type of request this is
        is_binary = any(x in mime.lower() for x in ['image', 'font', 'woff', 'video', 'audio', 'octet-stream'])
        is_script = any(x in mime.lower() for x in ['javascript', 'css'])
        is_important = is_api_or_json(entry) or 'html' in mime.lower()

        if is_binary:
            # Remove binary content entirely
            if 'text' in content:
                del content['text']
            if 'encoding' in content:
                del content['encoding']
            removed_count += 1
        elif is_script and not is_important:
            # Keep script metadata but remove the actual code
            if 'text' in content and len(content.get('text', '')) > 1000:
                content['text'] = f"[CONTENT REMOVED - {format_size(len(content['text']))}]"
            removed_count += 1
        else:
            kept_count += 1

        new_entries.append(entry)

    har_data['log']['entries'] = new_entries

    # Write minimized file
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(har_data, f, separators=(',', ':'))  # No indentation, compact

    new_size = os.path.getsize(output_file)
    reduction = ((original_size - new_size) / original_size) * 100

    print(f"\nMinimization complete:")
    print(f"  Original: {format_size(original_size)}")
    print(f"  Minimized: {format_size(new_size)}")
    print(f"  Reduction: {reduction:.1f}%")
    print(f"  Kept full content: {kept_count} entries")
    print(f"  Stripped content: {removed_count} entries")
    print(f"\nOutput: {output_file}")

    return new_size

def analyze_har(input_file, output_dir):
    """Analyze the HAR file and create reports"""
    print(f"Loading HAR file: {input_file}")

    parser = HarParser.from_file(input_file)

    print(f"HAR Version: {parser.version}")
    print(f"Creator: {parser.creator}")
    print(f"Number of pages: {len(parser.pages)}")

    # Create output directory
    os.makedirs(output_dir, exist_ok=True)

    # Create main summary
    summary_path = os.path.join(output_dir, "00_RACE_GENDER_SUMMARY.txt")
    with open(summary_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("FACEIQ RACE & GENDER HAR ANALYSIS REPORT\n")
        f.write("=" * 80 + "\n\n")
        f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"Source File: {input_file}\n")
        f.write(f"HAR Version: {parser.version}\n")
        f.write(f"Creator: {parser.creator}\n\n")

        for i, page in enumerate(parser.pages):
            f.write(f"\nPage {i + 1}: {page.page_id}\n")
            f.write(f"  Entries: {len(page.entries)}\n")

    print(f"\nCreated: {summary_path}")

    # Find all API calls and interesting endpoints
    api_calls_path = os.path.join(output_dir, "01_API_CALLS.txt")
    json_responses_path = os.path.join(output_dir, "02_JSON_RESPONSES.json")

    api_entries = []
    json_responses = []

    for page in parser.pages:
        for i, entry in enumerate(page.entries):
            try:
                url = entry.request.url
                mime = entry.response.mimeType or ''
                method = entry.request.method
                status = entry.response.status

                # Check if it's an API call or JSON response
                is_api = '/api/' in url or 'api.' in url
                is_json = 'json' in mime.lower()
                has_interesting_path = any(x in url.lower() for x in [
                    'analyze', 'detect', 'predict', 'score', 'landmark',
                    'race', 'gender', 'age', 'ethnicity', 'face'
                ])

                if is_api or is_json or has_interesting_path:
                    api_entries.append({
                        'index': i + 1,
                        'method': method,
                        'url': url,
                        'status': status,
                        'mime': mime,
                        'size': entry.response.bodySize,
                        'time': entry.time
                    })

                    # Try to get JSON response content
                    if is_json:
                        try:
                            content = entry.raw_entry.get('response', {}).get('content', {})
                            text = content.get('text', '')
                            if text:
                                try:
                                    json_data = json.loads(text)
                                    json_responses.append({
                                        'url': url,
                                        'method': method,
                                        'status': status,
                                        'response': json_data
                                    })
                                except:
                                    pass
                        except:
                            pass
            except Exception as e:
                pass

    # Write API calls report
    with open(api_calls_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("API CALLS & INTERESTING ENDPOINTS\n")
        f.write(f"Total found: {len(api_entries)}\n")
        f.write("=" * 80 + "\n\n")

        for entry in api_entries:
            f.write(f"\n#{entry['index']} [{entry['method']}] Status: {entry['status']}\n")
            f.write(f"URL: {entry['url']}\n")
            f.write(f"MIME: {entry['mime']}\n")
            f.write(f"Size: {format_size(entry['size'])} | Time: {format_time(entry['time'])}\n")
            f.write("-" * 60 + "\n")

    print(f"Created: {api_calls_path}")
    print(f"  Found {len(api_entries)} API/JSON entries")

    # Write JSON responses
    with open(json_responses_path, 'w', encoding='utf-8') as f:
        json.dump(json_responses, f, indent=2, default=str)

    print(f"Created: {json_responses_path}")
    print(f"  Extracted {len(json_responses)} JSON responses")

    # Create domain breakdown
    domains_path = os.path.join(output_dir, "03_DOMAINS.txt")
    domains = defaultdict(lambda: {'count': 0, 'size': 0})

    for page in parser.pages:
        for entry in page.entries:
            try:
                domain = get_domain(entry.request.url)
                domains[domain]['count'] += 1
                domains[domain]['size'] += entry.response.bodySize if entry.response.bodySize > 0 else 0
            except:
                pass

    with open(domains_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("DOMAIN BREAKDOWN\n")
        f.write("=" * 80 + "\n\n")

        for domain in sorted(domains.keys(), key=lambda x: -domains[x]['count']):
            data = domains[domain]
            f.write(f"{domain}\n")
            f.write(f"  Requests: {data['count']}\n")
            f.write(f"  Size: {format_size(data['size'])}\n\n")

    print(f"Created: {domains_path}")

    return len(api_entries), len(json_responses)

def main():
    input_file = "/Users/imorgado/Desktop/FACEIQPAID/FACEIQ-RACE&GENDER.har"
    output_dir = "/Users/imorgado/Desktop/FACEIQHAR/RACE_GENDER_ANALYSIS"
    minimized_file = "/Users/imorgado/Desktop/FACEIQPAID/FACEIQ-RACE&GENDER-MINI.har"

    # Step 1: Analyze
    print("=" * 80)
    print("STEP 1: ANALYZING HAR FILE")
    print("=" * 80)
    api_count, json_count = analyze_har(input_file, output_dir)

    # Step 2: Minimize
    print("\n" + "=" * 80)
    print("STEP 2: CREATING MINIMIZED VERSION")
    print("=" * 80)
    minimize_har(input_file, minimized_file)

    print("\n" + "=" * 80)
    print("COMPLETE!")
    print("=" * 80)
    print(f"\nAnalysis output: {output_dir}")
    print(f"Minimized HAR: {minimized_file}")

if __name__ == "__main__":
    main()
