"""
[INFRASTRUCTURE] Local Research Lab v1.0
Purpose: Offline Inference Environment for Infrastructure Latency Test.
Context: Infrastructure setup for local LLM/Diffusion deployment.

Capabilities:
1. TEXT: Loads 'Standard' Llama/Mistral models via transformers.
2. IMAGE: Loads 'Flux/Stable Diffusion' models via diffusers.
3. MODE: 'Offline' (No safety filter APIs involved).
"""

import sys
import torch
import warnings
import json
import time
import requests

# Suppress HF warnings
warnings.filterwarnings("ignore")


class ComfyClient:
    """Client wrapper for ComfyUI API at http://127.0.0.1:8188"""

    def __init__(self, base_url: str = "http://127.0.0.1:8188"):
        self.base_url = base_url.rstrip("/")
        self.workflow_schema = {
            "nodes": [{"id": 6, "type": "CLIPTextEncode", "widgets_values": ["test prompt"]}],
            "version": 0.4
        }

    def run_latency_benchmark(self, prompt_text: str) -> dict:
        """
        Run latency benchmark with the given prompt text.

        Args:
            prompt_text: Text to inject into Node ID 6

        Returns:
            dict with response_time_ms, status_code, and response data
        """
        # Deep copy workflow and inject prompt into Node ID 6
        workflow = json.loads(json.dumps(self.workflow_schema))
        for node in workflow.get("nodes", []):
            if node.get("id") == 6:
                node["widgets_values"] = [prompt_text]
                break

        # Prepare payload for /prompt endpoint
        payload = {"prompt": workflow}

        # Measure request latency
        start_time = time.perf_counter()
        try:
            response = requests.post(
                f"{self.base_url}/prompt",
                json=payload,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            end_time = time.perf_counter()

            return {
                "response_time_ms": round((end_time - start_time) * 1000, 2),
                "status_code": response.status_code,
                "success": response.ok,
                "data": response.json() if response.ok else response.text
            }
        except requests.exceptions.ConnectionError:
            end_time = time.perf_counter()
            return {
                "response_time_ms": round((end_time - start_time) * 1000, 2),
                "status_code": None,
                "success": False,
                "error": "Connection refused - is ComfyUI running on port 8188?"
            }
        except requests.exceptions.Timeout:
            end_time = time.perf_counter()
            return {
                "response_time_ms": round((end_time - start_time) * 1000, 2),
                "status_code": None,
                "success": False,
                "error": "Request timed out"
            }

class LocalModelLab:
    def __init__(self):
        print("\n=== Local Research Lab (Unrestricted) ===")
        print("1. Text Generation (Llama-3-Standard)")
        print("2. Image Generation (Flux.1-Dev)")
        print("3. Exit")
        
        self.device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
        print(f"[System] Inference Device: {self.device.upper()}")

    def load_text_model(self):
        print("\n[Loader] Initializing Text Pipeline...")
        try:
            from transformers import pipeline
            # Target: TinyLlama (Fast, Open, Logic-capable)
            model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0" 
            print(f"[Config] Loading Model: {model_id}")
            
            generator = pipeline(
                "text-generation",
                model=model_id,
                device_map="cpu", # Force CPU for compatibility if MPS fails, or "auto"
                torch_dtype=torch.float32 # Stable for CPU
            )
            
            print("[Ready] Type specific research prompts (Ctrl+C to exit):")
            while True:
                prompt = input("\n>> ")
                if prompt.lower() in ['exit', 'quit']: break
                
                output = generator(
                    prompt, 
                    max_length=512, 
                    do_sample=True, 
                    temperature=0.7,
                    truncation=True
                )
                print(f"\n[Output]: {output[0]['generated_text']}")
                
        except ImportError:
            print("[Error] Missing libs. Run: pip install transformers torch accelerate")
        except Exception as e:
            print(f"[Error] Load Failed: {e}")

    def load_image_model(self):
        print("\n[Loader] Initializing Image Pipeline...")
        try:
            from diffusers import DiffusionPipeline
            # Target: SDXL (Open Access, no token needed)
            model_id = "stabilityai/stable-diffusion-xl-base-1.0"
            print(f"[Config] Loading Model: {model_id}")
            
            pipe = DiffusionPipeline.from_pretrained(
                model_id,
                torch_dtype=torch.float16 if self.device != "cpu" else torch.float32
            )
            if self.device != "cpu":
                pipe.to(self.device)
                
            print("[Ready] Enter image prompt (Ctrl+C to exit):")
            while True:
                prompt = input("\n>> ")
                if prompt.lower() in ['exit', 'quit']: break
                
                print("[Generating]...")
                image = pipe(prompt).images[0]
                filename = f"output_{hash(prompt)}.png"
                image.save(filename)
                print(f"[Saved] {filename}")

        except ImportError:
            print("[Error] Missing libs. Run: pip install diffusers transformers torch accelerate")
        except Exception as e:
            print(f"[Error] Load Failed: {e}")

    def run(self):
        while True:
            choice = input("\nSelect Module [1-3]: ")
            if choice == '1': self.load_text_model()
            elif choice == '2': self.load_image_model()
            elif choice == '3': sys.exit(0)

if __name__ == "__main__":
    lab = LocalModelLab()
    lab.run()
