import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMExtractionStrategy, LLMConfig

GEMINI_API_KEY = "AIzaSyCwpp0YtdHB56WZ1bhtWdWrPqPS005I6U8"


async def crawl_glm_docs():
    llm_config = LLMConfig(
        provider="gemini/gemini-2.0-flash-exp",
        api_token=GEMINI_API_KEY,
        temperature=0.5,
    )

    extraction_strategy = LLMExtractionStrategy(
        llm_config=llm_config,
        instruction="""
        Extract comprehensive GLM-4 API documentation including:
        - All available model names and IDs (especially GLM-4 variants)
        - API endpoints and authentication methods
        - Supported features: tools/function calling, streaming, vision, embeddings
        - Request/response formats and parameters
        - Rate limits and pricing
        - Code examples for different languages
        - Any specific capabilities or limitations of GLM-4-Flash model

        Format as structured markdown with clear sections.
        """,
    )

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://docs.zhipuai.cn/docs/api/glm-4",
            config=CrawlerRunConfig(
                extraction_strategy=extraction_strategy,
                word_count_threshold=10,
                wait_for=3000,  # Wait for JS to load
                verbose=True,
            ),
        )

        # Save to markdown
        output_file = "/Users/imorgado/Desktop/Tools/crawl4ai-scripts/glm4_api_docs.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write("# GLM-4 API Documentation\n\n")
            f.write("Scraped from: https://docs.zhipuai.cn/docs/api/glm-4\n\n")
            f.write(result.markdown)

        print(f"âœ… Saved to {output_file}")
        print(f"ðŸ“Š Extracted {len(result.markdown)} characters")

        # Also save raw HTML for reference
        raw_file = "/Users/imorgado/Desktop/Tools/crawl4ai-scripts/glm4_raw.html"
        with open(raw_file, "w", encoding="utf-8") as f:
            f.write(result.html)
        print(f"ðŸ“„ Raw HTML saved to {raw_file}")


asyncio.run(crawl_glm_docs())
