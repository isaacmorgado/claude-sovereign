"""
Face Detection Service
Returns landmarks for both side and front profiles using InsightFace
"""

import base64
import math
from typing import Optional, List, Dict, Any
import cv2
import numpy as np

# Try to import InsightFace (for side profile)
try:
    from insightface.app import FaceAnalysis
    INSIGHTFACE_AVAILABLE = True
    print("[DetectionService] InsightFace imported successfully")
except ImportError as e:
    INSIGHTFACE_AVAILABLE = False
    FaceAnalysis = None
    print(f"[DetectionService] InsightFace import failed: {e}")
except Exception as e:
    INSIGHTFACE_AVAILABLE = False
    FaceAnalysis = None
    print(f"[DetectionService] InsightFace import error: {type(e).__name__}: {e}")

# ============================================
# INSIGHTFACE 106 LANDMARK INDEX MAPPING
# Based on FaceIQ's side profile landmark naming
# ============================================

# InsightFace 106 landmark indices mapped to FaceIQ names
INSIGHTFACE_106_MAPPING = {
    # Face contour (indices 0-32)
    0: "faceContour_0",
    1: "faceContour_1",
    2: "faceContour_2",
    3: "faceContour_3",
    4: "faceContour_4",
    5: "faceContour_5",
    6: "faceContour_6",
    7: "faceContour_7",
    8: "faceContour_8",
    9: "gonionTop",           # Upper jaw angle
    10: "gonionTop_alt",
    11: "gonionBottom",       # Lower jaw angle
    12: "gonionBottom_alt",
    13: "faceContour_13",
    14: "faceContour_14",
    15: "faceContour_15",
    16: "menton",             # Chin point
    17: "faceContour_17",
    18: "faceContour_18",
    19: "faceContour_19",
    20: "faceContour_20",
    21: "faceContour_21",
    22: "faceContour_22",
    23: "faceContour_23",
    24: "faceContour_24",
    25: "faceContour_25",
    26: "faceContour_26",
    27: "faceContour_27",
    28: "faceContour_28",
    29: "faceContour_29",
    30: "faceContour_30",
    31: "faceContour_31",
    32: "faceContour_32",

    # Left eyebrow (indices 33-37)
    33: "leftBrowInner",
    34: "leftBrowMid1",
    35: "leftBrowPeak",
    36: "leftBrowMid2",
    37: "leftBrowOuter",

    # Right eyebrow (indices 38-42)
    38: "rightBrowInner",
    39: "rightBrowMid1",
    40: "rightBrowPeak",
    41: "rightBrowMid2",
    42: "rightBrowOuter",

    # Nose bridge (indices 43-50)
    43: "nasion",             # Top of nose bridge
    44: "rhinion_upper",
    45: "rhinion",            # Nose bridge
    46: "rhinion_lower",
    47: "supratip",           # Above nose tip
    48: "noseLeft",
    49: "noseRight",
    50: "pronasale",          # Nose tip

    # Nose base (indices 51-54)
    51: "glabella",           # Between eyebrows
    52: "subnasale",          # Below nose
    53: "columella",          # Nose column
    54: "pronasale_alt",      # Nose tip alt

    # Nostrils (indices 55-59)
    55: "infratip",           # Below nose tip
    56: "subalare_left",      # Left nostril
    57: "subalare_right",     # Right nostril
    58: "nostril_left",
    59: "nostril_right",

    # Left eye (indices 60-67)
    60: "leftEyeInner",       # Left eye inner corner
    61: "leftEyeUpperInner",
    62: "leftEyeUpperMid",
    63: "leftEyeUpperOuter",
    64: "leftEyeOuter",       # Left eye outer corner
    65: "leftEyeLowerOuter",
    66: "leftEyeLowerMid",
    67: "leftEyeLowerInner",

    # Right eye (indices 68-75)
    68: "rightEyeInner",      # Right eye inner corner
    69: "rightEyeUpperInner",
    70: "rightEyeUpperMid",
    71: "rightEyeUpperOuter",
    72: "rightEyeOuter",      # Right eye outer corner
    73: "rightEyeLowerOuter",
    74: "rightEyeLowerMid",
    75: "rightEyeLowerInner",

    # Lips - outer (indices 76-87)
    76: "labraleSuperius",    # Upper lip top center
    77: "upperLipLeft",
    78: "upperLipLeftOuter",
    79: "cheilion_left",      # Left mouth corner
    80: "lowerLipLeftOuter",
    81: "lowerLipLeft",
    82: "labraleInferius",    # Lower lip bottom center
    83: "sublabiale",         # Below lower lip
    84: "cheilion_right",     # Right mouth corner
    85: "lowerLipRight",
    86: "lowerLipRightOuter",
    87: "upperLipRightOuter",

    # Lips - inner (indices 88-95)
    88: "upperLipInnerLeft",
    89: "upperLipInnerCenter",
    90: "upperLipInnerRight",
    91: "lowerLipInnerRight",
    92: "lowerLipInnerCenter",
    93: "lowerLipInnerLeft",
    94: "mouthInnerLeft",
    95: "mouthInnerRight",

    # Pupils (indices 96-97)
    96: "leftPupil",
    97: "rightPupil",

    # Additional points (indices 98-105)
    98: "leftEyeCenter",
    99: "rightEyeCenter",
    100: "noseCenter",
    101: "leftCheek",
    102: "rightCheek",
    103: "pogonion",           # Chin projection
    104: "cervicalPoint",      # Neck point
    105: "trichion",           # Hairline
}

# FaceIQ-style cephalometric landmark mapping for side profile analysis
SIDE_PROFILE_LANDMARKS = {
    # Forehead/Upper face
    "trichion": 105,          # Hairline
    "glabella": 51,           # Between eyebrows

    # Nose
    "nasion": 43,             # Top of nose bridge (radix)
    "rhinion": 45,            # Nose bridge
    "supratip": 47,           # Above nose tip
    "pronasale": 50,          # Nasal tip projection
    "infratip": 55,           # Below tip
    "columella": 53,          # Nose column
    "subnasale": 52,          # Base of nose
    "subalare": 56,           # Nostril base

    # Eyes
    "orbitale": 66,           # Lower orbital rim
    "cornealApex": 98,        # Corneal projection
    "leftEyeCenter": 98,
    "rightEyeCenter": 99,

    # Ears (estimated - not directly in InsightFace)
    "porion": 37,             # Top of ear canal (estimated from brow)
    "tragus": 37,             # Ear tragus (estimated)
    "intertragicNotch": 37,   # Below tragus (estimated)

    # Lips
    "labraleSuperius": 76,    # Upper lip margin
    "cheilion": 79,           # Mouth corner
    "labraleInferius": 82,    # Lower lip margin
    "sublabiale": 83,         # Below lower lip

    # Chin/Jaw
    "pogonion": 103,          # Chin projection
    "menton": 16,             # Lowest chin point
    "gonionTop": 9,           # Upper jaw angle
    "gonionBottom": 11,       # Lower jaw angle

    # Neck
    "cervicalPoint": 104,     # C-point
    "neckPoint": 104,         # Throat/neck

    # Cheekbone
    "cheekbone": 101,         # Cheek prominence
}


class DetectionService:
    """
    FaceIQ-compatible face detection service
    Returns landmarks in exact FaceIQ format
    """
    _instance: Optional["DetectionService"] = None
    _face_analyzer: Optional[FaceAnalysis] = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    async def initialize(self):
        """Initialize InsightFace model on startup"""
        if self._face_analyzer is not None:
            return

        if not INSIGHTFACE_AVAILABLE:
            print("[DetectionService] InsightFace not installed - detection disabled")
            return

        # Use buffalo_s by default (pre-downloaded in Dockerfile, 30MB vs 190MB)
        # This provides faster startup and lower memory usage
        try:
            self._face_analyzer = FaceAnalysis(
                name="buffalo_s",
                providers=["CPUExecutionProvider"]
            )
            self._face_analyzer.prepare(ctx_id=0, det_size=(320, 320))  # Reduced for memory
            print("[DetectionService] InsightFace buffalo_s model loaded")
        except Exception as e:
            print(f"[DetectionService] Failed to load buffalo_s: {e}")
            try:
                # Fallback to larger model if available
                self._face_analyzer = FaceAnalysis(
                    name="buffalo_l",
                    providers=["CPUExecutionProvider"]
                )
                self._face_analyzer.prepare(ctx_id=0, det_size=(320, 320))  # Reduced for memory
                print("[DetectionService] InsightFace buffalo_l model loaded (fallback)")
            except Exception as e2:
                print(f"[DetectionService] Failed to load any model: {e2}")

    def _decode_image(self, image_data: str) -> np.ndarray:
        """Decode base64 image to OpenCV format"""
        # Handle data URL format
        if "," in image_data:
            image_data = image_data.split(",")[1]

        image_bytes = base64.b64decode(image_data)
        nparr = np.frombuffer(image_bytes, np.uint8)
        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if img is None:
            raise ValueError("Failed to decode image")

        return img

    def _detect_direction(self, landmarks: np.ndarray) -> str:
        """
        Detect if face is facing left or right based on nose position
        Returns: "left" or "right"
        """
        if landmarks is None or len(landmarks) < 60:
            return "unknown"

        # Compare nose tip x to face center x
        nose_tip = landmarks[50]  # pronasale
        left_contour = landmarks[0]
        right_contour = landmarks[32]
        face_center_x = (left_contour[0] + right_contour[0]) / 2

        return "left" if nose_tip[0] < face_center_x else "right"

    def _calculate_rotation(self, landmarks: np.ndarray) -> float:
        """
        Calculate head rotation angle in radians
        Based on eye corner positions
        """
        if landmarks is None or len(landmarks) < 76:
            return 0.0

        # Use eye corners
        left_eye_outer = landmarks[64]
        right_eye_outer = landmarks[72]

        dx = right_eye_outer[0] - left_eye_outer[0]
        dy = right_eye_outer[1] - left_eye_outer[1]

        return math.atan2(dy, dx)  # Return in radians like FaceIQ

    def _calculate_center(self, landmarks: np.ndarray) -> Dict[str, float]:
        """Calculate face center point"""
        if landmarks is None or len(landmarks) < 17:
            return {"x": 0.0, "y": 0.0}

        # Average of key points
        nose = landmarks[50]
        chin = landmarks[16]
        center_x = nose[0]
        center_y = (nose[1] + chin[1]) / 2

        return {"x": float(center_x), "y": float(center_y)}

    def _calculate_crop(
        self,
        bbox: np.ndarray,
        img_width: int,
        img_height: int
    ) -> Dict[str, Any]:
        """Calculate crop region similar to FaceIQ"""
        x1, y1, x2, y2 = bbox[:4]
        width = x2 - x1
        height = y2 - y1

        # Add padding (20%)
        padding = 0.2
        pad_x = width * padding
        pad_y = height * padding

        crop_x = max(0, int(x1 - pad_x))
        crop_y = max(0, int(y1 - pad_y))
        crop_width = min(img_width - crop_x, int(width + 2 * pad_x))
        crop_height = min(img_height - crop_y, int(height + 2 * pad_y))

        # Scale factor (normalized to 1024px reference)
        scale = 1024 / max(crop_width, crop_height)

        return {
            "x": crop_x,
            "y": crop_y,
            "width": crop_width,
            "height": crop_height,
            "scale": scale
        }

    async def detect_side_landmarks(self, image_data: str) -> Dict[str, Any]:
        """
        Detect side profile landmarks - FaceIQ-compatible response

        Endpoint: POST /api/side-landmarks

        Returns exact FaceIQ format:
        {
            "success": true,
            "data": {
                "rotationAngle": float,
                "direction": "left" | "right",
                "center": {"x": float, "y": float},
                "crop": {"x": int, "y": int, "width": int, "height": int, "scale": float},
                "landmarks": [{"x": float, "y": float}, ...],  // 106 points
                "bbox": [x1, y1, x2, y2],
                "namedLandmarks": {"landmarkName": {"x": float, "y": float}, ...}
            }
        }
        """
        # Lazy load model on first request
        if self._face_analyzer is None:
            await self.initialize()

        if self._face_analyzer is None:
            return {
                "success": False,
                "error": "Detection model not loaded. InsightFace not available."
            }

        try:
            # Decode image
            img = self._decode_image(image_data)
            img_height, img_width = img.shape[:2]
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            # Detect faces
            faces = self._face_analyzer.get(img_rgb)

            if not faces:
                return {
                    "success": False,
                    "error": "No face detected in image"
                }

            # Use first (largest) face
            face = faces[0]

            # Check for 106 landmarks
            if not hasattr(face, 'landmark_2d_106') or face.landmark_2d_106 is None:
                return {
                    "success": False,
                    "error": "Full 106 landmarks not available"
                }

            landmarks_106 = face.landmark_2d_106
            bbox = face.bbox

            # Calculate FaceIQ-style response fields
            direction = self._detect_direction(landmarks_106)
            rotation_angle = self._calculate_rotation(landmarks_106)
            center = self._calculate_center(landmarks_106)
            crop = self._calculate_crop(bbox, img_width, img_height)

            # Convert landmarks to FaceIQ format (pixel coordinates)
            landmarks_list = [
                {"x": float(pt[0]), "y": float(pt[1])}
                for pt in landmarks_106
            ]

            # Create named landmarks mapping
            named_landmarks = {}
            for name, idx in SIDE_PROFILE_LANDMARKS.items():
                if idx < len(landmarks_106):
                    pt = landmarks_106[idx]
                    named_landmarks[name] = {
                        "x": float(pt[0]),
                        "y": float(pt[1])
                    }

            # Also add all indexed landmarks with their names
            indexed_landmarks = {}
            for idx, name in INSIGHTFACE_106_MAPPING.items():
                if idx < len(landmarks_106):
                    pt = landmarks_106[idx]
                    indexed_landmarks[name] = {
                        "x": float(pt[0]),
                        "y": float(pt[1])
                    }

            return {
                "success": True,
                "data": {
                    "rotationAngle": rotation_angle,
                    "direction": direction,
                    "center": center,
                    "crop": crop,
                    "landmarks": landmarks_list,
                    "bbox": [float(x) for x in bbox[:4]],
                    "namedLandmarks": named_landmarks,
                    "allLandmarks": indexed_landmarks,
                    "landmarkCount": len(landmarks_106)
                }
            }

        except ValueError as e:
            return {"success": False, "error": str(e)}
        except Exception as e:
            return {"success": False, "error": f"Detection failed: {str(e)}"}

    async def detect_front_landmarks(self, image_data: str) -> Dict[str, Any]:
        """
        Detect front profile landmarks

        Note: FaceIQ uses client-side MediaPipe for front profile.
        This endpoint provides server-side fallback using InsightFace.

        Returns FaceIQ-compatible format with InsightFace landmarks.
        """
        # Lazy load model on first request
        if self._face_analyzer is None:
            await self.initialize()

        if self._face_analyzer is None:
            return {
                "success": False,
                "error": "Detection model not loaded"
            }

        try:
            img = self._decode_image(image_data)
            img_height, img_width = img.shape[:2]
            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

            faces = self._face_analyzer.get(img_rgb)

            if not faces:
                return {
                    "success": False,
                    "error": "No face detected"
                }

            face = faces[0]

            if not hasattr(face, 'landmark_2d_106') or face.landmark_2d_106 is None:
                return {
                    "success": False,
                    "error": "Landmarks not available"
                }

            landmarks_106 = face.landmark_2d_106
            bbox = face.bbox

            # For front profile, normalize coordinates to 0-1 range
            landmarks_normalized = [
                {"x": float(pt[0]) / img_width, "y": float(pt[1]) / img_height}
                for pt in landmarks_106
            ]

            # Create front profile landmark mapping
            front_landmarks = {
                # Eyes
                "leftEyeMedialCanthus": landmarks_normalized[60],
                "leftEyeLateralCanthus": landmarks_normalized[64],
                "leftEyePupil": landmarks_normalized[96],
                "leftEyeUpperEyelid": landmarks_normalized[62],
                "leftEyeLowerEyelid": landmarks_normalized[66],
                "rightEyeMedialCanthus": landmarks_normalized[68],
                "rightEyeLateralCanthus": landmarks_normalized[72],
                "rightEyePupil": landmarks_normalized[97],
                "rightEyeUpperEyelid": landmarks_normalized[70],
                "rightEyeLowerEyelid": landmarks_normalized[74],

                # Eyebrows
                "leftBrowInnerCorner": landmarks_normalized[33],
                "leftBrowPeak": landmarks_normalized[35],
                "leftBrowOuterCorner": landmarks_normalized[37],
                "rightBrowInnerCorner": landmarks_normalized[38],
                "rightBrowPeak": landmarks_normalized[40],
                "rightBrowOuterCorner": landmarks_normalized[42],

                # Nose
                "nasion": landmarks_normalized[43],
                "noseBottom": landmarks_normalized[52],
                "noseLeft": landmarks_normalized[48],
                "noseRight": landmarks_normalized[49],
                "nasalBase": landmarks_normalized[52],

                # Mouth
                "mouthLeft": landmarks_normalized[79],
                "mouthRight": landmarks_normalized[84],
                "cupidsBow": landmarks_normalized[76],
                "lowerLip": landmarks_normalized[82],

                # Face contour
                "leftCheek": landmarks_normalized[101],
                "rightCheek": landmarks_normalized[102],
                "chinBottom": landmarks_normalized[16],
                "leftBottomGonion": landmarks_normalized[11],
                "rightBottomGonion": landmarks_normalized[11],  # Mirrored

                # Forehead (estimated)
                "hairline": landmarks_normalized[105] if 105 < len(landmarks_normalized) else {"x": 0.5, "y": 0.1},
            }

            return {
                "success": True,
                "data": {
                    "landmarks": landmarks_normalized,
                    "namedLandmarks": front_landmarks,
                    "bbox": [
                        float(bbox[0]) / img_width,
                        float(bbox[1]) / img_height,
                        float(bbox[2]) / img_width,
                        float(bbox[3]) / img_height
                    ],
                    "imageWidth": img_width,
                    "imageHeight": img_height,
                    "landmarkCount": len(landmarks_106)
                }
            }

        except ValueError as e:
            return {"success": False, "error": str(e)}
        except Exception as e:
            return {"success": False, "error": f"Detection failed: {str(e)}"}


# Singleton instance
detection_service = DetectionService()
