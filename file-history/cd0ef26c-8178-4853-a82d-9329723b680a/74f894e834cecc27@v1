# RunPod GPU Selection Quick Reference

**Last Updated:** 2026-01-05

---

## TL;DR - What GPU Should I Use?

| Model | Deploy On | Monthly Cost* |
|-------|-----------|---------------|
| Budget (4B) | RTX 3090 | ~$8-12 |
| Architect (32B) | A40 | ~$20-30 |
| Code Writer (30B) | A40 | ~$20-30 |
| Researcher (80B) | A100 40GB | ~$35-45 |

*100 requests/month with auto-scaling (`workersMin: 0`)

---

## GPU Pricing & Specs

### RTX 3090 - $0.34/hr
- **VRAM:** 24GB
- **Best for:** 4B models
- **Deploy:** Budget (4B)
- **Pros:** Cheapest option
- **Cons:** Tight for 30B+ models

### RTX 4090 - $0.69/hr
- **VRAM:** 24GB
- **Best for:** 4B-32B models (tight fit)
- **Deploy:** Budget (4B), Architect (32B) if A40 unavailable
- **Pros:** Fast, affordable
- **Cons:** Limited VRAM for 30B+

### A40 - $0.79/hr ⭐ **BEST VALUE**
- **VRAM:** 48GB
- **Best for:** 30-32B models
- **Deploy:** Architect (32B), Code Writer (30B)
- **Pros:** Perfect balance of VRAM and cost
- **Cons:** None for these models

### A100 40GB - $1.14/hr
- **VRAM:** 40GB
- **Best for:** 80B MoE models
- **Deploy:** Researcher (80B)
- **Pros:** Enough for MoE, professional grade
- **Cons:** More expensive than A40

### A100 80GB - $1.44/hr ⚠️
- **VRAM:** 80GB
- **Best for:** Nothing you're deploying
- **Deploy:** Avoid - overkill
- **Pros:** Maximum VRAM
- **Cons:** 45% more expensive than A100 40GB for no benefit

### H100 - $2.50-4.00/hr ⚠️
- **VRAM:** 80GB
- **Best for:** Production/business use only
- **Deploy:** Only if speed is critical
- **Pros:** 2-3x faster than A100
- **Cons:** 2-3x more expensive

---

## Cost Comparison Examples

### Scenario: 100 Requests/Month Mixed Usage
*(25 Budget, 35 Architect, 30 Code, 10 Research)*

**Option A: All on A100 80GB**
- Cost: ~$60/month
- ❌ Wastes money on over-provisioned GPUs

**Option B: All on A100 40GB**
- Cost: ~$45/month
- ❌ Still overpaying for smaller models

**Option C: Optimized GPUs** ✅
```
Budget (25):    RTX 3090 → $4.25
Architect (35): A40      → $14
Code (30):      A40      → $12
Research (10):  A100 40GB→ $5.70
Total: ~$36/month
```
- ✅ **Saves $24/month (40% reduction) vs Option A**
- ✅ **Saves $9/month (20% reduction) vs Option B**

### Scenario: Heavy Use - 500 Requests/Month
*(100 Budget, 175 Architect, 175 Code, 50 Research)*

**Optimized GPUs:**
```
Budget (100):   RTX 3090 → $17
Architect (175):A40      → $70
Code (175):     A40      → $70
Research (50):  A100 40GB→ $28.50
Total: ~$185/month
```

**All on A100 80GB:** ~$300/month
**Savings:** $115/month (38% reduction)

---

## When to Choose Each GPU

### Choose RTX 3090 when:
- ✅ Model is 4B parameters
- ✅ Budget is tightest constraint
- ✅ Cold starts are acceptable

### Choose RTX 4090 when:
- ✅ Model is 4B-32B
- ✅ Need faster than 3090 but cheaper than A40
- ✅ RTX 3090 unavailable

### Choose A40 when:
- ✅ Model is 30-32B parameters
- ✅ Best value for mid-sized models
- ✅ This is the sweet spot for most deployments

### Choose A100 40GB when:
- ✅ Model is 80B MoE (Mixture of Experts)
- ✅ Need professional-grade reliability
- ✅ Handling complex workloads

### Choose A100 80GB when:
- ❌ Don't - it's overkill for your use case
- Only if: A100 40GB is completely unavailable

### Choose H100 when:
- ✅ Production business use
- ✅ Speed is critical (2-3x faster)
- ✅ Cost is not primary concern
- ❌ Not for personal/experimental use

---

## RunPod Deployment Commands

When creating endpoint, select GPU:

```bash
# For Budget (4B)
GPU Type: RTX 3090 24GB

# For Architect (32B)
GPU Type: A40 48GB

# For Code Writer (30B)
GPU Type: A40 48GB

# For Researcher (80B MoE)
GPU Type: A100 40GB
```

---

## Monthly Cost Estimator

**Formula:**
```
Cost = (Requests × Avg Duration in hours × GPU $/hr)
```

**Typical request duration:** 30-60 seconds (0.0083-0.0167 hours)

**Examples:**

**50 requests on A40 ($0.79/hr):**
- 50 × 0.0125 hrs × $0.79 = **~$0.49/month** per model

**100 requests on RTX 3090 ($0.34/hr):**
- 100 × 0.0125 hrs × $0.34 = **~$0.43/month**

**100 requests on A100 40GB ($1.14/hr):**
- 100 × 0.0125 hrs × $1.14 = **~$1.43/month**

**Important:** These are COMPUTE costs only. Auto-scaling (`workersMin: 0`) means you only pay when actually processing requests.

---

## Key Takeaways

1. **Use A40 for most models** - Best value for 30-32B models
2. **Use RTX 3090 for 4B** - Cheapest option that works
3. **Use A100 40GB for 80B MoE** - Perfect fit, not overkill
4. **Avoid A100 80GB** - 45% more expensive with no benefit
5. **Save 40-60%** - By choosing right GPUs vs default A100 80GB

---

## Questions?

See `roo-code-model-guide.md` → "GPU Selection Guide" for full details.
