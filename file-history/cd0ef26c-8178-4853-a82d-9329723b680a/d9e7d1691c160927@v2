#!/usr/bin/env python3
"""
Browser Automation with Account Management
Playwright-based browser automation with persistent login sessions
"""
import os
import json
from playwright.sync_api import sync_playwright, Browser, BrowserContext, Page
from pathlib import Path
from typing import Optional, Dict, List
import time

# Directories for storing browser data
BROWSER_DATA_DIR = Path.home() / ".browser_automation"
COOKIES_DIR = BROWSER_DATA_DIR / "cookies"
SCREENSHOTS_DIR = BROWSER_DATA_DIR / "screenshots"

# Create directories if they don't exist
COOKIES_DIR.mkdir(parents=True, exist_ok=True)
SCREENSHOTS_DIR.mkdir(parents=True, exist_ok=True)


class BrowserSession:
    """
    Managed browser session with account login support
    """

    def __init__(self, headless: bool = False, account_name: Optional[str] = None):
        """
        Initialize browser session

        Args:
            headless: Run in headless mode (no visible browser)
            account_name: Name for this account session (for cookie persistence)
        """
        self.headless = headless
        self.account_name = account_name or "default"
        self.playwright = None
        self.browser: Optional[Browser] = None
        self.context: Optional[BrowserContext] = None
        self.page: Optional[Page] = None

        self.cookie_file = COOKIES_DIR / f"{self.account_name}_cookies.json"

    def __enter__(self):
        """Context manager entry"""
        self.start()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit"""
        self.close()

    def start(self):
        """Start browser session"""
        print(f"üåê Starting browser session: {self.account_name}")
        print(f"   Headless: {self.headless}")

        self.playwright = sync_playwright().start()

        # Launch browser
        self.browser = self.playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--disable-blink-features=AutomationControlled',  # Hide automation
            ]
        )

        # Create context (separate session)
        context_options = {
            "viewport": {"width": 1920, "height": 1080},
            "user_agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }

        # Load cookies if they exist
        if self.cookie_file.exists():
            print(f"   Loading cookies from: {self.cookie_file}")
            with open(self.cookie_file) as f:
                cookies = json.load(f)
                context_options["storage_state"] = {"cookies": cookies}

        self.context = self.browser.new_context(**context_options)

        # Create page
        self.page = self.context.new_page()

        # Set up console log capture
        self.console_logs = []
        self.page.on("console", lambda msg: self.console_logs.append({
            "type": msg.type,
            "text": msg.text,
            "location": str(msg.location)
        }))

        # Set up error capture
        self.errors = []
        self.page.on("pageerror", lambda exc: self.errors.append(str(exc)))

        # Set up network request capture
        self.network_requests = []
        self.page.on("request", lambda req: self.network_requests.append({
            "url": req.url,
            "method": req.method,
            "headers": dict(req.headers),
            "post_data": req.post_data
        }))

        print("‚úÖ Browser session started")

    def save_cookies(self):
        """Save current cookies for future sessions"""
        print(f"üíæ Saving cookies to: {self.cookie_file}")
        cookies = self.context.cookies()
        with open(self.cookie_file, "w") as f:
            json.dump(cookies, f, indent=2)
        print(f"‚úÖ Saved {len(cookies)} cookies")

    def close(self):
        """Close browser session"""
        if self.page:
            self.page.close()
        if self.context:
            self.context.close()
        if self.browser:
            self.browser.close()
        if self.playwright:
            self.playwright.stop()
        print("üõë Browser session closed")

    def navigate(self, url: str, wait_for: str = "networkidle"):
        """
        Navigate to URL

        Args:
            url: URL to navigate to
            wait_for: What to wait for (load, domcontentloaded, networkidle)
        """
        print(f"üîó Navigating to: {url}")
        self.page.goto(url, wait_until=wait_for)
        print(f"‚úÖ Loaded: {self.page.title()}")

    def screenshot(self, name: str = None, full_page: bool = True) -> Path:
        """
        Take screenshot

        Args:
            name: Screenshot filename (default: timestamp)
            full_page: Capture full scrollable page

        Returns:
            Path to screenshot file
        """
        if not name:
            name = f"screenshot_{int(time.time())}.png"

        screenshot_path = SCREENSHOTS_DIR / name
        self.page.screenshot(path=str(screenshot_path), full_page=full_page)
        print(f"üì∏ Screenshot saved: {screenshot_path}")

        return screenshot_path

    def wait_for_manual_login(self, success_url_pattern: str, timeout: int = 300):
        """
        Wait for user to manually login

        Args:
            success_url_pattern: URL pattern that indicates successful login
            timeout: How long to wait (seconds)
        """
        print(f"\n{'='*80}")
        print("üîê MANUAL LOGIN REQUIRED")
        print(f"{'='*80}")
        print(f"Please login manually in the browser window.")
        print(f"Waiting for URL to match: {success_url_pattern}")
        print(f"Timeout: {timeout} seconds")
        print(f"{'='*80}\n")

        try:
            self.page.wait_for_url(f"**{success_url_pattern}**", timeout=timeout*1000)
            print("\n‚úÖ Login successful!")
            self.save_cookies()
            return True
        except Exception as e:
            print(f"\n‚ùå Login timeout or failed: {e}")
            return False

    def get_devtools_data(self) -> Dict:
        """
        Get captured DevTools data

        Returns:
            Dictionary with console logs, errors, and network requests
        """
        return {
            "console": self.console_logs,
            "errors": self.errors,
            "network": self.network_requests[:100]  # Limit to avoid huge files
        }

    def save_devtools_data(self, filename: str = "devtools_data.json"):
        """Save DevTools data to file"""
        data = self.get_devtools_data()
        filepath = BROWSER_DATA_DIR / filename

        with open(filepath, "w") as f:
            json.dump(data, f, indent=2)

        print(f"üíæ DevTools data saved: {filepath}")
        print(f"   Console logs: {len(data['console'])}")
        print(f"   Errors: {len(data['errors'])}")
        print(f"   Network requests: {len(data['network'])}")

        return filepath


# Pre-configured account helpers
class GoogleAccount:
    """Helper for Google account management"""

    @staticmethod
    def login(session: BrowserSession, manual: bool = True):
        """
        Login to Google account

        Args:
            session: Browser session
            manual: Use manual login (safer, recommended)
        """
        session.navigate("https://accounts.google.com")

        if manual:
            success = session.wait_for_manual_login(
                success_url_pattern="myaccount.google.com",
                timeout=300
            )
            if success:
                print("‚úÖ Google account logged in")
                return True
        else:
            print("‚ùå Automated Google login not recommended (blocked by Google)")
            print("   Please use manual=True")
            return False

        return False


class RedditAccount:
    """Helper for Reddit account management"""

    @staticmethod
    def login(session: BrowserSession, manual: bool = True):
        """Login to Reddit"""
        session.navigate("https://www.reddit.com/login")

        if manual:
            success = session.wait_for_manual_login(
                success_url_pattern="/",
                timeout=300
            )
            if success:
                print("‚úÖ Reddit account logged in")
                return True

        return False


class TwitterAccount:
    """Helper for Twitter/X account management"""

    @staticmethod
    def login(session: BrowserSession, manual: bool = True):
        """Login to Twitter/X"""
        session.navigate("https://twitter.com/i/flow/login")

        if manual:
            success = session.wait_for_manual_login(
                success_url_pattern="home",
                timeout=300
            )
            if success:
                print("‚úÖ Twitter account logged in")
                return True

        return False


# Example workflows
def workflow_screenshot_to_code(url: str, output_name: str = None):
    """
    Complete workflow: Navigate ‚Üí Screenshot ‚Üí Analyze with RunPod

    Args:
        url: URL to capture
        output_name: Name for screenshot file
    """
    print("\n" + "="*80)
    print("SCREENSHOT TO CODE WORKFLOW")
    print("="*80 + "\n")

    with BrowserSession(headless=False) as browser:
        # Navigate
        browser.navigate(url)

        # Take screenshot
        screenshot_path = browser.screenshot(name=output_name)

        # Call RunPod API for code generation
        print("\nüì§ Sending to RunPod for code generation...")
        os.system(f"python3 runpod_api.py screenshot-to-code {screenshot_path} code")


def workflow_login_and_scrape(account_name: str, site: str, scrape_url: str):
    """
    Workflow: Login ‚Üí Scrape data ‚Üí Analyze with RunPod

    Args:
        account_name: Account session name (for cookie persistence)
        site: Which site (google/reddit/twitter)
        scrape_url: URL to scrape after login
    """
    print("\n" + "="*80)
    print("LOGIN + SCRAPE + ANALYZE WORKFLOW")
    print("="*80 + "\n")

    with BrowserSession(headless=False, account_name=account_name) as browser:
        # Login if needed
        if not browser.cookie_file.exists():
            print("‚ö†Ô∏è  No saved cookies found. Need to login first...")

            if site.lower() == "google":
                GoogleAccount.login(browser, manual=True)
            elif site.lower() == "reddit":
                RedditAccount.login(browser, manual=True)
            elif site.lower() == "twitter":
                TwitterAccount.login(browser, manual=True)
            else:
                print(f"Unknown site: {site}")
                return

        # Navigate to scrape URL
        browser.navigate(scrape_url)

        # Example: Extract data (customize based on site)
        data = browser.page.evaluate("""
            () => {
                // Extract all text from main content
                const main = document.querySelector('main') || document.body;
                return {
                    title: document.title,
                    url: window.location.href,
                    text_content: main.innerText.slice(0, 5000)
                };
            }
        """)

        # Save data
        data_file = BROWSER_DATA_DIR / "scraped_data.json"
        with open(data_file, "w") as f:
            json.dump(data, f, indent=2)

        print(f"\nüíæ Data saved: {data_file}")

        # Analyze with RunPod
        print("\nüî¨ Analyzing with RunPod Research model...")
        os.system(f"python3 runpod_api.py analyze-patterns {data_file} research")


def workflow_devtools_debug(url: str):
    """
    Workflow: Navigate ‚Üí Capture DevTools ‚Üí Analyze with Architect

    Args:
        url: URL to debug
    """
    print("\n" + "="*80)
    print("DEVTOOLS DEBUG WORKFLOW")
    print("="*80 + "\n")

    with BrowserSession(headless=False) as browser:
        # Navigate
        browser.navigate(url)

        # Let page run for a bit to capture logs
        print("\n‚è±Ô∏è  Capturing DevTools data (10 seconds)...")
        time.sleep(10)

        # Save DevTools data
        devtools_file = browser.save_devtools_data()

        # Analyze with RunPod
        print("\nüèóÔ∏è  Analyzing with Architect model...")

        # Extract console and network into separate files for analysis
        data = browser.get_devtools_data()

        console_file = BROWSER_DATA_DIR / "console.json"
        with open(console_file, "w") as f:
            json.dump(data["console"], f, indent=2)

        network_file = BROWSER_DATA_DIR / "network.json"
        with open(network_file, "w") as f:
            json.dump(data["network"], f, indent=2)

        os.system(f"python3 runpod_api.py analyze-devtools {console_file} {network_file} architect")


# Main CLI
def main():
    import sys

    if len(sys.argv) < 2:
        print("""
Browser Automation Tool

USAGE:
  python browser_automation.py <workflow> [args...]

WORKFLOWS:

  screenshot-to-code <url> [output_name]
    Navigate to URL, take screenshot, convert to code with RunPod
    Example: python browser_automation.py screenshot-to-code https://example.com pricing_page.png

  login-and-scrape <account_name> <site> <url>
    Login (if needed), scrape data, analyze with RunPod
    Sites: google, reddit, twitter
    Example: python browser_automation.py login-and-scrape my_google google https://mail.google.com

  devtools-debug <url>
    Capture DevTools data and analyze with Architect model
    Example: python browser_automation.py devtools-debug http://localhost:3000

DATA STORAGE:
  Browser data: ~/.browser_automation/
  Cookies: ~/.browser_automation/cookies/
  Screenshots: ~/.browser_automation/screenshots/

ACCOUNT MANAGEMENT:
  - First run: Browser opens, you login manually
  - Cookies saved automatically
  - Next run: Auto-logged in (cookies reused)
  - Different accounts: Use different account_name

CAPTCHA / HUMAN VERIFICATION:
  - Browser runs visible (headless=False)
  - You solve CAPTCHAs manually when they appear
  - Script waits for you to complete verification
  - Then continues automatically
""")
        sys.exit(0)

    workflow = sys.argv[1]

    if workflow == "screenshot-to-code":
        if len(sys.argv) < 3:
            print("‚ùå Error: URL required")
            sys.exit(1)
        url = sys.argv[2]
        output_name = sys.argv[3] if len(sys.argv) > 3 else None
        workflow_screenshot_to_code(url, output_name)

    elif workflow == "login-and-scrape":
        if len(sys.argv) < 5:
            print("‚ùå Error: account_name, site, and URL required")
            sys.exit(1)
        account_name = sys.argv[2]
        site = sys.argv[3]
        url = sys.argv[4]
        workflow_login_and_scrape(account_name, site, url)

    elif workflow == "devtools-debug":
        if len(sys.argv) < 3:
            print("‚ùå Error: URL required")
            sys.exit(1)
        url = sys.argv[2]
        workflow_devtools_debug(url)

    else:
        print(f"‚ùå Unknown workflow: {workflow}")
        print("Run without arguments to see usage")
        sys.exit(1)


if __name__ == "__main__":
    main()
