#!/usr/bin/env python3
"""
Comprehensive MCP Integration Test for Qwen Proxy
Tests actual API calls with real MCP tool definitions
"""

import json
import requests
import sys

PROXY_URL = "http://localhost:8000/v1"

# Real MCP tool definitions (from LibreChat's MCP servers)
MCP_TOOLS = [
    {
        "type": "function",
        "function": {
            "name": "web_search",
            "description": "Search the web using DuckDuckGo",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "The search query"},
                    "max_results": {
                        "type": "integer",
                        "description": "Maximum number of results (default: 5)",
                    },
                },
                "required": ["query"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "github_search_repos",
            "description": "Search GitHub repositories",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "Search query"},
                    "language": {
                        "type": "string",
                        "description": "Programming language filter",
                    },
                },
                "required": ["query"],
            },
        },
    },
    {
        "type": "function",
        "function": {
            "name": "grep_search",
            "description": "Search for text patterns in files",
            "parameters": {
                "type": "object",
                "properties": {
                    "pattern": {
                        "type": "string",
                        "description": "Text pattern to search for",
                    },
                    "path": {"type": "string", "description": "Path to search in"},
                },
                "required": ["pattern"],
            },
        },
    },
]


def print_section(title):
    """Print formatted section header"""
    print("\n" + "=" * 60)
    print(title)
    print("=" * 60)


def test_model_with_mcp(model_name, prompt, expected_tools=None):
    """
    Test a model with MCP tool definitions

    Args:
        model_name: Full model name
        prompt: User prompt that should trigger tool usage
        expected_tools: List of expected tool names (for validation)

    Returns:
        (success, tool_calls, error_message)
    """
    print_section(f"Testing: {model_name}")
    print(f"Prompt: {prompt}")
    print(f"Expected tools: {expected_tools or 'any'}")

    request_data = {
        "model": model_name,
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant with access to various tools. When the user asks you to search, analyze, or fetch information, use the appropriate tool.",
            },
            {"role": "user", "content": prompt},
        ],
        "tools": MCP_TOOLS,
        "max_tokens": 1000,
        "temperature": 0.7,
    }

    try:
        print("\nüì§ Sending request to proxy...")
        response = requests.post(
            f"{PROXY_URL}/chat/completions", json=request_data, timeout=60
        )

        if response.status_code != 200:
            print(f"‚ùå Request failed with status {response.status_code}")
            print(f"Response: {response.text}")
            return False, None, f"HTTP {response.status_code}"

        data = response.json()

        # Validate response structure
        if "choices" not in data or not data["choices"]:
            print("‚ùå Response missing 'choices'")
            return False, None, "Invalid response structure"

        message = data["choices"][0].get("message", {})
        content = message.get("content", "")
        tool_calls = message.get("tool_calls", [])

        print("\nüì• Response received:")
        print(f"‚úì Content length: {len(content)} chars")
        print(f"‚úì Tool calls found: {len(tool_calls)}")

        if content:
            print(f"\nüìù Model response: {content[:200]}...")

        if tool_calls:
            print("\nüîß Tool calls detected:")
            for i, tool_call in enumerate(tool_calls):
                print(f"\n  Tool Call #{i + 1}:")
                print(f"    ID: {tool_call.get('id')}")
                print(f"    Type: {tool_call.get('type')}")
                print(f"    Function: {tool_call.get('function', {}).get('name')}")

                try:
                    args = json.loads(
                        tool_call.get("function", {}).get("arguments", "{}")
                    )
                    print(f"    Arguments: {json.dumps(args, indent=6)}")
                except json.JSONDecodeError:
                    print(
                        f"    Arguments (raw): {tool_call.get('function', {}).get('arguments')}"
                    )

                # Validate OpenAI format
                required_keys = ["id", "type", "function"]
                if all(key in tool_call for key in required_keys):
                    print("    ‚úì OpenAI format: VALID")
                else:
                    print("    ‚ùå OpenAI format: INVALID (missing keys)")
                    return False, tool_calls, "Invalid tool call format"

                # Validate function structure
                func = tool_call.get("function", {})
                if "name" in func and "arguments" in func:
                    print("    ‚úì Function structure: VALID")

                    # Validate arguments is a JSON string
                    try:
                        json.loads(func["arguments"])
                        print("    ‚úì Arguments JSON: VALID")
                    except:
                        print("    ‚ùå Arguments JSON: INVALID")
                        return False, tool_calls, "Invalid JSON arguments"
                else:
                    print("    ‚ùå Function structure: INVALID")
                    return False, tool_calls, "Invalid function structure"

            # Check if expected tools were called
            if expected_tools:
                called_tools = [tc.get("function", {}).get("name") for tc in tool_calls]
                if any(expected in called_tools for expected in expected_tools):
                    print(f"\n‚úì Expected tool called: {called_tools}")
                else:
                    print(
                        f"\n‚ö†Ô∏è  Expected tools {expected_tools} not called, got: {called_tools}"
                    )

            return True, tool_calls, None
        else:
            print("\n‚ö†Ô∏è  No tool calls in response")
            print("Model may have responded without using tools")
            print("This could mean:")
            print("  - Prompt didn't clearly request tool usage")
            print("  - Model chose to answer directly")
            print("  - Model needs more explicit tool instruction")
            return True, None, "No tool calls generated"

    except requests.exceptions.Timeout:
        print("‚ùå Request timed out (>60s)")
        return False, None, "Timeout"
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback

        traceback.print_exc()
        return False, None, str(e)


def main():
    """Run comprehensive MCP integration tests"""
    print_section("MCP Integration Test Suite for Qwen Proxy")
    print("Testing abliterated models with real MCP tool definitions")

    # Test configurations
    tests = [
        {
            "model": "huihui-ai/Qwen2.5-Coder-32B-Instruct-abliterated",
            "prompt": "Use web search to find the latest Python FastAPI best practices for 2025",
            "expected_tools": ["web_search"],
        },
        {
            "model": "huihui-ai/QwQ-32B-abliterated",
            "prompt": "Search GitHub for popular machine learning repositories written in Python",
            "expected_tools": ["github_search_repos"],
        },
        {
            "model": "huihui-ai/Qwen2.5-Coder-32B-Instruct-abliterated",
            "prompt": "Use the grep tool to search for TODO comments in my project files",
            "expected_tools": ["grep_search"],
        },
    ]

    results = []

    for test_config in tests:
        success, tool_calls, error = test_model_with_mcp(
            test_config["model"], test_config["prompt"], test_config["expected_tools"]
        )

        results.append(
            {
                "model": test_config["model"].split("/")[-1],  # Short name
                "success": success,
                "tool_calls": len(tool_calls) if tool_calls else 0,
                "error": error,
            }
        )

    # Summary
    print_section("Test Results Summary")

    print(f"\n{'Model':<40} {'Status':<10} {'Tools':<8} {'Notes':<30}")
    print("-" * 90)

    for result in results:
        status = "‚úì PASS" if result["success"] else "‚ùå FAIL"
        error_note = result["error"] or "Working correctly"
        print(
            f"{result['model']:<40} {status:<10} {result['tool_calls']:<8} {error_note:<30}"
        )

    passed = sum(1 for r in results if r["success"])
    total = len(results)

    print(f"\n{'Total:':<40} {passed}/{total} tests passed")

    if passed == total:
        print("\n‚úÖ ALL TESTS PASSED")
        print("\nüéâ Proxy is working perfectly!")
        print("Abliterated models can now call MCP tools via XML‚ÜíJSON conversion.")
        return 0
    else:
        print("\n‚ö†Ô∏è  SOME TESTS FAILED")
        print("Check logs above for details.")
        return 1


if __name__ == "__main__":
    sys.exit(main())
