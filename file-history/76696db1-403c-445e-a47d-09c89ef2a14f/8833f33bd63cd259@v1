# Best Abliterated Models with Tool Calling Support

**Date:** 2026-01-15
**Research Method:** Featherless API + grep/github MCP + internet search

---

## Executive Summary

Found **78 abliterated models** on Featherless.ai. Of these, **14 models explicitly support tool calling** with `"tool_use": true` feature flag.

**Key Finding:** Qwen3 models (not Qwen2.5) have native JSON tool calling support!

---

## Recommended Models for Your Use Cases

### 1. Architecture Research (Abliterated + Tool Calling)

**Primary:** `roslein/Qwen3-32B-abliterated`
- **Size:** 32B parameters
- **Context:** 32,768 tokens
- **Tool calling:** ‚úÖ `"tool_use": true`
- **Why:** Largest abliterated model with tool calling support

**Backup:** `huihui-ai/Huihui-Qwen3-14B-abliterated-v2`
- **Size:** 14B parameters
- **Context:** 32,768 tokens
- **Tool calling:** ‚úÖ `"tool_use": true`
- **Why:** Smaller but still powerful for architecture analysis

---

### 2. General Research (Abliterated + Tool Calling)

**Primary:** `mlabonne/Qwen3-14B-abliterated`
- **Size:** 14B parameters
- **Context:** 32,768 tokens
- **Tool calling:** ‚úÖ `"tool_use": true`
- **Why:** Balanced size for research with tool support

**Backup:** `Goekdeniz-Guelmez/Josiefied-Qwen3-14B-abliterated-v3`
- **Size:** 14B parameters
- **Context:** 32,768 tokens
- **Tool calling:** ‚úÖ `"tool_use": true`
- **Why:** Alternative variant with fine-tuning

---

### 3. Coding (Abliterated + Tool Calling)

**Primary:** `mlabonne/Qwen3-8B-abliterated`
- **Size:** 8B parameters
- **Context:** 32,768 tokens
- **Tool calling:** ‚úÖ `"tool_use": true`
- **Why:** Fast, efficient for coding tasks

**Backup:** `huihui-ai/Huihui-Qwen3-8B-abliterated-v2`
- **Size:** 8B parameters
- **Context:** 32,768 tokens
- **Tool calling:** ‚úÖ `"tool_use": true`
- **Why:** Alternative 8B variant

---

## All Models with Tool Calling Support (14 total)

### Large Models (14B-32B)
1. `roslein/Qwen3-32B-abliterated` - 32B, 32K context ‚≠ê BEST FOR ARCHITECTURE
2. `mlabonne/Qwen3-14B-abliterated` - 14B, 32K context
3. `huihui-ai/Huihui-Qwen3-14B-abliterated-v2` - 14B, 32K context
4. `Goekdeniz-Guelmez/Josiefied-Qwen3-14B-abliterated-v3` - 14B, 32K context

### Medium Models (8B)
5. `Goekdeniz-Guelmez/Josiefied-Qwen3-8B-abliterated-v1` - 8B, 32K context
6. `huihui-ai/Huihui-Qwen3-8B-abliterated-v2` - 8B, 32K context
7. `mlabonne/Qwen3-8B-abliterated` - 8B, 32K context ‚≠ê BEST FOR CODING
8. `mlx-community/Josiefied-DeepSeek-R1-0528-Qwen3-8B-abliterated-v1-bf16` - 8B, 32K context

### Small Models (4B)
9. `mlabonne/Qwen3-4B-abliterated` - 4B, 40K context
10. `Goekdeniz-Guelmez/Josiefied-Qwen3-4B-abliterated-v1` - 4B, 40K context
11. `huihui-ai/Huihui-Qwen3-4B-abliterated-v2` - 4B, 40K context
12. `huihui-ai/Huihui-Jan-nano-128k-abliterated` - 4B, 40K context
13. `huihui-ai/Huihui-Qwen3-4B-Instruct-2507-abliterated` - 4B, 40K context
14. `Goekdeniz-Guelmez/Josiefied-Qwen3-4B-Instruct-2507-gabliterated-v1` - 4B, 40K context

---

## Why These Work (Technical Details)

### Qwen3 vs Qwen2.5
- **Qwen2.5:** Uses XML-based tool calling (incompatible with LibreChat)
- **Qwen3:** Uses JSON-based OpenAI-compatible tool calling (works with LibreChat)

### Evidence
```json
{
  "features": {
    "tool_use": true
  }
}
```

Models with this feature flag in the Featherless API support OpenAI-compatible JSON function calling.

---

## Comparison with Non-Abliterated Models

### DeepSeek Models (Non-Abliterated)
- `deepseek-ai/DeepSeek-V3-0324` - ‚úÖ **CONFIRMED WORKING** with MCP tools
- `deepseek-ai/DeepSeek-R1-0528` - ‚è≥ Currently being tested
- **Limitation:** Not abliterated, may refuse certain prompts

### Qwen2.5 Models (Currently in Config)
- `zetasepic/Qwen2.5-72B-Instruct-abliterated-v2` - ‚ùå Outputs XML, not JSON
- **Problem:** LibreChat doesn't execute XML-formatted tool calls

---

## Recommended Configuration Update

Replace current Qwen models with Qwen3 models that have `tool_use: true`:

### librechat.yaml (Updated)
```yaml
endpoints:
  custom:
    - name: "Featherless"
      apiKey: "${FEATHERLESS_API_KEY}"
      baseURL: "https://api.featherless.ai/v1"
      models:
        default:
          # Architecture Research (Abliterated + Tool Calling)
          - "roslein/Qwen3-32B-abliterated"

          # General Research (Abliterated + Tool Calling)
          - "mlabonne/Qwen3-14B-abliterated"

          # Coding (Abliterated + Tool Calling)
          - "mlabonne/Qwen3-8B-abliterated"

          # Non-abliterated (Proven working)
          - "deepseek-ai/DeepSeek-V3-0324"
          - "deepseek-ai/DeepSeek-R1-0528"
        fetch: false
      titleConvo: true
      titleModel: "deepseek-ai/DeepSeek-V3-0324"
      summarize: false
      forcePrompt: false
      modelDisplayLabel: "Featherless"
      addParams:
        max_tokens: 2048
        temperature: 0.7
        top_p: 0.9
```

---

## Testing Status

### ‚úÖ Confirmed Working
- **deepseek-ai/DeepSeek-V3-0324:** Successfully used web search MCP tool
  - Test: "Search the web for DeepSeek V3 model architecture"
  - Result: Returned actual search results with structured information

### ‚è≥ Currently Testing
- **deepseek-ai/DeepSeek-R1-0528:** Reasoning model with chain-of-thought

### ‚ùå Not Working (XML Output)
- **zetasepic/Qwen2.5-72B-Instruct-abliterated-v2:** Outputs XML instead of JSON
- **Qwen/Qwen3-Coder-30B-A3B-Instruct:** Token limit error (16384 max)

### üìã Needs Testing
- All Qwen3 models with `tool_use: true` (14 models listed above)

---

## Action Plan

1. ‚úÖ **Research completed:** Found 78 abliterated models, identified 14 with tool calling
2. ‚è≥ **Update configuration:** Replace Qwen2.5 models with Qwen3 models
3. ‚è≥ **Test Qwen3 models:** Verify tool calling works with MCP servers
4. ‚è≥ **Document results:** Final report with all findings

---

## Sources

- [Featherless.ai Tool Calling Documentation](https://featherless.ai/docs/tool-calling)
- [Model Context Protocol (MCP) - LibreChat](https://www.librechat.ai/docs/features/mcp)
- [LibreChat MCP Servers](https://www.librechat.ai/docs/configuration/librechat_yaml/object_structure/mcp_servers)
- Featherless API Models Endpoint: `https://api.featherless.ai/v1/models`

---

## Additional Findings

### Llama 3.3 Abliterated Models (No Tool Calling)
- `huihui-ai/Llama-3.3-70B-Instruct-abliterated` - 70B, 32K context
- No `tool_use: true` flag (likely XML-based)

### Mistral Abliterated Models (No Tool Calling)
- `huihui-ai/Mistral-Small-24B-Instruct-2501-abliterated` - 24B, 32K context
- `huihui-ai/Arcee-Blitz-abliterated` - 24B, 32K context
- No `tool_use: true` flag

### Gemma 3 Abliterated Models (No Tool Calling)
- `huihui-ai/gemma-3-27b-it-abliterated` - 27B, 32K context
- `mlabonne/gemma-3-27b-it-abliterated` - 27B, 32K context
- No `tool_use: true` flag

**Conclusion:** Qwen3 models are the ONLY abliterated models with confirmed JSON tool calling support on Featherless.
