# Complete Setup Guide: Abliterated Models + RunPod + Full Stack

## Overview

This guide sets up:
- 4 abliterated AI models on RunPod (uncensored)
- VS Code extension for GPU control
- Browser automation (Playwright)
- Reverse engineering toolkit
- MCP servers for Roo Code + Claude Code
- Cost tracking

**Total time:** ~30 minutes
**Cost:** ~$5-10 for initial testing

---

## Phase 1: RunPod Account & API Key (5 min)

### Step 1.1: Create RunPod Account
```
1. Go to: https://www.runpod.io/
2. Click "Sign Up"
3. Add payment method (required for serverless)
```

### Step 1.2: Get API Key
```
1. Go to: https://www.runpod.io/console/user/settings
2. Click "API Keys" section
3. Click "Create API Key"
4. Copy the key (starts with "rp_...")
```

### Step 1.3: Save API Key Locally
```bash
# Add to your shell config
echo 'export RUNPOD_API_KEY="rp_YOUR_KEY_HERE"' >> ~/.zshrc
source ~/.zshrc

# Verify it's set
echo $RUNPOD_API_KEY
```

---

## Phase 2: Create Serverless Endpoints (15 min)

Go to: https://www.runpod.io/console/serverless

### Endpoint 1: Architect (DeepSeek-R1-32B)
```
Click "New Endpoint"

Name: architect
Select Template: "vLLM - OpenAI Compatible"
GPU: A40 48GB ($0.79/hr)

Environment Variables:
  MODEL_NAME = huihui-ai/DeepSeek-R1-Distill-Qwen-32B-abliterated
  MAX_MODEL_LEN = 32768
  TENSOR_PARALLEL_SIZE = 1

Workers:
  Min Workers: 0 (saves money when idle)
  Max Workers: 1
  Idle Timeout: 5 seconds

Click "Create Endpoint"
Copy the Endpoint ID (e.g., "abc123xyz")
```

### Endpoint 2: Code Writer (Qwen3-Coder-30B)
```
Click "New Endpoint"

Name: code
Select Template: "vLLM - OpenAI Compatible"
GPU: A40 48GB ($0.79/hr)

Environment Variables:
  MODEL_NAME = huihui-ai/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated
  MAX_MODEL_LEN = 32768

Workers:
  Min Workers: 0
  Max Workers: 1
  Idle Timeout: 5 seconds

Copy the Endpoint ID
```

### Endpoint 3: Research (Qwen3-80B MoE)
```
Click "New Endpoint"

Name: research
Select Template: "vLLM - OpenAI Compatible"
GPU: A100 40GB ($1.14/hr) - needs more VRAM for 80B

Environment Variables:
  MODEL_NAME = huihui-ai/Huihui-Qwen3-Next-80B-A3B-Thinking-Abliterated
  MAX_MODEL_LEN = 16384

Workers:
  Min Workers: 0
  Max Workers: 1
  Idle Timeout: 5 seconds

Copy the Endpoint ID
```

### Endpoint 4: Quick (Qwen3-4B)
```
Click "New Endpoint"

Name: quick
Select Template: "vLLM - OpenAI Compatible"
GPU: RTX 3090 24GB ($0.44/hr) - cheapest option

Environment Variables:
  MODEL_NAME = DavidAU/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-Heretic-Abliterated
  MAX_MODEL_LEN = 32768

Workers:
  Min Workers: 0
  Max Workers: 1
  Idle Timeout: 5 seconds

Copy the Endpoint ID
```

### Step 2.5: Save Your Endpoint IDs
```bash
# You should have 4 endpoint IDs like:
# architect: abc123...
# code:      def456...
# research:  ghi789...
# quick:     jkl012...
```

---

## Phase 3: Install Local Tools (10 min)

### Step 3.1: Install Prerequisites
```bash
# Homebrew (if not installed)
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"

# Python packages
pip3 install runpod requests python-dotenv playwright openai

# Playwright browsers
playwright install chromium

# Reverse engineering tools
brew install radare2 binwalk

# Node.js (for MCP servers)
brew install node
```

### Step 3.2: Install MCP Servers
```bash
# RunPod MCP (official)
npm install -g @runpod/mcp

# Ghidra MCP
pip3 install ghidra-mcp
```

### Step 3.3: Optional - Install Ghidra
```bash
# Download from https://ghidra-sre.org/
# Extract to /Applications/ghidra
# Add to PATH:
echo 'export PATH="/Applications/ghidra:$PATH"' >> ~/.zshrc
```

---

## Phase 4: Configure Your Files (5 min)

### Step 4.1: Update Endpoint IDs
```bash
# Edit runpod_api.py with your endpoint IDs
nano ~/runpod_api.py
```

Find this section (around line 27) and replace with your IDs:
```python
ENDPOINTS = {
    "architect": {
        "id": "YOUR_ARCHITECT_ID",  # <-- Replace
        "url": "https://api.runpod.ai/v2/YOUR_ARCHITECT_ID/openai/v1",
        "model": "huihui-ai/DeepSeek-R1-Distill-Qwen-32B-abliterated"
    },
    "code": {
        "id": "YOUR_CODE_ID",  # <-- Replace
        "url": "https://api.runpod.ai/v2/YOUR_CODE_ID/openai/v1",
        "model": "huihui-ai/Huihui-Qwen3-Coder-30B-A3B-Instruct-abliterated"
    },
    "research": {
        "id": "YOUR_RESEARCH_ID",  # <-- Replace
        "url": "https://api.runpod.ai/v2/YOUR_RESEARCH_ID/openai/v1",
        "model": "huihui-ai/Huihui-Qwen3-Next-80B-A3B-Thinking-Abliterated"
    },
    "quick": {
        "id": "YOUR_QUICK_ID",  # <-- Replace
        "url": "https://api.runpod.ai/v2/YOUR_QUICK_ID/openai/v1",
        "model": "DavidAU/Qwen3-4B-Thinking-2507-Claude-4.5-Opus-High-Reasoning-Distill-Heretic-Abliterated"
    }
}
```

### Step 4.2: Build VS Code Extension
```bash
cd ~/vscode-runpod-gpu
npm install
npm run compile
npx @vscode/vsce package --allow-missing-repository
code --install-extension *.vsix
```

### Step 4.3: Configure VS Code Extension
Open VS Code Settings (Cmd+,) and add:
```json
{
  "runpod.apiKey": "rp_YOUR_KEY_HERE",
  "runpod.refreshInterval": 30,
  "runpod.showCostInStatusBar": true
}
```

---

## Phase 5: Configure MCP Servers (5 min)

### For Claude Code
```bash
mkdir -p ~/.claude
cat > ~/.claude/mcp_servers.json << 'EOF'
{
  "mcpServers": {
    "runpod": {
      "command": "npx",
      "args": ["-y", "@runpod/mcp"],
      "env": {
        "RUNPOD_API_KEY": "${RUNPOD_API_KEY}"
      }
    },
    "ghidra": {
      "command": "python3",
      "args": ["-m", "ghidra_mcp"]
    }
  }
}
EOF
```

### For Roo Code
In VS Code, go to Settings > Extensions > Roo Code > MCP Servers and add:
```json
{
  "runpod": {
    "command": "npx",
    "args": ["-y", "@runpod/mcp"],
    "env": {
      "RUNPOD_API_KEY": "rp_YOUR_KEY"
    }
  }
}
```

---

## Phase 6: Test Everything (5 min)

### Test 1: RunPod API
```bash
python3 ~/runpod_api.py ask quick "Hello, are you working?"
```

### Test 2: Cost Tracker
```bash
python3 ~/cost_tracker.py status
```

### Test 3: RE Toolkit
```bash
python3 ~/re_toolkit.py tools
python3 ~/re_toolkit.py info /bin/ls
```

### Test 4: VS Code Extension
```
1. Open VS Code
2. Look for "RunPod: X/X Active" in status bar
3. Click it to see control menu
```

### Test 5: Browser Automation
```bash
python3 ~/browser_automation.py screenshot https://example.com test.png
```

### Test 6: MCP in Claude Code
```bash
claude
# Then ask: "List my RunPod endpoints"
```

---

## Quick Reference Card

### Daily Commands
```bash
# Check costs
python3 ~/cost_tracker.py status

# Use models
python3 ~/runpod_api.py ask architect "Your question"
python3 ~/runpod_api.py ask code "Write a Python script that..."
python3 ~/runpod_api.py ask research "Deep analysis of..."
python3 ~/runpod_api.py ask quick "Quick question"

# Control endpoints
python3 ~/runpod-control.py list
python3 ~/runpod-control.py pause architect
python3 ~/runpod-control.py resume architect

# RE analysis
python3 ~/re_toolkit.py ai suspicious.bin architect

# Browser automation
python3 ~/browser_automation.py scrape https://example.com
```

### VS Code Commands (Cmd+Shift+P)
```
RunPod: Show GPU Status
RunPod: Pause All Endpoints
RunPod: Resume All Endpoints
RunPod: Show Cost Report
```

### Roo Code Custom Modes
```
/architect - System design, debugging, RE
/code      - Code generation
/research  - Deep analysis
/quick     - Fast responses
```

---

## Cost Management

### Set Daily Budget
```bash
python3 ~/cost_tracker.py set-budget daily 10.00
```

### Monitor Spending
```bash
python3 ~/cost_tracker.py daily
python3 ~/cost_tracker.py models
```

### GPU Pricing Reference
| GPU | $/hour | Best For |
|-----|--------|----------|
| RTX 3090 | $0.44 | 4B models (quick) |
| RTX 4090 | $0.69 | 7-14B models |
| A40 | $0.79 | 30-32B models |
| A100 40GB | $1.14 | 70-80B MoE models |
| H100 | $2.99+ | Largest models |

---

## Troubleshooting

### "RUNPOD_API_KEY not set"
```bash
source ~/.zshrc
echo $RUNPOD_API_KEY  # Should show your key
```

### "Endpoint not responding"
```bash
# Check endpoint status on RunPod console
# Or use:
python3 ~/runpod-control.py list
```

### "Cold start taking too long"
First request to an idle endpoint takes 30-60 seconds to load the model.
This is normal. Subsequent requests are fast.

### VS Code extension not showing
```bash
code --list-extensions | grep runpod
# If not listed, reinstall the extension
```

---

## Files Summary

| File | Purpose |
|------|---------|
| `~/runpod_api.py` | Call abliterated models |
| `~/runpod-control.py` | Manage endpoints |
| `~/browser_automation.py` | Playwright automation |
| `~/cost_tracker.py` | Track costs |
| `~/re_toolkit.py` | Reverse engineering |
| `~/vscode-runpod-gpu/` | VS Code extension |
| `~/.claude/mcp_servers.json` | Claude Code MCP config |

---

## You're Done!

Your setup now includes:
- [x] 4 uncensored AI models on RunPod
- [x] VS Code status bar for GPU control
- [x] Cost tracking with budget alerts
- [x] Browser automation with Playwright
- [x] Reverse engineering toolkit
- [x] MCP integration for Roo Code + Claude Code

**Start using:**
```bash
# Ask the architect model
python3 ~/runpod_api.py ask architect "How do I reverse engineer this binary?"

# Or in VS Code, click the RunPod status bar item
```
