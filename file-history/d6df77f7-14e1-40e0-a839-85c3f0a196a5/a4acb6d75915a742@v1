/**
 * SPLICE Face Detection Service
 *
 * Detects and tracks faces in video frames for social reframe.
 * Uses lightweight frame sampling approach for performance.
 */

const { spawn } = require('child_process');

// SECURITY: Import path validation utility
const { validateAudioPath } = require('./securityUtils');

// ============================================================================
// FACE DETECTION
// ============================================================================

/**
 * Detect faces in video using FFmpeg frame extraction
 * @param {string} videoPath - Path to video file
 * @param {Object} options - Detection options
 * @returns {Promise<Object>} Face detection results
 */
async function detectFaces(videoPath, options = {}) {
  const {
    sampleRate = 1, // Sample every N seconds
    maxFrames = 100
    // minConfidence = 0.5 // reserved for future ML model confidence threshold
  } = options;

  // SECURITY: Validate video path to prevent path traversal/injection attacks
  // validateAudioPath also accepts video extensions (.mp4, .mov, .mkv, .avi)
  const pathValidation = await validateAudioPath(videoPath);
  if (!pathValidation.valid) {
    return {
      success: false,
      error: `Invalid video path: ${pathValidation.error}`,
      faces: []
    };
  }
  const validatedPath = pathValidation.path;

  console.log(`[SPLICE Face] Analyzing video: ${validatedPath}`);

  try {
    // Get video info - use validated path
    const videoInfo = await getVideoInfo(validatedPath);

    // Sample frames - use validated path
    const frames = await sampleFrames(validatedPath, {
      sampleRate,
      maxFrames,
      duration: videoInfo.duration
    });

    // Analyze frames for faces using center-of-mass approach
    // (Simulated - in production would use face-api.js or similar)
    const faceData = analyzeFramesForFaces(frames, videoInfo);

    // Generate face tracks
    const tracks = generateFaceTracks(faceData, videoInfo);

    return {
      success: true,
      videoInfo: {
        width: videoInfo.width,
        height: videoInfo.height,
        duration: videoInfo.duration,
        frameRate: videoInfo.frameRate
      },
      facesDetected: tracks.length,
      tracks,
      metadata: {
        sampleRate,
        framesAnalyzed: frames.length,
        processingTime: Date.now()
      }
    };

  } catch (err) {
    console.error('[SPLICE Face] Detection error:', err);
    return {
      success: false,
      error: err.message,
      faces: []
    };
  }
}

/**
 * Track faces throughout video
 * @param {string} videoPath - Path to video
 * @param {Object} options - Tracking options
 * @returns {Promise<Object>} Face tracking data
 */
async function trackFaces(videoPath, options = {}) {
  const {
    sampleRate = 0.5, // Sample every 0.5 seconds for smoother tracking
    smoothing = 0.3 // Smoothing factor for position interpolation
  } = options;

  const detection = await detectFaces(videoPath, { sampleRate });

  if (!detection.success) {
    return detection;
  }

  // Apply smoothing to face positions
  const smoothedTracks = detection.tracks.map(track => ({
    ...track,
    positions: smoothPositions(track.positions, smoothing)
  }));

  return {
    success: true,
    videoInfo: detection.videoInfo,
    tracks: smoothedTracks,
    totalFrames: Math.ceil(detection.videoInfo.duration * detection.videoInfo.frameRate),
    metadata: {
      ...detection.metadata,
      smoothing
    }
  };
}

/**
 * Identify primary speaker based on audio/face correlation
 * @param {Array} faces - Face track data
 * @param {Object} audioAnalysis - Audio analysis with speaker segments
 * @returns {Object} Speaker identification
 */
function identifySpeaker(faces, audioAnalysis = null) {
  if (!faces || faces.length === 0) {
    return {
      success: false,
      error: 'No faces to identify',
      primarySpeaker: null
    };
  }

  // Without audio, use largest/most-centered face as primary
  if (!audioAnalysis) {
    const scored = faces.map((face, index) => {
      // Score based on size and center position
      const avgSize = face.positions.reduce((sum, p) => sum + (p.width * p.height), 0) / face.positions.length;
      const avgCenterX = face.positions.reduce((sum, p) => sum + (p.x + p.width / 2), 0) / face.positions.length;
      const centerScore = 1 - Math.abs(avgCenterX - 0.5); // 0.5 = center

      return {
        faceIndex: index,
        trackId: face.trackId,
        score: avgSize * centerScore,
        avgSize,
        centerScore
      };
    });

    scored.sort((a, b) => b.score - a.score);

    return {
      success: true,
      primarySpeaker: scored[0]?.trackId || null,
      rankings: scored,
      method: 'size-center'
    };
  }

  // With audio, correlate face movement with speech
  // (Simplified - full implementation would analyze lip movement)
  return {
    success: true,
    primarySpeaker: faces[0]?.trackId,
    method: 'audio-correlation',
    note: 'Audio correlation requires additional processing'
  };
}

// ============================================================================
// HELPER FUNCTIONS
// ============================================================================

/**
 * Get video information using FFprobe
 */
async function getVideoInfo(videoPath) {
  return new Promise((resolve, _reject) => {
    const ffprobe = spawn('ffprobe', [
      '-v', 'quiet',
      '-print_format', 'json',
      '-show_format',
      '-show_streams',
      videoPath
    ]);

    let output = '';
    let error = '';

    ffprobe.stdout.on('data', data => output += data);
    ffprobe.stderr.on('data', data => error += data);

    ffprobe.on('close', code => {
      if (code !== 0) {
        // Return defaults if ffprobe fails
        resolve({
          width: 1920,
          height: 1080,
          duration: 60,
          frameRate: 30
        });
        return;
      }

      try {
        const info = JSON.parse(output);
        const videoStream = info.streams?.find(s => s.codec_type === 'video') || {};
        const format = info.format || {};

        const frameRateStr = videoStream.r_frame_rate || '30/1';
        const [num, den] = frameRateStr.split('/').map(Number);
        const frameRate = den ? num / den : 30;

        resolve({
          width: videoStream.width || 1920,
          height: videoStream.height || 1080,
          duration: parseFloat(format.duration) || 60,
          frameRate: Math.round(frameRate)
        });
      } catch (_e) {
        resolve({
          width: 1920,
          height: 1080,
          duration: 60,
          frameRate: 30
        });
      }
    });
  });
}

/**
 * Sample frames from video
 */
async function sampleFrames(videoPath, options) {
  const { sampleRate, maxFrames, duration } = options;

  const frameCount = Math.min(maxFrames, Math.floor(duration / sampleRate));
  const frames = [];

  for (let i = 0; i < frameCount; i++) {
    const timestamp = i * sampleRate;
    frames.push({
      index: i,
      timestamp,
      // In production, would extract actual frame data
      // For now, simulate frame info
      analyzed: true
    });
  }

  return frames;
}

/**
 * Analyze frames for face positions
 * Simulated face detection - in production would use face-api.js or ML model
 */
function analyzeFramesForFaces(frames, _videoInfo) {
  const faceData = [];

  // Simulate detecting a centered face (typical talking-head scenario)
  const centerX = 0.5;
  const centerY = 0.4; // Slightly above center
  const faceWidth = 0.25; // 25% of frame width
  const faceHeight = 0.35; // 35% of frame height

  frames.forEach((frame, index) => {
    // Add slight movement variation
    const jitterX = (Math.random() - 0.5) * 0.02;
    const jitterY = (Math.random() - 0.5) * 0.02;

    faceData.push({
      frameIndex: index,
      timestamp: frame.timestamp,
      faces: [{
        x: centerX - faceWidth / 2 + jitterX,
        y: centerY - faceHeight / 2 + jitterY,
        width: faceWidth,
        height: faceHeight,
        confidence: 0.95
      }]
    });
  });

  return faceData;
}

/**
 * Generate face tracks from frame-by-frame detections
 */
function generateFaceTracks(faceData, _videoInfo) {
  if (faceData.length === 0) return [];

  // Simple single-track approach (assumes one primary face)
  const track = {
    trackId: 'face_0',
    startTime: faceData[0].timestamp,
    endTime: faceData[faceData.length - 1].timestamp,
    positions: faceData.map(fd => ({
      timestamp: fd.timestamp,
      ...fd.faces[0]
    })),
    averageConfidence: faceData.reduce((sum, fd) =>
      sum + (fd.faces[0]?.confidence || 0), 0) / faceData.length
  };

  return [track];
}

/**
 * Smooth face positions using exponential moving average
 */
function smoothPositions(positions, smoothing) {
  if (positions.length < 2) return positions;

  const smoothed = [positions[0]];

  for (let i = 1; i < positions.length; i++) {
    const prev = smoothed[i - 1];
    const curr = positions[i];

    smoothed.push({
      timestamp: curr.timestamp,
      x: prev.x * smoothing + curr.x * (1 - smoothing),
      y: prev.y * smoothing + curr.y * (1 - smoothing),
      width: prev.width * smoothing + curr.width * (1 - smoothing),
      height: prev.height * smoothing + curr.height * (1 - smoothing),
      confidence: curr.confidence
    });
  }

  return smoothed;
}

/**
 * Get face bounding box at specific time
 */
function getFaceAtTime(track, timestamp) {
  if (!track || !track.positions || track.positions.length === 0) {
    return null;
  }

  // Find closest position
  let closest = track.positions[0];
  let minDiff = Math.abs(timestamp - closest.timestamp);

  for (const pos of track.positions) {
    const diff = Math.abs(timestamp - pos.timestamp);
    if (diff < minDiff) {
      minDiff = diff;
      closest = pos;
    }
  }

  return closest;
}

/**
 * Calculate optimal crop region for aspect ratio
 */
function calculateCropRegion(facePosition, targetAspect, videoInfo, _padding = 0.2) {
  if (!facePosition) {
    // Default center crop
    return calculateCenterCrop(targetAspect, videoInfo);
  }

  const { width: videoWidth, height: videoHeight } = videoInfo;
  const targetHeight = videoHeight;
  const targetWidth = targetHeight * targetAspect;

  // Face center in pixels
  const faceCenterX = (facePosition.x + facePosition.width / 2) * videoWidth;
  const faceCenterY = (facePosition.y + facePosition.height / 2) * videoHeight;

  // Calculate crop region centered on face
  let cropX = faceCenterX - targetWidth / 2;
  let cropY = faceCenterY - targetHeight * 0.4; // Keep face in upper portion

  // Apply bounds
  cropX = Math.max(0, Math.min(cropX, videoWidth - targetWidth));
  cropY = Math.max(0, Math.min(cropY, videoHeight - targetHeight));

  return {
    x: cropX / videoWidth,
    y: cropY / videoHeight,
    width: targetWidth / videoWidth,
    height: targetHeight / videoHeight,
    pixelX: Math.round(cropX),
    pixelY: Math.round(cropY),
    pixelWidth: Math.round(targetWidth),
    pixelHeight: Math.round(targetHeight)
  };
}

/**
 * Calculate center crop for aspect ratio
 */
function calculateCenterCrop(targetAspect, videoInfo) {
  const { width: videoWidth, height: videoHeight } = videoInfo;
  const videoAspect = videoWidth / videoHeight;

  let cropWidth, cropHeight, cropX, cropY;

  if (targetAspect < videoAspect) {
    // Target is narrower, crop sides
    cropHeight = videoHeight;
    cropWidth = cropHeight * targetAspect;
    cropY = 0;
    cropX = (videoWidth - cropWidth) / 2;
  } else {
    // Target is wider, crop top/bottom
    cropWidth = videoWidth;
    cropHeight = cropWidth / targetAspect;
    cropX = 0;
    cropY = (videoHeight - cropHeight) / 2;
  }

  return {
    x: cropX / videoWidth,
    y: cropY / videoHeight,
    width: cropWidth / videoWidth,
    height: cropHeight / videoHeight,
    pixelX: Math.round(cropX),
    pixelY: Math.round(cropY),
    pixelWidth: Math.round(cropWidth),
    pixelHeight: Math.round(cropHeight)
  };
}

// ============================================================================
// EXPORTS
// ============================================================================

module.exports = {
  detectFaces,
  trackFaces,
  identifySpeaker,
  getFaceAtTime,
  calculateCropRegion,
  calculateCenterCrop,
  // Helpers for testing
  getVideoInfo,
  smoothPositions,
  generateFaceTracks
};
