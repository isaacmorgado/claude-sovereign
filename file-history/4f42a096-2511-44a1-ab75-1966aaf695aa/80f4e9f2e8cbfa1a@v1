# SPLICE Plugin - Comprehensive Research Document

**Document Version:** 2.0
**Date:** December 16, 2025
**Purpose:** Research compilation for Adobe Premiere Pro auto-cut plugin development
**Research Sources:** 10 parallel research agents investigating all aspects of plugin development

---

## Executive Summary

SPLICE is an Adobe Premiere Pro plugin designed to automate the organization of video clips by detecting multiple takes of the same dialogue, color-coding them for easy selection, removing dead space, and importing organized clips into the timeline.

### Core Workflow
1. User drags and drops clips into the plugin window
2. Plugin isolates voice using ElevenLabs API (for analysis accuracy)
3. Transcription API generates word-level timestamps
4. LLM analyzes transcript to detect repeated phrases (different takes)
5. Plugin organizes clips by take number (Take 1, Take 2, Take 3...)
6. Color codes clips for easy editor selection
7. Removes dead space/silence
8. Imports organized clips into Premiere timeline

### Unique Value Proposition
**No competitor currently offers take detection** - SPLICE will be the first tool to automatically identify and group multiple takes of the same dialogue, a pain point every video editor faces.

---

## Table of Contents

1. [Adobe Premiere Pro Plugin Development](#1-adobe-premiere-pro-plugin-development)
2. [Voice Isolation APIs](#2-voice-isolation-apis)
3. [Speech-to-Text/Transcription APIs](#3-speech-to-texttranscription-apis)
4. [LLM APIs for Text Analysis](#4-llm-apis-for-text-analysis)
5. [Silence/Dead Space Detection](#5-silencedead-space-detection)
6. [Premiere Timeline Manipulation](#6-premiere-timeline-manipulation)
7. [Take Detection Algorithms](#7-take-detection-algorithms)
8. [CEP/UXP Panel Development](#8-cepuxp-panel-development)
9. [Plugin Architecture Patterns](#9-plugin-architecture-patterns)
10. [Competitive Analysis](#10-competitive-analysis)
11. [Strategic Recommendations](#11-strategic-recommendations)
12. [Technology Stack Summary](#12-technology-stack-summary)

---

## 1. Adobe Premiere Pro Plugin Development

### Development Pathways

| Type | Technology | Use Case | Status |
|------|------------|----------|--------|
| **UXP Plugins** | HTML/CSS/JavaScript | Modern panel development | Beta (Premiere Pro 25.6+) |
| **CEP Extensions** | HTML/CSS/JavaScript + ExtendScript | Production-ready panels | Supported through Sept 2026 |
| **C++ Native Plugins** | C++ | Video effects, transitions, importers | Stable |

### Recommendation for SPLICE: **CEP with Bolt CEP Framework**

**Reasons:**
- CEP is mature, stable, and fully documented
- Bolt CEP provides modern React/Vue/Svelte + TypeScript development
- Full ExtendScript API access for timeline manipulation
- UXP is still in beta for Premiere Pro (not production-ready)
- Architecture can be designed for future UXP migration

### Key Technologies

```
Frontend: React 18+ with TypeScript (via Bolt CEP)
Backend: ExtendScript for Premiere Pro API
Build Tool: Vite (10-100x faster than webpack)
Packaging: ZXP format for distribution
```

### System Requirements

- Adobe Premiere Pro 22.0+ (CEP 11/12)
- Windows 10+ / macOS 10.15+
- Node.js 16+ for development

### Development Setup (Bolt CEP)

```bash
# Create new project
yarn create bolt-cep

# Development with Hot Module Replacement
yarn dev

# Production build
yarn build

# Create signed ZXP package
yarn zxp
```

---

## 2. Voice Isolation APIs

### Recommended: ElevenLabs Audio Isolation API

**Endpoint:** `POST https://api.elevenlabs.io/v1/audio-isolation`

| Specification | Value |
|---------------|-------|
| Max file size | 500 MB |
| Max duration | 1 hour |
| Output quality | 128 kbps @ 44.1kHz (paid: higher) |
| Cost | 1,000 characters per minute |
| Latency optimization | Use `pcm_s16le_16` format |

### Pricing (per 1,000 characters)

| Tier | Cost |
|------|------|
| Creator | $0.30 |
| Pro | $0.24 |
| Scale | $0.18 |
| Business | $0.12 |

### Implementation

```python
import requests

url = "https://api.elevenlabs.io/v1/audio-isolation"
headers = {"xi-api-key": "YOUR_API_KEY"}
files = {"audio": open("input.mp4", "rb")}

response = requests.post(url, headers=headers, files=files)
with open("isolated_audio.mp3", "wb") as f:
    f.write(response.content)
```

### Alternative: Dolby.io Media Enhance API

| Feature | Value |
|---------|-------|
| Capabilities | Speech isolation + noise reduction + loudness correction |
| Pricing | $0.05/minute (pay-as-you-go) |
| Free credits | $200 on signup |
| Quality | Professional-grade |

### Audio Processing Modes (User Feature)

```
Mode 1: "Keep Original Audio" (Default)
├── Voice isolation for analysis only
├── Transcription runs on isolated voice (better accuracy)
└── Final clips retain ORIGINAL audio (music, ambience)

Mode 2: "Isolated Voice Only"
├── Voice isolation applied to final output
└── Useful for podcasts/interviews in noisy environments

Mode 3: "No Voice Isolation"
├── Skip isolation entirely
└── Faster processing, lower API costs
```

---

## 3. Speech-to-Text/Transcription APIs

### Recommended: Deepgram Nova-3

**Why Deepgram:**
- Native word-level timestamps included (essential for video editing)
- Speaker diarization included at no extra cost
- 50%+ lower Word Error Rate than competitors
- Sub-300ms latency for real-time
- Best value at scale

### Pricing Comparison

| Provider | Batch Cost | Word Timestamps | Diarization | Free Credits |
|----------|------------|-----------------|-------------|--------------|
| **Deepgram Nova-3** | $0.0043/min | Native | Included | $200 |
| AssemblyAI Universal | $0.0025/min | Native | +$0.02/hr | $50 |
| OpenAI Whisper | $0.006/min | Via parameter | Not available | None |
| Rev.ai | Contact | Native | Included | - |

### Implementation (Deepgram)

```python
from deepgram import Deepgram

response = await dg.transcription.prerecorded(
    {"url": "video_url"},
    {"punctuate": True, "diarize": True, "utterances": True}
)

for word in response['results']['channels'][0]['alternatives'][0]['words']:
    print(f"{word['word']}: {word['start']}s - {word['end']}s")
```

### Critical for SPLICE

Word-level timestamps are **essential** for:
- Precise clip cutting at word boundaries
- Aligning transcript to video frames
- Accurate silence detection between words
- Determining exact take boundaries

---

## 4. LLM APIs for Text Analysis

### Recommended: GPT-4o-mini

| Specification | Value |
|---------------|-------|
| Context Window | 128,000 tokens (~96,000 words) |
| Input Cost | $0.15 per 1M tokens |
| Output Cost | $0.60 per 1M tokens |
| Best for | Analyzing full transcripts in single call |

### Take Detection Prompt Strategy

```python
system_prompt = """
You are a transcript editor assistant specializing in identifying repeated takes.

A "repeated take" occurs when:
- A speaker says the same sentence/phrase multiple times
- Content is semantically identical but with minor wording changes
- The same point is restated (indicating the speaker re-recorded)

For each repeated section:
1. Quote the repeated content
2. List all occurrences with timestamps/line numbers
3. Rate similarity: EXACT / MINOR_VARIATION / PARAPHRASE
4. Suggest which take to keep (if determinable from text quality)
"""
```

### Hybrid Approach (Recommended for Cost Efficiency)

```
1. Pre-filter with embeddings (sentence-transformers)
   └── Identify candidate pairs with similarity > 0.80

2. Send only candidates to LLM for nuanced assessment
   └── ~80% reduction in API calls

3. LLM confirms groupings and suggests best takes
```

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(transcript_lines)
similarity_matrix = cosine_similarity(embeddings)

# Find pairs with similarity > 0.80 for LLM analysis
candidates = find_high_similarity_pairs(similarity_matrix, threshold=0.80)
```

### Cost Estimate (1-hour transcript)

| Component | Cost |
|-----------|------|
| Input (~15K tokens) | ~$0.002 |
| Output (~3K tokens) | ~$0.002 |
| **Total** | **~$0.004** |

### Local LLM Option (Privacy)

For users concerned about cloud processing:
- **Ollama** with Llama 3.2 8B or Mistral 7B
- 100% local processing, no per-minute costs
- Requires 8GB+ RAM

---

## 5. Silence/Dead Space Detection

### Recommended: Hybrid Approach

**Pass 1: FFmpeg for initial detection**
```bash
ffmpeg -i input.mp4 -af "silencedetect=noise=-40dB:d=1.5" -f null -
```

**Pass 2: Silero VAD for accurate speech detection**
```python
import torch

model, utils = torch.hub.load('snakers4/silero-vad', 'silero_vad')
get_speech_timestamps, _, read_audio, _, _ = utils

wav = read_audio('audio.wav', sampling_rate=16000)
speech_timestamps = get_speech_timestamps(
    wav, model,
    threshold=0.5,
    min_speech_duration_ms=250,
    min_silence_duration_ms=1000  # Remove 1+ second silences
)
```

### Threshold Recommendations

| Detection Type | Threshold | Duration | Action |
|----------------|-----------|----------|--------|
| Dead Space | -40dB to -50dB | 1.5s+ | Remove |
| Natural Pauses | -35dB | <800ms | Preserve |

### Key Distinction

- **Dead Space:** Extended silence (1+ seconds) - should be removed
- **Natural Pauses:** Brief pauses (200-800ms) - should be preserved for natural speech rhythm

### Transcript-Based Gap Analysis

```javascript
function detectSilence(words, options) {
  const { minDuration = 1.5 } = options;
  const silentSections = [];

  for (let i = 0; i < words.length - 1; i++) {
    const gap = words[i + 1].start - words[i].end;

    if (gap >= minDuration) {
      silentSections.push({
        startTime: words[i].end,
        endTime: words[i + 1].start,
        duration: gap,
        type: 'cuttable'
      });
    }
  }

  return silentSections;
}
```

---

## 6. Premiere Timeline Manipulation

### Core ExtendScript Operations

**Adding Clips to Timeline:**
```javascript
var seq = app.project.activeSequence;
var clip = app.project.rootItem.children[0];

// Insert at specific time (pushes existing clips)
seq.videoTracks[0].insertClip(clip, 5.0);

// Overwrite at position (replaces existing)
seq.videoTracks[0].overwriteClip(clip, playerPosition);
```

**Setting In/Out Points:**
```javascript
var projectItem = app.project.rootItem.children[0];
projectItem.setInPoint(2.0, 4);   // Start at 2 seconds
projectItem.setOutPoint(10.0, 4); // End at 10 seconds
```

**Color Coding Clips (0-15):**
```javascript
// Color values: 0=Violet, 1=Iris, 2=Caribbean, 3=Cerulean,
// 4=Forest, 5=Rose, 6=Mango, 7=Purple, 8=Cerulean, 9=Lavender...

projectItem.setColorLabel(5);  // Rose for Take 1
projectItem.setColorLabel(3);  // Cerulean for Take 2
projectItem.setColorLabel(6);  // Mango for Take 3
```

**Creating Bins for Organization:**
```javascript
var rootBin = app.project.rootItem;
var takesBin = rootBin.createBin("SPLICE Takes");
var take1Bin = takesBin.createBin("Take 1 - Rose");
var take2Bin = takesBin.createBin("Take 2 - Cerulean");

// Move clips to bins
projectItem.moveBin(take1Bin);
```

**Creating Markers:**
```javascript
var seq = app.project.activeSequence;
var marker = seq.markers.createMarker(5.0);
marker.name = "Take 1 - Best";
marker.setTypeAsChapter();
marker.setColorByIndex(1, 0);  // Red
```

### Known Limitations

1. **No API for moving clips between tracks** - Must delete and re-insert
2. **TrackItem colors inherit from ProjectItem** - Set color before timeline insert
3. **No native split/razor API** - Must calculate and create subclips
4. **ExtendScript uses ECMAScript 3** - Legacy JavaScript syntax

---

## 7. Take Detection Algorithms

### Recommended Pipeline

```
┌─────────────────────────────────────────────────────────────────┐
│                     INPUT: Raw Video/Audio                      │
└─────────────────────────────────────────────────────────────────┘
                               │
                               ▼
┌─────────────────────────────────────────────────────────────────┐
│  STAGE 1: Transcribe all audio segments (Deepgram)             │
└─────────────────────────────────────────────────────────────────┘
                               │
                               ▼
┌─────────────────────────────────────────────────────────────────┐
│  STAGE 2: Generate embeddings (sentence-transformers)          │
│           Model: all-MiniLM-L6-v2 (384-dim vectors)            │
└─────────────────────────────────────────────────────────────────┘
                               │
                               ▼
┌─────────────────────────────────────────────────────────────────┐
│  STAGE 3: Compute cosine similarity matrix                     │
└─────────────────────────────────────────────────────────────────┘
                               │
                               ▼
┌─────────────────────────────────────────────────────────────────┐
│  STAGE 4: Apply clustering (Agglomerative, threshold=0.25)     │
│           Each cluster = one dialogue line's takes             │
└─────────────────────────────────────────────────────────────────┘
                               │
                               ▼
┌─────────────────────────────────────────────────────────────────┐
│  STAGE 5: Post-process clusters                                │
│           - Sort takes by timestamp                            │
│           - Flag partial takes (short duration)                │
│           - Identify "best take" candidates                    │
└─────────────────────────────────────────────────────────────────┘
                               │
                               ▼
┌─────────────────────────────────────────────────────────────────┐
│  OUTPUT: Organized take groups with metadata                   │
│  {                                                              │
│    "line_1": [take_1, take_2, take_3],                         │
│    "line_2": [take_1, take_2],                                 │
│  }                                                              │
└─────────────────────────────────────────────────────────────────┘
```

### Clustering Implementation

```python
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_distances

# Compute distance matrix (distance = 1 - similarity)
distance_matrix = cosine_distances(embeddings)

# Cluster with distance threshold
# threshold=0.25 means similarity > 0.75 will cluster together
clustering = AgglomerativeClustering(
    n_clusters=None,
    distance_threshold=0.25,
    metric='precomputed',
    linkage='average'
)
labels = clustering.fit_predict(distance_matrix)
```

### Similarity Thresholds

| Threshold | Meaning |
|-----------|---------|
| 0.95+ | Near-exact duplicates (same words, minor punctuation) |
| 0.85-0.95 | Minor variations (word substitutions, filler words) |
| 0.70-0.85 | Paraphrases (same meaning, different wording) |

---

## 8. CEP/UXP Panel Development

### Bolt CEP Setup

```bash
# Create new project
yarn create bolt-cep

# Select: React + TypeScript
# Target: Premiere Pro (PPRO)
```

### Project Structure

```
splice-plugin/
├── src/
│   ├── js/
│   │   └── main/
│   │       ├── components/     # React components
│   │       ├── hooks/          # Custom hooks
│   │       ├── services/       # API services
│   │       └── main.tsx        # Entry point
│   └── jsx/
│       ├── index.ts            # ExtendScript entry
│       ├── timeline.ts         # Timeline operations
│       └── clips.ts            # Clip operations
├── cep.config.ts               # Panel configuration
└── package.json
```

### Drag and Drop Implementation

**From OS to Panel:**
```javascript
dropzone.addEventListener('drop', (e) => {
    e.preventDefault();
    const files = e.dataTransfer.files;
    for (let file of files) {
        console.log('File path:', file.path);
    }
});
```

**From Panel to Premiere:**
```javascript
element.addEventListener('dragstart', (event) => {
    event.dataTransfer.setData('com.adobe.cep.dnd.file.0', '/path/to/file.mp4');
});
```

**Note:** Dragging FROM Premiere TO panel is NOT supported.

### Type-Safe ExtendScript Communication (Bolt CEP)

```typescript
// Panel side (React/TypeScript)
import { evalTS } from '../lib/utils/bolt';

const result = await evalTS('getSequenceInfo', sequenceId);

// ExtendScript side (src/jsx/index.ts)
export function getSequenceInfo(sequenceId: string) {
    const seq = app.project.sequences[sequenceId];
    return JSON.stringify({ name: seq.name, duration: seq.end });
}
```

### Debug Mode Setup

**macOS:**
```bash
defaults write com.adobe.CSXS.12 PlayerDebugMode 1
```

**Windows (PowerShell as Admin):**
```powershell
reg add "HKCU\Software\Adobe\CSXS.12" /v PlayerDebugMode /t REG_SZ /d 1
```

**Debug URL:** `http://localhost:8088` (Chrome DevTools)

---

## 9. Plugin Architecture Patterns

### Recommended Architecture: Hybrid Local + Cloud

```
┌─────────────────────────────────────────────────────────────┐
│                    SPLICE PLUGIN                            │
├─────────────────────────────────────────────────────────────┤
│  LOCAL PROCESSING                                           │
│  ├── UI/Panel (React + TypeScript)                         │
│  ├── File management and caching                           │
│  ├── Premiere Pro API integration (ExtendScript)           │
│  ├── FFmpeg silence pre-detection                          │
│  └── Embedding generation (optional local model)           │
├─────────────────────────────────────────────────────────────┤
│  CLOUD PROCESSING                                           │
│  ├── Voice isolation (ElevenLabs API)                      │
│  ├── Transcription (Deepgram API)                          │
│  ├── Take detection (GPT-4o-mini API)                      │
│  └── Batch processing jobs                                  │
└─────────────────────────────────────────────────────────────┘
```

### Processing Pipeline

```
USER DROPS CLIP(S)
       │
       ▼
┌──────────────────┐
│ 1. Extract Audio │ (FFmpeg, local)
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 2. Voice Isolate │ (ElevenLabs API)
│    [Optional]    │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 3. Transcribe    │ (Deepgram API)
│   + Timestamps   │
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 4. Detect Takes  │ (Embeddings + LLM)
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 5. Detect Silence│ (Silero VAD / FFmpeg)
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 6. Organize &    │
│   Color Code     │ (ExtendScript)
└────────┬─────────┘
         │
         ▼
┌──────────────────┐
│ 7. Import to     │
│   Timeline       │ (ExtendScript)
└──────────────────┘
```

### Error Handling

```typescript
async function processWithRetry(operation, maxRetries = 3) {
    for (let attempt = 0; attempt < maxRetries; attempt++) {
        try {
            return await operation();
        } catch (error) {
            if (attempt === maxRetries - 1) throw error;
            await sleep(Math.pow(2, attempt) * 1000);  // Exponential backoff
        }
    }
}
```

### API Key Management (Secure)

```typescript
// Store keys in OS keychain (not in code or localStorage)
import keyring from 'keyring';

class SecureCredentialStore {
    static SERVICE_NAME = 'SPLICE_Plugin';

    static store(key: string, value: string) {
        keyring.set_password(this.SERVICE_NAME, key, value);
    }

    static get(key: string): string | null {
        return keyring.get_password(this.SERVICE_NAME, key);
    }
}
```

---

## 10. Competitive Analysis

### Market Landscape

| Tool | Focus | Price | Key Strength | Key Weakness |
|------|-------|-------|--------------|--------------|
| **AutoPod** | Podcast multicam | $29/mo | Multi-cam switching | Audio-only, no take detection |
| **Descript** | Transcript editing | $12-24/mo | Edit video like text | Performance issues, bugs |
| **CapCut** | Social content | Free+ | Consumer-friendly | Not for professionals |
| **TimeBolt** | Silence removal | One-time | Fast processing | No take detection |
| **Gling AI** | YouTube creators | $10-15/mo | Filler word removal | Over-aggressive cutting |

### Gap Analysis - SPLICE Differentiators

| Feature | AutoPod | Descript | TimeBolt | Gling | **SPLICE** |
|---------|---------|----------|----------|-------|------------|
| Take Detection | ❌ | ❌ | ❌ | ❌ | ✅ |
| Color Coding by Take | ❌ | ❌ | ❌ | ❌ | ✅ |
| Voice Isolation | ❌ | ✅ | ❌ | ❌ | ✅ |
| Silence Removal | ✅ | ✅ | ✅ | ✅ | ✅ |
| Premiere Integration | ✅ | ❌ | ✅ | ❌ | ✅ |
| Word-Level Timestamps | ❌ | ✅ | ❌ | ✅ | ✅ |
| Editor Final Control | ❌ | ✅ | ❌ | ❌ | ✅ |

### Market Gaps (Opportunities)

1. **Take detection** - No competitor offers intelligent take grouping
2. **Voice isolation integration** - Most rely on raw audio for analysis
3. **Color-coded organization** - Unique visual workflow for editors
4. **Editor control** - Organize but let editor make final selection

---

## 11. Strategic Recommendations

### Phase 1: MVP (4-6 weeks)

**Features:**
- Drag-and-drop clip import to panel
- Basic transcription with word-level timestamps (Deepgram)
- Simple take detection (embedding similarity clustering)
- Color coding clips by take number
- Basic silence detection and removal
- Timeline import with organized bins

**Tech Stack:**
- CEP + Bolt CEP (React + TypeScript)
- Deepgram API for transcription
- sentence-transformers for embeddings
- GPT-4o-mini for take analysis confirmation
- ExtendScript for Premiere integration

### Phase 2: Enhanced (4 weeks)

**Features:**
- Voice isolation option (ElevenLabs)
- Advanced take grouping with LLM analysis
- "Best take" suggestions based on audio clarity
- Batch processing multiple clips
- Progress tracking UI
- Settings/preferences panel

### Phase 3: Polish (4 weeks)

**Features:**
- Local LLM option (Ollama) for privacy
- Custom color schemes
- Export presets
- Undo/history for applied changes
- Keyboard shortcuts
- Tutorial/onboarding flow

### Pricing Strategy

**Subscription Model (Recommended):**
- Starter: $19/mo (10 hours processing)
- Pro: $39/mo (40 hours processing)
- Enterprise: Contact for volume

**Alternative: Credit-Based**
- $0.05 per minute of video processed

### Distribution

1. **Direct (Initial):** ZXP via website download
2. **Adobe Exchange (Later):** After v1.0 stability
3. **Enterprise:** Custom licensing for studios

---

## 12. Technology Stack Summary

### Frontend (Panel UI)
```
Framework: React 18+ with TypeScript
Build Tool: Vite via Bolt CEP
Styling: Tailwind CSS or Adobe Spectrum
State Management: Zustand or React Context
```

### Backend (Processing)
```
Audio Extraction: FFmpeg (bundled)
Voice Isolation: ElevenLabs API
Transcription: Deepgram Nova-3
Embeddings: sentence-transformers (all-MiniLM-L6-v2)
Take Analysis: GPT-4o-mini
Silence Detection: Silero VAD
```

### Premiere Integration
```
Panel Platform: CEP (future: UXP)
Scripting: ExtendScript via evalTS()
Communication: JSON serialization
```

### Packaging
```
Format: ZXP (signed)
Tool: Bolt CEP `yarn zxp`
Distribution: Direct download → Adobe Exchange
```

---

## Appendix A: API Cost Estimates

### Per 1-Hour Video

| Service | Calculation | Cost |
|---------|-------------|------|
| ElevenLabs Voice Isolation | 60 min × $0.30/min | ~$18.00 |
| Deepgram Transcription | 60 min × $0.0043/min | ~$0.26 |
| GPT-4o-mini Analysis | ~15K tokens | ~$0.01 |
| **Total with Voice Isolation** | | **~$18.27** |
| **Total without Voice Isolation** | | **~$0.27** |

### Cost Optimization
- Voice isolation often not needed for clean studio audio
- Local embedding generation eliminates LLM calls for initial filtering
- Batch API discounts available (50% off with Claude Batch API)

---

## Appendix B: File Structure

```
splice-plugin/
├── src/
│   ├── js/
│   │   └── main/
│   │       ├── components/
│   │       │   ├── DropZone.tsx
│   │       │   ├── ClipList.tsx
│   │       │   ├── TakeGroups.tsx
│   │       │   ├── ProgressBar.tsx
│   │       │   └── Settings.tsx
│   │       ├── hooks/
│   │       │   ├── useProcessing.ts
│   │       │   └── usePremiere.ts
│   │       ├── services/
│   │       │   ├── elevenlabs.ts
│   │       │   ├── deepgram.ts
│   │       │   ├── openai.ts
│   │       │   └── silenceDetection.ts
│   │       ├── utils/
│   │       │   ├── embeddings.ts
│   │       │   └── clustering.ts
│   │       └── main.tsx
│   └── jsx/
│       ├── index.ts
│       ├── timeline.ts
│       ├── clips.ts
│       └── markers.ts
├── cep.config.ts
├── package.json
└── README.md
```

---

## Appendix C: Key Resources

### Official Documentation
- [Adobe Premiere Pro Developer Portal](https://developer.adobe.com/premiere-pro/)
- [Premiere Pro Scripting Guide](https://ppro-scripting.docsforadobe.dev/)
- [Bolt CEP Framework](https://github.com/hyperbrew/bolt-cep)
- [Adobe CEP Resources](https://github.com/Adobe-CEP/CEP-Resources)

### API Documentation
- [ElevenLabs API](https://elevenlabs.io/docs/api-reference)
- [Deepgram API](https://developers.deepgram.com/docs)
- [OpenAI API](https://platform.openai.com/docs)
- [Silero VAD](https://github.com/snakers4/silero-vad)

### Tools
- [sentence-transformers](https://www.sbert.net/)
- [scikit-learn clustering](https://scikit-learn.org/stable/modules/clustering.html)
- [FFmpeg](https://ffmpeg.org/documentation.html)

---

**Document prepared by:** 10 parallel research agents
**Next step:** Architecture planning and BUILD.md creation
**Last updated:** December 16, 2025
