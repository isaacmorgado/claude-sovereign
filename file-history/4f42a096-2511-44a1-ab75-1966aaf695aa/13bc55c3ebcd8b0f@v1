# SPLICE - Architecture Documentation

**Version:** 1.0.0
**Date:** December 16, 2025
**Status:** Production-Ready Architecture

---

## Table of Contents

1. [System Architecture Overview](#1-system-architecture-overview)
2. [Technical Stack](#2-technical-stack)
3. [Core Modules](#3-core-modules)
4. [Data Models](#4-data-models)
5. [Processing Pipeline](#5-processing-pipeline)
6. [API Integration Patterns](#6-api-integration-patterns)
7. [Premiere Pro Integration](#7-premiere-pro-integration)
8. [UI/UX Design](#8-uiux-design)
9. [Development Phases](#9-development-phases)
10. [File Structure](#10-file-structure)
11. [Build and Distribution](#11-build-and-distribution)

---

## 1. System Architecture Overview

### High-Level Component Diagram

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         SPLICE PLUGIN ARCHITECTURE                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                      PREMIERE PRO HOST                               │    │
│  │  ┌─────────────────┐    ┌──────────────────────────────────────┐   │    │
│  │  │   Project       │    │          SPLICE CEP PANEL            │   │    │
│  │  │   - Bins        │◄──►│  ┌────────────────────────────────┐  │   │    │
│  │  │   - Clips       │    │  │       React + TypeScript       │  │   │    │
│  │  │   - Sequences   │    │  │  ┌──────────┐ ┌─────────────┐  │  │   │    │
│  │  └─────────────────┘    │  │  │ DropZone │ │ TakeGroups  │  │  │   │    │
│  │                         │  │  └──────────┘ └─────────────┘  │  │   │    │
│  │  ┌─────────────────┐    │  │  ┌──────────┐ ┌─────────────┐  │  │   │    │
│  │  │   Timeline      │    │  │  │ Progress │ │  Settings   │  │  │   │    │
│  │  │   - Tracks      │◄──►│  │  └──────────┘ └─────────────┘  │  │   │    │
│  │  │   - Markers     │    │  └────────────────────────────────┘  │   │    │
│  │  └─────────────────┘    │               │                       │   │    │
│  │                         │               │ evalTS()               │   │    │
│  │  ┌─────────────────┐    │               ▼                       │   │    │
│  │  │  ExtendScript   │◄───│  ┌────────────────────────────────┐  │   │    │
│  │  │  - timeline.ts  │    │  │      ExtendScript Bridge       │  │   │    │
│  │  │  - clips.ts     │    │  │  - Timeline manipulation       │  │   │    │
│  │  │  - markers.ts   │    │  │  - Color coding                │  │   │    │
│  │  └─────────────────┘    │  │  - Bin organization            │  │   │    │
│  │                         │  └────────────────────────────────┘  │   │    │
│  │                         └──────────────────────────────────────┘   │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                        │                                     │
│                                        │ HTTP/HTTPS                          │
│                                        ▼                                     │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                        PROCESSING LAYER                              │    │
│  │                                                                      │    │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌────────────┐ │    │
│  │  │   FFmpeg    │  │ ElevenLabs  │  │  Deepgram   │  │ GPT-4o-mini│ │    │
│  │  │   (Local)   │  │    API      │  │    API      │  │    API     │ │    │
│  │  │             │  │             │  │             │  │            │ │    │
│  │  │ - Extract   │  │ - Voice     │  │ - Transcribe│  │ - Analyze  │ │    │
│  │  │   Audio     │  │   Isolation │  │ - Timestamps│  │   Takes    │ │    │
│  │  └─────────────┘  └─────────────┘  └─────────────┘  └────────────┘ │    │
│  │                                                                      │    │
│  │  ┌─────────────┐  ┌─────────────────────────────────────────────┐  │    │
│  │  │ Silero VAD  │  │           sentence-transformers              │  │    │
│  │  │   (Local)   │  │                 (Local)                      │  │    │
│  │  │             │  │                                              │  │    │
│  │  │ - Silence   │  │ - Generate embeddings (all-MiniLM-L6-v2)    │  │    │
│  │  │   Detection │  │ - Cosine similarity clustering              │  │    │
│  │  └─────────────┘  └─────────────────────────────────────────────┘  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Data Flow

```
┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐
│  User    │     │   Extract    │     │    Voice      │     │  Transcribe  │
│  Drops   │────►│   Audio      │────►│   Isolation   │────►│  + Timestamp │
│  Clips   │     │   (FFmpeg)   │     │  (ElevenLabs) │     │  (Deepgram)  │
└──────────┘     └──────────────┘     └───────────────┘     └──────────────┘
                                                                    │
                                                                    ▼
┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐
│ Timeline │     │   Organize   │     │    Detect     │     │   Detect     │
│  Import  │◄────│   & Color    │◄────│    Silence    │◄────│   Takes      │
│          │     │   Code       │     │  (Silero VAD) │     │  (Embed+LLM) │
└──────────┘     └──────────────┘     └───────────────┘     └──────────────┘
```

### Technology Decisions

| Component | Technology | Rationale |
|-----------|------------|-----------|
| Plugin Platform | CEP | Production-ready, full API access, UXP still in beta |
| UI Framework | React + TypeScript | Best Bolt CEP support, type safety |
| Build Tool | Vite | 10-100x faster than webpack |
| Voice Isolation | ElevenLabs | Best quality/price, 500MB limit |
| Transcription | Deepgram Nova-3 | Native word timestamps, diarization included |
| Take Detection | Embeddings + GPT-4o-mini | Hybrid approach: fast filtering + LLM analysis |
| Silence Detection | Silero VAD | ML-based, 95%+ accuracy, local processing |
| Premiere API | ExtendScript | Only option for timeline manipulation |

---

## 2. Technical Stack

### Frontend (CEP Panel)

```yaml
Framework: React 18.2+
Language: TypeScript 5.0+
Build Tool: Vite 5.0+ (via Bolt CEP)
UI Components: Adobe Spectrum Web Components
State Management: Zustand (or React Context)
Styling: Tailwind CSS + CSS Modules
```

### Backend (Processing Services)

```yaml
Audio Extraction: FFmpeg 6.0+ (bundled binary)
Voice Isolation: ElevenLabs API v1
Transcription: Deepgram Nova-3 API
Embeddings: sentence-transformers (all-MiniLM-L6-v2)
Clustering: scikit-learn AgglomerativeClustering
LLM Analysis: OpenAI GPT-4o-mini API
Silence Detection: Silero VAD 4.0+
```

### Premiere Integration

```yaml
Panel Platform: CEP 12
Scripting: ExtendScript (ECMAScript 3)
Communication: evalTS() via Bolt CEP
Supported Premiere: 22.0+ (2022 and later)
```

### Development Tools

```yaml
Package Manager: pnpm or yarn
Node.js: 18.0+ (LTS)
IDE: VS Code
Debugging: Chrome DevTools (localhost:8088)
ExtendScript Debug: Adobe ExtendScript Debugger extension
```

---

## 3. Core Modules

### 3.1 DropZone/Import Module

```typescript
// src/js/main/modules/import/DropZone.ts

interface ImportModule {
  // Events
  onFilesDropped: (files: File[]) => void;
  onClipsSelected: (clips: ClipInfo[]) => void;

  // Methods
  validateFiles(files: File[]): ValidationResult;
  extractClipInfo(files: File[]): Promise<ClipInfo[]>;
  getSelectedClips(): Promise<ClipInfo[]>;
}

interface ValidationResult {
  valid: boolean;
  errors: string[];
  warnings: string[];
  supportedFormats: string[];
}
```

### 3.2 Audio Extraction Module

```typescript
// src/js/main/modules/audio/AudioExtractor.ts

interface AudioExtractorModule {
  extractAudio(clipPath: string, options: ExtractionOptions): Promise<AudioBuffer>;
  getAudioInfo(clipPath: string): Promise<AudioInfo>;
  splitAudio(buffer: AudioBuffer, chunks: number): AudioBuffer[];
}

interface ExtractionOptions {
  format: 'wav' | 'mp3' | 'pcm';
  sampleRate: 16000 | 44100 | 48000;
  channels: 1 | 2;
  bitDepth?: 16 | 24 | 32;
}

interface AudioInfo {
  duration: number;
  sampleRate: number;
  channels: number;
  format: string;
  bitRate: number;
}
```

### 3.3 Voice Isolation Module

```typescript
// src/js/main/modules/isolation/VoiceIsolator.ts

interface VoiceIsolationModule {
  isolate(audioBuffer: ArrayBuffer): Promise<ArrayBuffer>;
  isAvailable(): boolean;
  estimateCost(durationMinutes: number): number;
}

interface IsolationConfig {
  apiKey: string;
  outputFormat: 'pcm_s16le_16' | 'mp3' | 'wav';
  timeout: number;
}
```

### 3.4 Transcription Module

```typescript
// src/js/main/modules/transcription/Transcriber.ts

interface TranscriptionModule {
  transcribe(audioBuffer: ArrayBuffer): Promise<Transcript>;
  transcribeWithDiarization(audioBuffer: ArrayBuffer): Promise<DiarizedTranscript>;
  estimateCost(durationMinutes: number): number;
}

interface TranscriptWord {
  text: string;
  start: number;      // seconds
  end: number;        // seconds
  confidence: number; // 0-1
  speaker?: string;   // "Speaker 1", "Speaker 2"
}

interface Transcript {
  text: string;
  words: TranscriptWord[];
  duration: number;
  language: string;
}
```

### 3.5 Take Detection Module

```typescript
// src/js/main/modules/takes/TakeDetector.ts

interface TakeDetectionModule {
  detectTakes(transcript: Transcript): Promise<TakeGroup[]>;
  generateEmbeddings(sentences: string[]): Promise<number[][]>;
  clusterBySimilarity(embeddings: number[][], threshold: number): number[];
  analyzeTakesWithLLM(candidates: TakeCandidate[]): Promise<TakeAnalysis>;
}

interface TakeCandidate {
  sentences: string[];
  timestamps: TimeRange[];
  similarityScore: number;
}

interface TakeAnalysis {
  groups: TakeGroup[];
  bestTakes: Record<string, number>; // groupId -> takeIndex
  confidence: number;
}
```

### 3.6 Silence Detection Module

```typescript
// src/js/main/modules/silence/SilenceDetector.ts

interface SilenceDetectionModule {
  detectSilence(audioBuffer: ArrayBuffer): Promise<SilentSection[]>;
  detectFromTranscript(words: TranscriptWord[]): SilentSection[];
  filterNaturalPauses(sections: SilentSection[]): SilentSection[];
}

interface SilenceConfig {
  minDuration: number;      // seconds (default: 1.5)
  threshold: number;        // dB (default: -40)
  preservePauseDuration: number; // seconds (default: 0.8)
}
```

### 3.7 Timeline Integration Module

```typescript
// src/js/main/modules/premiere/TimelineIntegrator.ts

interface TimelineModule {
  // Bin operations
  createBin(name: string, parentPath?: string): Promise<string>;
  moveToBin(clipId: string, binPath: string): Promise<void>;

  // Color coding
  setClipColor(clipId: string, colorIndex: number): Promise<void>;

  // Timeline operations
  insertClip(clipId: string, trackIndex: number, time: number): Promise<void>;
  setInOutPoints(clipId: string, inPoint: number, outPoint: number): Promise<void>;

  // Markers
  createMarker(time: number, name: string, color: number): Promise<void>;
}

// Color mapping for takes
const TAKE_COLORS = {
  1: 5,   // Rose
  2: 3,   // Cerulean
  3: 6,   // Mango
  4: 7,   // Purple
  5: 4,   // Forest
  6: 9,   // Lavender
  7: 12,  // Orange
  8: 14,  // Blue
} as const;
```

### 3.8 Settings Module

```typescript
// src/js/main/modules/settings/SettingsManager.ts

interface SettingsModule {
  get<T>(key: string): T | undefined;
  set<T>(key: string, value: T): void;
  getAll(): Settings;
  reset(): void;
  export(): string;
  import(json: string): void;
}

interface Settings {
  // Voice Isolation
  useVoiceIsolation: boolean;
  audioOutputMode: 'original' | 'isolated' | 'none';

  // Transcription
  transcriptionProvider: 'deepgram' | 'assemblyai' | 'whisper';
  language: string;

  // Take Detection
  similarityThreshold: number;  // 0.70-0.95
  useLLMAnalysis: boolean;

  // Silence Detection
  minSilenceDuration: number;   // seconds
  silenceThreshold: number;     // dB
  preserveNaturalPauses: boolean;

  // Timeline
  autoColorCode: boolean;
  createBins: boolean;
  addMarkers: boolean;

  // API Keys
  elevenLabsApiKey: string;
  deepgramApiKey: string;
  openaiApiKey: string;
}
```

---

## 4. Data Models

### Core Interfaces

```typescript
// src/js/main/types/models.ts

// === CLIP MODELS ===

interface ClipInfo {
  id: string;
  name: string;
  path: string;
  duration: number;        // seconds
  inPoint: number;         // source in point
  outPoint: number;        // source out point
  hasAudio: boolean;
  hasVideo: boolean;
  audioChannels: number;
  frameRate: number;
  resolution: { width: number; height: number };
}

// === TAKE MODELS ===

interface Take {
  id: string;
  clipId: string;
  takeNumber: number;
  text: string;
  startTime: number;       // seconds in source
  endTime: number;
  duration: number;
  confidence: number;      // 0-1
  isPartial: boolean;      // incomplete take
  isBest: boolean;         // suggested best take
  audioClarity: number;    // 0-1 (if analyzed)
}

interface TakeGroup {
  id: string;
  canonicalText: string;   // reference text for this line
  takes: Take[];
  similarityType: 'exact' | 'minor_variation' | 'paraphrase';
  suggestedBestTakeId: string | null;
}

// === TRANSCRIPT MODELS ===

interface Word {
  text: string;
  start: number;           // seconds
  end: number;             // seconds
  confidence: number;
  speaker?: string;
  punctuatedText?: string;
}

interface Sentence {
  text: string;
  words: Word[];
  start: number;
  end: number;
  speaker?: string;
}

interface Transcript {
  id: string;
  clipId: string;
  fullText: string;
  words: Word[];
  sentences: Sentence[];
  speakers: string[];
  language: string;
  duration: number;
}

// === SILENCE MODELS ===

interface SilentSection {
  id: string;
  startTime: number;
  endTime: number;
  duration: number;
  type: 'dead_space' | 'natural_pause' | 'take_boundary';
  shouldRemove: boolean;
  afterWord?: string;
  beforeWord?: string;
}

// === PROCESSING MODELS ===

interface ProcessingJob {
  id: string;
  clipId: string;
  status: JobStatus;
  progress: number;        // 0-100
  currentStep: ProcessingStep;
  startedAt: Date;
  completedAt?: Date;
  error?: ProcessingError;
  result?: ProcessingResult;
}

type JobStatus = 'pending' | 'processing' | 'completed' | 'failed' | 'cancelled';

type ProcessingStep =
  | 'extracting_audio'
  | 'isolating_voice'
  | 'transcribing'
  | 'detecting_takes'
  | 'detecting_silence'
  | 'organizing'
  | 'complete';

interface ProcessingError {
  code: string;
  message: string;
  step: ProcessingStep;
  retryable: boolean;
}

interface ProcessingResult {
  clipId: string;
  transcript: Transcript;
  takeGroups: TakeGroup[];
  silentSections: SilentSection[];
  processingTime: number;  // ms
  apiCosts: {
    voiceIsolation: number;
    transcription: number;
    llmAnalysis: number;
    total: number;
  };
}

// === PREMIERE MODELS ===

interface PremiereClip {
  projectItemId: string;
  name: string;
  binPath: string;
  colorLabel: number;
  inPoint: number;
  outPoint: number;
}

interface PremiereTimeline {
  sequenceId: string;
  name: string;
  duration: number;
  videoTracks: number;
  audioTracks: number;
}
```

---

## 5. Processing Pipeline

### Main Processing Flow

```typescript
// src/js/main/services/ProcessingPipeline.ts

class ProcessingPipeline {
  private audioExtractor: AudioExtractorModule;
  private voiceIsolator: VoiceIsolationModule;
  private transcriber: TranscriptionModule;
  private takeDetector: TakeDetectionModule;
  private silenceDetector: SilenceDetectionModule;
  private timelineIntegrator: TimelineModule;

  async processClip(clip: ClipInfo, options: ProcessingOptions): Promise<ProcessingResult> {
    const job = this.createJob(clip);

    try {
      // Step 1: Extract audio from video
      this.updateProgress(job, 'extracting_audio', 10);
      const audioBuffer = await this.audioExtractor.extractAudio(clip.path, {
        format: 'wav',
        sampleRate: 16000,
        channels: 1
      });

      // Step 2: Voice isolation (optional)
      let analysisAudio = audioBuffer;
      let outputAudio = audioBuffer;

      if (options.useVoiceIsolation) {
        this.updateProgress(job, 'isolating_voice', 25);
        const isolatedAudio = await this.voiceIsolator.isolate(audioBuffer);
        analysisAudio = isolatedAudio;

        if (options.audioOutputMode === 'isolated') {
          outputAudio = isolatedAudio;
        }
      }

      // Step 3: Transcription with word-level timestamps
      this.updateProgress(job, 'transcribing', 40);
      const transcript = await this.transcriber.transcribe(analysisAudio);

      // Step 4: Take detection
      this.updateProgress(job, 'detecting_takes', 60);
      const takeGroups = await this.takeDetector.detectTakes(transcript);

      // Step 5: Silence detection
      this.updateProgress(job, 'detecting_silence', 75);
      const silentSections = await this.silenceDetector.detectFromTranscript(transcript.words);
      const filteredSilence = this.silenceDetector.filterNaturalPauses(silentSections);

      // Step 6: Organize in Premiere
      this.updateProgress(job, 'organizing', 90);
      await this.organizeInPremiere(clip, takeGroups, filteredSilence, options);

      this.updateProgress(job, 'complete', 100);

      return {
        clipId: clip.id,
        transcript,
        takeGroups,
        silentSections: filteredSilence,
        processingTime: Date.now() - job.startedAt.getTime(),
        apiCosts: this.calculateCosts(job)
      };

    } catch (error) {
      job.status = 'failed';
      job.error = this.createError(error);
      throw error;
    }
  }

  private async organizeInPremiere(
    clip: ClipInfo,
    takeGroups: TakeGroup[],
    silentSections: SilentSection[],
    options: ProcessingOptions
  ): Promise<void> {

    // Create bin structure
    if (options.createBins) {
      const mainBin = await this.timelineIntegrator.createBin('SPLICE Takes');

      for (const group of takeGroups) {
        const groupBin = await this.timelineIntegrator.createBin(
          `Line ${group.id}`,
          mainBin
        );

        for (const take of group.takes) {
          // Create subclip with in/out points
          // Move to appropriate bin
          // Set color based on take number
          await this.timelineIntegrator.setClipColor(
            take.clipId,
            TAKE_COLORS[take.takeNumber] || 0
          );
        }
      }
    }

    // Add markers for take boundaries
    if (options.addMarkers) {
      for (const group of takeGroups) {
        for (const take of group.takes) {
          await this.timelineIntegrator.createMarker(
            take.startTime,
            `Take ${take.takeNumber}: ${group.canonicalText.substring(0, 30)}...`,
            take.isBest ? 1 : 0  // Red for best, green for others
          );
        }
      }
    }
  }
}
```

### Take Detection Algorithm

```typescript
// src/js/main/services/TakeDetectionService.ts

class TakeDetectionService implements TakeDetectionModule {
  private embeddingModel: SentenceTransformer;
  private openaiClient: OpenAI;

  async detectTakes(transcript: Transcript): Promise<TakeGroup[]> {
    // 1. Segment transcript into sentences
    const sentences = this.segmentIntoSentences(transcript);

    // 2. Generate embeddings for all sentences
    const embeddings = await this.generateEmbeddings(
      sentences.map(s => s.text)
    );

    // 3. Compute similarity matrix
    const similarityMatrix = this.computeCosineSimilarity(embeddings);

    // 4. Find candidate pairs (similarity > threshold)
    const candidates = this.findCandidatePairs(
      sentences,
      similarityMatrix,
      0.80  // threshold
    );

    // 5. Cluster into groups
    const clusters = this.clusterCandidates(candidates);

    // 6. Analyze with LLM for confirmation and best-take selection
    const analysis = await this.analyzeTakesWithLLM(clusters);

    // 7. Build TakeGroup objects
    return this.buildTakeGroups(sentences, clusters, analysis);
  }

  private computeCosineSimilarity(embeddings: number[][]): number[][] {
    const n = embeddings.length;
    const matrix: number[][] = Array(n).fill(null).map(() => Array(n).fill(0));

    for (let i = 0; i < n; i++) {
      for (let j = i; j < n; j++) {
        const sim = this.cosineSim(embeddings[i], embeddings[j]);
        matrix[i][j] = sim;
        matrix[j][i] = sim;
      }
    }

    return matrix;
  }

  private cosineSim(a: number[], b: number[]): number {
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;

    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }

    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }

  private async analyzeTakesWithLLM(clusters: TakeCluster[]): Promise<LLMAnalysis> {
    const prompt = this.buildAnalysisPrompt(clusters);

    const response = await this.openaiClient.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: TAKE_ANALYSIS_SYSTEM_PROMPT },
        { role: 'user', content: prompt }
      ],
      response_format: { type: 'json_object' }
    });

    return JSON.parse(response.choices[0].message.content);
  }
}

const TAKE_ANALYSIS_SYSTEM_PROMPT = `
You are analyzing transcript segments to identify repeated takes.

For each group of similar sentences, determine:
1. Whether they represent the same intended dialogue (different takes)
2. The similarity type: exact, minor_variation, or paraphrase
3. Which take appears to be the "best" (most complete, clearest)

Return JSON with this structure:
{
  "groups": [
    {
      "id": "string",
      "isTakeGroup": boolean,
      "similarityType": "exact" | "minor_variation" | "paraphrase",
      "bestTakeIndex": number,
      "confidence": number
    }
  ]
}
`;
```

---

## 6. API Integration Patterns

### Rate Limiting and Retry Logic

```typescript
// src/js/main/utils/apiClient.ts

interface RetryConfig {
  maxRetries: number;
  baseDelay: number;
  maxDelay: number;
  retryableStatuses: number[];
}

const DEFAULT_RETRY_CONFIG: RetryConfig = {
  maxRetries: 3,
  baseDelay: 1000,
  maxDelay: 30000,
  retryableStatuses: [429, 500, 502, 503, 504]
};

async function fetchWithRetry<T>(
  url: string,
  options: RequestInit,
  config: RetryConfig = DEFAULT_RETRY_CONFIG
): Promise<T> {
  let lastError: Error;

  for (let attempt = 0; attempt <= config.maxRetries; attempt++) {
    try {
      const response = await fetch(url, options);

      if (!response.ok) {
        if (config.retryableStatuses.includes(response.status)) {
          throw new RetryableError(response.status, await response.text());
        }
        throw new ApiError(response.status, await response.text());
      }

      return await response.json();

    } catch (error) {
      lastError = error;

      if (error instanceof RetryableError && attempt < config.maxRetries) {
        const delay = Math.min(
          config.baseDelay * Math.pow(2, attempt),
          config.maxDelay
        );
        await sleep(delay);
        continue;
      }

      throw error;
    }
  }

  throw lastError;
}
```

### API Key Management

```typescript
// src/js/main/services/CredentialManager.ts

import keytar from 'keytar';

const SERVICE_NAME = 'SPLICE_Plugin';

class CredentialManager {
  private static instance: CredentialManager;
  private cache: Map<string, string> = new Map();

  static getInstance(): CredentialManager {
    if (!this.instance) {
      this.instance = new CredentialManager();
    }
    return this.instance;
  }

  async setApiKey(provider: ApiProvider, key: string): Promise<void> {
    await keytar.setPassword(SERVICE_NAME, provider, key);
    this.cache.set(provider, key);
  }

  async getApiKey(provider: ApiProvider): Promise<string | null> {
    if (this.cache.has(provider)) {
      return this.cache.get(provider)!;
    }

    const key = await keytar.getPassword(SERVICE_NAME, provider);
    if (key) {
      this.cache.set(provider, key);
    }
    return key;
  }

  async deleteApiKey(provider: ApiProvider): Promise<void> {
    await keytar.deletePassword(SERVICE_NAME, provider);
    this.cache.delete(provider);
  }

  async validateApiKey(provider: ApiProvider, key: string): Promise<boolean> {
    switch (provider) {
      case 'elevenlabs':
        return this.validateElevenLabsKey(key);
      case 'deepgram':
        return this.validateDeepgramKey(key);
      case 'openai':
        return this.validateOpenAIKey(key);
      default:
        return false;
    }
  }
}

type ApiProvider = 'elevenlabs' | 'deepgram' | 'openai';
```

### Cost Tracking

```typescript
// src/js/main/services/CostTracker.ts

interface CostRates {
  elevenlabs: {
    perMinute: number;  // $0.30 at Creator tier
  };
  deepgram: {
    perMinute: number;  // $0.0043
  };
  openai: {
    inputPer1M: number;   // $0.15
    outputPer1M: number;  // $0.60
  };
}

const DEFAULT_RATES: CostRates = {
  elevenlabs: { perMinute: 0.30 },
  deepgram: { perMinute: 0.0043 },
  openai: { inputPer1M: 0.15, outputPer1M: 0.60 }
};

class CostTracker {
  private rates: CostRates;
  private usage: UsageRecord = {
    elevenlabs: { minutes: 0, cost: 0 },
    deepgram: { minutes: 0, cost: 0 },
    openai: { inputTokens: 0, outputTokens: 0, cost: 0 }
  };

  estimateCost(durationMinutes: number, options: ProcessingOptions): CostEstimate {
    let total = 0;

    // Transcription (always needed)
    const transcriptionCost = durationMinutes * this.rates.deepgram.perMinute;
    total += transcriptionCost;

    // Voice isolation (optional)
    let isolationCost = 0;
    if (options.useVoiceIsolation) {
      isolationCost = durationMinutes * this.rates.elevenlabs.perMinute;
      total += isolationCost;
    }

    // LLM analysis (estimated based on transcript length)
    const estimatedTokens = durationMinutes * 500; // ~500 tokens per minute
    const llmCost = (estimatedTokens / 1_000_000) * this.rates.openai.inputPer1M +
                    (estimatedTokens / 4 / 1_000_000) * this.rates.openai.outputPer1M;
    total += llmCost;

    return {
      transcription: transcriptionCost,
      voiceIsolation: isolationCost,
      llmAnalysis: llmCost,
      total
    };
  }
}
```

---

## 7. Premiere Pro Integration

### ExtendScript Functions

```typescript
// src/jsx/index.ts

// === CLIP OPERATIONS ===

export function getSelectedClips(): string {
  const selection = app.project.activeSequence?.getSelection() || [];
  const clips = [];

  for (let i = 0; i < selection.length; i++) {
    clips.push({
      id: selection[i].projectItem.nodeId,
      name: selection[i].name,
      duration: selection[i].duration.seconds
    });
  }

  return JSON.stringify(clips);
}

export function setClipColor(clipId: string, colorIndex: number): string {
  const item = findProjectItemById(clipId);
  if (item) {
    item.setColorLabel(colorIndex);
    return JSON.stringify({ success: true });
  }
  return JSON.stringify({ success: false, error: 'Clip not found' });
}

export function setInOutPoints(clipId: string, inPoint: number, outPoint: number): string {
  const item = findProjectItemById(clipId);
  if (item) {
    item.setInPoint(inPoint, 4);  // 4 = update UI
    item.setOutPoint(outPoint, 4);
    return JSON.stringify({ success: true });
  }
  return JSON.stringify({ success: false, error: 'Clip not found' });
}

// === BIN OPERATIONS ===

export function createBin(name: string, parentPath?: string): string {
  const parent = parentPath
    ? findBinByPath(parentPath)
    : app.project.rootItem;

  if (parent) {
    const newBin = parent.createBin(name);
    return JSON.stringify({
      success: true,
      binId: newBin.nodeId,
      path: getBinPath(newBin)
    });
  }
  return JSON.stringify({ success: false, error: 'Parent bin not found' });
}

export function moveClipToBin(clipId: string, binPath: string): string {
  const clip = findProjectItemById(clipId);
  const bin = findBinByPath(binPath);

  if (clip && bin) {
    clip.moveBin(bin);
    return JSON.stringify({ success: true });
  }
  return JSON.stringify({ success: false, error: 'Clip or bin not found' });
}

// === TIMELINE OPERATIONS ===

export function insertClipToTimeline(
  clipId: string,
  trackIndex: number,
  timeSeconds: number
): string {
  const seq = app.project.activeSequence;
  const clip = findProjectItemById(clipId);

  if (!seq || !clip) {
    return JSON.stringify({ success: false, error: 'Sequence or clip not found' });
  }

  try {
    seq.videoTracks[trackIndex].insertClip(clip, timeSeconds);
    return JSON.stringify({ success: true });
  } catch (e) {
    return JSON.stringify({ success: false, error: e.toString() });
  }
}

// === MARKER OPERATIONS ===

export function createMarker(
  timeSeconds: number,
  name: string,
  colorIndex: number
): string {
  const seq = app.project.activeSequence;
  if (!seq) {
    return JSON.stringify({ success: false, error: 'No active sequence' });
  }

  const marker = seq.markers.createMarker(timeSeconds);
  marker.name = name;
  marker.setTypeAsComment();
  marker.setColorByIndex(colorIndex, 0);

  return JSON.stringify({ success: true, markerId: marker.guid });
}

// === HELPER FUNCTIONS ===

function findProjectItemById(nodeId: string): ProjectItem | null {
  return searchInBin(app.project.rootItem, nodeId);
}

function searchInBin(bin: ProjectItem, nodeId: string): ProjectItem | null {
  for (let i = 0; i < bin.children.numItems; i++) {
    const item = bin.children[i];
    if (item.nodeId === nodeId) {
      return item;
    }
    if (item.type === ProjectItemType.BIN) {
      const found = searchInBin(item, nodeId);
      if (found) return found;
    }
  }
  return null;
}

function findBinByPath(path: string): ProjectItem | null {
  const parts = path.split('/').filter(p => p);
  let current = app.project.rootItem;

  for (const part of parts) {
    let found = false;
    for (let i = 0; i < current.children.numItems; i++) {
      const child = current.children[i];
      if (child.name === part && child.type === ProjectItemType.BIN) {
        current = child;
        found = true;
        break;
      }
    }
    if (!found) return null;
  }

  return current;
}
```

### Color Coding System

```typescript
// src/js/main/constants/colors.ts

export const TAKE_COLOR_MAP = {
  1: { index: 5,  name: 'Rose',     hex: '#F77FBE' },
  2: { index: 3,  name: 'Cerulean', hex: '#00A0DC' },
  3: { index: 6,  name: 'Mango',    hex: '#F5A623' },
  4: { index: 7,  name: 'Purple',   hex: '#9B59B6' },
  5: { index: 4,  name: 'Forest',   hex: '#27AE60' },
  6: { index: 9,  name: 'Lavender', hex: '#B39DDB' },
  7: { index: 12, name: 'Orange',   hex: '#FF5722' },
  8: { index: 14, name: 'Blue',     hex: '#2196F3' },
} as const;

export const MARKER_COLORS = {
  BEST_TAKE: 1,    // Red
  REGULAR_TAKE: 4, // Green
  SILENCE: 3,      // Yellow
} as const;
```

### Bin Organization Structure

```
Project Panel
└── SPLICE Takes
    ├── Line 1 - "Hello everyone..."
    │   ├── Take 1 [Rose]
    │   ├── Take 2 [Cerulean] ★ Best
    │   └── Take 3 [Mango]
    ├── Line 2 - "Today we're going to..."
    │   ├── Take 1 [Rose] ★ Best
    │   └── Take 2 [Cerulean]
    └── Silence Removed
        ├── Gap 1 (1.5s)
        └── Gap 2 (2.3s)
```

---

## 8. UI/UX Design

### Panel Layout

```tsx
// src/js/main/components/Panel.tsx

import React from 'react';
import { DropZone } from './DropZone';
import { ClipList } from './ClipList';
import { TakeGroups } from './TakeGroups';
import { ProgressBar } from './ProgressBar';
import { SettingsPanel } from './SettingsPanel';

export function SplicePanel() {
  return (
    <sp-theme color="dark" scale="medium">
      <div className="splice-panel">
        {/* Header */}
        <header className="panel-header">
          <sp-heading size="M">SPLICE Auto-Cut</sp-heading>
          <sp-action-button quiet onClick={openSettings}>
            <sp-icon name="Settings" />
          </sp-action-button>
        </header>

        {/* Drop Zone / Clip Selection */}
        <section className="import-section">
          <DropZone onFilesDropped={handleFiles}>
            <sp-illustrated-message>
              <sp-icon name="Upload" />
              <sp-heading>Drop clips here</sp-heading>
              <sp-body>or click to select from project</sp-body>
            </sp-illustrated-message>
          </DropZone>
        </section>

        {/* Selected Clips */}
        <section className="clips-section">
          <ClipList
            clips={selectedClips}
            onRemove={removeClip}
          />
        </section>

        {/* Processing Options */}
        <section className="options-section">
          <sp-field-group>
            <sp-checkbox checked={useVoiceIsolation}>
              Use Voice Isolation
            </sp-checkbox>
            <sp-checkbox checked={detectTakes}>
              Detect Multiple Takes
            </sp-checkbox>
            <sp-checkbox checked={removeSilence}>
              Remove Dead Space
            </sp-checkbox>
          </sp-field-group>

          <sp-slider
            label="Silence Threshold"
            min={0.5}
            max={3.0}
            value={silenceThreshold}
            step={0.1}
          />
        </section>

        {/* Process Button */}
        <section className="action-section">
          <sp-button
            variant="cta"
            onClick={processClips}
            disabled={isProcessing || selectedClips.length === 0}
          >
            {isProcessing ? 'Processing...' : 'Process Clips'}
          </sp-button>
        </section>

        {/* Progress */}
        {isProcessing && (
          <section className="progress-section">
            <ProgressBar
              value={progress}
              label={currentStep}
            />
          </section>
        )}

        {/* Results */}
        {results && (
          <section className="results-section">
            <sp-tabs>
              <sp-tab label="Takes" panel="takes-panel" />
              <sp-tab label="Silence" panel="silence-panel" />
            </sp-tabs>

            <sp-tab-panel id="takes-panel">
              <TakeGroups groups={results.takeGroups} />
            </sp-tab-panel>

            <sp-tab-panel id="silence-panel">
              <SilenceList sections={results.silentSections} />
            </sp-tab-panel>
          </section>
        )}

        {/* Apply Actions */}
        {results && (
          <section className="apply-section">
            <sp-button-group>
              <sp-button onClick={applyColorCoding}>
                Apply Colors
              </sp-button>
              <sp-button onClick={createBins}>
                Create Bins
              </sp-button>
              <sp-button variant="cta" onClick={applyAll}>
                Apply All
              </sp-button>
            </sp-button-group>
          </section>
        )}

        {/* Footer */}
        <footer className="panel-footer">
          <sp-body size="XS">
            Est. cost: ${estimatedCost.toFixed(2)}
          </sp-body>
        </footer>
      </div>
    </sp-theme>
  );
}
```

### Settings Panel

```tsx
// src/js/main/components/SettingsPanel.tsx

export function SettingsPanel() {
  return (
    <sp-dialog>
      <sp-heading slot="heading">Settings</sp-heading>

      <div className="settings-content">
        {/* API Keys */}
        <sp-field-group>
          <sp-field-label>API Keys</sp-field-label>

          <sp-textfield
            label="ElevenLabs API Key"
            type="password"
            value={elevenLabsKey}
            onChange={setElevenLabsKey}
          />

          <sp-textfield
            label="Deepgram API Key"
            type="password"
            value={deepgramKey}
            onChange={setDeepgramKey}
          />

          <sp-textfield
            label="OpenAI API Key"
            type="password"
            value={openaiKey}
            onChange={setOpenaiKey}
          />
        </sp-field-group>

        {/* Voice Isolation Settings */}
        <sp-field-group>
          <sp-field-label>Voice Isolation</sp-field-label>

          <sp-dropdown label="Audio Output Mode">
            <sp-menu-item value="original" selected>
              Keep Original Audio
            </sp-menu-item>
            <sp-menu-item value="isolated">
              Use Isolated Voice
            </sp-menu-item>
            <sp-menu-item value="none">
              Skip Isolation (faster)
            </sp-menu-item>
          </sp-dropdown>
        </sp-field-group>

        {/* Take Detection Settings */}
        <sp-field-group>
          <sp-field-label>Take Detection</sp-field-label>

          <sp-slider
            label="Similarity Threshold"
            min={0.70}
            max={0.95}
            value={similarityThreshold}
            step={0.05}
          />

          <sp-checkbox checked={useLLMAnalysis}>
            Use LLM for Better Accuracy
          </sp-checkbox>
        </sp-field-group>

        {/* Silence Detection Settings */}
        <sp-field-group>
          <sp-field-label>Silence Detection</sp-field-label>

          <sp-slider
            label="Minimum Silence Duration (seconds)"
            min={0.5}
            max={3.0}
            value={minSilenceDuration}
            step={0.1}
          />

          <sp-checkbox checked={preserveNaturalPauses}>
            Preserve Natural Pauses
          </sp-checkbox>
        </sp-field-group>

        {/* Timeline Settings */}
        <sp-field-group>
          <sp-field-label>Timeline Integration</sp-field-label>

          <sp-checkbox checked={autoColorCode}>
            Auto Color Code Takes
          </sp-checkbox>

          <sp-checkbox checked={createBins}>
            Organize in Bins
          </sp-checkbox>

          <sp-checkbox checked={addMarkers}>
            Add Timeline Markers
          </sp-checkbox>
        </sp-field-group>
      </div>

      <sp-button-group slot="button">
        <sp-button variant="secondary" onClick={resetDefaults}>
          Reset Defaults
        </sp-button>
        <sp-button variant="cta" onClick={saveSettings}>
          Save
        </sp-button>
      </sp-button-group>
    </sp-dialog>
  );
}
```

---

## 9. Development Phases

### Phase 1: MVP (Weeks 1-6)

**Goals:**
- Working plugin with core functionality
- Basic take detection
- Silence removal
- Timeline integration

**Features:**
- [ ] CEP panel setup with Bolt CEP
- [ ] Drag-and-drop clip import
- [ ] Audio extraction (FFmpeg)
- [ ] Deepgram transcription integration
- [ ] Basic take detection (embeddings only)
- [ ] Silence detection (threshold-based)
- [ ] Color coding in Premiere
- [ ] Bin organization
- [ ] Basic settings panel

**Tech Debt:**
- Skip voice isolation
- Skip LLM analysis
- Minimal error handling
- No progress streaming

### Phase 2: Enhanced (Weeks 7-10)

**Goals:**
- Production-ready quality
- Full feature set
- Robust error handling

**Features:**
- [ ] ElevenLabs voice isolation
- [ ] GPT-4o-mini take analysis
- [ ] Advanced similarity clustering
- [ ] "Best take" suggestions
- [ ] Batch processing
- [ ] Progress streaming
- [ ] Cost estimation UI
- [ ] Full settings panel
- [ ] Error recovery

### Phase 3: Polish (Weeks 11-14)

**Goals:**
- Professional quality
- User delight
- Market readiness

**Features:**
- [ ] Local LLM option (Ollama)
- [ ] Custom color schemes
- [ ] Export/import settings
- [ ] Keyboard shortcuts
- [ ] Tutorial/onboarding
- [ ] Usage analytics
- [ ] Undo/redo for applied changes
- [ ] Performance optimization
- [ ] Comprehensive testing

---

## 10. File Structure

```
splice-plugin/
├── .vscode/
│   └── launch.json              # Debug configurations
├── .github/
│   └── workflows/
│       └── release.yml          # GitHub Actions release
├── bin/
│   ├── mac/
│   │   └── ffmpeg               # macOS FFmpeg binary
│   └── win/
│       └── ffmpeg.exe           # Windows FFmpeg binary
├── src/
│   ├── js/
│   │   └── main/
│   │       ├── components/
│   │       │   ├── DropZone.tsx
│   │       │   ├── ClipList.tsx
│   │       │   ├── TakeGroups.tsx
│   │       │   ├── SilenceList.tsx
│   │       │   ├── ProgressBar.tsx
│   │       │   ├── SettingsPanel.tsx
│   │       │   └── Panel.tsx
│   │       ├── hooks/
│   │       │   ├── useProcessing.ts
│   │       │   ├── usePremiere.ts
│   │       │   ├── useSettings.ts
│   │       │   └── useApi.ts
│   │       ├── services/
│   │       │   ├── ProcessingPipeline.ts
│   │       │   ├── AudioExtractor.ts
│   │       │   ├── VoiceIsolator.ts
│   │       │   ├── Transcriber.ts
│   │       │   ├── TakeDetector.ts
│   │       │   ├── SilenceDetector.ts
│   │       │   ├── CredentialManager.ts
│   │       │   └── CostTracker.ts
│   │       ├── modules/
│   │       │   ├── audio/
│   │       │   ├── takes/
│   │       │   ├── silence/
│   │       │   └── premiere/
│   │       ├── utils/
│   │       │   ├── apiClient.ts
│   │       │   ├── embeddings.ts
│   │       │   ├── clustering.ts
│   │       │   └── helpers.ts
│   │       ├── constants/
│   │       │   ├── colors.ts
│   │       │   └── config.ts
│   │       ├── types/
│   │       │   ├── models.ts
│   │       │   ├── api.ts
│   │       │   └── premiere.ts
│   │       ├── store/
│   │       │   └── index.ts     # Zustand store
│   │       ├── main.tsx         # Entry point
│   │       └── index.html
│   └── jsx/
│       ├── index.ts             # ExtendScript entry
│       ├── timeline.ts          # Timeline operations
│       ├── clips.ts             # Clip operations
│       ├── bins.ts              # Bin operations
│       ├── markers.ts           # Marker operations
│       └── helpers.ts           # Utility functions
├── public/
│   └── icons/
│       ├── icon-light.png
│       └── icon-dark.png
├── tests/
│   ├── unit/
│   │   ├── TakeDetector.test.ts
│   │   └── SilenceDetector.test.ts
│   └── integration/
│       └── Pipeline.test.ts
├── cep.config.ts                # Bolt CEP configuration
├── vite.config.ts               # Vite configuration
├── tsconfig.json                # TypeScript config
├── tsconfig.jsx.json            # ExtendScript TypeScript config
├── tailwind.config.js           # Tailwind CSS config
├── package.json
├── pnpm-lock.yaml
├── .gitignore
├── .env.example                 # Environment variables template
├── README.md
├── LICENSE
└── CHANGELOG.md
```

---

## 11. Build and Distribution

### Development Workflow

```bash
# Install dependencies
pnpm install

# Start development server with HMR
pnpm dev

# Build for production
pnpm build

# Create signed ZXP package
pnpm zxp

# Run tests
pnpm test

# Lint and type check
pnpm lint
pnpm typecheck
```

### CEP Configuration

```typescript
// cep.config.ts

import { CEPConfig } from 'bolt-cep';

const config: CEPConfig = {
  id: 'com.splice.autocut',
  displayName: 'SPLICE Auto-Cut',
  version: '1.0.0',

  hosts: [
    {
      name: 'PPRO',
      version: '[22.0,99.9]'
    }
  ],

  panels: {
    main: {
      type: 'Panel',
      name: 'SPLICE Auto-Cut',
      root: './src/js/main/index.html',
      script: './src/jsx/index.ts',
      size: {
        width: 350,
        height: 600
      },
      minSize: {
        width: 300,
        height: 400
      }
    }
  },

  lifecycle: {
    autoVisible: true,
    startOnEvents: ['applicationActivate']
  },

  cepVersion: '11.0',
  runtimeVersion: '9.0'
};

export default config;
```

### ZXP Signing

```bash
# Generate self-signed certificate (development)
ZXPSignCmd -selfSignedCert US "SPLICE" "SPLICE Auto-Cut" password ./cert.p12

# Sign the package
pnpm zxp  # Uses Bolt CEP's built-in signing
```

### Distribution Strategy

**Phase 1: Direct Distribution**
- Host ZXP on website
- Install via aescripts ZXP Installer or Anastasiy's Extension Manager
- Free trial + subscription model

**Phase 2: Adobe Exchange**
- Submit after v1.0 stability
- Requires Adobe review
- Broader discovery

**Phase 3: Enterprise**
- Custom licensing
- Volume discounts
- Priority support

### Environment Variables

```bash
# .env.example

# API Keys (stored securely, not in env)
# ELEVENLABS_API_KEY=
# DEEPGRAM_API_KEY=
# OPENAI_API_KEY=

# Development
DEBUG=true
LOG_LEVEL=info

# Feature Flags
ENABLE_VOICE_ISOLATION=true
ENABLE_LLM_ANALYSIS=true
ENABLE_LOCAL_LLM=false

# Pricing (for cost estimation)
ELEVENLABS_RATE_PER_MIN=0.30
DEEPGRAM_RATE_PER_MIN=0.0043
OPENAI_INPUT_RATE_PER_1M=0.15
OPENAI_OUTPUT_RATE_PER_1M=0.60
```

---

## Quick Start for Developers

```bash
# 1. Clone repository
git clone https://github.com/your-org/splice-plugin.git
cd splice-plugin

# 2. Install dependencies
pnpm install

# 3. Enable CEP debug mode
# macOS:
defaults write com.adobe.CSXS.12 PlayerDebugMode 1
# Windows (PowerShell as Admin):
# reg add "HKCU\Software\Adobe\CSXS.12" /v PlayerDebugMode /t REG_SZ /d 1

# 4. Start development
pnpm dev

# 5. Open Premiere Pro
# Window > Extensions > SPLICE Auto-Cut

# 6. Debug in Chrome
# Navigate to http://localhost:8088
```

---

**Architecture by:** Claude (Anthropic)
**Version:** 1.0.0
**Last Updated:** December 16, 2025
