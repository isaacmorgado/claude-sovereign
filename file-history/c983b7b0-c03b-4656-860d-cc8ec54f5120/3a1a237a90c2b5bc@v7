# Splice v3

Cloud-powered SaaS for Adobe Premiere Pro with AI-driven auto-cutting, take detection, and subscription billing.

## Monorepo Structure

```
/                         # Root
├── src/                  # Premiere Pro UXP Plugin
│   ├── components/
│   │   ├── panels/       # PluginApp (main container)
│   │   └── common/       # UI: Header, SelectionPanel, AIToolsPanel, AuthPanel, CuttingPanel
│   ├── types/            # TypeScript: plugin, api, premiere, audio, auto-cutting
│   ├── utils/
│   │   ├── api/          # premiere.ts, backend-client.ts, audio-processor.ts
│   │   ├── cutting/      # silence-detector.ts (gap detection, cut points)
│   │   └── helpers/      # formatters, validators, audio-utils
│   └── styles/           # Global CSS, Spectrum theming
│
├── backend/              # Node.js + Express API
│   └── src/
│       ├── routes/       # auth, billing, jobs, audio, usage
│       ├── controllers/  # Route handlers
│       ├── services/     # audio-service, take-detection-service
│       ├── workers/      # BullMQ: transcription, vocal-isolation, take-detection
│       ├── db/           # migrations/, repositories/
│       └── middleware/   # auth, rate-limit, usage-limit
│
├── dashboard/            # Next.js Web Dashboard
│   └── app/              # login, signup, dashboard (usage, billing, settings)
│
├── tests/                # Vitest unit + Playwright e2e
└── public/               # manifest.json, icons
```

## Tech Stack

| Layer     | Stack                                                             |
| --------- | ----------------------------------------------------------------- |
| Plugin    | Lit 3.2 + Spectrum Web Components + TypeScript                    |
| Backend   | Node.js + Express + PostgreSQL + BullMQ/Redis                     |
| Dashboard | Next.js + React                                                   |
| Audio     | Whisper/Deepgram (transcription) + Demucs/Dolby (vocal isolation) |
| Payments  | Stripe                                                            |

## Auto-Cutting Workflow

### User Flow

1. User selects clips in Premiere Pro timeline
2. Plugin extracts audio from selected clips
3. **Options:** Isolate vocals (optional), choose audio source for timeline
4. Backend transcribes audio (uses isolated vocals for accuracy if available)
5. LLM analyzes transcript to detect repeated takes (similar statements)
6. Cut silences (preserving natural pauses 150-500ms)
7. Color-code and label clips by take groups

### Audio Options (User-Selectable)

| Option         | Values              | Default  | Description                                |
| -------------- | ------------------- | -------- | ------------------------------------------ |
| Isolate Audio  | checkbox            | off      | Run vocal isolation on clips               |
| Timeline Audio | Original / Isolated | Original | Which audio appears on timeline after cuts |

### Take Detection (LLM-Powered)

The LLM groups similar statements as takes:

```
Input: "Hey guys welcome back" at 0:05, "Hey guys, welcome back" at 0:45
Output: Take Group "Hey Guys Welcome" with Take 1, Take 2
```

### Clip Color Coding

| Takes | Label Index | Color   | Meaning           |
| ----- | ----------- | ------- | ----------------- |
| 1     | 13          | Green   | Single take ✓     |
| 2     | 15          | Yellow  | Minor retry       |
| 3     | 7           | Mango   | Multiple attempts |
| 4+    | 11          | Magenta | Many retakes      |

### Silence Thresholds

| Duration  | Action | Description                             |
| --------- | ------ | --------------------------------------- |
| 150-500ms | KEEP   | Natural pause (breath, comma, sentence) |
| >750ms    | CUT    | Dead air, between-take gap              |
| >2000ms   | CUT    | Definitely cut (between takes)          |
| Padding   | 100ms  | Buffer to avoid clipping words          |

### Processing Pipeline

```
Selected Clips → Extract Audio → Upload to S3 →
  → Vocal Isolation (optional) → Transcription →
  → LLM Take Detection → Silence Detection →
  → Apply Cuts → Color & Label Clips
```

## Code Quality - Zero Tolerance

After editing ANY file, run:

```bash
npm run lint
npm run typecheck
```

Fix ALL errors/warnings before continuing.

## Current Focus

Section: Phase 3 - Auto-Cutting & Take Detection
Files: `backend/src/services/take-detection-service.ts`, `src/components/common/CuttingPanel.ts`

## Last Session (2025-12-16)

### Completed

- **LLM-powered Take Detection service**:
  - Created `backend/src/services/take-detection-service.ts` with Anthropic/OpenAI support
  - Added `/audio/detect-takes` endpoint in controller/routes
  - Groups similar statements as takes, assigns color codes by take count
  - Mock mode support for development testing

- **Plugin Clip Labeling APIs**:
  - Added `setClipLabelColor()`, `setClipName()`, `applyTakeGroupColors()` to `premiere.ts`
  - Uses Premiere Pro label colors (0-15) for take visualization
  - Added `submitTakeDetection()` to backend-client

- **Enhanced CuttingPanel UI**:
  - Audio Options: Isolate Audio checkbox, Timeline Audio Source dropdown
  - Take Detection section with expandable take groups and color swatches
  - Apply Colors & Labels button for timeline integration

- **Types and Documentation**:
  - Added TakeGroup, Take, TakeDetectionResult types to `auto-cutting.ts`
  - Added PREMIERE_LABEL_COLORS constants
  - Updated CLAUDE.md with complete Auto-Cutting Workflow documentation

### Previous Session

- End-to-end upload flow, AUDIO_MOCK_MODE, silence-based cutting implemented

### Test Credentials

- Email: `test2@example.com`
- Password: `TestPass123`

### Mock Mode (Development)

Set `AUDIO_MOCK_MODE=true` in `backend/.env` to test without external services.
Workers will return mock results instead of calling Whisper/Deepgram/Dolby/Demucs.

To use real services, set `AUDIO_MOCK_MODE=false` and configure:

- `DEEPGRAM_API_KEY` or `WHISPER_API_URL` for transcription
- `DOLBY_API_KEY` or `DEMUCS_API_URL` for vocal isolation

### S3 Setup (LocalStack - Already Configured)

LocalStack is configured in `docker-compose.yml` with auto-bucket creation.
Credentials in `backend/.env` are set for LocalStack (test/test).

To switch to real AWS S3, update `backend/.env`:

```
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_real_key
AWS_SECRET_ACCESS_KEY=your_real_secret
S3_BUCKET_NAME=your-bucket-name
S3_ENDPOINT=  # Remove or comment out for real AWS
```

## Next Steps

1. **Add TakeGroup types** to `src/types/auto-cutting.ts`
2. **Backend: take-detection-service.ts** - LLM integration for grouping similar statements
3. **Backend: take-detection-worker.ts** - New BullMQ job type
4. **Plugin: Audio extraction** - Extract audio from selected clips
5. **Plugin: Clip colors & names** - setClipColor, setClipName APIs
6. **UI: CuttingPanel updates** - Audio options, take groups preview
7. Test complete flow in Premiere Pro

## Dev Server Commands

```bash
# Start all services (Postgres, Redis, LocalStack)
docker-compose up -d

# Plugin (Vite) - runs on port 3000 (or 3002 if port in use)
npm run dev

# Backend - runs on port 3001
cd backend && npm run dev

# Health check
curl http://localhost:3001/health

# Verify LocalStack S3
curl http://localhost:4566/_localstack/health
```

## Backend Client API

```typescript
import { backendClient } from '@/utils/api/backend-client'

// Auth
backendClient.login(email, password)
backendClient.register(email, password, name?)
backendClient.logout()
backendClient.getMe()
backendClient.isAuthenticated()
backendClient.onAuthChange(callback) // returns unsubscribe fn

// Usage
backendClient.getUsage()
backendClient.checkUsageLimit(minutes)
backendClient.canSubmitAudioJob(minutes)

// File Upload
backendClient.getPresignedUploadUrl(filename, contentType, fileSize)
backendClient.uploadFileToS3(file, presignedUrl)
backendClient.uploadFileToS3WithProgress(file, presignedUrl, onProgress?) // XHR with progress
backendClient.uploadFile(file) // convenience: presign + upload in one call
backendClient.uploadFileWithProgress(file, onProgress?) // convenience with progress callback

// Audio Jobs
backendClient.submitTranscription(audioUrl, durationMinutes)
backendClient.submitVocalIsolation(audioUrl, durationMinutes)
backendClient.submitTakeDetection(transcriptionResult) // NEW: LLM take detection
backendClient.getJobs(limit?, offset?)
backendClient.getJob(jobId)
backendClient.pollJobUntilComplete(jobId, onProgress?, {intervalMs?, maxAttempts?, signal?})

// Billing
backendClient.createCheckout(tier, interval?)
backendClient.createPortal()
backendClient.getBillingStatus()
```

## Key Patterns

| Area           | Pattern                                                                            |
| -------------- | ---------------------------------------------------------------------------------- |
| AuthPanel      | `isMounted` flag prevents async writes on unmounted component                      |
| AuthPanel      | `if (this.isLoading) return` prevents double-submission                            |
| AuthPanel      | `getSafePlanClass()` validates plan enum before CSS class use                      |
| SettingsPanel  | User prop passed from PluginApp, logout dispatches event                           |
| SettingsPanel  | Billing buttons call `backendClient.createCheckout/createPortal` and `window.open` |
| backend-client | Refresh token mutex prevents concurrent refresh race condition                     |
| backend-client | localStorage fallback for browser dev mode (no UXP)                                |
| tsconfig       | `useDefineForClassFields: false` fixes Lit decorator issue                         |
