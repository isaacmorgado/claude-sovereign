import { env } from '../config/env.js'
import type { TranscriptionSegment } from '../types/audio.js'

// ============================================================================
// Types
// ============================================================================

export interface Take {
  takeNumber: number
  startTime: number
  endTime: number
  text: string
  confidence?: number
}

export interface TakeGroup {
  id: string
  label: string
  takes: Take[]
  colorIndex: number
}

export interface TakeDetectionResult {
  groups: TakeGroup[]
  totalTakes: number
  uniqueSegments: {
    startTime: number
    endTime: number
    text: string
  }[]
  metadata: {
    model: string
    processingTimeMs: number
    segmentsAnalyzed: number
  }
}

// Premiere Pro label color indices
const LABEL_COLORS = {
  GREEN: 13,   // 1 take
  YELLOW: 15,  // 2 takes
  MANGO: 7,    // 3 takes
  MAGENTA: 11, // 4+ takes
}

// ============================================================================
// LLM Prompt
// ============================================================================

const TAKE_DETECTION_PROMPT = `You are analyzing a transcript to identify repeated takes (multiple attempts at the same line or phrase).

Analyze the transcript segments and group similar statements together. Similar statements are:
- The same line said multiple times with slight variations
- Repeated phrases that appear to be different takes of the same content
- Lines that convey the same meaning but with minor wording differences

For each group, provide:
1. A short label (3-5 words, title case) that describes the content
2. The individual takes with their timestamps

IMPORTANT: Only group segments that are clearly repeated takes. Segments that are unique content should NOT be grouped.

Return your response as valid JSON in this exact format:
{
  "groups": [
    {
      "label": "Hey Guys Welcome",
      "takes": [
        { "startTime": 5.2, "endTime": 8.1, "text": "Hey guys welcome back to my channel" },
        { "startTime": 45.0, "endTime": 47.5, "text": "Hey guys, welcome back" }
      ]
    }
  ],
  "uniqueSegmentIndices": [2, 5, 7]
}

The uniqueSegmentIndices array should contain the indices (0-based) of segments that are NOT part of any take group.

TRANSCRIPT SEGMENTS:
`

// ============================================================================
// Service Implementation
// ============================================================================

export class TakeDetectionError extends Error {
  constructor(
    message: string,
    public statusCode: number = 500
  ) {
    super(message)
    this.name = 'TakeDetectionError'
  }
}

function getColorIndex(takeCount: number): number {
  if (takeCount <= 1) return LABEL_COLORS.GREEN
  if (takeCount === 2) return LABEL_COLORS.YELLOW
  if (takeCount === 3) return LABEL_COLORS.MANGO
  return LABEL_COLORS.MAGENTA
}

function generateGroupId(): string {
  return `tg_${Date.now()}_${Math.random().toString(36).substring(2, 9)}`
}

async function callAnthropicAPI(segments: TranscriptionSegment[]): Promise<string> {
  if (!env.ANTHROPIC_API_KEY) {
    throw new TakeDetectionError('ANTHROPIC_API_KEY not configured', 500)
  }

  const segmentText = segments
    .map((seg, i) => `[${i}] ${seg.start.toFixed(2)}s - ${seg.end.toFixed(2)}s: "${seg.text}"`)
    .join('\n')

  const response = await fetch('https://api.anthropic.com/v1/messages', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'x-api-key': env.ANTHROPIC_API_KEY,
      'anthropic-version': '2023-06-01',
    },
    body: JSON.stringify({
      model: 'claude-3-haiku-20240307',
      max_tokens: 4096,
      messages: [
        {
          role: 'user',
          content: TAKE_DETECTION_PROMPT + segmentText,
        },
      ],
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    throw new TakeDetectionError(`Anthropic API error: ${error}`, response.status)
  }

  const data = await response.json()
  return data.content[0].text
}

async function callOpenAIAPI(segments: TranscriptionSegment[]): Promise<string> {
  if (!env.OPENAI_API_KEY) {
    throw new TakeDetectionError('OPENAI_API_KEY not configured', 500)
  }

  const segmentText = segments
    .map((seg, i) => `[${i}] ${seg.start.toFixed(2)}s - ${seg.end.toFixed(2)}s: "${seg.text}"`)
    .join('\n')

  const response = await fetch('https://api.openai.com/v1/chat/completions', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      Authorization: `Bearer ${env.OPENAI_API_KEY}`,
    },
    body: JSON.stringify({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'user',
          content: TAKE_DETECTION_PROMPT + segmentText,
        },
      ],
      response_format: { type: 'json_object' },
    }),
  })

  if (!response.ok) {
    const error = await response.text()
    throw new TakeDetectionError(`OpenAI API error: ${error}`, response.status)
  }

  const data = await response.json()
  return data.choices[0].message.content
}

function generateMockResult(segments: TranscriptionSegment[]): TakeDetectionResult {
  // Mock implementation for testing without LLM
  const groups: TakeGroup[] = []
  const usedIndices = new Set<number>()

  // Simple similarity check - find segments with similar starting words
  for (let i = 0; i < segments.length; i++) {
    if (usedIndices.has(i)) continue

    const words1 = segments[i].text.toLowerCase().split(' ').slice(0, 3).join(' ')
    const similarIndices = [i]

    for (let j = i + 1; j < segments.length; j++) {
      if (usedIndices.has(j)) continue
      const words2 = segments[j].text.toLowerCase().split(' ').slice(0, 3).join(' ')

      if (words1 === words2 && words1.length > 5) {
        similarIndices.push(j)
      }
    }

    if (similarIndices.length > 1) {
      const takes: Take[] = similarIndices.map((idx, takeNum) => ({
        takeNumber: takeNum + 1,
        startTime: segments[idx].start,
        endTime: segments[idx].end,
        text: segments[idx].text,
        confidence: segments[idx].confidence,
      }))

      // Create label from first few words
      const labelWords = segments[i].text.split(' ').slice(0, 3)
      const label = labelWords
        .map(w => w.charAt(0).toUpperCase() + w.slice(1).toLowerCase())
        .join(' ')

      groups.push({
        id: generateGroupId(),
        label,
        takes,
        colorIndex: getColorIndex(takes.length),
      })

      similarIndices.forEach(idx => usedIndices.add(idx))
    }
  }

  const uniqueSegments = segments
    .filter((_, i) => !usedIndices.has(i))
    .map(seg => ({
      startTime: seg.start,
      endTime: seg.end,
      text: seg.text,
    }))

  return {
    groups,
    totalTakes: groups.reduce((sum, g) => sum + g.takes.length, 0),
    uniqueSegments,
    metadata: {
      model: 'mock',
      processingTimeMs: 0,
      segmentsAnalyzed: segments.length,
    },
  }
}

interface LLMResponse {
  groups: {
    label: string
    takes: {
      startTime: number
      endTime: number
      text: string
    }[]
  }[]
  uniqueSegmentIndices: number[]
}

function parseLLMResponse(
  response: string,
  segments: TranscriptionSegment[]
): TakeDetectionResult {
  // Extract JSON from response (may be wrapped in markdown code blocks)
  let jsonStr = response
  const jsonMatch = response.match(/```(?:json)?\s*([\s\S]*?)```/)
  if (jsonMatch) {
    jsonStr = jsonMatch[1]
  }

  const parsed: LLMResponse = JSON.parse(jsonStr)

  const groups: TakeGroup[] = parsed.groups.map(g => ({
    id: generateGroupId(),
    label: g.label,
    takes: g.takes.map((t, i) => ({
      takeNumber: i + 1,
      startTime: t.startTime,
      endTime: t.endTime,
      text: t.text,
    })),
    colorIndex: getColorIndex(g.takes.length),
  }))

  const uniqueSegments = (parsed.uniqueSegmentIndices || [])
    .filter(i => i >= 0 && i < segments.length)
    .map(i => ({
      startTime: segments[i].start,
      endTime: segments[i].end,
      text: segments[i].text,
    }))

  return {
    groups,
    totalTakes: groups.reduce((sum, g) => sum + g.takes.length, 0),
    uniqueSegments,
    metadata: {
      model: env.ANTHROPIC_API_KEY ? 'claude-3-haiku' : 'gpt-4o-mini',
      processingTimeMs: 0, // Will be set by caller
      segmentsAnalyzed: segments.length,
    },
  }
}

// ============================================================================
// Public API
// ============================================================================

export async function detectTakes(
  segments: TranscriptionSegment[]
): Promise<TakeDetectionResult> {
  if (!segments || segments.length === 0) {
    return {
      groups: [],
      totalTakes: 0,
      uniqueSegments: [],
      metadata: {
        model: 'none',
        processingTimeMs: 0,
        segmentsAnalyzed: 0,
      },
    }
  }

  const startTime = Date.now()

  // Use mock mode if enabled or no API keys configured
  if (env.AUDIO_MOCK_MODE || (!env.ANTHROPIC_API_KEY && !env.OPENAI_API_KEY)) {
    const result = generateMockResult(segments)
    result.metadata.processingTimeMs = Date.now() - startTime
    return result
  }

  try {
    // Prefer Anthropic, fall back to OpenAI
    const llmResponse = env.ANTHROPIC_API_KEY
      ? await callAnthropicAPI(segments)
      : await callOpenAIAPI(segments)

    const result = parseLLMResponse(llmResponse, segments)
    result.metadata.processingTimeMs = Date.now() - startTime
    return result
  } catch (error) {
    console.error('Take detection error:', error)

    // Fall back to mock if LLM fails
    const result = generateMockResult(segments)
    result.metadata.model = 'mock-fallback'
    result.metadata.processingTimeMs = Date.now() - startTime
    return result
  }
}
