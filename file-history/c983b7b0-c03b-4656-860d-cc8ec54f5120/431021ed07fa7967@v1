/**
 * Silence detection from transcription segments
 * Identifies gaps between speech segments that indicate potential cut points
 */

export interface TranscriptionSegment {
  start: number // seconds
  end: number // seconds
  text: string
  confidence: number
}

export interface SilenceGap {
  startTime: number // seconds - end of previous segment
  endTime: number // seconds - start of next segment
  duration: number // seconds
  beforeSegmentIndex: number // index of segment before this gap
  afterSegmentIndex: number // index of segment after this gap
}

export interface CutPoint {
  time: number // seconds - where to make the cut
  type: 'silence' | 'manual'
  silenceGap?: SilenceGap // reference to the gap that triggered this cut
}

export interface SilenceDetectionConfig {
  minSilenceDuration: number // minimum gap duration to consider (seconds), default 0.5
  maxSilenceDuration: number // maximum gap to consider as single cut point (seconds), default 10
  cutPosition: 'start' | 'middle' | 'end' // where in the silence to place the cut
}

const DEFAULT_CONFIG: SilenceDetectionConfig = {
  minSilenceDuration: 0.5,
  maxSilenceDuration: 10,
  cutPosition: 'middle',
}

/**
 * Detect silence gaps between transcription segments
 */
export function detectSilenceGaps(
  segments: TranscriptionSegment[],
  config: Partial<SilenceDetectionConfig> = {}
): SilenceGap[] {
  const { minSilenceDuration, maxSilenceDuration } = { ...DEFAULT_CONFIG, ...config }

  if (segments.length < 2) {
    return []
  }

  // Sort segments by start time to ensure correct order
  const sortedSegments = [...segments].sort((a, b) => a.start - b.start)

  const gaps: SilenceGap[] = []

  for (let i = 0; i < sortedSegments.length - 1; i++) {
    const currentSegment = sortedSegments[i]
    const nextSegment = sortedSegments[i + 1]

    const gapStart = currentSegment.end
    const gapEnd = nextSegment.start
    const duration = gapEnd - gapStart

    // Only include gaps that meet the threshold criteria
    if (duration >= minSilenceDuration && duration <= maxSilenceDuration) {
      gaps.push({
        startTime: gapStart,
        endTime: gapEnd,
        duration,
        beforeSegmentIndex: i,
        afterSegmentIndex: i + 1,
      })
    }
  }

  return gaps
}

/**
 * Convert silence gaps to cut points
 */
export function gapsToCutPoints(
  gaps: SilenceGap[],
  config: Partial<SilenceDetectionConfig> = {}
): CutPoint[] {
  const { cutPosition } = { ...DEFAULT_CONFIG, ...config }

  return gaps.map((gap) => {
    let time: number

    switch (cutPosition) {
      case 'start':
        time = gap.startTime
        break
      case 'end':
        time = gap.endTime
        break
      case 'middle':
      default:
        time = gap.startTime + gap.duration / 2
        break
    }

    return {
      time,
      type: 'silence' as const,
      silenceGap: gap,
    }
  })
}

/**
 * Detect cut points from transcription segments
 * Convenience function that combines gap detection and cut point generation
 */
export function detectCutPoints(
  segments: TranscriptionSegment[],
  config: Partial<SilenceDetectionConfig> = {}
): CutPoint[] {
  const gaps = detectSilenceGaps(segments, config)
  return gapsToCutPoints(gaps, config)
}

/**
 * Get statistics about detected silences
 */
export function getSilenceStats(gaps: SilenceGap[]): {
  count: number
  totalDuration: number
  averageDuration: number
  minDuration: number
  maxDuration: number
} {
  if (gaps.length === 0) {
    return {
      count: 0,
      totalDuration: 0,
      averageDuration: 0,
      minDuration: 0,
      maxDuration: 0,
    }
  }

  const durations = gaps.map((g) => g.duration)
  const totalDuration = durations.reduce((sum, d) => sum + d, 0)

  return {
    count: gaps.length,
    totalDuration,
    averageDuration: totalDuration / gaps.length,
    minDuration: Math.min(...durations),
    maxDuration: Math.max(...durations),
  }
}
