#!/usr/bin/env node
/**
 * Multi-Provider Proxy Server for Claude Code
 * Inspired by claudish - enables using GLM, Featherless.ai, and other providers
 *
 * This proxy translates between Anthropic API format and various provider formats,
 * allowing Claude Code to work with any LLM provider seamlessly.
 *
 * Usage:
 *   node model-proxy-server.js [port]
 *
 * Then start Claude Code with:
 *   ANTHROPIC_BASE_URL=http://localhost:PORT claude
 *
 * Model Prefixes:
 *   glm/glm-4           -> GLM (ZhipuAI)
 *   featherless/...     -> Featherless.ai
 *   anthropic/...       -> Native Anthropic (passthrough)
 *   (no prefix)         -> Native Anthropic (passthrough)
 */

const http = require('http');
const https = require('https');
const { URL } = require('url');

// Configuration
const PORT = process.env.CLAUDISH_PORT || process.argv[2] || 3000;
const GLM_API_KEY = process.env.GLM_API_KEY || '9a58c7331504f3cbaef3f2f95cb375b.BrfNpV8TbeF5tCaK';
const GLM_BASE_URL = 'https://open.bigmodel.cn/api/paas/v4';
const FEATHERLESS_API_KEY = process.env.FEATHERLESS_API_KEY || '';
const FEATHERLESS_BASE_URL = 'https://api.featherless.ai/v1';
const ANTHROPIC_API_KEY = process.env.ANTHROPIC_API_KEY || '';
const ANTHROPIC_BASE_URL = 'https://api.anthropic.com';

// Color codes for logging
const colors = {
  reset: '\x1b[0m',
  bright: '\x1b[1m',
  dim: '\x1b[2m',
  red: '\x1b[31m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  magenta: '\x1b[35m',
  cyan: '\x1b[36m',
};

function log(message, color = 'reset') {
  const timestamp = new Date().toISOString().split('T')[1].split('.')[0];
  console.error(`${colors.dim}[${timestamp}]${colors.reset} ${colors[color]}${message}${colors.reset}`);
}

/**
 * Parse model string to extract provider and model name
 */
function parseModel(modelString) {
  if (!modelString) {
    return { provider: 'anthropic', model: 'claude-sonnet-4-5-20250929' };
  }

  // Check for prefixes
  const prefixMatch = modelString.match(/^(glm|featherless|anthropic)\/(.*)/);

  if (prefixMatch) {
    return {
      provider: prefixMatch[1],
      model: prefixMatch[2]
    };
  }

  // Default to anthropic if no prefix
  return {
    provider: 'anthropic',
    model: modelString
  };
}

/**
 * Convert Anthropic message format to OpenAI format
 */
function anthropicToOpenAI(anthropicBody) {
  const messages = [];

  // Add system message if present
  if (anthropicBody.system) {
    messages.push({
      role: 'system',
      content: anthropicBody.system
    });
  }

  // Convert messages
  for (const msg of anthropicBody.messages || []) {
    const openaiMsg = {
      role: msg.role,
      content: ''
    };

    // Handle content
    if (typeof msg.content === 'string') {
      openaiMsg.content = msg.content;
    } else if (Array.isArray(msg.content)) {
      // Combine text blocks
      openaiMsg.content = msg.content
        .filter(block => block.type === 'text')
        .map(block => block.text)
        .join('\n');
    }

    messages.push(openaiMsg);
  }

  return {
    model: anthropicBody.model,
    messages: messages,
    max_tokens: anthropicBody.max_tokens || 2048,
    temperature: anthropicBody.temperature || 0.7,
    top_p: anthropicBody.top_p,
    stream: anthropicBody.stream || false
  };
}

/**
 * Convert OpenAI response to Anthropic format
 */
function openaiToAnthropic(openaiResponse, isStreaming = false) {
  if (isStreaming) {
    // Handle streaming chunk
    if (openaiResponse.choices && openaiResponse.choices[0]) {
      const delta = openaiResponse.choices[0].delta;
      if (delta && delta.content) {
        return {
          type: 'content_block_delta',
          index: 0,
          delta: {
            type: 'text_delta',
            text: delta.content
          }
        };
      }
      if (openaiResponse.choices[0].finish_reason) {
        return {
          type: 'message_delta',
          delta: {
            stop_reason: openaiResponse.choices[0].finish_reason
          },
          usage: openaiResponse.usage || {}
        };
      }
    }
    return null;
  }

  // Handle complete response
  const choice = openaiResponse.choices?.[0];
  const content = choice?.message?.content || '';

  return {
    id: openaiResponse.id || `msg_${Date.now()}`,
    type: 'message',
    role: 'assistant',
    content: [
      {
        type: 'text',
        text: content
      }
    ],
    model: openaiResponse.model,
    stop_reason: choice?.finish_reason || 'end_turn',
    usage: {
      input_tokens: openaiResponse.usage?.prompt_tokens || 0,
      output_tokens: openaiResponse.usage?.completion_tokens || 0
    }
  };
}

/**
 * Make HTTP/HTTPS request
 */
function makeRequest(url, options, body) {
  return new Promise((resolve, reject) => {
    const parsedUrl = new URL(url);
    const protocol = parsedUrl.protocol === 'https:' ? https : http;

    const reqOptions = {
      hostname: parsedUrl.hostname,
      port: parsedUrl.port,
      path: parsedUrl.pathname + parsedUrl.search,
      method: options.method || 'POST',
      headers: options.headers || {}
    };

    const req = protocol.request(reqOptions, (res) => {
      let data = '';

      res.on('data', (chunk) => {
        data += chunk;
      });

      res.on('end', () => {
        resolve({
          status: res.statusCode,
          headers: res.headers,
          body: data
        });
      });
    });

    req.on('error', (err) => {
      reject(err);
    });

    if (body) {
      req.write(typeof body === 'string' ? body : JSON.stringify(body));
    }

    req.end();
  });
}

/**
 * Handle GLM provider requests
 */
async function handleGLM(anthropicBody, res) {
  const { model } = parseModel(anthropicBody.model);
  const openaiBody = anthropicToOpenAI(anthropicBody);
  openaiBody.model = model;

  log(`â†’ GLM: ${model}`, 'cyan');

  try {
    const response = await makeRequest(
      `${GLM_BASE_URL}/chat/completions`,
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${GLM_API_KEY}`,
          'Content-Type': 'application/json'
        }
      },
      openaiBody
    );

    if (response.status !== 200) {
      log(`âœ— GLM error: ${response.status}`, 'red');
      res.writeHead(response.status, { 'Content-Type': 'application/json' });
      res.end(response.body);
      return;
    }

    const openaiResponse = JSON.parse(response.body);
    const anthropicResponse = openaiToAnthropic(openaiResponse);

    log(`â† GLM: ${anthropicResponse.usage.output_tokens} tokens`, 'green');

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify(anthropicResponse));

  } catch (error) {
    log(`âœ— GLM error: ${error.message}`, 'red');
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'api_error',
        message: error.message
      }
    }));
  }
}

/**
 * Handle Featherless.ai provider requests
 */
async function handleFeatherless(anthropicBody, res) {
  if (!FEATHERLESS_API_KEY) {
    log(`âœ— Featherless: API key not configured`, 'red');
    res.writeHead(401, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'authentication_error',
        message: 'FEATHERLESS_API_KEY not set'
      }
    }));
    return;
  }

  const { model } = parseModel(anthropicBody.model);
  const openaiBody = anthropicToOpenAI(anthropicBody);
  openaiBody.model = model;

  log(`â†’ Featherless: ${model}`, 'magenta');

  try {
    const response = await makeRequest(
      `${FEATHERLESS_BASE_URL}/chat/completions`,
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${FEATHERLESS_API_KEY}`,
          'Content-Type': 'application/json'
        }
      },
      openaiBody
    );

    if (response.status !== 200) {
      log(`âœ— Featherless error: ${response.status}`, 'red');
      res.writeHead(response.status, { 'Content-Type': 'application/json' });
      res.end(response.body);
      return;
    }

    const openaiResponse = JSON.parse(response.body);
    const anthropicResponse = openaiToAnthropic(openaiResponse);

    log(`â† Featherless: ${anthropicResponse.usage.output_tokens} tokens`, 'green');

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify(anthropicResponse));

  } catch (error) {
    log(`âœ— Featherless error: ${error.message}`, 'red');
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'api_error',
        message: error.message
      }
    }));
  }
}

/**
 * Handle Anthropic (native) provider requests - passthrough
 */
async function handleAnthropic(anthropicBody, res, headers) {
  if (!ANTHROPIC_API_KEY) {
    log(`âœ— Anthropic: API key not configured`, 'red');
    res.writeHead(401, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'authentication_error',
        message: 'ANTHROPIC_API_KEY not set'
      }
    }));
    return;
  }

  const { model } = parseModel(anthropicBody.model);
  anthropicBody.model = model;

  log(`â†’ Anthropic: ${model}`, 'blue');

  try {
    const response = await makeRequest(
      `${ANTHROPIC_BASE_URL}/v1/messages`,
      {
        method: 'POST',
        headers: {
          'x-api-key': ANTHROPIC_API_KEY,
          'anthropic-version': '2023-06-01',
          'Content-Type': 'application/json'
        }
      },
      anthropicBody
    );

    log(`â† Anthropic: ${response.status}`, 'green');

    res.writeHead(response.status, { 'Content-Type': 'application/json' });
    res.end(response.body);

  } catch (error) {
    log(`âœ— Anthropic error: ${error.message}`, 'red');
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'api_error',
        message: error.message
      }
    }));
  }
}

/**
 * Main request handler
 */
function handleRequest(req, res) {
  // CORS headers
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization, x-api-key, anthropic-version');

  if (req.method === 'OPTIONS') {
    res.writeHead(200);
    res.end();
    return;
  }

  // Only handle /v1/messages endpoint
  if (!req.url.includes('/v1/messages')) {
    res.writeHead(404, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: 'Not found' }));
    return;
  }

  let body = '';
  req.on('data', (chunk) => {
    body += chunk;
  });

  req.on('end', async () => {
    try {
      const anthropicBody = JSON.parse(body);
      const { provider } = parseModel(anthropicBody.model);

      log(`${req.method} ${req.url} [${provider}]`, 'bright');

      // Route to appropriate provider
      switch (provider) {
        case 'glm':
          await handleGLM(anthropicBody, res);
          break;
        case 'featherless':
          await handleFeatherless(anthropicBody, res);
          break;
        case 'anthropic':
        default:
          await handleAnthropic(anthropicBody, res, req.headers);
          break;
      }

    } catch (error) {
      log(`âœ— Request error: ${error.message}`, 'red');
      res.writeHead(400, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({
        type: 'error',
        error: {
          type: 'invalid_request_error',
          message: error.message
        }
      }));
    }
  });
}

/**
 * Start the proxy server
 */
const server = http.createServer(handleRequest);

server.listen(PORT, '127.0.0.1', () => {
  console.log('');
  log('â•'.repeat(60), 'bright');
  log(`Multi-Provider Proxy Server for Claude Code`, 'bright');
  log('â•'.repeat(60), 'bright');
  console.log('');
  log(`ðŸš€ Server running on http://127.0.0.1:${PORT}`, 'green');
  console.log('');
  log('Supported Providers:', 'bright');
  log(`  ${GLM_API_KEY ? 'âœ“' : 'âœ—'} GLM (ZhipuAI)     - glm/glm-4`, GLM_API_KEY ? 'green' : 'dim');
  log(`  ${FEATHERLESS_API_KEY ? 'âœ“' : 'âœ—'} Featherless.ai   - featherless/model-name`, FEATHERLESS_API_KEY ? 'green' : 'dim');
  log(`  ${ANTHROPIC_API_KEY ? 'âœ“' : 'âœ—'} Anthropic        - anthropic/claude-sonnet-4-5 (or no prefix)`, ANTHROPIC_API_KEY ? 'green' : 'dim');
  console.log('');
  log('Usage:', 'bright');
  log(`  ANTHROPIC_BASE_URL=http://127.0.0.1:${PORT} claude`, 'cyan');
  log(`  /model glm/glm-4`, 'cyan');
  log(`  /model featherless/uncensored-model`, 'cyan');
  log(`  /model anthropic/claude-opus-4-5`, 'cyan');
  console.log('');
  log('Environment Variables:', 'bright');
  log(`  GLM_API_KEY=${GLM_API_KEY.substring(0, 20)}...`, 'dim');
  log(`  FEATHERLESS_API_KEY=${FEATHERLESS_API_KEY ? FEATHERLESS_API_KEY.substring(0, 20) + '...' : '(not set)'}`, 'dim');
  log(`  ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY ? ANTHROPIC_API_KEY.substring(0, 20) + '...' : '(not set)'}`, 'dim');
  console.log('');
  log('â•'.repeat(60), 'bright');
  console.log('');
  log('Ready to proxy requests...', 'green');
  console.log('');
});

// Graceful shutdown
process.on('SIGINT', () => {
  console.log('');
  log('Shutting down proxy server...', 'yellow');
  server.close(() => {
    log('Server stopped', 'dim');
    process.exit(0);
  });
});
