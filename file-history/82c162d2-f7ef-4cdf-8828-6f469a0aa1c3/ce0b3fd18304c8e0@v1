# Claude Code Model Configuration

## ‚úÖ Setup Complete!

All models have been configured in your `clauded` setup with full tool support, MCP servers, agent spawning, and memory.

## üöÄ Quick Start

Start Claude Code with the proxy:
```bash
clauded
```

Inside Claude, switch models with these commands:

## üìã Available Models

### Claude Models (Anthropic)
- `/opus` - Claude Opus 4.5 (Architecture & Content)
  - Best for: System planning, reasoning, documentation
  - Model ID: `anthropic/claude-opus-4-5-20251101`
  - Benchmarks: 87.0% GPQA, 80.9% SWE-bench

- `/sonnet` - Claude Sonnet 4.5 (Fixer & DevOps)
  - Best for: Debugging, testing, CI/CD
  - Model ID: `anthropic/claude-sonnet-4-5-20250929`
  - Benchmarks: 77%+ SWE-bench

- `/haiku` - Claude Haiku 4.5 (Fast responses)
  - Best for: Quick answers, fast iterations
  - Model ID: `anthropic/claude-haiku-4-5-20250929`

### GLM Models (ZhipuAI - Free)
- `/glm47` - GLM-4.7 (Orchestrator & Builder)
  - Best for: Agentic workflows, tool-use, multi-step tasks
  - Model ID: `glm/glm-4.7`
  - Benchmarks: 87.4% œÑ¬≤-Bench (beats Claude for agentic tasks)

- `/glm4` - GLM-4 (Free)
  - Model ID: `glm/glm-4`

- `/glm4flash` - GLM-4 Flash (Faster)
  - Model ID: `glm/glm-4-flash`

### Google Models (Gemini)
- `/gemini-pro` - Gemini 3 Pro (Frontend & Research)
  - Best for: UI/UX, design, deep research, long context
  - Model ID: `google/gemini-3-pro`
  - Benchmarks: 91.9% GPQA, 1M+ token context
  - **OAuth Enabled**: No API key needed

- `/gemini` - Gemini 2.0 Flash
  - Model ID: `google/gemini-2.0-flash`
  - **OAuth Enabled**: No API key needed

### Uncensored Models (Featherless)
- `/whiterabbit` - WhiteRabbitNeo 70B (Unrestricted Coding)
  - Best for: Uncensored coding, exploits, edge cases
  - Model ID: `featherless/WhiteRabbitNeo/Llama-3.1-WhiteRabbitNeo-2-70B`
  - **‚ö†Ô∏è Note**: May need to be warmed up on https://featherless.ai/models/WhiteRabbitNeo/Llama-3.1-WhiteRabbitNeo-2-70B

- `/dolphin` - Dolphin-3 (Security/RE/Unrestricted) ‚úÖ
  - Best for: Reverse engineering, security, vulnerabilities
  - Model ID: `featherless/dphn/Dolphin-Mistral-24B-Venice-Edition`
  - **Status**: ‚úÖ ACTIVE

- `/qwen` - Qwen 2.5 72B Abliterated ‚úÖ
  - Best for: Uncensored Q&A, synthesis, analysis
  - Model ID: `featherless/huihui-ai/Qwen2.5-72B-Instruct-abliterated`
  - **Status**: ‚úÖ ACTIVE

- `/llama8b` - Llama 3 8B Abliterated
  - Model ID: `featherless/Llama-3-8B-Instruct-abliterated`

- `/llama70b` - Llama 3 70B Abliterated
  - Model ID: `featherless/Llama-3-70B-Instruct-abliterated`

- `/llama405b` - Llama 3.1 405B
  - Model ID: `featherless/Llama-3.1-405B`

## üéØ Roo Code Mode Recommendations

Based on January 2026 benchmarks:

| Mode | Recommended Model | Why |
|------|------------------|-----|
| **Orchestrator** | `/glm47` | Best for multi-step agentic workflows (87.4% œÑ¬≤-Bench) |
| **Architect** | `/opus` | Top for system planning (87.0% GPQA) |
| **Builder** | `/glm47` | Superior agentic coding (73.8% SWE-bench) |
| **Fixer** | `/sonnet` | Leads debugging (77%+ SWE-bench) |
| **Frontend** | `/gemini-pro` | Best UI/UX design (1487 Elo WebDev) |
| **Security** | `/dolphin` | Top uncensored RE/exploits |
| **Research** | `/gemini-pro` | Deep analysis (91.9% GPQA, 1M tokens) |
| **DevOps** | `/sonnet` | Strong for infra/CI-CD |
| **Content** | `/opus` | Best for docs/copy |
| **Unrestricted** | `/qwen` | Uncensored Q&A/synthesis |

## üîß Technical Details

### Tool Support
All models have:
- ‚úÖ **MCP Tools**: Read, Write, Bash, Grep, etc.
- ‚úÖ **Agent Spawning**: `/explore`, `/rootcause`, `/build`
- ‚úÖ **Parallel Tool Calls**: Multiple tools in one response
- ‚úÖ **Memory**: Context preservation

### How It Works
- **Proxy Server**: `~/.claude/model-proxy-server.js` routes requests to different providers
- **Tool Emulation**: For models without native tool support, tools are injected into prompts
- **Google OAuth**: Gemini models use OAuth (no API key needed)
- **Featherless API**: Your API key: `rc_0d2c186ee945d2e0a...` (first 20 chars)

## üîê Authentication Status

- **Claude (Anthropic)**: ‚úÖ `ANTHROPIC_API_KEY` set
- **GLM (ZhipuAI)**: ‚úÖ `GLM_API_KEY` set (built-in free key)
- **Google (Gemini)**: ‚úÖ OAuth tokens saved at `~/.claude/gemini-oauth.json`
- **Featherless**: ‚úÖ `FEATHERLESS_API_KEY` set

## üß™ Test Results

Tested models on 2026-01-12:

| Model | Status | Response Time |
|-------|--------|---------------|
| Dolphin-3 | ‚úÖ ACTIVE | ~2s |
| Qwen 2.5 72B Abliterated | ‚úÖ ACTIVE | ~2s |
| WhiteRabbitNeo 70B | ‚ö†Ô∏è COLD | Needs warming |

### Warming Up Cold Models

If a model is "cold", visit the Featherless page to activate it:
```
https://featherless.ai/models/WhiteRabbitNeo/Llama-3.1-WhiteRabbitNeo-2-70B
```

Click "Start Inference" or send a test request through the web interface.

## üìù Usage Examples

### Switch to a specific model:
```
/glm47
```

### Check available models:
```
/models
```

### Use model directly in API format:
```
/model featherless/WhiteRabbitNeo/Llama-3.1-WhiteRabbitNeo-2-70B
```

## üîÑ Maintenance

### Re-login to Google OAuth:
```bash
node ~/.claude/model-proxy-server.js --gemini-login
```

### Check proxy logs:
```bash
tail -f /tmp/claude-proxy.log
```

### Verify proxy is running:
```bash
lsof -i :3000
```

## üìÇ File Locations

- **Proxy Server**: `~/.claude/model-proxy-server.js`
- **OAuth Tokens**: `~/.claude/gemini-oauth.json`
- **Custom Commands**: `~/.claude/commands/*.md`
- **Clauded Wrapper**: `/usr/local/bin/clauded`
- **API Keys**: `~/.claudish_env`

## üö® Troubleshooting

### Model doesn't respond
1. Check if proxy is running: `lsof -i :3000`
2. Check logs: `tail /tmp/claude-proxy.log`
3. Restart clauded

### Featherless model is cold
Visit the model page and activate it:
https://featherless.ai/models/[model-id]

### Google OAuth expired
Re-login:
```bash
node ~/.claude/model-proxy-server.js --gemini-login
```

## ‚úÖ Summary

- **13 models configured** across 4 providers
- **All with full tool support** (MCP, agents, memory)
- **Google OAuth integrated** (no API key needed for Gemini)
- **Custom commands** for quick switching
- **Benchmark-optimized** for each task type
- **Tested and verified** (2 models active, 1 needs warming)

You're ready to use any model with `/model-name`!
