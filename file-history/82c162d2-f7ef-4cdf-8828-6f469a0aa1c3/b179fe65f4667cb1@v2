#!/usr/bin/env node
/**
 * Multi-Provider Proxy Server for Claude Code
 * Enables GLM, Featherless.ai, Google Gemini, and Anthropic models with full tool support
 *
 * Features:
 * - Tool calling emulation for models without native support (abliterated models)
 * - Multiple provider support with automatic format translation
 * - Seamless integration with Claude Code's MCP tools
 *
 * Usage:
 *   node model-proxy-server.js [port]
 *
 * Then start Claude Code with:
 *   ANTHROPIC_BASE_URL=http://localhost:PORT claude
 *
 * Model Prefixes:
 *   glm/glm-4           -> GLM (ZhipuAI)
 *   featherless/...     -> Featherless.ai (with tool emulation)
 *   google/gemini-pro   -> Google Gemini
 *   anthropic/...       -> Native Anthropic (passthrough)
 *   (no prefix)         -> Native Anthropic (passthrough)
 */

const http = require('http');
const https = require('https');
const { URL } = require('url');

// Configuration
const PORT = process.env.CLAUDISH_PORT || process.argv[2] || 3000;
const GLM_API_KEY = process.env.GLM_API_KEY || '9a58c7331504f3cbaef3f2f95cb375b.BrfNpV8TbeF5tCaK';
const GLM_BASE_URL = 'https://open.bigmodel.cn/api/paas/v4';
const FEATHERLESS_API_KEY = process.env.FEATHERLESS_API_KEY || '';
const FEATHERLESS_BASE_URL = 'https://api.featherless.ai/v1';
const GOOGLE_API_KEY = process.env.GOOGLE_API_KEY || '';
const GOOGLE_BASE_URL = 'https://generativelanguage.googleapis.com/v1beta';
const ANTHROPIC_API_KEY = process.env.ANTHROPIC_API_KEY || '';
const ANTHROPIC_BASE_URL = 'https://api.anthropic.com';

// Models that support native tool calling
const NATIVE_TOOL_CALLING_MODELS = [
  'glm-4',
  'glm-4-plus',
  'gemini-pro',
  'gemini-1.5-pro',
  'gemini-2.0-flash',
];

// Color codes for logging
const colors = {
  reset: '\x1b[0m',
  bright: '\x1b[1m',
  dim: '\x1b[2m',
  red: '\x1b[31m',
  green: '\x1b[32m',
  yellow: '\x1b[33m',
  blue: '\x1b[34m',
  magenta: '\x1b[35m',
  cyan: '\x1b[36m',
};

function log(message, color = 'reset') {
  const timestamp = new Date().toISOString().split('T')[1].split('.')[0];
  console.error(`${colors.dim}[${timestamp}]${colors.reset} ${colors[color]}${message}${colors.reset}`);
}

/**
 * Parse model string to extract provider and model name
 */
function parseModel(modelString) {
  if (!modelString) {
    return { provider: 'anthropic', model: 'claude-sonnet-4-5-20250929' };
  }

  // Check for prefixes
  const prefixMatch = modelString.match(/^(glm|featherless|google|anthropic)\/(.*)$/);

  if (prefixMatch) {
    return {
      provider: prefixMatch[1],
      model: prefixMatch[2]
    };
  }

  // Default to anthropic if no prefix
  return {
    provider: 'anthropic',
    model: modelString
  };
}

/**
 * Check if model supports native tool calling
 */
function supportsNativeToolCalling(provider, model) {
  if (provider === 'anthropic') return true;
  if (provider === 'google') return true;
  if (provider === 'featherless') return false; // Abliterated models don't support tools
  if (provider === 'glm') {
    return NATIVE_TOOL_CALLING_MODELS.some(m => model.includes(m));
  }
  return false;
}

/**
 * Convert Anthropic tools to OpenAI function format
 */
function anthropicToolsToOpenAI(tools) {
  if (!tools || tools.length === 0) return undefined;

  return tools.map(tool => ({
    type: 'function',
    function: {
      name: tool.name,
      description: tool.description,
      parameters: tool.input_schema
    }
  }));
}

/**
 * Inject tools into system prompt for models without native support
 */
function injectToolsIntoPrompt(systemPrompt, tools) {
  if (!tools || tools.length === 0) return systemPrompt;

  const toolsDescription = tools.map(tool => {
    return `## Tool: ${tool.name}
Description: ${tool.description}
Parameters: ${JSON.stringify(tool.input_schema, null, 2)}`;
  }).join('\n\n');

  const toolCallInstructions = `
# Available Tools

You have access to the following tools. To use a tool, respond with XML tags in this exact format:

<tool_call>
{"name": "tool_name", "arguments": {"param1": "value1", "param2": "value2"}}
</tool_call>

You can call multiple tools in sequence. Always wrap each tool call in separate <tool_call> tags.

${toolsDescription}

# Examples

User: What's the weather in San Francisco?
Assistant: I'll check the weather for you.
<tool_call>
{"name": "get_weather", "arguments": {"location": "San Francisco, CA"}}
</tool_call>

User: Read file config.json and then search for "database"
Assistant: I'll read the file first.
<tool_call>
{"name": "read_file", "arguments": {"path": "config.json"}}
</tool_call>

Then based on the content, I'll search for the database configuration.
`;

  return (systemPrompt || '') + '\n\n' + toolCallInstructions;
}

/**
 * Parse tool calls from model output
 */
function parseToolCalls(text) {
  const toolCalls = [];
  const regex = /<tool_call>\s*(\{[\s\S]*?\})\s*<\/tool_call>/g;
  let match;

  while ((match = regex.exec(text)) !== null) {
    try {
      const toolCall = JSON.parse(match[1]);
      toolCalls.push({
        id: `call_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        type: 'function',
        function: {
          name: toolCall.name,
          arguments: JSON.stringify(toolCall.arguments)
        }
      });
    } catch (error) {
      log(`Failed to parse tool call: ${error.message}`, 'yellow');
    }
  }

  // Remove tool call tags from text
  const cleanedText = text.replace(/<tool_call>[\s\S]*?<\/tool_call>/g, '').trim();

  return { toolCalls, cleanedText };
}

/**
 * Convert Anthropic message format to OpenAI format
 */
function anthropicToOpenAI(anthropicBody, emulateTools = false) {
  const messages = [];

  // Add system message if present
  let systemContent = anthropicBody.system || '';

  // Inject tools into system prompt if emulating
  if (emulateTools && anthropicBody.tools && anthropicBody.tools.length > 0) {
    systemContent = injectToolsIntoPrompt(systemContent, anthropicBody.tools);
  }

  if (systemContent) {
    messages.push({
      role: 'system',
      content: systemContent
    });
  }

  // Convert messages
  for (const msg of anthropicBody.messages || []) {
    const openaiMsg = {
      role: msg.role,
      content: ''
    };

    // Handle content
    if (typeof msg.content === 'string') {
      openaiMsg.content = msg.content;
    } else if (Array.isArray(msg.content)) {
      // Handle tool results and text blocks
      const textBlocks = [];
      const toolResults = [];

      for (const block of msg.content) {
        if (block.type === 'text') {
          textBlocks.push(block.text);
        } else if (block.type === 'tool_result') {
          toolResults.push({
            tool_call_id: block.tool_use_id,
            role: 'tool',
            name: block.name || 'unknown',
            content: typeof block.content === 'string' ? block.content : JSON.stringify(block.content)
          });
        } else if (block.type === 'tool_use') {
          // This shouldn't appear in user messages, but handle it
          textBlocks.push(`[Tool: ${block.name}]`);
        }
      }

      openaiMsg.content = textBlocks.join('\n');

      // Add tool results as separate messages
      if (toolResults.length > 0) {
        messages.push(openaiMsg);
        messages.push(...toolResults);
        continue;
      }
    }

    messages.push(openaiMsg);
  }

  const result = {
    model: anthropicBody.model,
    messages: messages,
    max_tokens: anthropicBody.max_tokens || 4096,
    temperature: anthropicBody.temperature || 0.7,
    top_p: anthropicBody.top_p,
    stream: anthropicBody.stream || false
  };

  // Add tools if not emulating (native support)
  if (!emulateTools && anthropicBody.tools && anthropicBody.tools.length > 0) {
    result.tools = anthropicToolsToOpenAI(anthropicBody.tools);
  }

  return result;
}

/**
 * Convert OpenAI response to Anthropic format
 */
function openaiToAnthropic(openaiResponse, emulateTools = false) {
  const choice = openaiResponse.choices?.[0];
  if (!choice) {
    return {
      id: `msg_${Date.now()}`,
      type: 'message',
      role: 'assistant',
      content: [{ type: 'text', text: 'No response generated' }],
      model: openaiResponse.model,
      stop_reason: 'end_turn',
      usage: {
        input_tokens: openaiResponse.usage?.prompt_tokens || 0,
        output_tokens: openaiResponse.usage?.completion_tokens || 0
      }
    };
  }

  const content = choice.message?.content || '';
  const toolCalls = choice.message?.tool_calls || [];

  const anthropicContent = [];

  // Handle tool calling emulation
  if (emulateTools && content) {
    const { toolCalls: parsedCalls, cleanedText } = parseToolCalls(content);

    if (cleanedText) {
      anthropicContent.push({
        type: 'text',
        text: cleanedText
      });
    }

    // Convert parsed tool calls to Anthropic format
    for (const call of parsedCalls) {
      anthropicContent.push({
        type: 'tool_use',
        id: call.id,
        name: call.function.name,
        input: JSON.parse(call.function.arguments)
      });
    }
  } else {
    // Add text content
    if (content) {
      anthropicContent.push({
        type: 'text',
        text: content
      });
    }

    // Add native tool calls
    for (const call of toolCalls) {
      anthropicContent.push({
        type: 'tool_use',
        id: call.id,
        name: call.function.name,
        input: JSON.parse(call.function.arguments)
      });
    }
  }

  return {
    id: openaiResponse.id || `msg_${Date.now()}`,
    type: 'message',
    role: 'assistant',
    content: anthropicContent.length > 0 ? anthropicContent : [{ type: 'text', text: '' }],
    model: openaiResponse.model,
    stop_reason: choice.finish_reason === 'tool_calls' ? 'tool_use' :
                 choice.finish_reason === 'stop' ? 'end_turn' :
                 choice.finish_reason || 'end_turn',
    usage: {
      input_tokens: openaiResponse.usage?.prompt_tokens || 0,
      output_tokens: openaiResponse.usage?.completion_tokens || 0
    }
  };
}

/**
 * Make HTTP/HTTPS request
 */
function makeRequest(url, options, body) {
  return new Promise((resolve, reject) => {
    const parsedUrl = new URL(url);
    const protocol = parsedUrl.protocol === 'https:' ? https : http;

    const reqOptions = {
      hostname: parsedUrl.hostname,
      port: parsedUrl.port,
      path: parsedUrl.pathname + parsedUrl.search,
      method: options.method || 'POST',
      headers: options.headers || {}
    };

    const req = protocol.request(reqOptions, (res) => {
      let data = '';

      res.on('data', (chunk) => {
        data += chunk;
      });

      res.on('end', () => {
        resolve({
          status: res.statusCode,
          headers: res.headers,
          body: data
        });
      });
    });

    req.on('error', (err) => {
      reject(err);
    });

    if (body) {
      req.write(typeof body === 'string' ? body : JSON.stringify(body));
    }

    req.end();
  });
}

/**
 * Handle GLM provider requests
 */
async function handleGLM(anthropicBody, res) {
  const { model } = parseModel(anthropicBody.model);
  const emulateTools = !supportsNativeToolCalling('glm', model) && anthropicBody.tools;
  const openaiBody = anthropicToOpenAI(anthropicBody, emulateTools);
  openaiBody.model = model;

  log(`â†’ GLM: ${model}${emulateTools ? ' (tool emulation)' : ''}`, 'cyan');

  try {
    const response = await makeRequest(
      `${GLM_BASE_URL}/chat/completions`,
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${GLM_API_KEY}`,
          'Content-Type': 'application/json'
        }
      },
      openaiBody
    );

    if (response.status !== 200) {
      log(`âœ— GLM error: ${response.status}`, 'red');
      res.writeHead(response.status, { 'Content-Type': 'application/json' });
      res.end(response.body);
      return;
    }

    const openaiResponse = JSON.parse(response.body);
    const anthropicResponse = openaiToAnthropic(openaiResponse, emulateTools);

    log(`â† GLM: ${anthropicResponse.usage.output_tokens} tokens`, 'green');

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify(anthropicResponse));

  } catch (error) {
    log(`âœ— GLM error: ${error.message}`, 'red');
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'api_error',
        message: error.message
      }
    }));
  }
}

/**
 * Handle Featherless.ai provider requests (with tool emulation)
 */
async function handleFeatherless(anthropicBody, res) {
  if (!FEATHERLESS_API_KEY) {
    log(`âœ— Featherless: API key not configured`, 'red');
    res.writeHead(401, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'authentication_error',
        message: 'FEATHERLESS_API_KEY not set'
      }
    }));
    return;
  }

  const { model } = parseModel(anthropicBody.model);
  // Featherless abliterated models always need tool emulation
  const emulateTools = anthropicBody.tools && anthropicBody.tools.length > 0;
  const openaiBody = anthropicToOpenAI(anthropicBody, emulateTools);
  openaiBody.model = model;

  log(`â†’ Featherless: ${model}${emulateTools ? ' (tool emulation)' : ''}`, 'magenta');

  try {
    const response = await makeRequest(
      `${FEATHERLESS_BASE_URL}/chat/completions`,
      {
        method: 'POST',
        headers: {
          'Authorization': `Bearer ${FEATHERLESS_API_KEY}`,
          'Content-Type': 'application/json'
        }
      },
      openaiBody
    );

    if (response.status !== 200) {
      log(`âœ— Featherless error: ${response.status}`, 'red');
      res.writeHead(response.status, { 'Content-Type': 'application/json' });
      res.end(response.body);
      return;
    }

    const openaiResponse = JSON.parse(response.body);
    const anthropicResponse = openaiToAnthropic(openaiResponse, emulateTools);

    log(`â† Featherless: ${anthropicResponse.usage.output_tokens} tokens`, 'green');

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify(anthropicResponse));

  } catch (error) {
    log(`âœ— Featherless error: ${error.message}`, 'red');
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'api_error',
        message: error.message
      }
    }));
  }
}

/**
 * Handle Google Gemini provider requests
 */
async function handleGoogle(anthropicBody, res) {
  if (!GOOGLE_API_KEY) {
    log(`âœ— Google: API key not configured`, 'red');
    res.writeHead(401, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'authentication_error',
        message: 'GOOGLE_API_KEY not set'
      }
    }));
    return;
  }

  const { model } = parseModel(anthropicBody.model);
  const openaiBody = anthropicToOpenAI(anthropicBody, false);

  log(`â†’ Google: ${model}`, 'blue');

  try {
    // Google uses a different endpoint structure
    const response = await makeRequest(
      `${GOOGLE_BASE_URL}/models/${model}:generateContent?key=${GOOGLE_API_KEY}`,
      {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        }
      },
      {
        contents: openaiBody.messages.map(msg => ({
          role: msg.role === 'assistant' ? 'model' : 'user',
          parts: [{ text: msg.content }]
        })),
        generationConfig: {
          maxOutputTokens: openaiBody.max_tokens,
          temperature: openaiBody.temperature,
          topP: openaiBody.top_p
        }
      }
    );

    if (response.status !== 200) {
      log(`âœ— Google error: ${response.status}`, 'red');
      res.writeHead(response.status, { 'Content-Type': 'application/json' });
      res.end(response.body);
      return;
    }

    const googleResponse = JSON.parse(response.body);
    const content = googleResponse.candidates?.[0]?.content?.parts?.[0]?.text || '';

    const anthropicResponse = {
      id: `msg_${Date.now()}`,
      type: 'message',
      role: 'assistant',
      content: [{ type: 'text', text: content }],
      model: model,
      stop_reason: 'end_turn',
      usage: {
        input_tokens: googleResponse.usageMetadata?.promptTokenCount || 0,
        output_tokens: googleResponse.usageMetadata?.candidatesTokenCount || 0
      }
    };

    log(`â† Google: ${anthropicResponse.usage.output_tokens} tokens`, 'green');

    res.writeHead(200, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify(anthropicResponse));

  } catch (error) {
    log(`âœ— Google error: ${error.message}`, 'red');
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'api_error',
        message: error.message
      }
    }));
  }
}

/**
 * Handle Anthropic (native) provider requests - passthrough
 */
async function handleAnthropic(anthropicBody, res) {
  if (!ANTHROPIC_API_KEY) {
    log(`âœ— Anthropic: API key not configured`, 'red');
    res.writeHead(401, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'authentication_error',
        message: 'ANTHROPIC_API_KEY not set'
      }
    }));
    return;
  }

  const { model } = parseModel(anthropicBody.model);
  anthropicBody.model = model;

  log(`â†’ Anthropic: ${model}`, 'blue');

  try {
    const response = await makeRequest(
      `${ANTHROPIC_BASE_URL}/v1/messages`,
      {
        method: 'POST',
        headers: {
          'x-api-key': ANTHROPIC_API_KEY,
          'anthropic-version': '2023-06-01',
          'Content-Type': 'application/json'
        }
      },
      anthropicBody
    );

    log(`â† Anthropic: ${response.status}`, 'green');

    res.writeHead(response.status, { 'Content-Type': 'application/json' });
    res.end(response.body);

  } catch (error) {
    log(`âœ— Anthropic error: ${error.message}`, 'red');
    res.writeHead(500, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({
      type: 'error',
      error: {
        type: 'api_error',
        message: error.message
      }
    }));
  }
}

/**
 * Main request handler
 */
function handleRequest(req, res) {
  // CORS headers
  res.setHeader('Access-Control-Allow-Origin', '*');
  res.setHeader('Access-Control-Allow-Methods', 'GET, POST, OPTIONS');
  res.setHeader('Access-Control-Allow-Headers', 'Content-Type, Authorization, x-api-key, anthropic-version');

  if (req.method === 'OPTIONS') {
    res.writeHead(200);
    res.end();
    return;
  }

  // Only handle /v1/messages endpoint
  if (!req.url.includes('/v1/messages')) {
    res.writeHead(404, { 'Content-Type': 'application/json' });
    res.end(JSON.stringify({ error: 'Not found' }));
    return;
  }

  let body = '';
  req.on('data', (chunk) => {
    body += chunk;
  });

  req.on('end', async () => {
    try {
      const anthropicBody = JSON.parse(body);
      const { provider } = parseModel(anthropicBody.model);

      log(`${req.method} ${req.url} [${provider}]`, 'bright');

      // Route to appropriate provider
      switch (provider) {
        case 'glm':
          await handleGLM(anthropicBody, res);
          break;
        case 'featherless':
          await handleFeatherless(anthropicBody, res);
          break;
        case 'google':
          await handleGoogle(anthropicBody, res);
          break;
        case 'anthropic':
        default:
          await handleAnthropic(anthropicBody, res);
          break;
      }

    } catch (error) {
      log(`âœ— Request error: ${error.message}`, 'red');
      res.writeHead(400, { 'Content-Type': 'application/json' });
      res.end(JSON.stringify({
        type: 'error',
        error: {
          type: 'invalid_request_error',
          message: error.message
        }
      }));
    }
  });
}

/**
 * Start the proxy server
 */
const server = http.createServer(handleRequest);

server.listen(PORT, '127.0.0.1', () => {
  console.log('');
  log('â•'.repeat(70), 'bright');
  log(`Multi-Provider Proxy Server for Claude Code`, 'bright');
  log(`With Tool Calling Emulation for Abliterated Models`, 'bright');
  log('â•'.repeat(70), 'bright');
  console.log('');
  log(`ðŸš€ Server running on http://127.0.0.1:${PORT}`, 'green');
  console.log('');
  log('Supported Providers:', 'bright');
  log(`  ${GLM_API_KEY ? 'âœ“' : 'âœ—'} GLM (ZhipuAI)     - glm/glm-4`, GLM_API_KEY ? 'green' : 'dim');
  log(`  ${FEATHERLESS_API_KEY ? 'âœ“' : 'âœ—'} Featherless.ai   - featherless/model-name (tool emulation)`, FEATHERLESS_API_KEY ? 'green' : 'dim');
  log(`  ${GOOGLE_API_KEY ? 'âœ“' : 'âœ—'} Google Gemini    - google/gemini-pro`, GOOGLE_API_KEY ? 'green' : 'dim');
  log(`  ${ANTHROPIC_API_KEY ? 'âœ“' : 'âœ—'} Anthropic        - anthropic/claude-sonnet-4-5 (or no prefix)`, ANTHROPIC_API_KEY ? 'green' : 'dim');
  console.log('');
  log('Features:', 'bright');
  log('  âœ“ Tool calling emulation for abliterated models', 'green');
  log('  âœ“ Seamless model switching with /model command', 'green');
  log('  âœ“ Full MCP tool support across all providers', 'green');
  console.log('');
  log('Usage:', 'bright');
  log(`  # Start Claude Code with proxy:`, 'cyan');
  log(`  ANTHROPIC_BASE_URL=http://127.0.0.1:${PORT} claude`, 'cyan');
  console.log('');
  log(`  # Switch models:`, 'cyan');
  log(`  /model glm/glm-4`, 'cyan');
  log(`  /model featherless/Llama-3-8B-Instruct-abliterated`, 'cyan');
  log(`  /model google/gemini-pro`, 'cyan');
  log(`  /model anthropic/claude-opus-4-5`, 'cyan');
  console.log('');
  log('Environment Variables:', 'bright');
  log(`  GLM_API_KEY=${GLM_API_KEY ? GLM_API_KEY.substring(0, 20) + '...' : '(not set)'}`, 'dim');
  log(`  FEATHERLESS_API_KEY=${FEATHERLESS_API_KEY ? FEATHERLESS_API_KEY.substring(0, 20) + '...' : '(not set)'}`, 'dim');
  log(`  GOOGLE_API_KEY=${GOOGLE_API_KEY ? GOOGLE_API_KEY.substring(0, 20) + '...' : '(not set)'}`, 'dim');
  log(`  ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY ? ANTHROPIC_API_KEY.substring(0, 20) + '...' : '(not set)'}`, 'dim');
  console.log('');
  log('â•'.repeat(70), 'bright');
  console.log('');
  log('Ready to proxy requests with full tool support...', 'green');
  console.log('');
});

// Graceful shutdown
process.on('SIGINT', () => {
  console.log('');
  log('Shutting down proxy server...', 'yellow');
  server.close(() => {
    log('Server stopped', 'dim');
    process.exit(0);
  });
});
