# SPLICE - Architecture Documentation

**Version:** 1.1.0
**Date:** December 16, 2025
**Status:** Production-Ready Architecture (Updated with Research Findings)

---

## Table of Contents

1. [System Architecture Overview](#1-system-architecture-overview)
2. [Technical Stack](#2-technical-stack)
3. [Core Modules](#3-core-modules)
4. [Data Models](#4-data-models)
5. [Processing Pipeline](#5-processing-pipeline)
6. [API Integration Patterns](#6-api-integration-patterns)
7. [Premiere Pro Integration](#7-premiere-pro-integration)
8. [UI/UX Design](#8-uiux-design)
9. [Development Phases](#9-development-phases)
10. [File Structure](#10-file-structure)
11. [Build and Distribution](#11-build-and-distribution)
12. [Pricing and Business Model](#12-pricing-and-business-model)
13. [SaaS Web Application](#13-saas-web-application)

---

## 1. System Architecture Overview

### High-Level Component Diagram

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         SPLICE PLUGIN ARCHITECTURE                           │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                              │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                      PREMIERE PRO HOST                               │    │
│  │  ┌─────────────────┐    ┌──────────────────────────────────────┐   │    │
│  │  │   Project       │    │          SPLICE CEP PANEL            │   │    │
│  │  │   - Bins        │◄──►│  ┌────────────────────────────────┐  │   │    │
│  │  │   - Clips       │    │  │       React + TypeScript       │  │   │    │
│  │  │   - Sequences   │    │  │  ┌──────────┐ ┌─────────────┐  │  │   │    │
│  │  └─────────────────┘    │  │  │ DropZone │ │ TakeGroups  │  │  │   │    │
│  │                         │  │  └──────────┘ └─────────────┘  │  │   │    │
│  │  ┌─────────────────┐    │  │  ┌──────────┐ ┌─────────────┐  │  │   │    │
│  │  │   Timeline      │    │  │  │ Progress │ │  Settings   │  │  │   │    │
│  │  │   - Tracks      │◄──►│  │  └──────────┘ └─────────────┘  │  │   │    │
│  │  │   - Markers     │    │  └────────────────────────────────┘  │   │    │
│  │  └─────────────────┘    │               │                       │   │    │
│  │                         │               │ evalTS()               │   │    │
│  │  ┌─────────────────┐    │               ▼                       │   │    │
│  │  │  ExtendScript   │◄───│  ┌────────────────────────────────┐  │   │    │
│  │  │  - timeline.ts  │    │  │      ExtendScript Bridge       │  │   │    │
│  │  │  - clips.ts     │    │  │  - Timeline manipulation       │  │   │    │
│  │  │  - markers.ts   │    │  │  - Color coding                │  │   │    │
│  │  └─────────────────┘    │  │  - Bin organization            │  │   │    │
│  │                         │  └────────────────────────────────┘  │   │    │
│  │                         └──────────────────────────────────────┘   │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                        │                                     │
│                                        │ HTTP/HTTPS                          │
│                                        ▼                                     │
│  ┌─────────────────────────────────────────────────────────────────────┐    │
│  │                        PROCESSING LAYER                              │    │
│  │                                                                      │    │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌────────────┐ │    │
│  │  │   FFmpeg    │  │LALAL.AI/    │  │  Deepgram   │  │ GPT-4o-mini│ │    │
│  │  │   (Local)   │  │Auphonic/    │  │  Nova-3     │  │ Batch API  │ │    │
│  │  │             │  │Dolby.io     │  │             │  │            │ │    │
│  │  │ - Extract   │  │ - Voice     │  │ - Transcribe│  │ - Analyze  │ │    │
│  │  │   Audio     │  │   Isolation │  │ - Timestamps│  │   Takes    │ │    │
│  │  └─────────────┘  └─────────────┘  └─────────────┘  └────────────┘ │    │
│  │                                                                      │    │
│  │  ┌─────────────┐  ┌─────────────────────────────────────────────┐  │    │
│  │  │ Silero VAD  │  │           sentence-transformers              │  │    │
│  │  │   (Local)   │  │                 (Local)                      │  │    │
│  │  │             │  │                                              │  │    │
│  │  │ - Silence   │  │ - Generate embeddings (all-MiniLM-L6-v2)    │  │    │
│  │  │   Detection │  │ - Cosine similarity clustering              │  │    │
│  │  └─────────────┘  └─────────────────────────────────────────────┘  │    │
│  └─────────────────────────────────────────────────────────────────────┘    │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Data Flow

```
┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐
│  User    │     │   Extract    │     │    Voice      │     │  Transcribe  │
│ Selects  │────►│   Audio      │────►│   Isolation   │────►│  + Timestamp │
│  Clips   │     │   (FFmpeg)   │     │(LALAL.AI/etc) │     │  (Deepgram)  │
└──────────┘     └──────────────┘     └───────────────┘     └──────────────┘
                                                                    │
                                                                    ▼
┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐
│ Timeline │     │   Organize   │     │    Detect     │     │   Detect     │
│  Import  │◄────│   & Color    │◄────│    Silence    │◄────│   Takes      │
│          │     │   Code       │     │  (Silero VAD) │     │  (Embed+LLM) │
└──────────┘     └──────────────┘     └───────────────┘     └──────────────┘
```

### Technology Decisions

| Component | Technology | Rationale |
|-----------|------------|-----------|
| Plugin Platform | CEP | Production-ready, full API access, UXP still in beta |
| UI Framework | React + TypeScript | Best Bolt CEP support, type safety |
| Build Tool | Vite | 10-100x faster than webpack |
| Voice Isolation | LALAL.AI / Auphonic / Dolby.io | Best value (~$0.02-$0.04/min), handles loud music/wind |
| Voice Isolation (Local) | Demucs v4 | Free open source option, high quality |
| Transcription | Deepgram Nova-3 | Native word timestamps, $0.0043/min |
| Take Detection | Embeddings + GPT-4o-mini Batch | Hybrid approach: fast filtering + LLM analysis |
| Silence Detection | Silero VAD | ML-based, 95%+ accuracy, local processing |
| Premiere API | ExtendScript | Only option for timeline manipulation |
| User Interaction | Panel with Selection Detection | Native context menus cannot be customized |

### Voice Isolation Cost Comparison

> **Important:** ElevenLabs (~$0.20/min) is NOT recommended due to high cost relative to alternatives.

| Service | Cost per Minute | Best For |
|---------|----------------|----------|
| **Picovoice Koala** | ~$0.005/min | Best SDK, on-device processing |
| **Auphonic** | ~$0.015-$0.02/min | Best value with full REST API |
| **LALAL.AI** | ~$0.02-$0.04/min | Excellent for music separation |
| **Dolby.io** | ~$0.03/min | Best for mixed conditions (wind, noise) |
| **Demucs v4** | Free | Open source, high quality, local processing |

**Primary Recommendation:** Auphonic or LALAL.AI for cloud processing. These handle loud music, wind, and low dialogue volume effectively at 10x lower cost than ElevenLabs.

---

## 2. Technical Stack

### Frontend (CEP Panel)

```yaml
Framework: React 18.2+
Language: TypeScript 5.0+
Build Tool: Vite 5.0+ (via Bolt CEP)
UI Components: Adobe Spectrum Web Components
State Management: Zustand (or React Context)
Styling: Tailwind CSS + CSS Modules
```

### Backend (Processing Services)

```yaml
Audio Extraction: FFmpeg 6.0+ (bundled binary)
Voice Isolation: LALAL.AI / Auphonic / Dolby.io (cloud) or Demucs v4 (local)
Transcription: Deepgram Nova-3 API ($0.0043/min)
Embeddings: sentence-transformers (all-MiniLM-L6-v2)
Clustering: scikit-learn AgglomerativeClustering
LLM Analysis: OpenAI GPT-4o-mini Batch API ($0.075/1M input tokens)
Silence Detection: Silero VAD 4.0+
Rate Limiting: Redis + rate-limiter-flexible + BullMQ
```

### Premiere Integration

```yaml
Panel Platform: CEP 12
Scripting: ExtendScript (ECMAScript 3)
Communication: evalTS() via Bolt CEP
Supported Premiere: 22.0+ (2022 and later)
```

### Development Tools

```yaml
Package Manager: pnpm or yarn
Node.js: 18.0+ (LTS)
IDE: VS Code
Debugging: Chrome DevTools (localhost:8088)
ExtendScript Debug: Adobe ExtendScript Debugger extension
```

---

## 3. Core Modules

### 3.1 DropZone/Import Module

```typescript
// src/js/main/modules/import/DropZone.ts

interface ImportModule {
  // Events
  onFilesDropped: (files: File[]) => void;
  onClipsSelected: (clips: ClipInfo[]) => void;

  // Methods
  validateFiles(files: File[]): ValidationResult;
  extractClipInfo(files: File[]): Promise<ClipInfo[]>;
  getSelectedClips(): Promise<ClipInfo[]>;
}

interface ValidationResult {
  valid: boolean;
  errors: string[];
  warnings: string[];
  supportedFormats: string[];
}
```

### 3.2 Audio Extraction Module

```typescript
// src/js/main/modules/audio/AudioExtractor.ts

interface AudioExtractorModule {
  extractAudio(clipPath: string, options: ExtractionOptions): Promise<AudioBuffer>;
  getAudioInfo(clipPath: string): Promise<AudioInfo>;
  splitAudio(buffer: AudioBuffer, chunks: number): AudioBuffer[];
}

interface ExtractionOptions {
  format: 'wav' | 'mp3' | 'pcm';
  sampleRate: 16000 | 44100 | 48000;
  channels: 1 | 2;
  bitDepth?: 16 | 24 | 32;
}

interface AudioInfo {
  duration: number;
  sampleRate: number;
  channels: number;
  format: string;
  bitRate: number;
}
```

### 3.3 Voice Isolation Module

```typescript
// src/js/main/modules/isolation/VoiceIsolator.ts

interface VoiceIsolationModule {
  isolate(audioBuffer: ArrayBuffer): Promise<ArrayBuffer>;
  isAvailable(): boolean;
  estimateCost(durationMinutes: number): number;
}

interface IsolationConfig {
  apiKey: string;
  outputFormat: 'pcm_s16le_16' | 'mp3' | 'wav';
  timeout: number;
}
```

### 3.4 Transcription Module

```typescript
// src/js/main/modules/transcription/Transcriber.ts

interface TranscriptionModule {
  transcribe(audioBuffer: ArrayBuffer): Promise<Transcript>;
  transcribeWithDiarization(audioBuffer: ArrayBuffer): Promise<DiarizedTranscript>;
  estimateCost(durationMinutes: number): number;
}

interface TranscriptWord {
  text: string;
  start: number;      // seconds
  end: number;        // seconds
  confidence: number; // 0-1
  speaker?: string;   // "Speaker 1", "Speaker 2"
}

interface Transcript {
  text: string;
  words: TranscriptWord[];
  duration: number;
  language: string;
}
```

### 3.5 Take Detection Module

```typescript
// src/js/main/modules/takes/TakeDetector.ts

interface TakeDetectionModule {
  detectTakes(transcript: Transcript): Promise<TakeGroup[]>;
  generateEmbeddings(sentences: string[]): Promise<number[][]>;
  clusterBySimilarity(embeddings: number[][], threshold: number): number[];
  analyzeTakesWithLLM(candidates: TakeCandidate[]): Promise<TakeAnalysis>;
}

interface TakeCandidate {
  sentences: string[];
  timestamps: TimeRange[];
  similarityScore: number;
}

interface TakeAnalysis {
  groups: TakeGroup[];
  bestTakes: Record<string, number>; // groupId -> takeIndex
  confidence: number;
}
```

### 3.6 Silence Detection Module

```typescript
// src/js/main/modules/silence/SilenceDetector.ts

interface SilenceDetectionModule {
  detectSilence(audioBuffer: ArrayBuffer): Promise<SilentSection[]>;
  detectFromTranscript(words: TranscriptWord[]): SilentSection[];
  filterNaturalPauses(sections: SilentSection[]): SilentSection[];
}

interface SilenceConfig {
  minDuration: number;      // seconds (default: 1.5)
  threshold: number;        // dB (default: -40)
  preservePauseDuration: number; // seconds (default: 0.8)
}
```

### 3.7 Timeline Integration Module

```typescript
// src/js/main/modules/premiere/TimelineIntegrator.ts

interface TimelineModule {
  // Bin operations
  createBin(name: string, parentPath?: string): Promise<string>;
  moveToBin(clipId: string, binPath: string): Promise<void>;

  // Color coding
  setClipColor(clipId: string, colorIndex: number): Promise<void>;

  // Timeline operations
  insertClip(clipId: string, trackIndex: number, time: number): Promise<void>;
  setInOutPoints(clipId: string, inPoint: number, outPoint: number): Promise<void>;

  // Markers
  createMarker(time: number, name: string, color: number): Promise<void>;
}

// Color mapping for takes
const TAKE_COLORS = {
  1: 5,   // Rose
  2: 3,   // Cerulean
  3: 6,   // Mango
  4: 7,   // Purple
  5: 4,   // Forest
  6: 9,   // Lavender
  7: 12,  // Orange
  8: 14,  // Blue
} as const;
```

### 3.8 Settings Module

```typescript
// src/js/main/modules/settings/SettingsManager.ts

interface SettingsModule {
  get<T>(key: string): T | undefined;
  set<T>(key: string, value: T): void;
  getAll(): Settings;
  reset(): void;
  export(): string;
  import(json: string): void;
}

interface Settings {
  // Voice Isolation
  useVoiceIsolation: boolean;
  voiceIsolationProvider: 'auphonic' | 'lalalai' | 'dolbyio' | 'picovoice' | 'demucs';
  audioOutputMode: 'original' | 'isolated' | 'none';

  // Transcription
  transcriptionProvider: 'deepgram' | 'assemblyai' | 'whisper';
  language: string;

  // Take Detection
  similarityThreshold: number;  // 0.70-0.95
  useLLMAnalysis: boolean;

  // Silence Detection
  minSilenceDuration: number;   // seconds
  silenceThreshold: number;     // dB
  preserveNaturalPauses: boolean;

  // Timeline
  autoColorCode: boolean;
  createBins: boolean;
  addMarkers: boolean;

  // API Keys (stored securely via CredentialManager)
  auphonicApiKey?: string;
  lalalaiApiKey?: string;
  dolbyioApiKey?: string;
  deepgramApiKey: string;
  openaiApiKey: string;
}
```

---

## 4. Data Models

### Core Interfaces

```typescript
// src/js/main/types/models.ts

// === CLIP MODELS ===

interface ClipInfo {
  id: string;
  name: string;
  path: string;
  duration: number;        // seconds
  inPoint: number;         // source in point
  outPoint: number;        // source out point
  hasAudio: boolean;
  hasVideo: boolean;
  audioChannels: number;
  frameRate: number;
  resolution: { width: number; height: number };
}

// === TAKE MODELS ===

interface Take {
  id: string;
  clipId: string;
  takeNumber: number;
  text: string;
  startTime: number;       // seconds in source
  endTime: number;
  duration: number;
  confidence: number;      // 0-1
  isPartial: boolean;      // incomplete take
  isBest: boolean;         // suggested best take
  audioClarity: number;    // 0-1 (if analyzed)
}

interface TakeGroup {
  id: string;
  canonicalText: string;   // reference text for this line
  takes: Take[];
  similarityType: 'exact' | 'minor_variation' | 'paraphrase';
  suggestedBestTakeId: string | null;
}

// === TRANSCRIPT MODELS ===

interface Word {
  text: string;
  start: number;           // seconds
  end: number;             // seconds
  confidence: number;
  speaker?: string;
  punctuatedText?: string;
}

interface Sentence {
  text: string;
  words: Word[];
  start: number;
  end: number;
  speaker?: string;
}

interface Transcript {
  id: string;
  clipId: string;
  fullText: string;
  words: Word[];
  sentences: Sentence[];
  speakers: string[];
  language: string;
  duration: number;
}

// === SILENCE MODELS ===

interface SilentSection {
  id: string;
  startTime: number;
  endTime: number;
  duration: number;
  type: 'dead_space' | 'natural_pause' | 'take_boundary';
  shouldRemove: boolean;
  afterWord?: string;
  beforeWord?: string;
}

// === PROCESSING MODELS ===

interface ProcessingJob {
  id: string;
  clipId: string;
  status: JobStatus;
  progress: number;        // 0-100
  currentStep: ProcessingStep;
  startedAt: Date;
  completedAt?: Date;
  error?: ProcessingError;
  result?: ProcessingResult;
}

type JobStatus = 'pending' | 'processing' | 'completed' | 'failed' | 'cancelled';

type ProcessingStep =
  | 'extracting_audio'
  | 'isolating_voice'
  | 'transcribing'
  | 'detecting_takes'
  | 'detecting_silence'
  | 'organizing'
  | 'complete';

interface ProcessingError {
  code: string;
  message: string;
  step: ProcessingStep;
  retryable: boolean;
}

interface ProcessingResult {
  clipId: string;
  transcript: Transcript;
  takeGroups: TakeGroup[];
  silentSections: SilentSection[];
  processingTime: number;  // ms
  apiCosts: {
    voiceIsolation: number;
    transcription: number;
    llmAnalysis: number;
    total: number;
  };
}

// === PREMIERE MODELS ===

interface PremiereClip {
  projectItemId: string;
  name: string;
  binPath: string;
  colorLabel: number;
  inPoint: number;
  outPoint: number;
}

interface PremiereTimeline {
  sequenceId: string;
  name: string;
  duration: number;
  videoTracks: number;
  audioTracks: number;
}
```

---

## 5. Processing Pipeline

### Main Processing Flow

```typescript
// src/js/main/services/ProcessingPipeline.ts

class ProcessingPipeline {
  private audioExtractor: AudioExtractorModule;
  private voiceIsolator: VoiceIsolationModule;
  private transcriber: TranscriptionModule;
  private takeDetector: TakeDetectionModule;
  private silenceDetector: SilenceDetectionModule;
  private timelineIntegrator: TimelineModule;

  async processClip(clip: ClipInfo, options: ProcessingOptions): Promise<ProcessingResult> {
    const job = this.createJob(clip);

    try {
      // Step 1: Extract audio from video
      this.updateProgress(job, 'extracting_audio', 10);
      const audioBuffer = await this.audioExtractor.extractAudio(clip.path, {
        format: 'wav',
        sampleRate: 16000,
        channels: 1
      });

      // Step 2: Voice isolation (optional)
      let analysisAudio = audioBuffer;
      let outputAudio = audioBuffer;

      if (options.useVoiceIsolation) {
        this.updateProgress(job, 'isolating_voice', 25);
        const isolatedAudio = await this.voiceIsolator.isolate(audioBuffer);
        analysisAudio = isolatedAudio;

        if (options.audioOutputMode === 'isolated') {
          outputAudio = isolatedAudio;
        }
      }

      // Step 3: Transcription with word-level timestamps
      this.updateProgress(job, 'transcribing', 40);
      const transcript = await this.transcriber.transcribe(analysisAudio);

      // Step 4: Take detection
      this.updateProgress(job, 'detecting_takes', 60);
      const takeGroups = await this.takeDetector.detectTakes(transcript);

      // Step 5: Silence detection
      this.updateProgress(job, 'detecting_silence', 75);
      const silentSections = await this.silenceDetector.detectFromTranscript(transcript.words);
      const filteredSilence = this.silenceDetector.filterNaturalPauses(silentSections);

      // Step 6: Organize in Premiere
      this.updateProgress(job, 'organizing', 90);
      await this.organizeInPremiere(clip, takeGroups, filteredSilence, options);

      this.updateProgress(job, 'complete', 100);

      return {
        clipId: clip.id,
        transcript,
        takeGroups,
        silentSections: filteredSilence,
        processingTime: Date.now() - job.startedAt.getTime(),
        apiCosts: this.calculateCosts(job)
      };

    } catch (error) {
      job.status = 'failed';
      job.error = this.createError(error);
      throw error;
    }
  }

  private async organizeInPremiere(
    clip: ClipInfo,
    takeGroups: TakeGroup[],
    silentSections: SilentSection[],
    options: ProcessingOptions
  ): Promise<void> {

    // Create bin structure
    if (options.createBins) {
      const mainBin = await this.timelineIntegrator.createBin('SPLICE Takes');

      for (const group of takeGroups) {
        const groupBin = await this.timelineIntegrator.createBin(
          `Line ${group.id}`,
          mainBin
        );

        for (const take of group.takes) {
          // Create subclip with in/out points
          // Move to appropriate bin
          // Set color based on take number
          await this.timelineIntegrator.setClipColor(
            take.clipId,
            TAKE_COLORS[take.takeNumber] || 0
          );
        }
      }
    }

    // Add markers for take boundaries
    if (options.addMarkers) {
      for (const group of takeGroups) {
        for (const take of group.takes) {
          await this.timelineIntegrator.createMarker(
            take.startTime,
            `Take ${take.takeNumber}: ${group.canonicalText.substring(0, 30)}...`,
            take.isBest ? 1 : 0  // Red for best, green for others
          );
        }
      }
    }
  }
}
```

### Take Detection Algorithm

```typescript
// src/js/main/services/TakeDetectionService.ts

class TakeDetectionService implements TakeDetectionModule {
  private embeddingModel: SentenceTransformer;
  private openaiClient: OpenAI;

  async detectTakes(transcript: Transcript): Promise<TakeGroup[]> {
    // 1. Segment transcript into sentences
    const sentences = this.segmentIntoSentences(transcript);

    // 2. Generate embeddings for all sentences
    const embeddings = await this.generateEmbeddings(
      sentences.map(s => s.text)
    );

    // 3. Compute similarity matrix
    const similarityMatrix = this.computeCosineSimilarity(embeddings);

    // 4. Find candidate pairs (similarity > threshold)
    const candidates = this.findCandidatePairs(
      sentences,
      similarityMatrix,
      0.80  // threshold
    );

    // 5. Cluster into groups
    const clusters = this.clusterCandidates(candidates);

    // 6. Analyze with LLM for confirmation and best-take selection
    const analysis = await this.analyzeTakesWithLLM(clusters);

    // 7. Build TakeGroup objects
    return this.buildTakeGroups(sentences, clusters, analysis);
  }

  private computeCosineSimilarity(embeddings: number[][]): number[][] {
    const n = embeddings.length;
    const matrix: number[][] = Array(n).fill(null).map(() => Array(n).fill(0));

    for (let i = 0; i < n; i++) {
      for (let j = i; j < n; j++) {
        const sim = this.cosineSim(embeddings[i], embeddings[j]);
        matrix[i][j] = sim;
        matrix[j][i] = sim;
      }
    }

    return matrix;
  }

  private cosineSim(a: number[], b: number[]): number {
    let dotProduct = 0;
    let normA = 0;
    let normB = 0;

    for (let i = 0; i < a.length; i++) {
      dotProduct += a[i] * b[i];
      normA += a[i] * a[i];
      normB += b[i] * b[i];
    }

    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));
  }

  private async analyzeTakesWithLLM(clusters: TakeCluster[]): Promise<LLMAnalysis> {
    const prompt = this.buildAnalysisPrompt(clusters);

    const response = await this.openaiClient.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: TAKE_ANALYSIS_SYSTEM_PROMPT },
        { role: 'user', content: prompt }
      ],
      response_format: { type: 'json_object' }
    });

    return JSON.parse(response.choices[0].message.content);
  }
}

const TAKE_ANALYSIS_SYSTEM_PROMPT = `
You are analyzing transcript segments to identify repeated takes.

For each group of similar sentences, determine:
1. Whether they represent the same intended dialogue (different takes)
2. The similarity type: exact, minor_variation, or paraphrase
3. Which take appears to be the "best" (most complete, clearest)

Return JSON with this structure:
{
  "groups": [
    {
      "id": "string",
      "isTakeGroup": boolean,
      "similarityType": "exact" | "minor_variation" | "paraphrase",
      "bestTakeIndex": number,
      "confidence": number
    }
  ]
}
`;
```

---

## 6. API Integration Patterns

### Rate Limiting and Retry Logic

```typescript
// src/js/main/utils/apiClient.ts

interface RetryConfig {
  maxRetries: number;
  baseDelay: number;
  maxDelay: number;
  retryableStatuses: number[];
}

const DEFAULT_RETRY_CONFIG: RetryConfig = {
  maxRetries: 3,
  baseDelay: 1000,
  maxDelay: 30000,
  retryableStatuses: [429, 500, 502, 503, 504]
};

async function fetchWithRetry<T>(
  url: string,
  options: RequestInit,
  config: RetryConfig = DEFAULT_RETRY_CONFIG
): Promise<T> {
  let lastError: Error;

  for (let attempt = 0; attempt <= config.maxRetries; attempt++) {
    try {
      const response = await fetch(url, options);

      if (!response.ok) {
        if (config.retryableStatuses.includes(response.status)) {
          throw new RetryableError(response.status, await response.text());
        }
        throw new ApiError(response.status, await response.text());
      }

      return await response.json();

    } catch (error) {
      lastError = error;

      if (error instanceof RetryableError && attempt < config.maxRetries) {
        const delay = Math.min(
          config.baseDelay * Math.pow(2, attempt),
          config.maxDelay
        );
        await sleep(delay);
        continue;
      }

      throw error;
    }
  }

  throw lastError;
}
```

### Production Rate Limiting Architecture

**Algorithm Selection:**
- **API-level limiting:** Token Bucket algorithm
- **Monthly quotas:** Sliding Window Counter
- **Storage:** Redis with `rate-limiter-flexible` library
- **Job queue:** BullMQ for processing queue management

**External API Limits:**

| Service | Concurrent Limit | Notes |
|---------|-----------------|-------|
| LALAL.AI | 10 requests | Per API key |
| Deepgram | 100 requests | STT concurrent limit |
| OpenAI | Varies | TPM/RPM limits by tier |

```typescript
// src/js/main/services/RateLimiter.ts

import { RateLimiterRedis } from 'rate-limiter-flexible';
import Redis from 'ioredis';

const redis = new Redis();

// Token bucket for API calls
const apiLimiter = new RateLimiterRedis({
  storeClient: redis,
  keyPrefix: 'splice_api',
  points: 10,           // requests
  duration: 1,          // per second
  blockDuration: 60,    // block for 60s if exceeded
});

// Sliding window for monthly quotas
const monthlyLimiter = new RateLimiterRedis({
  storeClient: redis,
  keyPrefix: 'splice_monthly',
  points: 600,          // 10 hours = 600 minutes (Starter tier)
  duration: 30 * 24 * 60 * 60, // 30 days
});

interface RateLimitResult {
  allowed: boolean;
  retryAfter?: number;
  reason?: 'quota_exceeded' | 'rate_limited';
}

// Middleware implementation
async function rateLimitMiddleware(userId: string, minutes: number): Promise<RateLimitResult> {
  try {
    await apiLimiter.consume(userId, 1);
    await monthlyLimiter.consume(userId, minutes);
    return { allowed: true };
  } catch (error: any) {
    return {
      allowed: false,
      retryAfter: error.msBeforeNext / 1000,
      reason: error.remainingPoints === 0 ? 'quota_exceeded' : 'rate_limited'
    };
  }
}
```

**Graceful Degradation:**
- Return `Retry-After` headers when rate limited
- Queue jobs in BullMQ when at capacity
- Per-user usage counters for quota tracking

### API Key Management

```typescript
// src/js/main/services/CredentialManager.ts

import keytar from 'keytar';

const SERVICE_NAME = 'SPLICE_Plugin';

class CredentialManager {
  private static instance: CredentialManager;
  private cache: Map<string, string> = new Map();

  static getInstance(): CredentialManager {
    if (!this.instance) {
      this.instance = new CredentialManager();
    }
    return this.instance;
  }

  async setApiKey(provider: ApiProvider, key: string): Promise<void> {
    await keytar.setPassword(SERVICE_NAME, provider, key);
    this.cache.set(provider, key);
  }

  async getApiKey(provider: ApiProvider): Promise<string | null> {
    if (this.cache.has(provider)) {
      return this.cache.get(provider)!;
    }

    const key = await keytar.getPassword(SERVICE_NAME, provider);
    if (key) {
      this.cache.set(provider, key);
    }
    return key;
  }

  async deleteApiKey(provider: ApiProvider): Promise<void> {
    await keytar.deletePassword(SERVICE_NAME, provider);
    this.cache.delete(provider);
  }

  async validateApiKey(provider: ApiProvider, key: string): Promise<boolean> {
    switch (provider) {
      case 'elevenlabs':
        return this.validateElevenLabsKey(key);
      case 'deepgram':
        return this.validateDeepgramKey(key);
      case 'openai':
        return this.validateOpenAIKey(key);
      default:
        return false;
    }
  }
}

type ApiProvider = 'elevenlabs' | 'deepgram' | 'openai';
```

### Cost Tracking

```typescript
// src/js/main/services/CostTracker.ts

interface CostRates {
  voiceIsolation: {
    lalalai: { perMinute: number };
    dolbyio: { perMinute: number };
    auphonic: { perMinute: number };
    picovoice: { perMinute: number };
    demucs: { perMinute: number };
  };
  deepgram: {
    perMinute: number;  // $0.0043
  };
  openai: {
    inputPer1M: number;   // $0.075 (batch pricing)
    outputPer1M: number;  // $0.30
  };
}

const DEFAULT_RATES: CostRates = {
  voiceIsolation: {
    lalalai: { perMinute: 0.03 },
    dolbyio: { perMinute: 0.03 },
    auphonic: { perMinute: 0.02 },
    picovoice: { perMinute: 0.005 },
    demucs: { perMinute: 0 }  // Free, local processing
  },
  deepgram: { perMinute: 0.0043 },
  openai: { inputPer1M: 0.075, outputPer1M: 0.30 }  // GPT-4o-mini batch pricing
};

interface VoiceIsolationProvider {
  id: 'lalalai' | 'dolbyio' | 'auphonic' | 'picovoice' | 'demucs';
  name: string;
  costPerMinute: number;
  isLocal: boolean;
}

const RECOMMENDED_PROVIDERS: VoiceIsolationProvider[] = [
  { id: 'auphonic', name: 'Auphonic', costPerMinute: 0.02, isLocal: false },
  { id: 'lalalai', name: 'LALAL.AI', costPerMinute: 0.03, isLocal: false },
  { id: 'dolbyio', name: 'Dolby.io', costPerMinute: 0.03, isLocal: false },
  { id: 'demucs', name: 'Demucs v4', costPerMinute: 0, isLocal: true }
];

class CostTracker {
  private rates: CostRates;
  private usage: UsageRecord = {
    elevenlabs: { minutes: 0, cost: 0 },
    deepgram: { minutes: 0, cost: 0 },
    openai: { inputTokens: 0, outputTokens: 0, cost: 0 }
  };

  estimateCost(durationMinutes: number, options: ProcessingOptions): CostEstimate {
    let total = 0;

    // Transcription (always needed)
    const transcriptionCost = durationMinutes * this.rates.deepgram.perMinute;
    total += transcriptionCost;

    // Voice isolation (optional)
    let isolationCost = 0;
    if (options.useVoiceIsolation) {
      isolationCost = durationMinutes * this.rates.elevenlabs.perMinute;
      total += isolationCost;
    }

    // LLM analysis (estimated based on transcript length)
    const estimatedTokens = durationMinutes * 500; // ~500 tokens per minute
    const llmCost = (estimatedTokens / 1_000_000) * this.rates.openai.inputPer1M +
                    (estimatedTokens / 4 / 1_000_000) * this.rates.openai.outputPer1M;
    total += llmCost;

    return {
      transcription: transcriptionCost,
      voiceIsolation: isolationCost,
      llmAnalysis: llmCost,
      total
    };
  }
}
```

---

## 7. Premiere Pro Integration

> **Important Discovery:** Native right-click context menu customization is NOT possible in Adobe Premiere Pro. The solution is a CEP panel with selection detection.

### Selection Detection and Panel Workflow

Since native context menus cannot be customized, SPLICE uses a panel-based approach with automatic selection detection.

**Workflow:**
1. User selects clips on timeline
2. SPLICE panel detects selection change
3. Panel displays action buttons
4. User clicks "Process with Voice Isolation" or "Process without Voice Isolation"

```
┌──────────────────────────────────────────────────────────────────┐
│                        UPDATED WORKFLOW                           │
├──────────────────────────────────────────────────────────────────┤
│                                                                   │
│  1. Select clips on timeline                                      │
│           │                                                       │
│           ▼                                                       │
│  2. SPLICE panel auto-detects selection                          │
│           │                                                       │
│           ▼                                                       │
│  3. Panel shows: [Process with Voice Isolation]                   │
│                  [Process without Voice Isolation]                │
│           │                                                       │
│           ▼                                                       │
│  4. Click button → Processing begins                              │
│                                                                   │
└──────────────────────────────────────────────────────────────────┘
```

**Selection Detection Hook:**

```typescript
// src/js/main/hooks/useSelectionDetection.ts

import { useEffect, useState } from 'react';
import { evalTS } from '../utils/bolt';

interface ClipInfo {
  id: string;
  name: string;
  duration: number;
}

export function useSelectionDetection() {
  const [selectedClips, setSelectedClips] = useState<ClipInfo[]>([]);

  useEffect(() => {
    // Bind to Premiere's selection change event
    const csInterface = new CSInterface();

    const handleSelectionChange = async () => {
      const result = await evalTS('getSelectedClips');
      const clips = JSON.parse(result);
      setSelectedClips(clips);
    };

    csInterface.addEventListener(
      'com.adobe.csxs.events.SelectionChanged',
      handleSelectionChange
    );

    // Initial check
    handleSelectionChange();

    // Cleanup
    return () => {
      csInterface.removeEventListener(
        'com.adobe.csxs.events.SelectionChanged',
        handleSelectionChange
      );
    };
  }, []);

  return selectedClips;
}
```

### ExtendScript Functions

```typescript
// src/jsx/index.ts

// === CLIP OPERATIONS ===

export function getSelectedClips(): string {
  const selection = app.project.activeSequence?.getSelection() || [];
  const clips = [];

  for (let i = 0; i < selection.length; i++) {
    clips.push({
      id: selection[i].projectItem.nodeId,
      name: selection[i].name,
      duration: selection[i].duration.seconds
    });
  }

  return JSON.stringify(clips);
}

export function setClipColor(clipId: string, colorIndex: number): string {
  const item = findProjectItemById(clipId);
  if (item) {
    item.setColorLabel(colorIndex);
    return JSON.stringify({ success: true });
  }
  return JSON.stringify({ success: false, error: 'Clip not found' });
}

export function setInOutPoints(clipId: string, inPoint: number, outPoint: number): string {
  const item = findProjectItemById(clipId);
  if (item) {
    item.setInPoint(inPoint, 4);  // 4 = update UI
    item.setOutPoint(outPoint, 4);
    return JSON.stringify({ success: true });
  }
  return JSON.stringify({ success: false, error: 'Clip not found' });
}

// === BIN OPERATIONS ===

export function createBin(name: string, parentPath?: string): string {
  const parent = parentPath
    ? findBinByPath(parentPath)
    : app.project.rootItem;

  if (parent) {
    const newBin = parent.createBin(name);
    return JSON.stringify({
      success: true,
      binId: newBin.nodeId,
      path: getBinPath(newBin)
    });
  }
  return JSON.stringify({ success: false, error: 'Parent bin not found' });
}

export function moveClipToBin(clipId: string, binPath: string): string {
  const clip = findProjectItemById(clipId);
  const bin = findBinByPath(binPath);

  if (clip && bin) {
    clip.moveBin(bin);
    return JSON.stringify({ success: true });
  }
  return JSON.stringify({ success: false, error: 'Clip or bin not found' });
}

// === TIMELINE OPERATIONS ===

export function insertClipToTimeline(
  clipId: string,
  trackIndex: number,
  timeSeconds: number
): string {
  const seq = app.project.activeSequence;
  const clip = findProjectItemById(clipId);

  if (!seq || !clip) {
    return JSON.stringify({ success: false, error: 'Sequence or clip not found' });
  }

  try {
    seq.videoTracks[trackIndex].insertClip(clip, timeSeconds);
    return JSON.stringify({ success: true });
  } catch (e) {
    return JSON.stringify({ success: false, error: e.toString() });
  }
}

// === MARKER OPERATIONS ===

export function createMarker(
  timeSeconds: number,
  name: string,
  colorIndex: number
): string {
  const seq = app.project.activeSequence;
  if (!seq) {
    return JSON.stringify({ success: false, error: 'No active sequence' });
  }

  const marker = seq.markers.createMarker(timeSeconds);
  marker.name = name;
  marker.setTypeAsComment();
  marker.setColorByIndex(colorIndex, 0);

  return JSON.stringify({ success: true, markerId: marker.guid });
}

// === HELPER FUNCTIONS ===

function findProjectItemById(nodeId: string): ProjectItem | null {
  return searchInBin(app.project.rootItem, nodeId);
}

function searchInBin(bin: ProjectItem, nodeId: string): ProjectItem | null {
  for (let i = 0; i < bin.children.numItems; i++) {
    const item = bin.children[i];
    if (item.nodeId === nodeId) {
      return item;
    }
    if (item.type === ProjectItemType.BIN) {
      const found = searchInBin(item, nodeId);
      if (found) return found;
    }
  }
  return null;
}

function findBinByPath(path: string): ProjectItem | null {
  const parts = path.split('/').filter(p => p);
  let current = app.project.rootItem;

  for (const part of parts) {
    let found = false;
    for (let i = 0; i < current.children.numItems; i++) {
      const child = current.children[i];
      if (child.name === part && child.type === ProjectItemType.BIN) {
        current = child;
        found = true;
        break;
      }
    }
    if (!found) return null;
  }

  return current;
}
```

### Color Coding System

```typescript
// src/js/main/constants/colors.ts

export const TAKE_COLOR_MAP = {
  1: { index: 5,  name: 'Rose',     hex: '#F77FBE' },
  2: { index: 3,  name: 'Cerulean', hex: '#00A0DC' },
  3: { index: 6,  name: 'Mango',    hex: '#F5A623' },
  4: { index: 7,  name: 'Purple',   hex: '#9B59B6' },
  5: { index: 4,  name: 'Forest',   hex: '#27AE60' },
  6: { index: 9,  name: 'Lavender', hex: '#B39DDB' },
  7: { index: 12, name: 'Orange',   hex: '#FF5722' },
  8: { index: 14, name: 'Blue',     hex: '#2196F3' },
} as const;

export const MARKER_COLORS = {
  BEST_TAKE: 1,    // Red
  REGULAR_TAKE: 4, // Green
  SILENCE: 3,      // Yellow
} as const;
```

### Bin Organization Structure

```
Project Panel
└── SPLICE Takes
    ├── Line 1 - "Hello everyone..."
    │   ├── Take 1 [Rose]
    │   ├── Take 2 [Cerulean] ★ Best
    │   └── Take 3 [Mango]
    ├── Line 2 - "Today we're going to..."
    │   ├── Take 1 [Rose] ★ Best
    │   └── Take 2 [Cerulean]
    └── Silence Removed
        ├── Gap 1 (1.5s)
        └── Gap 2 (2.3s)
```

---

## 8. UI/UX Design

### Panel Layout

```tsx
// src/js/main/components/Panel.tsx

import React from 'react';
import { useSelectionDetection } from '../hooks/useSelectionDetection';
import { ClipList } from './ClipList';
import { TakeGroups } from './TakeGroups';
import { ProgressBar } from './ProgressBar';
import { SettingsPanel } from './SettingsPanel';
import { formatDuration, estimateCost } from '../utils/helpers';

export function SplicePanel() {
  const selectedClips = useSelectionDetection();
  const totalDuration = selectedClips.reduce((sum, c) => sum + c.duration, 0);

  return (
    <sp-theme color="dark" scale="medium">
      <div className="splice-panel">
        {/* Header */}
        <header className="panel-header">
          <sp-heading size="M">SPLICE Auto-Cut</sp-heading>
          <sp-action-button quiet onClick={openSettings}>
            <sp-icon name="Settings" />
          </sp-action-button>
        </header>

        {/* Selection Detection Section */}
        <section className="selection-section">
          {selectedClips.length === 0 ? (
            <sp-illustrated-message>
              <sp-icon name="SelectionOutline" />
              <sp-heading>No clips selected</sp-heading>
              <sp-body>Select clips on the timeline to process</sp-body>
            </sp-illustrated-message>
          ) : (
            <div className="selection-info">
              <sp-body>
                {selectedClips.length} clip{selectedClips.length > 1 ? 's' : ''} selected
              </sp-body>
              <sp-body size="S" class="secondary">
                Total duration: {formatDuration(totalDuration)}
              </sp-body>
            </div>
          )}
        </section>

        {/* Quick Action Buttons - Selection-Based Workflow */}
        {selectedClips.length > 0 && (
          <section className="quick-actions">
            <sp-button-group vertical>
              <sp-button
                variant="cta"
                onClick={() => processClips({ useVoiceIsolation: true })}
                disabled={isProcessing}
              >
                Process with Voice Isolation
              </sp-button>
              <sp-button
                variant="secondary"
                onClick={() => processClips({ useVoiceIsolation: false })}
                disabled={isProcessing}
              >
                Process without Voice Isolation
              </sp-button>
            </sp-button-group>
            <sp-body size="XS" class="cost-estimate">
              Est. cost: ${estimateCost(selectedClips, true).toFixed(2)} with /
              ${estimateCost(selectedClips, false).toFixed(2)} without isolation
            </sp-body>
          </section>
        )}

        {/* Selected Clips List */}
        {selectedClips.length > 0 && (
          <section className="clips-section">
            <ClipList clips={selectedClips} />
          </section>
        )}

        {/* Advanced Options (collapsible) */}
        <sp-accordion>
          <sp-accordion-item label="Advanced Options">
            <section className="options-section">
              <sp-field-group>
                <sp-checkbox checked={detectTakes}>
                  Detect Multiple Takes
                </sp-checkbox>
                <sp-checkbox checked={removeSilence}>
                  Remove Dead Space
                </sp-checkbox>
              </sp-field-group>

              <sp-slider
                label="Silence Threshold"
                min={0.5}
                max={3.0}
                value={silenceThreshold}
                step={0.1}
              />
            </section>
          </sp-accordion-item>
        </sp-accordion>

        {/* Progress */}
        {isProcessing && (
          <section className="progress-section">
            <ProgressBar
              value={progress}
              label={currentStep}
            />
          </section>
        )}

        {/* Results */}
        {results && (
          <section className="results-section">
            <sp-tabs>
              <sp-tab label="Takes" panel="takes-panel" />
              <sp-tab label="Silence" panel="silence-panel" />
            </sp-tabs>

            <sp-tab-panel id="takes-panel">
              <TakeGroups groups={results.takeGroups} />
            </sp-tab-panel>

            <sp-tab-panel id="silence-panel">
              <SilenceList sections={results.silentSections} />
            </sp-tab-panel>
          </section>
        )}

        {/* Apply Actions */}
        {results && (
          <section className="apply-section">
            <sp-button-group>
              <sp-button onClick={applyColorCoding}>
                Apply Colors
              </sp-button>
              <sp-button onClick={createBins}>
                Create Bins
              </sp-button>
              <sp-button variant="cta" onClick={applyAll}>
                Apply All
              </sp-button>
            </sp-button-group>
          </section>
        )}

        {/* Footer */}
        <footer className="panel-footer">
          <sp-body size="XS">
            Est. cost: ${estimatedCost.toFixed(2)}
          </sp-body>
        </footer>
      </div>
    </sp-theme>
  );
}
```

### Settings Panel

```tsx
// src/js/main/components/SettingsPanel.tsx

export function SettingsPanel() {
  return (
    <sp-dialog>
      <sp-heading slot="heading">Settings</sp-heading>

      <div className="settings-content">
        {/* API Keys */}
        <sp-field-group>
          <sp-field-label>API Keys</sp-field-label>

          <sp-textfield
            label="ElevenLabs API Key"
            type="password"
            value={elevenLabsKey}
            onChange={setElevenLabsKey}
          />

          <sp-textfield
            label="Deepgram API Key"
            type="password"
            value={deepgramKey}
            onChange={setDeepgramKey}
          />

          <sp-textfield
            label="OpenAI API Key"
            type="password"
            value={openaiKey}
            onChange={setOpenaiKey}
          />
        </sp-field-group>

        {/* Voice Isolation Settings */}
        <sp-field-group>
          <sp-field-label>Voice Isolation</sp-field-label>

          <sp-dropdown label="Audio Output Mode">
            <sp-menu-item value="original" selected>
              Keep Original Audio
            </sp-menu-item>
            <sp-menu-item value="isolated">
              Use Isolated Voice
            </sp-menu-item>
            <sp-menu-item value="none">
              Skip Isolation (faster)
            </sp-menu-item>
          </sp-dropdown>
        </sp-field-group>

        {/* Take Detection Settings */}
        <sp-field-group>
          <sp-field-label>Take Detection</sp-field-label>

          <sp-slider
            label="Similarity Threshold"
            min={0.70}
            max={0.95}
            value={similarityThreshold}
            step={0.05}
          />

          <sp-checkbox checked={useLLMAnalysis}>
            Use LLM for Better Accuracy
          </sp-checkbox>
        </sp-field-group>

        {/* Silence Detection Settings */}
        <sp-field-group>
          <sp-field-label>Silence Detection</sp-field-label>

          <sp-slider
            label="Minimum Silence Duration (seconds)"
            min={0.5}
            max={3.0}
            value={minSilenceDuration}
            step={0.1}
          />

          <sp-checkbox checked={preserveNaturalPauses}>
            Preserve Natural Pauses
          </sp-checkbox>
        </sp-field-group>

        {/* Timeline Settings */}
        <sp-field-group>
          <sp-field-label>Timeline Integration</sp-field-label>

          <sp-checkbox checked={autoColorCode}>
            Auto Color Code Takes
          </sp-checkbox>

          <sp-checkbox checked={createBins}>
            Organize in Bins
          </sp-checkbox>

          <sp-checkbox checked={addMarkers}>
            Add Timeline Markers
          </sp-checkbox>
        </sp-field-group>
      </div>

      <sp-button-group slot="button">
        <sp-button variant="secondary" onClick={resetDefaults}>
          Reset Defaults
        </sp-button>
        <sp-button variant="cta" onClick={saveSettings}>
          Save
        </sp-button>
      </sp-button-group>
    </sp-dialog>
  );
}
```

---

## 9. Development Phases

### Phase 1: MVP (Weeks 1-6)

**Goals:**
- Working plugin with core functionality
- Basic take detection
- Silence removal
- Timeline integration

**Features:**
- [ ] CEP panel setup with Bolt CEP
- [ ] Drag-and-drop clip import
- [ ] Audio extraction (FFmpeg)
- [ ] Deepgram transcription integration
- [ ] Basic take detection (embeddings only)
- [ ] Silence detection (threshold-based)
- [ ] Color coding in Premiere
- [ ] Bin organization
- [ ] Basic settings panel

**Tech Debt:**
- Skip voice isolation
- Skip LLM analysis
- Minimal error handling
- No progress streaming

### Phase 2: Enhanced (Weeks 7-10)

**Goals:**
- Production-ready quality
- Full feature set
- Robust error handling

**Features:**
- [ ] ElevenLabs voice isolation
- [ ] GPT-4o-mini take analysis
- [ ] Advanced similarity clustering
- [ ] "Best take" suggestions
- [ ] Batch processing
- [ ] Progress streaming
- [ ] Cost estimation UI
- [ ] Full settings panel
- [ ] Error recovery

### Phase 3: Polish (Weeks 11-14)

**Goals:**
- Professional quality
- User delight
- Market readiness

**Features:**
- [ ] Local LLM option (Ollama)
- [ ] Custom color schemes
- [ ] Export/import settings
- [ ] Keyboard shortcuts
- [ ] Tutorial/onboarding
- [ ] Usage analytics
- [ ] Undo/redo for applied changes
- [ ] Performance optimization
- [ ] Comprehensive testing

---

## 10. File Structure

```
splice-plugin/
├── .vscode/
│   └── launch.json              # Debug configurations
├── .github/
│   └── workflows/
│       └── release.yml          # GitHub Actions release
├── bin/
│   ├── mac/
│   │   └── ffmpeg               # macOS FFmpeg binary
│   └── win/
│       └── ffmpeg.exe           # Windows FFmpeg binary
├── src/
│   ├── js/
│   │   └── main/
│   │       ├── components/
│   │       │   ├── DropZone.tsx
│   │       │   ├── ClipList.tsx
│   │       │   ├── TakeGroups.tsx
│   │       │   ├── SilenceList.tsx
│   │       │   ├── ProgressBar.tsx
│   │       │   ├── SettingsPanel.tsx
│   │       │   └── Panel.tsx
│   │       ├── hooks/
│   │       │   ├── useProcessing.ts
│   │       │   ├── usePremiere.ts
│   │       │   ├── useSettings.ts
│   │       │   └── useApi.ts
│   │       ├── services/
│   │       │   ├── ProcessingPipeline.ts
│   │       │   ├── AudioExtractor.ts
│   │       │   ├── VoiceIsolator.ts
│   │       │   ├── Transcriber.ts
│   │       │   ├── TakeDetector.ts
│   │       │   ├── SilenceDetector.ts
│   │       │   ├── CredentialManager.ts
│   │       │   └── CostTracker.ts
│   │       ├── modules/
│   │       │   ├── audio/
│   │       │   ├── takes/
│   │       │   ├── silence/
│   │       │   └── premiere/
│   │       ├── utils/
│   │       │   ├── apiClient.ts
│   │       │   ├── embeddings.ts
│   │       │   ├── clustering.ts
│   │       │   └── helpers.ts
│   │       ├── constants/
│   │       │   ├── colors.ts
│   │       │   └── config.ts
│   │       ├── types/
│   │       │   ├── models.ts
│   │       │   ├── api.ts
│   │       │   └── premiere.ts
│   │       ├── store/
│   │       │   └── index.ts     # Zustand store
│   │       ├── main.tsx         # Entry point
│   │       └── index.html
│   └── jsx/
│       ├── index.ts             # ExtendScript entry
│       ├── timeline.ts          # Timeline operations
│       ├── clips.ts             # Clip operations
│       ├── bins.ts              # Bin operations
│       ├── markers.ts           # Marker operations
│       └── helpers.ts           # Utility functions
├── public/
│   └── icons/
│       ├── icon-light.png
│       └── icon-dark.png
├── tests/
│   ├── unit/
│   │   ├── TakeDetector.test.ts
│   │   └── SilenceDetector.test.ts
│   └── integration/
│       └── Pipeline.test.ts
├── cep.config.ts                # Bolt CEP configuration
├── vite.config.ts               # Vite configuration
├── tsconfig.json                # TypeScript config
├── tsconfig.jsx.json            # ExtendScript TypeScript config
├── tailwind.config.js           # Tailwind CSS config
├── package.json
├── pnpm-lock.yaml
├── .gitignore
├── .env.example                 # Environment variables template
├── README.md
├── LICENSE
└── CHANGELOG.md
```

---

## 11. Build and Distribution

### Development Workflow

```bash
# Install dependencies
pnpm install

# Start development server with HMR
pnpm dev

# Build for production
pnpm build

# Create signed ZXP package
pnpm zxp

# Run tests
pnpm test

# Lint and type check
pnpm lint
pnpm typecheck
```

### CEP Configuration

```typescript
// cep.config.ts

import { CEPConfig } from 'bolt-cep';

const config: CEPConfig = {
  id: 'com.splice.autocut',
  displayName: 'SPLICE Auto-Cut',
  version: '1.0.0',

  hosts: [
    {
      name: 'PPRO',
      version: '[22.0,99.9]'
    }
  ],

  panels: {
    main: {
      type: 'Panel',
      name: 'SPLICE Auto-Cut',
      root: './src/js/main/index.html',
      script: './src/jsx/index.ts',
      size: {
        width: 350,
        height: 600
      },
      minSize: {
        width: 300,
        height: 400
      }
    }
  },

  lifecycle: {
    autoVisible: true,
    startOnEvents: ['applicationActivate']
  },

  cepVersion: '11.0',
  runtimeVersion: '9.0'
};

export default config;
```

### ZXP Signing

```bash
# Generate self-signed certificate (development)
ZXPSignCmd -selfSignedCert US "SPLICE" "SPLICE Auto-Cut" password ./cert.p12

# Sign the package
pnpm zxp  # Uses Bolt CEP's built-in signing
```

### Distribution Strategy

**Phase 1: Direct Distribution**
- Host ZXP on website
- Install via aescripts ZXP Installer or Anastasiy's Extension Manager
- Free trial + subscription model

**Phase 2: Adobe Exchange**
- Submit after v1.0 stability
- Requires Adobe review
- Broader discovery

**Phase 3: Enterprise**
- Custom licensing
- Volume discounts
- Priority support

### Environment Variables

```bash
# .env.example

# API Keys (stored securely, not in env)
# ELEVENLABS_API_KEY=
# DEEPGRAM_API_KEY=
# OPENAI_API_KEY=

# Development
DEBUG=true
LOG_LEVEL=info

# Feature Flags
ENABLE_VOICE_ISOLATION=true
ENABLE_LLM_ANALYSIS=true
ENABLE_LOCAL_LLM=false

# Pricing (for cost estimation)
ELEVENLABS_RATE_PER_MIN=0.30
DEEPGRAM_RATE_PER_MIN=0.0043
OPENAI_INPUT_RATE_PER_1M=0.075
OPENAI_OUTPUT_RATE_PER_1M=0.30

# Voice Isolation Provider Rates
AUPHONIC_RATE_PER_MIN=0.02
LALALAI_RATE_PER_MIN=0.03
DOLBYIO_RATE_PER_MIN=0.03
```

---

## 12. Pricing and Business Model

### All-In API Cost Stack

Voice isolation is **included in all tiers** - no separate add-ons required.

| Component | Service | Cost per Minute |
|-----------|---------|-----------------|
| Transcription | Deepgram Nova-3 | $0.0043 |
| LLM Analysis | GPT-4o-mini Batch API | ~$0.001 |
| Voice Isolation | LALAL.AI | $0.03 |
| **Total** | — | **$0.0353/min ($2.12/hr)** |

### Integrated Subscription Tiers

All tiers include full functionality: transcription, take detection, silence removal, AND voice isolation.

| Tier | Hours/Month | Price | Cost | Profit | Margin |
|------|-------------|-------|------|--------|--------|
| **Starter** | 5 hours | $49/month | $10.60 | $38.40 | **78%** |
| **Creator** | 15 hours | $129/month | $31.80 | $97.20 | **75%** |
| **Pro** | 40 hours | $349/month | $84.80 | $264.20 | **76%** |
| **Studio** | 100 hours | $849/month | $212.00 | $637.00 | **75%** |

### Tier Target Users

| Tier | Target User | Use Case |
|------|-------------|----------|
| Starter | Part-time YouTubers, hobbyists | 1-2 videos/month |
| Creator | Full-time content creators | 3-4 videos/week |
| Pro | Production studios, agencies | Daily production |
| Studio | Enterprise, post-production houses | High-volume workflows |

### Revenue Projections

```
1,000 users:
- 50% Starter ($49)   = 500 × $49  = $24,500
- 30% Creator ($129)  = 300 × $129 = $38,700
- 15% Pro ($349)      = 150 × $349 = $52,350
- 5% Studio ($849)    = 50 × $849  = $42,450
                        Monthly: $158,000
                        Annual:  $1,896,000

Profit (at 75% avg margin):
                        Monthly: $118,500
                        Annual:  $1,422,000
```

### Overage Pricing

When users exceed their monthly hours:
- **$4.99/hour** (overage rate maintains ~58% margin)
- Users can upgrade tier mid-cycle with prorated credit

---

## 13. SaaS Web Application

### Recommended Tech Stack

```yaml
Framework: Next.js 15 (App Router)
Authentication: Clerk (10K free MAU, built-in billing portal)
Database: Supabase (PostgreSQL)
Payments: Stripe Billing with Meters API
UI Components: shadcn/ui + Tremor (for charts/dashboards)
Hosting: Vercel (auto-scaling)
```

### Architecture Diagram

```
┌─────────────────────────────────────────────────────────────────────┐
│                        SPLICE SaaS Platform                          │
├─────────────────────────────────────────────────────────────────────┤
│                                                                      │
│  ┌─────────────────┐     ┌─────────────────┐     ┌───────────────┐  │
│  │   Next.js 15    │     │     Clerk       │     │   Supabase    │  │
│  │   App Router    │◄───►│  Authentication │◄───►│  PostgreSQL   │  │
│  │                 │     │  + User Mgmt    │     │               │  │
│  └────────┬────────┘     └─────────────────┘     └───────────────┘  │
│           │                                                          │
│           ▼                                                          │
│  ┌─────────────────┐     ┌─────────────────┐                        │
│  │   shadcn/ui +   │     │ Stripe Billing  │                        │
│  │   Tremor Charts │     │ + Meters API    │                        │
│  └─────────────────┘     └─────────────────┘                        │
│                                                                      │
└─────────────────────────────────────────────────────────────────────┘
```

### Key Features

- **Usage Dashboard:** Real-time charts showing processing hours consumed
- **Subscription Management:** Stripe Customer Portal integration
- **API Key Generation:** Self-service API key creation and management
- **Processing History:** Logs of all processed files with timestamps
- **Billing History:** Invoice access and payment method management

### Database Schema (Supabase)

```sql
-- Users (synced from Clerk)
CREATE TABLE users (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  clerk_id TEXT UNIQUE NOT NULL,
  email TEXT NOT NULL,
  stripe_customer_id TEXT,
  subscription_tier TEXT DEFAULT 'free',
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- API Keys
CREATE TABLE api_keys (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  key_hash TEXT NOT NULL,
  name TEXT,
  last_used_at TIMESTAMPTZ,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Usage Records
CREATE TABLE usage_records (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  minutes_used DECIMAL NOT NULL,
  voice_isolation BOOLEAN DEFAULT FALSE,
  cost DECIMAL NOT NULL,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Processing Jobs
CREATE TABLE processing_jobs (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  user_id UUID REFERENCES users(id),
  status TEXT NOT NULL,
  clip_name TEXT,
  duration_minutes DECIMAL,
  voice_isolation BOOLEAN,
  result JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  completed_at TIMESTAMPTZ
);
```

### Usage Tracking API Route

```typescript
// app/api/usage/route.ts

import { auth } from '@clerk/nextjs';
import { createClient } from '@supabase/supabase-js';
import Stripe from 'stripe';

const stripe = new Stripe(process.env.STRIPE_SECRET_KEY!);
const supabase = createClient(
  process.env.SUPABASE_URL!,
  process.env.SUPABASE_SERVICE_KEY!
);

export async function POST(req: Request) {
  const { userId } = auth();
  if (!userId) return new Response('Unauthorized', { status: 401 });

  const { minutes, voiceIsolation } = await req.json();

  // Record usage in Supabase
  const cost = calculateCost(minutes, voiceIsolation);
  await supabase.from('usage_records').insert({
    user_id: userId,
    minutes_used: minutes,
    voice_isolation: voiceIsolation,
    cost
  });

  // Report to Stripe Meters for billing
  const user = await supabase
    .from('users')
    .select('stripe_customer_id')
    .eq('clerk_id', userId)
    .single();

  await stripe.billing.meterEvents.create({
    event_name: 'processing_minutes',
    payload: {
      stripe_customer_id: user.data.stripe_customer_id,
      value: Math.ceil(minutes).toString()
    }
  });

  return Response.json({ success: true, cost });
}

function calculateCost(minutes: number, voiceIsolation: boolean): number {
  const baseRate = 0.0043; // Deepgram
  const llmRate = 0.001;   // Estimated LLM cost per minute
  const voiceRate = voiceIsolation ? 0.03 : 0; // LALAL.AI

  return minutes * (baseRate + llmRate + voiceRate);
}
```

### Dashboard Component

```tsx
// app/dashboard/page.tsx

import { Card, Title, AreaChart, DonutChart } from '@tremor/react';
import { auth } from '@clerk/nextjs';
import { getUsageStats } from '@/lib/usage';

export default async function DashboardPage() {
  const { userId } = auth();
  const stats = await getUsageStats(userId);

  return (
    <div className="p-8 space-y-8">
      <div className="grid grid-cols-3 gap-4">
        <Card>
          <Title>Hours Used</Title>
          <p className="text-3xl font-bold">
            {stats.hoursUsed} / {stats.hoursLimit}
          </p>
        </Card>
        <Card>
          <Title>Cost This Month</Title>
          <p className="text-3xl font-bold">${stats.costThisMonth.toFixed(2)}</p>
        </Card>
        <Card>
          <Title>Jobs Processed</Title>
          <p className="text-3xl font-bold">{stats.jobsProcessed}</p>
        </Card>
      </div>

      <Card>
        <Title>Usage Over Time</Title>
        <AreaChart
          data={stats.dailyUsage}
          index="date"
          categories={['minutes']}
          colors={['blue']}
        />
      </Card>

      <Card>
        <Title>Usage by Type</Title>
        <DonutChart
          data={[
            { name: 'With Voice Isolation', value: stats.withIsolation },
            { name: 'Without Voice Isolation', value: stats.withoutIsolation }
          ]}
          category="value"
          index="name"
        />
      </Card>
    </div>
  );
}
```

### Alternative: Pre-built Boilerplate

**Supastarter** ($349 one-time) - Pre-built SaaS template with:
- Authentication already configured
- Stripe subscription management
- User dashboard
- Admin panel
- Email templates

This can save 2-4 weeks of development time on the SaaS infrastructure.

---

## Quick Start for Developers

```bash
# 1. Clone repository
git clone https://github.com/your-org/splice-plugin.git
cd splice-plugin

# 2. Install dependencies
pnpm install

# 3. Enable CEP debug mode
# macOS:
defaults write com.adobe.CSXS.12 PlayerDebugMode 1
# Windows (PowerShell as Admin):
# reg add "HKCU\Software\Adobe\CSXS.12" /v PlayerDebugMode /t REG_SZ /d 1

# 4. Start development
pnpm dev

# 5. Open Premiere Pro
# Window > Extensions > SPLICE Auto-Cut

# 6. Debug in Chrome
# Navigate to http://localhost:8088
```

---

**Architecture by:** Claude (Anthropic)
**Version:** 1.1.0
**Last Updated:** December 16, 2025

---

## Changelog

### v1.1.0 (December 16, 2025)
- **Voice Isolation:** Replaced ElevenLabs with LALAL.AI/Auphonic/Dolby.io (10x cost savings)
- **User Interaction:** Updated to selection-based workflow (context menus not customizable)
- **Pricing:** Added comprehensive pricing model for 70% profit margin
- **Rate Limiting:** Added production-ready rate limiting architecture
- **SaaS Platform:** Added full web application architecture with Clerk/Stripe/Supabase
- **Cost Tracking:** Updated with new provider pricing
