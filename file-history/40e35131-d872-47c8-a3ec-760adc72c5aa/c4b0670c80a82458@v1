/**
 * RMS-Based Silence Detection Service
 *
 * Advanced silence detection using Root Mean Square (RMS) audio analysis.
 * Based on Fireside's proven approach with enhancements.
 *
 * Features:
 * - RMS calculation from raw audio data
 * - dBFS (decibels relative to full scale) conversion
 * - Auto-threshold detection from audio histogram
 * - Gaussian blur smoothing for noise reduction
 * - Local maxima detection for speech/silence peak identification
 * - Configurable padding, seek step, min duration
 * - Silence merging for adjacent regions
 */

const { exec } = require('child_process');
const { promisify } = require('util');
const fs = require('fs');
const path = require('path');

const execAsync = promisify(exec);

// =============================================================================
// Configuration
// =============================================================================

const DEFAULT_OPTIONS = {
  threshold: -30,           // dBFS threshold (-60 to -20)
  minSilenceLength: 0.5,    // Minimum silence duration in seconds
  seekStep: 0.05,           // Analysis window step in seconds (50ms)
  paddingStart: 0.1,        // Buffer before silence cut (seconds)
  paddingEnd: 0.05,         // Buffer after silence cut (seconds)
  autoThreshold: false,     // Auto-detect optimal threshold
  mergeDistance: 0.2,       // Merge silences closer than this (seconds)
  sampleRate: 48000         // Default sample rate
};

// =============================================================================
// Core RMS Detection
// =============================================================================

/**
 * Detect silences using RMS-based audio analysis
 *
 * @param {string} audioPath - Path to audio file (WAV, MP3, etc.)
 * @param {Object} options - Detection options
 * @returns {Promise<Object>} Detection results with silences and metadata
 */
async function detectSilencesRMS(audioPath, options = {}) {
  const opts = { ...DEFAULT_OPTIONS, ...options };

  console.log(`[SPLICE RMS] Analyzing: ${audioPath}`);
  console.log(`[SPLICE RMS] Options: threshold=${opts.threshold}dB, minLength=${opts.minSilenceLength}s, seekStep=${opts.seekStep}s`);

  // Step 1: Extract raw audio data using FFmpeg
  const audioData = await extractAudioData(audioPath);

  // Step 2: Calculate RMS values for sliding windows
  const rmsValues = calculateRMSWindows(audioData.samples, {
    windowSize: Math.floor(opts.seekStep * audioData.sampleRate),
    hopSize: Math.floor(opts.seekStep * audioData.sampleRate / 2)
  });

  // Step 3: Convert to dBFS
  const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));

  // Step 4: Determine threshold (auto-detect or use provided)
  let threshold = opts.threshold;
  let thresholdInfo = { method: 'manual', value: threshold };

  if (opts.autoThreshold) {
    const autoResult = autoDetectThreshold(dBFSValues);
    threshold = autoResult.threshold;
    thresholdInfo = autoResult;
    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB (${autoResult.method})`);
  }

  // Step 5: Find silence regions
  const rawSilences = findSilenceRegions(dBFSValues, {
    threshold,
    seekStep: opts.seekStep,
    minSilenceLength: opts.minSilenceLength
  });

  // Step 6: Merge adjacent silences
  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);

  // Step 7: Apply padding
  const paddedSilences = applyPadding(mergedSilences, {
    paddingStart: opts.paddingStart,
    paddingEnd: opts.paddingEnd,
    audioDuration: audioData.duration
  });

  // Calculate statistics
  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);
  const speechDuration = audioData.duration - totalSilenceDuration;

  console.log(`[SPLICE RMS] Detected ${paddedSilences.length} silence(s), total ${totalSilenceDuration.toFixed(2)}s`);

  return {
    silences: paddedSilences,
    metadata: {
      audioDuration: audioData.duration,
      sampleRate: audioData.sampleRate,
      channels: audioData.channels,
      silenceCount: paddedSilences.length,
      totalSilenceDuration,
      speechDuration,
      silencePercentage: ((totalSilenceDuration / audioData.duration) * 100).toFixed(1),
      threshold: thresholdInfo,
      rmsAnalysis: {
        windowCount: dBFSValues.length,
        minDB: Math.min(...dBFSValues).toFixed(1),
        maxDB: Math.max(...dBFSValues).toFixed(1),
        avgDB: (dBFSValues.reduce((a, b) => a + b, 0) / dBFSValues.length).toFixed(1)
      }
    }
  };
}

// =============================================================================
// Audio Extraction
// =============================================================================

/**
 * Extract raw audio samples from file using FFmpeg
 *
 * @param {string} audioPath - Path to audio file
 * @returns {Promise<Object>} Audio data with samples, sampleRate, channels, duration
 */
async function extractAudioData(audioPath) {
  // First, get audio info
  const infoCmd = `ffprobe -v error -select_streams a:0 -show_entries stream=sample_rate,channels,duration -of json "${audioPath}"`;

  let sampleRate = 48000;
  let channels = 1;
  let duration = 0;

  try {
    const { stdout } = await execAsync(infoCmd);
    const info = JSON.parse(stdout);
    if (info.streams && info.streams[0]) {
      sampleRate = parseInt(info.streams[0].sample_rate) || 48000;
      channels = parseInt(info.streams[0].channels) || 1;
      duration = parseFloat(info.streams[0].duration) || 0;
    }
  } catch (err) {
    console.warn('[SPLICE RMS] Could not get audio info, using defaults:', err.message);
  }

  // If duration not available from stream, get from format
  if (!duration) {
    try {
      const durationCmd = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 "${audioPath}"`;
      const { stdout } = await execAsync(durationCmd);
      duration = parseFloat(stdout.trim()) || 0;
    } catch (err) {
      console.warn('[SPLICE RMS] Could not get duration:', err.message);
    }
  }

  // Extract audio as raw PCM floating point
  const tempFile = path.join('/tmp', `splice_rms_${Date.now()}.raw`);

  try {
    // Convert to mono, 16kHz for faster processing, output as signed 16-bit PCM
    const extractCmd = `ffmpeg -y -i "${audioPath}" -ac 1 -ar 16000 -f s16le -acodec pcm_s16le "${tempFile}" 2>/dev/null`;
    await execAsync(extractCmd, { maxBuffer: 100 * 1024 * 1024 });

    // Read the raw PCM data
    const rawData = fs.readFileSync(tempFile);

    // Convert to float samples (-1 to 1)
    const samples = new Float32Array(rawData.length / 2);
    for (let i = 0; i < samples.length; i++) {
      // Read signed 16-bit little-endian
      const int16 = rawData.readInt16LE(i * 2);
      samples[i] = int16 / 32768.0;
    }

    // Clean up temp file
    fs.unlinkSync(tempFile);

    return {
      samples,
      sampleRate: 16000, // We resampled to 16kHz
      channels: 1,
      duration
    };
  } catch (err) {
    // Clean up temp file if exists
    if (fs.existsSync(tempFile)) {
      fs.unlinkSync(tempFile);
    }
    throw new Error(`Failed to extract audio: ${err.message}`);
  }
}

// =============================================================================
// RMS Calculation
// =============================================================================

/**
 * Calculate RMS values for sliding windows across audio samples
 *
 * @param {Float32Array} samples - Audio samples (-1 to 1)
 * @param {Object} options - Window and hop size
 * @returns {Float32Array} RMS values for each window
 */
function calculateRMSWindows(samples, options = {}) {
  const { windowSize = 800, hopSize = 400 } = options; // Default ~50ms at 16kHz

  const numWindows = Math.floor((samples.length - windowSize) / hopSize) + 1;
  const rmsValues = new Float32Array(numWindows);

  for (let i = 0; i < numWindows; i++) {
    const start = i * hopSize;
    const end = Math.min(start + windowSize, samples.length);

    // Calculate RMS for this window
    let sumSquares = 0;
    for (let j = start; j < end; j++) {
      sumSquares += samples[j] * samples[j];
    }

    const rms = Math.sqrt(sumSquares / (end - start));
    rmsValues[i] = rms;
  }

  return rmsValues;
}

/**
 * Convert RMS value to dBFS (decibels relative to full scale)
 *
 * @param {number} rms - RMS value (0 to 1)
 * @returns {number} dBFS value (negative, -inf for silence)
 */
function rmsToDBFS(rms) {
  if (rms <= 0) return -100; // Effective silence floor
  const dbfs = 20 * Math.log10(rms);
  return Math.max(-100, dbfs); // Clamp to reasonable floor
}

// =============================================================================
// Auto-Threshold Detection
// =============================================================================

/**
 * Auto-detect optimal silence threshold from audio histogram
 * Uses Gaussian blur + local maxima detection
 *
 * @param {Float32Array} dBFSValues - Array of dBFS values
 * @returns {Object} Detected threshold and method info
 */
function autoDetectThreshold(dBFSValues) {
  // Build histogram of dBFS values
  const histBins = 100;
  const minDB = -80;
  const maxDB = 0;
  const binWidth = (maxDB - minDB) / histBins;

  const histogram = new Array(histBins).fill(0);

  for (const db of dBFSValues) {
    const binIndex = Math.min(histBins - 1, Math.max(0, Math.floor((db - minDB) / binWidth)));
    histogram[binIndex]++;
  }

  // Apply Gaussian blur to smooth histogram
  const smoothed = gaussianBlur(histogram, 5);

  // Find local maxima (peaks)
  const peaks = findLocalMaxima(smoothed, 3);

  // Sort peaks by value (highest first)
  peaks.sort((a, b) => b.value - a.value);

  if (peaks.length >= 2) {
    // Interpolate between the two highest peaks
    // Assume: higher peak = speech, lower peak = silence
    const peak1DB = minDB + peaks[0].index * binWidth;
    const peak2DB = minDB + peaks[1].index * binWidth;

    const silencePeakDB = Math.min(peak1DB, peak2DB);
    const speechPeakDB = Math.max(peak1DB, peak2DB);

    // Set threshold between peaks (closer to silence peak)
    const threshold = silencePeakDB + (speechPeakDB - silencePeakDB) * 0.3;

    return {
      threshold: Math.max(-60, Math.min(-20, threshold)),
      method: 'dual-peak',
      silencePeak: silencePeakDB.toFixed(1),
      speechPeak: speechPeakDB.toFixed(1)
    };
  } else if (peaks.length === 1) {
    // Single peak - use percentile-based fallback
    const sorted = [...dBFSValues].sort((a, b) => a - b);
    const percentile25 = sorted[Math.floor(sorted.length * 0.25)];

    return {
      threshold: Math.max(-60, Math.min(-20, percentile25)),
      method: 'percentile-25',
      value: percentile25.toFixed(1)
    };
  } else {
    // No peaks found - use conservative default
    return {
      threshold: -35,
      method: 'default-fallback'
    };
  }
}

/**
 * Apply Gaussian blur to an array
 *
 * @param {Array<number>} arr - Input array
 * @param {number} sigma - Standard deviation
 * @returns {Array<number>} Smoothed array
 */
function gaussianBlur(arr, sigma = 5) {
  const kernelSize = Math.ceil(sigma * 3) * 2 + 1;
  const kernel = [];
  const halfSize = Math.floor(kernelSize / 2);

  // Generate Gaussian kernel
  let sum = 0;
  for (let i = 0; i < kernelSize; i++) {
    const x = i - halfSize;
    const g = Math.exp(-(x * x) / (2 * sigma * sigma));
    kernel.push(g);
    sum += g;
  }

  // Normalize kernel
  for (let i = 0; i < kernelSize; i++) {
    kernel[i] /= sum;
  }

  // Apply convolution
  const result = new Array(arr.length).fill(0);
  for (let i = 0; i < arr.length; i++) {
    let value = 0;
    for (let j = 0; j < kernelSize; j++) {
      const idx = i + j - halfSize;
      if (idx >= 0 && idx < arr.length) {
        value += arr[idx] * kernel[j];
      }
    }
    result[i] = value;
  }

  return result;
}

/**
 * Find local maxima in an array
 *
 * @param {Array<number>} arr - Input array
 * @param {number} windowSize - Neighborhood size for local maximum detection
 * @returns {Array<{index: number, value: number}>} Array of peak positions and values
 */
function findLocalMaxima(arr, windowSize = 3) {
  const peaks = [];
  const halfWindow = Math.floor(windowSize / 2);

  for (let i = halfWindow; i < arr.length - halfWindow; i++) {
    let isMax = true;
    for (let j = i - halfWindow; j <= i + halfWindow; j++) {
      if (j !== i && arr[j] >= arr[i]) {
        isMax = false;
        break;
      }
    }
    if (isMax && arr[i] > 0) {
      peaks.push({ index: i, value: arr[i] });
    }
  }

  return peaks;
}

// =============================================================================
// Silence Region Detection
// =============================================================================

/**
 * Find silence regions based on dBFS threshold
 *
 * @param {Float32Array} dBFSValues - Array of dBFS values
 * @param {Object} options - Detection options
 * @returns {Array<{start: number, end: number, duration: number}>} Silence regions
 */
function findSilenceRegions(dBFSValues, options) {
  const { threshold, seekStep, minSilenceLength } = options;

  const silences = [];
  let silenceStart = null;

  for (let i = 0; i < dBFSValues.length; i++) {
    const time = i * seekStep;
    const isSilent = dBFSValues[i] < threshold;

    if (isSilent && silenceStart === null) {
      // Start of silence
      silenceStart = time;
    } else if (!isSilent && silenceStart !== null) {
      // End of silence
      const duration = time - silenceStart;
      if (duration >= minSilenceLength) {
        silences.push({
          start: silenceStart,
          end: time,
          duration
        });
      }
      silenceStart = null;
    }
  }

  // Handle silence at end of audio
  if (silenceStart !== null) {
    const endTime = dBFSValues.length * seekStep;
    const duration = endTime - silenceStart;
    if (duration >= minSilenceLength) {
      silences.push({
        start: silenceStart,
        end: endTime,
        duration
      });
    }
  }

  return silences;
}

// =============================================================================
// Post-Processing
// =============================================================================

/**
 * Merge silences that are close together
 *
 * @param {Array} silences - Array of silence regions
 * @param {number} mergeDistance - Maximum gap to merge (seconds)
 * @returns {Array} Merged silence regions
 */
function mergeSilences(silences, mergeDistance) {
  if (silences.length <= 1) return silences;

  const merged = [];
  let current = { ...silences[0] };

  for (let i = 1; i < silences.length; i++) {
    const next = silences[i];

    // If gap is small enough, merge
    if (next.start - current.end <= mergeDistance) {
      current.end = next.end;
      current.duration = current.end - current.start;
    } else {
      merged.push(current);
      current = { ...next };
    }
  }

  merged.push(current);
  return merged;
}

/**
 * Apply padding to silence boundaries
 *
 * @param {Array} silences - Array of silence regions
 * @param {Object} options - Padding options
 * @returns {Array} Padded silence regions
 */
function applyPadding(silences, options) {
  const { paddingStart, paddingEnd, audioDuration } = options;

  return silences
    .map(silence => {
      const paddedStart = Math.max(0, silence.start + paddingStart);
      const paddedEnd = Math.min(audioDuration, silence.end - paddingEnd);
      const duration = paddedEnd - paddedStart;

      // Skip if padding made the silence invalid
      if (duration <= 0) return null;

      return {
        start: parseFloat(paddedStart.toFixed(3)),
        end: parseFloat(paddedEnd.toFixed(3)),
        duration: parseFloat(duration.toFixed(3))
      };
    })
    .filter(s => s !== null);
}

// =============================================================================
// Sensitivity Mapping
// =============================================================================

/**
 * Map UI sensitivity (0-100) to detection parameters
 * Based on Fireside's aggressiveness mapping
 *
 * @param {number} sensitivity - UI sensitivity value (0-100)
 * @returns {Object} Detection parameters
 */
function sensitivityToParams(sensitivity) {
  // Clamp to 0-100
  const s = Math.max(0, Math.min(100, sensitivity));

  // Linear interpolation helper
  const lerp = (min, max, t) => min + (max - min) * (t / 100);

  return {
    // Threshold: -50dB (conservative) to -20dB (aggressive)
    threshold: lerp(-50, -20, s),

    // Min silence length: 2.0s (conservative) to 0.3s (aggressive)
    minSilenceLength: lerp(2.0, 0.3, s),

    // Padding start: 0.2s (conservative) to 0.05s (aggressive)
    paddingStart: lerp(0.2, 0.05, s),

    // Padding end: 0.15s (conservative) to 0.03s (aggressive)
    paddingEnd: lerp(0.15, 0.03, s),

    // Seek step stays constant
    seekStep: 0.05
  };
}

// =============================================================================
// Exports
// =============================================================================

module.exports = {
  detectSilencesRMS,
  sensitivityToParams,
  autoDetectThreshold,
  // Export internal functions for testing
  calculateRMSWindows,
  rmsToDBFS,
  gaussianBlur,
  findLocalMaxima,
  mergeSilences,
  applyPadding,
  DEFAULT_OPTIONS
};
