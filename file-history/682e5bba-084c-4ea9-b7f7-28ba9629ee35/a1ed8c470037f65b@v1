# Website Cloner

A comprehensive website cloning toolkit that combines two powerful approaches:
1. **Playwright-based cloner** - For JavaScript-heavy sites, SPAs, and interactive content
2. **Python static downloader** - For fast, multi-threaded downloading of static sites

## Features

### Playwright Cloner (`clone.js`)
- Captures JavaScript-rendered content (React, Vue, Next.js, etc.)
- Handles SPAs with client-side routing
- Interacts with dropdowns, modals, tabs, and accordions
- Handles infinite scroll and pagination
- Dismisses cookie consent banners automatically
- Captures all network requests including API calls
- Generates HAR files for network analysis
- Creates sitemap of all discovered URLs
- Supports login with credentials
- Interactive mode for step-by-step crawling

### Python Downloader (integrated from WEBSITE_DOWNLOADER)
- Fast multi-threaded asset downloading
- Automatic link rewriting for offline viewing
- Breadth-first crawling with page limits
- Robust path handling for long filenames
- Minimal dependencies (requests + beautifulsoup4)

## Installation

```bash
cd ~/Desktop/reverse-engineer/website-cloner

# Install Node.js dependencies
npm install

# Install Playwright browsers
npx playwright install chromium

# (Optional) Install Python dependencies for static downloader
pip install requests beautifulsoup4
```

## Usage

### Playwright Cloner (Recommended for SPAs/JS-heavy sites)

```bash
# Basic usage
node clone.js --url="https://example.com"

# Full options
node clone.js \
  --url="https://example.com" \
  --depth=3 \
  --delay=500 \
  --output="./my-clone" \
  --max-pages=100 \
  --screenshots \
  --interactive

# With login
node clone.js \
  --url="https://example.com/login" \
  --login="user@email.com:password" \
  --depth=5

# Headed mode (see the browser)
node clone.js --url="https://example.com" --headed --interactive
```

### Python Static Downloader (Fast for static sites)

```bash
python ~/Desktop/WEBSITE_DOWNLOADER/website-downloader.py \
  --url="https://example.com" \
  --destination="./static-clone" \
  --max-pages=100 \
  --threads=8
```

### Unified Wrapper

```bash
# Auto-detect site type and use best approach
node unified.js --url="https://example.com"

# Force specific mode
node unified.js --url="https://example.com" --mode=playwright
node unified.js --url="https://example.com" --mode=static
```

## CLI Options

| Option | Default | Description |
|--------|---------|-------------|
| `-u, --url` | (required) | Starting URL to clone |
| `-d, --depth` | 3 | Maximum crawl depth |
| `-D, --delay` | 500 | Delay between requests (ms) |
| `-o, --output` | ./site-clone | Output directory |
| `-i, --interactive` | false | Pause before major actions |
| `-I, --ignore-robots` | false | Ignore robots.txt |
| `-m, --max-pages` | 1000 | Maximum pages to crawl |
| `-t, --timeout` | 30000 | Page load timeout (ms) |
| `-e, --exclude` | /logout,/admin | URL patterns to exclude |
| `-l, --login` | - | Login as username:password |
| `-H, --headed` | false | Show browser window |
| `-s, --screenshots` | false | Take screenshots |
| `-b, --beautify` | false | Beautify JS/CSS files |

## Output Structure

```
./site-clone/
├── html/           # All HTML pages
├── css/            # Stylesheets
├── js/             # JavaScript files
├── images/         # Images (png, jpg, svg, webp, gif)
├── fonts/          # Font files (woff, woff2, ttf, eot)
├── api/            # Captured API responses (JSON)
├── har/            # HAR file of all network traffic
├── screenshots/    # Page screenshots (if enabled)
├── sitemap.json    # Map of all discovered URLs
├── assets-index.json    # Index of all captured assets
├── url-mapping.json     # Original URL -> local path mapping
└── robots.txt      # Site's robots.txt (if found)
```

## Post-Processing

After cloning, you can process the captured JS/CSS using the reverse-engineer tools:

```bash
# Beautify and analyze captured assets
node lib/post-processor.js ./site-clone

# Or use individual tools:

# Beautify JavaScript
cd ~/Desktop/reverse-engineer/js-beautify
npx js-beautify ../website-cloner/site-clone/js/*.js

# Unpack webpack bundles
cd ~/Desktop/reverse-engineer/wakaru
npx wakaru ../website-cloner/site-clone/js/bundle.js -o ./unpacked

# Deobfuscate (use webcrack web interface)
# https://webcrack.netlify.app
```

## Integration with Reverse-Engineer Toolkit

This tool is part of the reverse-engineer toolkit and works seamlessly with:

| Tool | Purpose | Integration |
|------|---------|-------------|
| `js-beautify` | Format minified JS/CSS | Post-process captured assets |
| `wakaru` | Unpack webpack bundles | Analyze bundled modules |
| `webcrack` | Deobfuscate JS | Handle obfuscated code |
| `humanify` | AI-powered variable renaming | Make code readable |
| `capture-scripts` | Browser-based capture | Complement with DevTools |
| `parse_har.py` | HAR file analysis | Analyze captured traffic |

## Examples

### Clone a React SPA

```bash
node clone.js \
  --url="https://react-app.example.com" \
  --depth=5 \
  --delay=1000 \
  --screenshots \
  --output="./react-clone"
```

### Clone with authentication

```bash
node clone.js \
  --url="https://app.example.com/login" \
  --login="myuser:mypassword" \
  --depth=10 \
  --exclude="/logout,/api/auth/signout" \
  --output="./authenticated-clone"
```

### Interactive exploration

```bash
node clone.js \
  --url="https://example.com" \
  --interactive \
  --headed \
  --depth=2
```

### Fast static site download

```bash
python ~/Desktop/WEBSITE_DOWNLOADER/website-downloader.py \
  --url="https://static-site.com" \
  --max-pages=500 \
  --threads=12
```

## Troubleshooting

### Site uses Cloudflare/Bot Protection
- Use `--headed` mode and solve CAPTCHAs manually
- Add `--delay=2000` to slow down requests
- Consider using the interactive mode

### JavaScript not rendering
- Increase `--timeout` value
- The site may require specific cookies/headers

### Too many pages
- Set `--max-pages` to limit crawl
- Use `--exclude` to skip sections
- Set lower `--depth`

### Login not working
- Check credentials format: `username:password`
- The site may use non-standard login forms
- Try `--headed --interactive` to see what's happening

## License

MIT
