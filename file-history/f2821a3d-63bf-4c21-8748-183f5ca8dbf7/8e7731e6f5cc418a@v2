/**
 * Slice 4: GPT-4o-mini Transcription Service
 *
 * Handles audio transcription using OpenAI's gpt-4o-mini-transcribe model.
 * 50% cheaper than Whisper ($0.003/min vs $0.006/min).
 * Returns timestamped segments for take detection.
 * Includes caching to avoid repeated API calls.
 */

const fs = require('fs');
const OpenAI = require('openai');

// Initialize OpenAI client
const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });

// In-memory cache: { wavPath: { mtime, result } }
const transcriptCache = new Map();

/**
 * Transcribe audio file using GPT-4o-mini-transcribe (with caching)
 * @param {string} wavPath - Path to the WAV file
 * @returns {Promise<{text: string, segments: Array, language: string, duration: number}>}
 */
async function transcribeAudio(wavPath) {
  // Check cache based on file modification time
  const stats = fs.statSync(wavPath);
  const mtime = stats.mtimeMs;

  const cached = transcriptCache.get(wavPath);
  if (cached && cached.mtime === mtime) {
    console.log('[SPLICE] Using cached transcription (file unchanged)');
    return cached.result;
  }

  console.log('[SPLICE] Starting GPT-4o-mini transcription...');

  // GPT-4o-mini-transcribe uses the same transcriptions API as Whisper
  // but only supports 'json' response format (not verbose_json)
  const transcription = await openai.audio.transcriptions.create({
    file: fs.createReadStream(wavPath),
    model: 'gpt-4o-mini-transcribe',
    response_format: 'json',
    language: 'en',
  });

  console.log(`[SPLICE] Transcription complete: ${transcription.text.slice(0, 100)}...`);

  // GPT-4o-mini-transcribe returns simpler response than verbose_json
  // We need to estimate segments from the text if not provided
  const result = {
    text: transcription.text,
    segments: transcription.segments || estimateSegments(transcription.text),
    language: transcription.language || 'en',
    duration: transcription.duration || 0
  };

  // Cache the result
  transcriptCache.set(wavPath, { mtime, result });
  console.log('[SPLICE] Transcription cached');

  return result;
}

/**
 * Estimate segments from text when not provided by API
 * Creates rough segments based on sentence boundaries
 * @param {string} text - Full transcript text
 * @returns {Array} Estimated segments
 */
function estimateSegments(text) {
  if (!text) return [];

  // Split by sentence boundaries
  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];

  // Create segments with estimated timing (will be refined by take detection)
  return sentences.map((sentence, index) => ({
    id: index,
    start: 0, // Will be refined by take detection
    end: 0,
    text: sentence.trim()
  }));
}

/**
 * Transcribe audio with word-level timestamps using Whisper
 * Required for profanity, repetition, and stutter detection endpoints.
 *
 * @param {string} wavPath - Path to the WAV file
 * @returns {Promise<{text: string, words: Array<{word: string, start: number, end: number}>}>}
 */
async function transcribeWithWords(wavPath) {
  // Check cache based on file modification time
  const stats = fs.statSync(wavPath);
  const mtime = stats.mtimeMs;
  const cacheKey = `words:${wavPath}`;

  const cached = transcriptCache.get(cacheKey);
  if (cached && cached.mtime === mtime) {
    console.log('[SPLICE] Using cached word-level transcription (file unchanged)');
    return cached.result;
  }

  console.log('[SPLICE] Starting Whisper transcription with word timestamps...');

  // Use whisper-1 with word-level timestamp granularity
  let transcription;
  try {
    transcription = await openai.audio.transcriptions.create({
      file: fs.createReadStream(wavPath),
      model: 'whisper-1',
      response_format: 'verbose_json',
      timestamp_granularities: ['word'],
      language: 'en',
    });
  } catch (err) {
    // Handle quota errors with clear message
    if (err.code === 'insufficient_quota' || err.message?.includes('quota')) {
      throw new Error('OpenAI API quota exceeded. Please check your billing at https://platform.openai.com/account/billing');
    }
    // Handle connection errors with retry hint
    if (err.code === 'ECONNRESET' || err.message?.includes('Connection error')) {
      throw new Error('OpenAI API connection failed. Please try again in a moment.');
    }
    throw err;
  }

  console.log(`[SPLICE] Word transcription complete: ${transcription.words?.length || 0} words`);

  // Map to consistent format
  const result = {
    text: transcription.text,
    words: (transcription.words || []).map(w => ({
      word: w.word,
      start: w.start,
      end: w.end
    })),
    language: transcription.language || 'en',
    duration: transcription.duration || 0
  };

  // Cache the result
  transcriptCache.set(cacheKey, { mtime, result });
  console.log(`[SPLICE] Word transcription cached (${result.words.length} words)`);

  return result;
}

/**
 * Clear transcription cache
 * Useful for testing or memory management
 */
function clearCache() {
  const size = transcriptCache.size;
  transcriptCache.clear();
  console.log(`[SPLICE] Cleared ${size} cached transcription(s)`);
  return size;
}

/**
 * Get cache statistics
 */
function getCacheStats() {
  return {
    entries: transcriptCache.size,
    keys: Array.from(transcriptCache.keys())
  };
}

module.exports = { transcribeAudio, transcribeWithWords, clearCache, getCacheStats };
