/**
 * Multitrack/Multicam Analysis Service
 *
 * Analyzes multiple audio tracks to determine optimal video angle selection.
 * Based on Fireside's proven multicam editing approach.
 *
 * Features:
 * - Per-speaker RMS audio level analysis
 * - Noise floor subtraction for cleaner detection
 * - Automatic video angle selection based on loudest speaker
 * - Wide shot detection (multiple speakers talking)
 * - Cutaway insertion for long clips
 * - Speaker screentime balancing
 * - Gaussian smoothing to reduce spurious cuts
 */

const fsPromises = require('fs').promises;
const { validateAudioPath, safeFFprobe, safeFFmpeg, safeTempPath } = require('./securityUtils');

// =============================================================================
// Configuration
// =============================================================================

const DEFAULT_OPTIONS = {
  // Analysis parameters
  analysisWindowMs: 100,       // RMS window size in milliseconds
  smoothingWindow: 5,          // Gaussian blur kernel size (frames)

  // Switching parameters
  switchingFrequency: 50,      // How often to allow cuts (0-100)
  minShotDuration: 2.0,        // Minimum seconds before next cut
  speakerBoosts: {},           // Per-speaker dB adjustments

  // Wide shot parameters
  wideShotEnabled: true,
  wideShotPercentage: 20,      // Target % of time in wide shots
  wideShotTolerance: 6,        // dB tolerance for multiple speaker detection

  // Cutaway parameters
  cutawayEnabled: false,
  cutawayMinDuration: 1.5,
  cutawayMaxDuration: 4.0,
  cutawayTracks: [],           // Video track indices for cutaways

  // Output
  frameRate: 30
};

// =============================================================================
// Core Analysis
// =============================================================================

/**
 * Analyze multiple audio tracks for multicam editing
 *
 * @param {Array<string>} audioPaths - Paths to audio files (one per speaker)
 * @param {Object} options - Analysis options
 * @returns {Promise<Object>} Analysis results with switching decisions
 */
async function analyzeMultitrack(audioPaths, options = {}) {
  const opts = { ...DEFAULT_OPTIONS, ...options };

  console.log(`[SPLICE Multitrack] Analyzing ${audioPaths.length} audio track(s)`);

  // Step 1: Extract RMS levels for each track
  const trackLevels = await Promise.all(
    audioPaths.map((audioPath, i) =>
      extractTrackLevels(audioPath, {
        windowMs: opts.analysisWindowMs,
        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`
      })
    )
  );

  // Validate all tracks have same duration
  const durations = trackLevels.map(t => t.duration);
  const maxDuration = Math.max(...durations);
  const minDuration = Math.min(...durations);
  if (maxDuration - minDuration > 1) {
    console.warn('[SPLICE Multitrack] Track durations differ significantly');
  }

  // Step 2: Calculate noise floor and clean up levels
  const cleanedLevels = cleanupAudioLevels(trackLevels, opts);

  // Step 3: Apply speaker boosts
  const boostedLevels = applySpeakerBoosts(cleanedLevels, opts.speakerBoosts);

  // Step 4: Apply Gaussian smoothing
  const smoothedLevels = opts.smoothingWindow > 0
    ? smoothAudioLevels(boostedLevels, opts.smoothingWindow)
    : boostedLevels;

  // Step 5: Generate switching decisions
  const decisions = generateSwitchingDecisions(smoothedLevels, {
    videoTrackMapping: opts.videoTrackMapping,
    minShotDuration: opts.minShotDuration,
    switchingFrequency: opts.switchingFrequency,
    wideShotEnabled: opts.wideShotEnabled,
    wideShotTolerance: opts.wideShotTolerance,
    wideShotTracks: opts.wideShotTracks,
    frameRate: opts.frameRate
  });

  // Step 6: Fine-tune wide shot percentage
  const tunedDecisions = opts.wideShotEnabled
    ? tuneWideShotPercentage(decisions, opts.wideShotPercentage)
    : decisions;

  // Step 7: Insert cutaways if enabled
  const finalDecisions = opts.cutawayEnabled
    ? insertCutaways(tunedDecisions, {
        cutawayTracks: opts.cutawayTracks,
        minDuration: opts.cutawayMinDuration,
        maxDuration: opts.cutawayMaxDuration
      })
    : tunedDecisions;

  // Step 8: Calculate statistics
  const stats = calculateMultitrackStats(finalDecisions, trackLevels[0].duration);

  console.log(`[SPLICE Multitrack] Generated ${finalDecisions.length} switching decision(s)`);

  return {
    decisions: finalDecisions,
    metadata: {
      trackCount: audioPaths.length,
      speakerNames: opts.speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),
      duration: maxDuration,
      frameRate: opts.frameRate,
      ...stats
    },
    levels: {
      raw: trackLevels.map(t => ({ speaker: t.speakerName, sampleCount: t.levels.length })),
      // Don't include full level data in response (too large)
    }
  };
}

/**
 * Analyze multitrack using pre-extracted levels (PERF-008)
 * Skips FFmpeg processing for faster iteration during auto-balance
 *
 * @param {Array<Object>} cachedLevels - Pre-extracted track levels from extractTrackLevels
 * @param {Object} options - Analysis options
 * @returns {Object} Analysis results with switching decisions
 */
function analyzeMultitrackFromLevels(cachedLevels, options = {}) {
  const opts = { ...DEFAULT_OPTIONS, ...options };

  // Validate all tracks have same duration
  const durations = cachedLevels.map(t => t.duration);
  const maxDuration = Math.max(...durations);
  const minDuration = Math.min(...durations);
  if (maxDuration - minDuration > 1) {
    console.warn('[SPLICE Multitrack] Track durations differ significantly');
  }

  // Step 2: Calculate noise floor and clean up levels
  const cleanedLevels = cleanupAudioLevels(cachedLevels, opts);

  // Step 3: Apply speaker boosts
  const boostedLevels = applySpeakerBoosts(cleanedLevels, opts.speakerBoosts);

  // Step 4: Apply Gaussian smoothing
  const smoothedLevels = opts.smoothingWindow > 0
    ? smoothAudioLevels(boostedLevels, opts.smoothingWindow)
    : boostedLevels;

  // Step 5: Generate switching decisions
  const decisions = generateSwitchingDecisions(smoothedLevels, {
    videoTrackMapping: opts.videoTrackMapping,
    minShotDuration: opts.minShotDuration,
    switchingFrequency: opts.switchingFrequency,
    wideShotEnabled: opts.wideShotEnabled,
    wideShotTolerance: opts.wideShotTolerance,
    wideShotTracks: opts.wideShotTracks,
    frameRate: opts.frameRate
  });

  // Step 6: Fine-tune wide shot percentage
  const tunedDecisions = opts.wideShotEnabled
    ? tuneWideShotPercentage(decisions, opts.wideShotPercentage)
    : decisions;

  // Step 7: Insert cutaways if enabled
  const finalDecisions = opts.cutawayEnabled
    ? insertCutaways(tunedDecisions, {
        cutawayTracks: opts.cutawayTracks,
        minDuration: opts.cutawayMinDuration,
        maxDuration: opts.cutawayMaxDuration
      })
    : tunedDecisions;

  // Step 8: Calculate statistics
  const stats = calculateMultitrackStats(finalDecisions, cachedLevels[0].duration);

  return {
    decisions: finalDecisions,
    metadata: {
      trackCount: cachedLevels.length,
      speakerNames: opts.speakerNames || cachedLevels.map((_, i) => `Speaker ${i + 1}`),
      duration: maxDuration,
      frameRate: opts.frameRate,
      ...stats
    },
    levels: {
      raw: cachedLevels.map(t => ({ speaker: t.speakerName, sampleCount: t.levels.length })),
    }
  };
}

// =============================================================================
// Audio Level Extraction
// =============================================================================

/**
 * Extract RMS levels from audio file
 *
 * @param {string} audioPath - Path to audio file
 * @param {Object} options - Extraction options
 * @returns {Promise<Object>} Track levels with timing
 */
async function extractTrackLevels(audioPath, options = {}) {
  const { windowMs = 100, speakerName = 'Unknown' } = options;

  // SECURITY: Validate input path to prevent path traversal
  const pathValidation = await validateAudioPath(audioPath);
  if (!pathValidation.valid) {
    throw new Error(`Invalid audio path: ${pathValidation.error}`);
  }
  const validatedPath = pathValidation.path;

  console.log(`[SPLICE Multitrack] Extracting levels: ${speakerName}`);

  // SECURITY: Use execFile with array arguments to prevent command injection
  const infoArgs = [
    '-v', 'error',
    '-select_streams', 'a:0',
    '-show_entries', 'stream=sample_rate,duration',
    '-of', 'json',
    validatedPath
  ];
  let duration = 0;

  try {
    const { stdout } = await safeFFprobe(infoArgs);
    const info = JSON.parse(stdout);
    if (info.streams?.[0]) {
      // Note: sample_rate available in info.streams[0].sample_rate for future resampling support
      duration = parseFloat(info.streams[0].duration) || 0;
    }
  } catch (err) {
    console.warn(`[SPLICE Multitrack] Could not get audio info: ${err.message}`);
  }

  // Get duration from format if not in stream
  if (!duration) {
    try {
      const durationArgs = [
        '-v', 'error',
        '-show_entries', 'format=duration',
        '-of', 'default=noprint_wrappers=1:nokey=1',
        validatedPath
      ];
      const { stdout } = await safeFFprobe(durationArgs);
      duration = parseFloat(stdout.trim()) || 0;
    } catch (err) {
      console.warn(`[SPLICE Multitrack] Could not get duration: ${err.message}`);
    }
  }

  // SECURITY: Use safe temp path generation
  const tempFile = safeTempPath('splice_multitrack', '.raw');

  try {
    // SECURITY: Use execFile with array arguments to prevent command injection
    const extractArgs = [
      '-y',
      '-i', validatedPath,
      '-ac', '1',
      '-ar', '16000',
      '-f', 's16le',
      '-acodec', 'pcm_s16le',
      tempFile
    ];
    await safeFFmpeg(extractArgs);

    // PERFORMANCE: Use async file read to avoid blocking event loop
    const rawData = await fsPromises.readFile(tempFile);
    const samples = new Float32Array(rawData.length / 2);
    for (let i = 0; i < samples.length; i++) {
      samples[i] = rawData.readInt16LE(i * 2) / 32768.0;
    }

    // Calculate RMS per window
    const windowSamples = Math.floor((windowMs / 1000) * 16000);
    const hopSamples = windowSamples; // Non-overlapping windows
    const numWindows = Math.floor(samples.length / hopSamples);

    const levels = new Float32Array(numWindows);
    const times = new Float32Array(numWindows);

    for (let i = 0; i < numWindows; i++) {
      const start = i * hopSamples;
      const end = Math.min(start + windowSamples, samples.length);

      let sumSquares = 0;
      for (let j = start; j < end; j++) {
        sumSquares += samples[j] * samples[j];
      }

      const rms = Math.sqrt(sumSquares / (end - start));
      const dbfs = rms > 0 ? 20 * Math.log10(rms) : -100;

      levels[i] = Math.max(-100, dbfs);
      times[i] = (start / 16000);
    }

    // PERFORMANCE: Use async file operations
    await fsPromises.unlink(tempFile);

    return { levels, times, duration, speakerName, sampleRate: 16000 };
  } catch (err) {
    // PERFORMANCE: Use async file operations for cleanup
    try {
      await fsPromises.access(tempFile);
      await fsPromises.unlink(tempFile);
    } catch {
      // File doesn't exist
    }
    throw new Error(`Failed to extract track levels: ${err.message}`);
  }
}

// =============================================================================
// Level Processing
// =============================================================================

/**
 * Clean up audio levels by subtracting noise floor
 *
 * @param {Array} trackLevels - Array of track level objects
 * @param {Object} options - Cleanup options
 * @returns {Array} Cleaned level arrays
 */
function cleanupAudioLevels(trackLevels, _options = {}) {
  const numTracks = trackLevels.length;
  const numSamples = trackLevels[0].levels.length;

  // Calculate average level (noise floor estimate)
  const averageLevels = new Float32Array(numSamples);
  for (let i = 0; i < numSamples; i++) {
    let sum = 0;
    for (let t = 0; t < numTracks; t++) {
      sum += trackLevels[t].levels[i];
    }
    averageLevels[i] = sum / numTracks;
  }

  // Subtract average from each track (relative loudness)
  const cleaned = trackLevels.map(track => {
    const cleanedLevels = new Float32Array(numSamples);
    for (let i = 0; i < numSamples; i++) {
      // Normalize to -100 to 0 range
      cleanedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] - averageLevels[i]));
    }
    return {
      ...track,
      levels: cleanedLevels
    };
  });

  return cleaned;
}

/**
 * Apply per-speaker dB boosts
 *
 * @param {Array} trackLevels - Cleaned track levels
 * @param {Object} boosts - Speaker boost mapping { "Speaker 1": 5, ... }
 * @returns {Array} Boosted levels
 */
function applySpeakerBoosts(trackLevels, boosts = {}) {
  return trackLevels.map(track => {
    const boost = boosts[track.speakerName] || 0;
    if (boost === 0) return track;

    const boostedLevels = new Float32Array(track.levels.length);
    for (let i = 0; i < track.levels.length; i++) {
      boostedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] + boost));
    }

    return { ...track, levels: boostedLevels };
  });
}

/**
 * Apply Gaussian smoothing to reduce noise
 *
 * @param {Array} trackLevels - Track levels
 * @param {number} windowSize - Gaussian kernel size
 * @returns {Array} Smoothed levels
 */
function smoothAudioLevels(trackLevels, windowSize = 5) {
  const kernel = generateGaussianKernel(windowSize);

  return trackLevels.map(track => {
    const smoothed = applyConvolution(track.levels, kernel);
    return { ...track, levels: smoothed };
  });
}

/**
 * Generate Gaussian kernel
 */
function generateGaussianKernel(size) {
  const sigma = size / 3;
  const kernel = new Float32Array(size);
  const half = Math.floor(size / 2);
  let sum = 0;

  for (let i = 0; i < size; i++) {
    const x = i - half;
    kernel[i] = Math.exp(-(x * x) / (2 * sigma * sigma));
    sum += kernel[i];
  }

  // Normalize
  for (let i = 0; i < size; i++) {
    kernel[i] /= sum;
  }

  return kernel;
}

/**
 * Apply 1D convolution
 */
function applyConvolution(data, kernel) {
  const result = new Float32Array(data.length);
  const half = Math.floor(kernel.length / 2);

  for (let i = 0; i < data.length; i++) {
    let sum = 0;
    for (let j = 0; j < kernel.length; j++) {
      const idx = i + j - half;
      if (idx >= 0 && idx < data.length) {
        sum += data[idx] * kernel[j];
      }
    }
    result[i] = sum;
  }

  return result;
}

// =============================================================================
// Switching Decision Generation
// =============================================================================

/**
 * Generate video switching decisions based on audio levels
 *
 * @param {Array} trackLevels - Processed track levels
 * @param {Object} options - Decision options
 * @returns {Array} Switching decisions
 */
function generateSwitchingDecisions(trackLevels, options = {}) {
  const {
    videoTrackMapping = {},
    minShotDuration = 2.0,
    switchingFrequency = 50,
    wideShotEnabled = true,
    wideShotTolerance = 6,
    wideShotTracks = []
    // frameRate reserved for future frame-aligned cutting
  } = options;

  const numSamples = trackLevels[0].levels.length;
  const times = trackLevels[0].times;
  const decisions = [];

  // Calculate min samples between cuts based on switching frequency
  const minSamplesPerCut = Math.ceil(minShotDuration / (times[1] - times[0]));
  const switchingCooldown = Math.ceil(minSamplesPerCut * (1 - switchingFrequency / 100));

  let currentSpeaker = -1;
  // currentDecisionStart reserved for future decision grouping
  let lastSwitchSample = 0;

  for (let i = 0; i < numSamples; i++) {
    // Find loudest speaker
    let maxLevel = -100;
    let loudestSpeaker = 0;
    let speakersAboveThreshold = [];

    for (let t = 0; t < trackLevels.length; t++) {
      const level = trackLevels[t].levels[i];
      if (level > maxLevel) {
        maxLevel = level;
        loudestSpeaker = t;
      }

      // Track speakers within tolerance of max (for wide shot detection)
      if (wideShotEnabled && level >= maxLevel - wideShotTolerance) {
        speakersAboveThreshold.push(t);
      }
    }

    // Determine if this should be a wide shot
    const isWideShot = wideShotEnabled && speakersAboveThreshold.length >= 2;
    const effectiveSpeaker = isWideShot ? -1 : loudestSpeaker; // -1 = wide shot

    // Check if we should switch
    const canSwitch = (i - lastSwitchSample) >= switchingCooldown;
    const shouldSwitch = canSwitch && effectiveSpeaker !== currentSpeaker;

    if (shouldSwitch || i === 0) {
      // Close previous decision
      if (i > 0 && decisions.length > 0) {
        decisions[decisions.length - 1].endTime = times[i];
        decisions[decisions.length - 1].endSample = i;
      }

      // Start new decision
      const speakerName = isWideShot
        ? 'Wide Shot'
        : trackLevels[loudestSpeaker].speakerName;

      const videoTrack = isWideShot
        ? (wideShotTracks[0] ?? 0)
        : (videoTrackMapping[loudestSpeaker] ?? loudestSpeaker);

      decisions.push({
        speakerIndex: effectiveSpeaker,
        speakerName,
        speakers: speakersAboveThreshold.map(s => trackLevels[s].speakerName),
        videoTrack,
        startTime: times[i],
        startSample: i,
        endTime: times[numSamples - 1],
        endSample: numSamples - 1,
        isWideShot,
        reason: isWideShot ? 'Multiple speakers' : 'Loudest speaker'
      });

      currentSpeaker = effectiveSpeaker;
      lastSwitchSample = i;
    }
  }

  // Finalize last decision
  if (decisions.length > 0) {
    decisions[decisions.length - 1].endTime = times[numSamples - 1];
    decisions[decisions.length - 1].endSample = numSamples - 1;
  }

  // Add duration to each decision
  return decisions.map(d => ({
    ...d,
    duration: parseFloat((d.endTime - d.startTime).toFixed(3))
  }));
}

// =============================================================================
// Wide Shot Tuning
// =============================================================================

/**
 * Fine-tune wide shot percentage using tolerance adjustment
 *
 * @param {Array} decisions - Initial decisions
 * @param {number} targetPercentage - Target wide shot percentage
 * @returns {Array} Tuned decisions
 */
function tuneWideShotPercentage(decisions, targetPercentage) {
  const totalDuration = decisions.reduce((sum, d) => sum + d.duration, 0);
  const currentWideTime = decisions
    .filter(d => d.isWideShot)
    .reduce((sum, d) => sum + d.duration, 0);
  const currentPercentage = (currentWideTime / totalDuration) * 100;

  // If within 5% of target, don't adjust
  if (Math.abs(currentPercentage - targetPercentage) <= 5) {
    return decisions;
  }

  console.log(`[SPLICE Multitrack] Wide shot: ${currentPercentage.toFixed(1)}% (target: ${targetPercentage}%)`);

  // Simple approach: mark some single-speaker decisions as wide shots
  // or vice versa based on whether we need more or fewer wide shots
  // (Full implementation would re-run with adjusted tolerance)

  return decisions; // Return as-is for now
}

// =============================================================================
// Cutaway Insertion
// =============================================================================

/**
 * Insert cutaways into long clips
 *
 * @param {Array} decisions - Switching decisions
 * @param {Object} options - Cutaway options
 * @returns {Array} Decisions with cutaways inserted
 */
function insertCutaways(decisions, options = {}) {
  const {
    cutawayTracks = [],
    minDuration = 1.5,
    maxDuration = 4.0,
    maxClipDuration = 30 // Insert cutaway if clip longer than this
  } = options;

  if (cutawayTracks.length === 0) return decisions;

  const result = [];

  for (const decision of decisions) {
    if (decision.duration <= maxClipDuration) {
      result.push(decision);
      continue;
    }

    // Split long clip with cutaways
    const numCutaways = Math.floor(decision.duration / maxClipDuration);
    const interval = decision.duration / (numCutaways + 1);

    let currentStart = decision.startTime;

    for (let i = 0; i < numCutaways; i++) {
      // Add main clip segment
      const cutawayStart = currentStart + interval - minDuration / 2;
      const cutawayDuration = minDuration + Math.random() * (maxDuration - minDuration);
      const cutawayEnd = cutawayStart + cutawayDuration;

      result.push({
        ...decision,
        startTime: currentStart,
        endTime: cutawayStart,
        duration: cutawayStart - currentStart
      });

      // Add cutaway
      const cutawayTrack = cutawayTracks[i % cutawayTracks.length];
      result.push({
        speakerIndex: -2, // Special index for cutaway
        speakerName: 'Cutaway',
        speakers: [],
        videoTrack: cutawayTrack,
        startTime: cutawayStart,
        endTime: cutawayEnd,
        duration: cutawayDuration,
        isWideShot: false,
        isCutaway: true,
        reason: 'Cutaway insert'
      });

      currentStart = cutawayEnd;
    }

    // Add final segment
    result.push({
      ...decision,
      startTime: currentStart,
      endTime: decision.endTime,
      duration: decision.endTime - currentStart
    });
  }

  return result;
}

// =============================================================================
// Statistics
// =============================================================================

/**
 * Calculate multitrack analysis statistics
 *
 * @param {Array} decisions - Final decisions
 * @param {number} totalDuration - Total audio duration
 * @returns {Object} Statistics
 */
function calculateMultitrackStats(decisions, totalDuration) {
  const speakerTimes = {};
  let wideShotTime = 0;
  let cutawayTime = 0;

  for (const d of decisions) {
    if (d.isCutaway) {
      cutawayTime += d.duration;
    } else if (d.isWideShot) {
      wideShotTime += d.duration;
    } else {
      speakerTimes[d.speakerName] = (speakerTimes[d.speakerName] || 0) + d.duration;
    }
  }

  const speakerPercentages = {};
  for (const [speaker, time] of Object.entries(speakerTimes)) {
    speakerPercentages[speaker] = parseFloat(((time / totalDuration) * 100).toFixed(1));
  }

  return {
    decisionCount: decisions.length,
    wideShotPercentage: parseFloat(((wideShotTime / totalDuration) * 100).toFixed(1)),
    cutawayPercentage: parseFloat(((cutawayTime / totalDuration) * 100).toFixed(1)),
    speakerPercentages,
    averageShotDuration: parseFloat((totalDuration / decisions.length).toFixed(2))
  };
}

// =============================================================================
// Auto-Balance Optimizer
// =============================================================================

/**
 * Auto-balance speaker screentime
 * Adjusts speaker boosts to achieve equal distribution
 * PERF-008: Caches audio levels to avoid re-extraction on each iteration
 *
 * @param {Array<string>} audioPaths - Audio file paths
 * @param {Object} options - Base analysis options
 * @returns {Promise<Object>} Optimized analysis with best parameters
 */
async function autoBalanceMultitrack(audioPaths, options = {}) {
  const opts = { ...DEFAULT_OPTIONS, ...options };
  const numSpeakers = audioPaths.length;
  const targetShare = 100 / numSpeakers;

  console.log(`[SPLICE Multitrack] Auto-balancing for ${numSpeakers} speakers (target: ${targetShare.toFixed(1)}% each)`);

  // PERF-008: Extract levels once and cache them (avoids re-running FFmpeg)
  const cachedLevels = await Promise.all(
    audioPaths.map((audioPath, i) =>
      extractTrackLevels(audioPath, {
        windowMs: opts.analysisWindowMs,
        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`
      })
    )
  );

  // Run initial analysis using cached levels
  let bestResult = analyzeMultitrackFromLevels(cachedLevels, opts);
  let bestError = calculateDistributionError(bestResult.metadata.speakerPercentages, targetShare);

  // Iterative optimization
  const maxIterations = 10;
  const boostStep = 3; // dB step

  for (let iter = 0; iter < maxIterations; iter++) {
    // Find most over/under-represented speakers
    const percentages = bestResult.metadata.speakerPercentages;
    let maxOver = { speaker: null, diff: 0 };
    let maxUnder = { speaker: null, diff: 0 };

    for (const [speaker, pct] of Object.entries(percentages)) {
      const diff = pct - targetShare;
      if (diff > maxOver.diff) maxOver = { speaker, diff };
      if (diff < maxUnder.diff) maxUnder = { speaker, diff };
    }

    if (Math.abs(maxOver.diff) < 1 && Math.abs(maxUnder.diff) < 1) {
      console.log(`[SPLICE Multitrack] Converged at iteration ${iter + 1}`);
      break;
    }

    // Adjust boosts
    const newBoosts = { ...opts.speakerBoosts };
    if (maxOver.speaker) {
      newBoosts[maxOver.speaker] = (newBoosts[maxOver.speaker] || 0) - boostStep;
    }
    if (maxUnder.speaker) {
      newBoosts[maxUnder.speaker] = (newBoosts[maxUnder.speaker] || 0) + boostStep;
    }

    // PERF-008: Use cached levels instead of re-extracting via analyzeMultitrack
    const testResult = analyzeMultitrackFromLevels(cachedLevels, { ...opts, speakerBoosts: newBoosts });
    const testError = calculateDistributionError(testResult.metadata.speakerPercentages, targetShare);

    if (testError < bestError) {
      bestResult = testResult;
      bestError = testError;
      opts.speakerBoosts = newBoosts;
      console.log(`[SPLICE Multitrack] Iteration ${iter + 1}: error ${testError.toFixed(2)}%`);
    }
  }

  return {
    ...bestResult,
    optimized: true,
    speakerBoosts: opts.speakerBoosts,
    finalError: bestError
  };
}

/**
 * Calculate distribution error (max absolute deviation from target)
 */
function calculateDistributionError(percentages, targetShare) {
  let maxError = 0;
  for (const pct of Object.values(percentages)) {
    maxError = Math.max(maxError, Math.abs(pct - targetShare));
  }
  return maxError;
}

/**
 * Advanced multitrack balancing with genetic algorithm optimization
 * Provides finer control over switching behavior with constraints
 *
 * @param {Array<string>} audioPaths - Paths to audio files
 * @param {Object} options - Advanced balancing options
 * @returns {Promise<Object>} Optimized analysis result
 */
async function advancedBalanceMultitrack(audioPaths, options = {}) {
  const {
    maxConsecutiveSeconds = 30,   // Max time for single speaker
    momentumFactor = 0.7,          // Bias toward current speaker (0-1)
    populationSize = 20,           // GA population size
    generations = 10,              // GA generations
    mutationRate = 0.1,            // GA mutation rate
    targetDistribution = null,     // Custom target percentages per speaker
    ...baseOptions
  } = options;

  const opts = { ...DEFAULT_OPTIONS, ...baseOptions };
  const numSpeakers = audioPaths.length;
  const targetShare = 100 / numSpeakers;

  console.log(`[SPLICE Multitrack] Advanced balancing for ${numSpeakers} speakers`);
  console.log(`  - Max consecutive: ${maxConsecutiveSeconds}s`);
  console.log(`  - Momentum factor: ${momentumFactor}`);
  console.log(`  - GA population: ${populationSize}, generations: ${generations}`);

  // Extract levels once and cache
  const cachedLevels = await Promise.all(
    audioPaths.map((audioPath, i) =>
      extractTrackLevels(audioPath, {
        windowMs: opts.analysisWindowMs,
        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`
      })
    )
  );

  // Get speaker names for target distribution
  const speakerNames = cachedLevels.map(l => l.speakerName);

  // Build target distribution
  const targetDist = targetDistribution || {};
  for (const name of speakerNames) {
    if (!targetDist[name]) {
      targetDist[name] = targetShare;
    }
  }

  // Generate initial population of boost configurations
  let population = generatePopulation(speakerNames, populationSize);

  // Evaluate fitness for each individual
  const evaluateFitness = (boosts) => {
    const result = analyzeMultitrackFromLevels(cachedLevels, {
      ...opts,
      speakerBoosts: boosts
    });

    // Calculate fitness score (lower is better)
    let fitness = 0;

    // Distribution error
    const pcts = result.metadata.speakerPercentages;
    for (const [speaker, target] of Object.entries(targetDist)) {
      fitness += Math.pow((pcts[speaker] || 0) - target, 2);
    }

    // Consecutive speaker penalty
    const decisions = result.decisions || [];
    let currentSpeaker = null;
    let currentDuration = 0;
    let consecutivePenalty = 0;

    for (const decision of decisions) {
      if (decision.speaker === currentSpeaker) {
        currentDuration += decision.duration;
        if (currentDuration > maxConsecutiveSeconds) {
          consecutivePenalty += (currentDuration - maxConsecutiveSeconds);
        }
      } else {
        currentSpeaker = decision.speaker;
        currentDuration = decision.duration;
      }
    }
    fitness += consecutivePenalty * 10;

    return { fitness, result };
  };

  // Genetic algorithm loop
  let bestIndividual = population[0];
  let bestFitness = Infinity;
  let bestResult = null;

  for (let gen = 0; gen < generations; gen++) {
    // Evaluate all individuals
    const evaluated = population.map(boosts => {
      const { fitness, result } = evaluateFitness(boosts);
      if (fitness < bestFitness) {
        bestFitness = fitness;
        bestIndividual = boosts;
        bestResult = result;
      }
      return { boosts, fitness };
    });

    // Sort by fitness
    evaluated.sort((a, b) => a.fitness - b.fitness);

    // Selection - keep top 50%
    const survivors = evaluated.slice(0, Math.floor(populationSize / 2));

    // Crossover and mutation
    const newPopulation = survivors.map(s => s.boosts);

    while (newPopulation.length < populationSize) {
      // Select two parents
      const parent1 = survivors[Math.floor(Math.random() * survivors.length)].boosts;
      const parent2 = survivors[Math.floor(Math.random() * survivors.length)].boosts;

      // Crossover
      const child = {};
      for (const speaker of speakerNames) {
        child[speaker] = Math.random() < 0.5 ? parent1[speaker] : parent2[speaker];

        // Mutation
        if (Math.random() < mutationRate) {
          child[speaker] += (Math.random() - 0.5) * 6; // +/- 3dB
        }
      }
      newPopulation.push(child);
    }

    population = newPopulation;

    if (gen % 2 === 0 || gen === generations - 1) {
      console.log(`[SPLICE Multitrack] Generation ${gen + 1}: best fitness ${bestFitness.toFixed(2)}`);
    }
  }

  // Apply momentum factor to final decisions
  if (bestResult && momentumFactor > 0) {
    bestResult = applyMomentum(bestResult, momentumFactor, maxConsecutiveSeconds);
  }

  return {
    ...bestResult,
    optimized: true,
    advanced: true,
    speakerBoosts: bestIndividual,
    finalError: bestFitness,
    constraints: {
      maxConsecutiveSeconds,
      momentumFactor,
      generations,
      populationSize
    }
  };
}

/**
 * Generate initial population of boost configurations
 */
function generatePopulation(speakerNames, size) {
  const population = [];

  // Always include baseline (no boosts)
  const baseline = {};
  for (const name of speakerNames) {
    baseline[name] = 0;
  }
  population.push(baseline);

  // Generate random variations
  for (let i = 1; i < size; i++) {
    const individual = {};
    for (const name of speakerNames) {
      individual[name] = (Math.random() - 0.5) * 12; // -6 to +6 dB
    }
    population.push(individual);
  }

  return population;
}

/**
 * Apply momentum factor to reduce rapid switching
 */
function applyMomentum(result, momentumFactor, maxConsecutive) {
  const decisions = result.decisions || [];
  if (decisions.length < 2) return result;

  const adjusted = [decisions[0]];
  let currentSpeaker = decisions[0].speaker;
  let currentDuration = decisions[0].duration;

  for (let i = 1; i < decisions.length; i++) {
    const decision = { ...decisions[i] };

    // Check if switch is preferred by momentum
    const switchProbability = 1 - momentumFactor;
    const forcedSwitch = currentDuration >= maxConsecutive;

    if (decision.speaker === currentSpeaker) {
      // Same speaker - accumulate duration
      currentDuration += decision.duration;
      adjusted.push(decision);
    } else if (forcedSwitch || Math.random() < switchProbability) {
      // Switch speakers (forced or allowed by momentum)
      currentSpeaker = decision.speaker;
      currentDuration = decision.duration;
      adjusted.push(decision);
    } else {
      // Momentum prevents switch - keep current speaker
      const merged = {
        ...adjusted[adjusted.length - 1],
        end: decision.end,
        duration: adjusted[adjusted.length - 1].duration + decision.duration
      };
      adjusted[adjusted.length - 1] = merged;
      currentDuration += decision.duration;
    }
  }

  // Recalculate speaker percentages
  const totalDuration = adjusted.reduce((sum, d) => sum + d.duration, 0);
  const speakerTimes = {};
  for (const d of adjusted) {
    speakerTimes[d.speaker] = (speakerTimes[d.speaker] || 0) + d.duration;
  }
  const speakerPercentages = {};
  for (const [speaker, time] of Object.entries(speakerTimes)) {
    speakerPercentages[speaker] = parseFloat(((time / totalDuration) * 100).toFixed(2));
  }

  return {
    ...result,
    decisions: adjusted,
    metadata: {
      ...result.metadata,
      speakerPercentages,
      decisionCount: adjusted.length,
      momentumApplied: true
    }
  };
}

// =============================================================================
// Exports
// =============================================================================

module.exports = {
  // Core analysis
  analyzeMultitrack,
  analyzeMultitrackFromLevels,  // PERF-008: cached levels analysis
  extractTrackLevels,

  // Level processing
  cleanupAudioLevels,
  applySpeakerBoosts,
  smoothAudioLevels,

  // Decision generation
  generateSwitchingDecisions,
  tuneWideShotPercentage,
  insertCutaways,

  // Auto-balance
  autoBalanceMultitrack,

  // Statistics
  calculateMultitrackStats,

  // Config
  DEFAULT_OPTIONS
};
