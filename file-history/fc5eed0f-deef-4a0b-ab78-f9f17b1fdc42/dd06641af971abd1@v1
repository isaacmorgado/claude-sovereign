/**
 * /research Command - Research Assistant
 *
 * Implements intelligent research workflow:
 * - GitHub code search
 * - Memory-based insights
 * - LLM-powered analysis
 * - Documentation compilation
 */

import chalk from 'chalk';
import { BaseCommand } from '../BaseCommand';
import type { CommandContext, CommandResult } from '../types';
import { MemoryManagerBridge } from '../../core/llm/bridge/BashBridge';

export interface ResearchConfig {
  query: string;
  sources?: ('github' | 'memory' | 'web')[];
  limit?: number;
  language?: string[];
  verbose?: boolean;
}

export class ResearchCommand extends BaseCommand {
  name = 'research';
  description = 'Research code patterns, solutions, and best practices';

  private memory: MemoryManagerBridge;

  constructor() {
    super();
    this.memory = new MemoryManagerBridge();
  }

  async execute(context: CommandContext, config: ResearchConfig): Promise<CommandResult> {
    try {
      if (!config.query) {
        return this.createFailure('Query is required. Usage: komplete research "your query"');
      }

      this.info(`ðŸ”¬ Researching: ${chalk.bold(config.query)}`);
      console.log('');

      const sources = config.sources || ['github', 'memory'];
      const results: any = {
        query: config.query,
        sources: {},
        summary: ''
      };

      // Search memory
      if (sources.includes('memory')) {
        this.startSpinner('Searching memory...');
        const memoryResults = await this.searchMemory(config.query);
        results.sources.memory = memoryResults;
        this.succeedSpinner(`Found ${memoryResults.length} memory results`);
      }

      // Search GitHub (if available)
      if (sources.includes('github')) {
        this.startSpinner('Searching GitHub...');
        try {
          const githubResults = await this.searchGitHub(config);
          results.sources.github = githubResults;
          this.succeedSpinner(`Found ${githubResults.length} GitHub results`);
        } catch (error) {
          this.warn('GitHub search not available');
        }
      }

      // Generate summary using LLM
      this.startSpinner('Generating research summary...');
      const summary = await this.generateSummary(context, config.query, results);
      results.summary = summary;
      this.succeedSpinner('Summary generated');

      // Record to memory
      await this.memory.recordEpisode(
        'research_complete',
        `Research: ${config.query}`,
        'success',
        JSON.stringify(results)
      );

      // Display results
      console.log('');
      this.success('Research completed');
      console.log('');
      console.log(chalk.bold('Summary:'));
      console.log(chalk.gray(summary));
      console.log('');

      if (results.sources.memory && results.sources.memory.length > 0) {
        console.log(chalk.bold('Memory Results:'));
        results.sources.memory.slice(0, 3).forEach((result: any, i: number) => {
          console.log(`  ${i + 1}. ${chalk.gray(result.episode || result.fact || 'Result')}`);
        });
        console.log('');
      }

      if (results.sources.github && results.sources.github.length > 0) {
        console.log(chalk.bold('GitHub Results:'));
        results.sources.github.slice(0, 5).forEach((result: any, i: number) => {
          console.log(`  ${i + 1}. ${chalk.cyan(result.repo || 'Repository')}`);
          console.log(`     ${chalk.gray(result.description || result.path || '')}`);
        });
        console.log('');
      }

      return this.createSuccess('Research complete', results);
    } catch (error) {
      const err = error as Error;
      this.error(err.message);
      return this.createFailure(err.message, err);
    }
  }

  /**
   * Search memory for relevant information
   */
  private async searchMemory(query: string): Promise<any[]> {
    try {
      // Search episodes
      const episodes = await this.memory.searchEpisodes(query, 5);

      // Parse episodes (JSON Lines format)
      const results: any[] = [];
      if (episodes) {
        const lines = episodes.split('\n').filter(l => l.trim());
        for (const line of lines) {
          try {
            const episode = JSON.parse(line);
            results.push(episode);
          } catch {
            // Skip invalid JSON
          }
        }
      }

      return results;
    } catch (error) {
      console.warn('Memory search failed:', error);
      return [];
    }
  }

  /**
   * Search GitHub for code examples
   */
  private async searchGitHub(config: ResearchConfig): Promise<any[]> {
    try {
      // Attempt to use GitHub MCP if available (via grep MCP server)
      // This would require the grep MCP server to be configured in Claude Code
      // For now, we use a structured fallback approach

      // TODO: When GitHub MCP is configured, use:
      // const results = await mcpClient.searchGitHub(config.query, {
      //   limit: config.limit || 10,
      //   languages: config.language
      // });

      // Fallback: Return structured placeholder that matches expected format
      this.warn('GitHub MCP integration not available - using mock data');
      this.info('To enable: Configure GitHub MCP server in ~/.claude/config.json');

      return [
        {
          repo: 'anthropics/anthropic-sdk-typescript',
          path: 'src/resources/messages.ts',
          description: `Code example for: ${config.query} (mock result)`,
          url: `https://github.com/search?q=${encodeURIComponent(config.query)}`,
          language: config.language?.[0] || 'typescript',
          score: 0.9
        },
        {
          repo: 'vercel/next.js',
          path: 'packages/next/src/server/api.ts',
          description: `Related implementation: ${config.query} (mock result)`,
          url: `https://github.com/search?q=${encodeURIComponent(config.query)}`,
          language: config.language?.[0] || 'typescript',
          score: 0.85
        }
      ];
    } catch (error) {
      const err = error as Error;
      this.warn(`GitHub search failed: ${err.message}`);
      return [];
    }
  }

  /**
   * Generate summary using LLM
   */
  private async generateSummary(
    context: CommandContext,
    query: string,
    results: any
  ): Promise<string> {
    const prompt = this.buildSummaryPrompt(query, results);

    const response = await context.llmRouter.route(
      {
        messages: [{ role: 'user', content: prompt }],
        system: 'You are a research assistant. Provide concise, actionable summaries.',
        max_tokens: 1000
      },
      {
        taskType: 'general',
        priority: 'quality'
      }
    );

    const firstContent = response.content[0];
    return firstContent.type === 'text' ? firstContent.text : 'Summary unavailable';
  }

  /**
   * Build prompt for summary generation
   */
  private buildSummaryPrompt(query: string, results: any): string {
    let prompt = `Research Query: ${query}\n\n`;

    if (results.sources.memory && results.sources.memory.length > 0) {
      prompt += '## Memory Results\n\n';
      results.sources.memory.forEach((result: any, i: number) => {
        prompt += `${i + 1}. ${JSON.stringify(result)}\n`;
      });
      prompt += '\n';
    }

    if (results.sources.github && results.sources.github.length > 0) {
      prompt += '## GitHub Results\n\n';
      results.sources.github.forEach((result: any, i: number) => {
        prompt += `${i + 1}. ${JSON.stringify(result)}\n`;
      });
      prompt += '\n';
    }

    prompt += `
Provide a comprehensive but concise summary of the research findings.
Include:
1. Key insights and patterns
2. Recommended approaches
3. Important caveats or considerations
4. Next steps or areas for deeper investigation

Keep the summary under 500 words and focus on actionable information.
`.trim();

    return prompt;
  }
}
