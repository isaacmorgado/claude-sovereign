#!/usr/bin/env python3
"""
Scrape all courses from kenkais.com agency site
Password-protected site with authentication
"""

import asyncio
import os
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMExtractionStrategy, LLMConfig
from crawl4ai.async_configs import BrowserConfig

# API Keys
GEMINI_API_KEY = "AIzaSyCwpp0YtdHB56WZ1bhtWdWrPqPS005I6U8"
ZHIPUAI_API_KEY = "9a58c7331504f3cbaef3f2f95cb375b.BrfNpV8TbeF5tCaK"

# Site credentials
SITE_URL = "https://www.kenkais.com/agency"
PASSWORD = "9111"


async def scrape_kenkais_courses():
    """Scrape all courses from kenkais.com agency site"""

    print("=" * 70)
    print("Scraping Kenkais.com Agency Courses")
    print("=" * 70)
    print()
    print(f"üîê Site: {SITE_URL}")
    print(f"üîë Password: {PASSWORD}")
    print()

    # Use GLM-4-Long for comprehensive extraction (1M context)
    llm_config = LLMConfig(
        provider="zhipu/glm-4-long",
        api_token=ZHIPUAI_API_KEY,
        temperature=0.7,
    )

    extraction_strategy = LLMExtractionStrategy(
        llm_config=llm_config,
        instruction="""
        Extract all courses from this page with comprehensive details:

        For each course, extract:
        1. Course title/name
        2. Course description
        3. Topics covered
        4. Duration or length
        5. Any pricing information
        6. Prerequisites (if mentioned)
        7. Learning outcomes
        8. Course materials or modules
        9. Instructor information (if available)
        10. Links to course pages

        Organize the output as:

        # Course 1: [Title]
        - Description: [description]
        - Topics: [list]
        - Duration: [duration]
        - Price: [price]
        - Prerequisites: [prereqs]
        - Learning Outcomes: [outcomes]
        - Modules: [modules]
        - Instructor: [instructor]
        - Link: [url]

        # Course 2: [Title]
        ...

        If there are multiple pages or sections, extract all courses you can find.
        """
    )

    # Browser config
    browser_config = BrowserConfig(
        headless=False,  # Show browser for debugging
        viewport_width=1920,
        viewport_height=1080,
    )

    print("üöÄ Starting crawl...")
    print("üìç Step 1: Navigating to site and handling password...")
    print()

    async with AsyncWebCrawler(config=browser_config, verbose=True) as crawler:
        # Try different password entry methods

        # Method 1: Direct navigation (password in URL)
        print("Attempting password entry...")

        try:
            # Navigate and handle password prompt
            result = await crawler.arun(
                url=SITE_URL,
                config=CrawlerRunConfig(
                    js_code=f"""
                    // Wait a bit for page to load
                    await new Promise(resolve => setTimeout(resolve, 2000));

                    // Look for password input field
                    const passwordInput = document.querySelector('input[type="password"]') ||
                                         document.querySelector('input[name="password"]') ||
                                         document.querySelector('input[placeholder*="password" i]') ||
                                         document.querySelector('#password');

                    if (passwordInput) {{
                        console.log('Found password field');
                        passwordInput.value = '{PASSWORD}';

                        // Find and click submit button
                        const submitBtn = document.querySelector('button[type="submit"]') ||
                                        document.querySelector('input[type="submit"]') ||
                                        document.querySelector('button:contains("Submit")') ||
                                        document.querySelector('button:contains("Enter")') ||
                                        document.querySelector('.submit-btn');

                        if (submitBtn) {{
                            console.log('Clicking submit button');
                            submitBtn.click();
                        }} else {{
                            // Try form submit
                            const form = passwordInput.closest('form');
                            if (form) {{
                                console.log('Submitting form');
                                form.submit();
                            }}
                        }}
                    }} else {{
                        console.log('No password field found - might already be authenticated');
                    }}

                    // Wait for page to load after authentication
                    await new Promise(resolve => setTimeout(resolve, 3000));
                    """,
                    wait_for="networkidle",
                    verbose=True,
                ),
            )

            print("‚úÖ Password entry attempted")
            print()

            # Now extract courses
            print("üìç Step 2: Extracting course information...")
            print()

            result = await crawler.arun(
                url=SITE_URL,
                config=CrawlerRunConfig(
                    extraction_strategy=extraction_strategy,
                    wait_for="networkidle",
                    verbose=True,
                ),
            )

            print("‚úÖ Extraction complete!")
            print(f"üìä Extracted: {len(result.markdown)} characters")
            print()

            # Save to file
            output_file = "kenkais_agency_courses.md"
            with open(output_file, "w", encoding="utf-8") as f:
                f.write(f"# Kenkais.com Agency Courses\n\n")
                f.write(f"**Source**: {SITE_URL}\n")
                f.write(f"**Scraped**: {asyncio.get_event_loop().time()}\n")
                f.write(f"**Model**: GLM-4-Long (1M context)\n\n")
                f.write("---\n\n")
                f.write(result.markdown)

            print(f"üíæ Saved to: {output_file}")
            print()

            # Also show preview
            print("=" * 70)
            print("Preview of Extracted Courses")
            print("=" * 70)
            print()
            print(result.markdown[:2000])  # Show first 2000 chars
            if len(result.markdown) > 2000:
                print("\n... (truncated, see full output in file)")
            print()

            print("=" * 70)
            print("‚úÖ Scraping Complete!")
            print("=" * 70)
            print()
            print(f"üìÅ Full results: {output_file}")
            print(f"üìä Total content: {len(result.markdown)} characters")
            print()

            return result

        except Exception as e:
            print(f"‚ùå Error during scraping: {e}")
            print()
            print("Troubleshooting tips:")
            print("1. Check if password is correct")
            print("2. Verify site is accessible")
            print("3. Check if page structure has changed")
            print()
            raise


async def scrape_with_manual_login():
    """
    Alternative: Show browser for manual login, then scrape
    Use this if automated password entry doesn't work
    """

    print("=" * 70)
    print("Manual Login Method")
    print("=" * 70)
    print()
    print("Browser will open - please log in manually")
    print("Then the script will extract course information")
    print()
    input("Press Enter to continue...")

    llm_config = LLMConfig(
        provider="zhipu/glm-4-long",
        api_token=ZHIPUAI_API_KEY,
        temperature=0.7,
    )

    extraction_strategy = LLMExtractionStrategy(
        llm_config=llm_config,
        instruction="""
        Extract all courses from this page with complete details.
        Include: title, description, topics, duration, price, prerequisites,
        learning outcomes, modules, instructor, and any links.
        """
    )

    browser_config = BrowserConfig(
        headless=False,  # Show browser
        viewport_width=1920,
        viewport_height=1080,
    )

    async with AsyncWebCrawler(config=browser_config, verbose=True) as crawler:
        # Navigate to site
        print("Opening site...")
        await crawler.arun(
            url=SITE_URL,
            config=CrawlerRunConfig(wait_for=5000)
        )

        print()
        print("üëâ Please enter password (9111) in the browser window")
        print("üëâ After logging in, press Enter here to continue...")
        input()

        # Extract courses
        result = await crawler.arun(
            url=SITE_URL,
            config=CrawlerRunConfig(
                extraction_strategy=extraction_strategy,
                wait_for="networkidle",
            )
        )

        # Save
        output_file = "kenkais_agency_courses_manual.md"
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(f"# Kenkais.com Agency Courses (Manual Login)\n\n")
            f.write(result.markdown)

        print(f"‚úÖ Saved to: {output_file}")
        return result


if __name__ == "__main__":
    print()
    print("Choose scraping method:")
    print("1. Automated (script enters password)")
    print("2. Manual (you enter password in browser)")
    print()

    choice = input("Enter choice (1 or 2, default=1): ").strip() or "1"
    print()

    if choice == "2":
        asyncio.run(scrape_with_manual_login())
    else:
        asyncio.run(scrape_kenkais_courses())
