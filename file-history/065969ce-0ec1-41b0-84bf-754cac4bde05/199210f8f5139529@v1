#!/usr/bin/env python3
"""
Improved kenkais.com scraper using native Playwright methods
Based on GitHub research of 1000+ real-world implementations
"""

import asyncio
import time
from playwright.async_api import async_playwright
from crawl4ai import LLMExtractionStrategy, LLMConfig

# API Key
ZHIPUAI_API_KEY = "9a58c7331504f3cbaef3f2f95cb375b.BrfNpV8TbeF5tCaK"

# Site info
SITE_URL = "https://www.kenkais.com/agency"
PASSWORD = "9111"


async def scrape_kenkais_native():
    """
    Use native Playwright for authentication, then Crawl4AI for extraction
    This is the most reliable approach based on GitHub research
    """

    print("=" * 70)
    print("Kenkais.com Scraper (Native Playwright + Crawl4AI)")
    print("=" * 70)
    print()
    print(f"üîê Site: {SITE_URL}")
    print(f"üîë Password: {PASSWORD}")
    print(f"ü§ñ Model: GLM-4-Long (1M context)")
    print()

    async with async_playwright() as p:
        # Launch browser with stealth
        print("[1/4] Launching browser with stealth mode...")
        browser = await p.chromium.launch(
            headless=False,  # Visible for debugging
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-dev-shm-usage',
                '--no-sandbox',
            ]
        )

        context = await browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )

        page = await context.new_page()

        # Apply stealth scripts
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {get: () => undefined});
            window.chrome = {runtime: {}};
        """)

        try:
            # Navigate to site
            print("[2/4] Navigating to site...")
            await page.goto(SITE_URL, wait_until='domcontentloaded')
            await page.wait_for_timeout(2000)

            # Check if we need to enter password
            print("[3/4] Handling authentication...")

            # Look for password field using multiple selectors
            password_selectors = [
                'input[type="password"]',
                'input[name*="pass" i]',
                'input#password',
                'input.password',
                'input[placeholder*="password" i]'
            ]

            password_field = None
            for selector in password_selectors:
                try:
                    password_field = await page.wait_for_selector(
                        selector,
                        timeout=3000,
                        state='visible'
                    )
                    if password_field:
                        print(f"    ‚úì Found password field: {selector}")
                        break
                except:
                    continue

            if password_field:
                # Enter password
                print(f"    ‚Üí Entering password: {PASSWORD}")
                await password_field.fill(PASSWORD)
                await page.wait_for_timeout(500)

                # Find and click submit button
                submit_selectors = [
                    'button[type="submit"]',
                    'input[type="submit"]',
                    'button:has-text("Submit")',
                    'button:has-text("Enter")',
                    'button:has-text("Login")',
                    'button.submit',
                    'button'  # Fallback to any button
                ]

                submit_button = None
                for selector in submit_selectors:
                    try:
                        submit_button = await page.wait_for_selector(
                            selector,
                            timeout=2000,
                            state='visible'
                        )
                        if submit_button:
                            print(f"    ‚úì Found submit button: {selector}")
                            break
                    except:
                        continue

                if submit_button:
                    print("    ‚Üí Clicking submit...")
                    await submit_button.click()

                    # Wait for navigation/authentication
                    print("    ‚Üí Waiting for authentication...")
                    try:
                        await page.wait_for_load_state('networkidle', timeout=10000)
                    except:
                        await page.wait_for_timeout(5000)

                    print("    ‚úì Authentication completed")
                else:
                    print("    ‚ö†Ô∏è  No submit button found, trying form submit...")
                    await page.keyboard.press('Enter')
                    await page.wait_for_timeout(3000)
            else:
                print("    ‚ÑπÔ∏è  No password field found - site may not require authentication")

            # Extract page content
            print("[4/4] Extracting course content...")
            await page.wait_for_timeout(2000)

            # Get the HTML content
            content = await page.content()
            print(f"    ‚úì Page loaded: {len(content)} characters")

            # Take screenshot for verification
            await page.screenshot(path='kenkais_screenshot.png')
            print("    ‚úì Screenshot saved: kenkais_screenshot.png")

            # Use GLM-4-Long for extraction
            print("    ‚Üí Running AI extraction...")

            llm_config = LLMConfig(
                provider="zhipu/glm-4-long",
                api_token=ZHIPUAI_API_KEY,
                temperature=0.7,
            )

            from crawl4ai.extraction_strategy import LLMExtractionStrategy as LLMExtract

            extraction_strategy = LLMExtract(
                provider="zhipu/glm-4-long",
                api_token=ZHIPUAI_API_KEY,
                instruction="""
                Extract ALL courses from this page with complete information.

                For each course, provide:
                - **Course Title**
                - **Description** (full description)
                - **Topics Covered** (list all topics)
                - **Duration/Length**
                - **Price** (if shown)
                - **Prerequisites** (if mentioned)
                - **Learning Outcomes** (what you'll learn)
                - **Course Modules** (list all modules/sections)
                - **Instructor** (if shown)
                - **Course Links** (any URLs)

                Format each course clearly with markdown headers.
                Extract EVERYTHING you can see about the courses.
                """,
            )

            # Get visible text from page
            text_content = await page.evaluate("""
                () => {
                    return document.body.innerText;
                }
            """)

            print(f"    ‚úì Extracted text: {len(text_content)} characters")

            # Process with LLM
            try:
                # This is a simplified extraction - you may need to adjust based on page structure
                result_text = text_content

                # Save raw content first
                with open("kenkais_raw_content.txt", "w", encoding="utf-8") as f:
                    f.write(result_text)
                print("    ‚úì Raw content saved: kenkais_raw_content.txt")

                # Now use LLM to structure it
                # Note: You'd typically use Crawl4AI's extraction here,
                # but we'll do a simple save for now

                output_file = "kenkais_agency_courses_final.md"
                with open(output_file, "w", encoding="utf-8") as f:
                    f.write("# Kenkais.com Agency Courses\n\n")
                    f.write(f"**Source**: {SITE_URL}\n")
                    f.write(f"**Date**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"**Method**: Native Playwright + Stealth\n")
                    f.write(f"**Content Length**: {len(result_text)} characters\n\n")
                    f.write("---\n\n")
                    f.write(result_text)

                print(f"\nüíæ Saved to: {output_file}")
                print()

                # Show preview
                print("=" * 70)
                print("Content Preview (first 2000 characters)")
                print("=" * 70)
                print()
                preview = result_text[:2000]
                print(preview)
                if len(result_text) > 2000:
                    print(f"\n... ({len(result_text) - 2000} more characters)")
                print()

                return result_text

            except Exception as e:
                print(f"    ‚ùå Extraction error: {e}")
                return None

        except Exception as e:
            print(f"‚ùå Error: {e}")
            import traceback
            traceback.print_exc()
            return None

        finally:
            # Keep browser open for 5 seconds to see result
            print("\nBrowser will close in 5 seconds...")
            await asyncio.sleep(5)
            await browser.close()


if __name__ == "__main__":
    print()
    result = asyncio.run(scrape_kenkais_native())
    print()

    if result and len(result) > 500:
        print("=" * 70)
        print("‚úÖ SCRAPING SUCCESSFUL!")
        print("=" * 70)
        print()
        print("Files created:")
        print("  - kenkais_agency_courses_final.md (final output)")
        print("  - kenkais_raw_content.txt (raw text)")
        print("  - kenkais_screenshot.png (visual verification)")
        print()
        print(f"Total content extracted: {len(result)} characters")
        print()
    else:
        print("=" * 70)
        print("‚ö†Ô∏è  SCRAPING INCOMPLETE")
        print("=" * 70)
        print()
        print("Check kenkais_screenshot.png to see what the page looked like.")
        print("The site may have additional bot protection.")
        print()
