#!/usr/bin/env python3
"""
Automated scraper for kenkais.com agency courses
Opens browser, enters password automatically, extracts all courses
"""

import asyncio
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMExtractionStrategy, LLMConfig
from crawl4ai.async_configs import BrowserConfig

# API Key
ZHIPUAI_API_KEY = "9a58c7331504f3cbaef3f2f95cb375b.BrfNpV8TbeF5tCaK"

# Site info
SITE_URL = "https://www.kenkais.com/agency"
PASSWORD = "9111"


async def scrape_kenkais_automated():
    """Fully automated scraping with password entry"""

    print("=" * 70)
    print("Kenkais.com Agency Course Scraper (Automated)")
    print("=" * 70)
    print()
    print(f"üîê Site: {SITE_URL}")
    print(f"üîë Password: {PASSWORD}")
    print(f"ü§ñ Using: GLM-4-Long (1M context)")
    print()
    print("Starting automated scraping...")
    print("(Browser will open briefly)")
    print()

    # GLM-4-Long for comprehensive extraction
    llm_config = LLMConfig(
        provider="zhipu/glm-4-long",
        api_token=ZHIPUAI_API_KEY,
        temperature=0.7,
    )

    extraction_strategy = LLMExtractionStrategy(
        llm_config=llm_config,
        instruction="""
        Extract ALL courses from this page with complete details.

        For each course, extract:
        - Course title
        - Full description
        - Topics/curriculum covered
        - Duration or time commitment
        - Price (if shown)
        - Prerequisites (if mentioned)
        - Learning outcomes
        - Course modules/sections
        - Instructor information
        - Any course links

        Format clearly with headers for each course.
        Include everything visible on the page about the courses.
        """,
    )

    # Browser config - visible for debugging, but automated
    browser_config = BrowserConfig(
        headless=False,
        viewport_width=1920,
        viewport_height=1080,
    )

    print("[1/3] Initializing browser...")
    async with AsyncWebCrawler(config=browser_config, verbose=False) as crawler:
        try:
            # Step 1: Navigate and enter password
            print("[2/3] Navigating to site and entering password...")

            # First, just navigate to see the page structure
            await crawler.arun(url=SITE_URL)

            # Wait a moment for page to load
            await asyncio.sleep(3)

            # Now try to enter password with JavaScript
            await crawler.arun(
                url=SITE_URL,
                config=CrawlerRunConfig(
                    js_code=f"""
                    // Find password input
                    let passwordField = document.querySelector('input[type="password"]');
                    if (!passwordField) {{
                        passwordField = document.querySelector('input[name*="pass" i]');
                    }}
                    if (!passwordField) {{
                        passwordField = document.querySelector('input#password');
                    }}

                    if (passwordField) {{
                        console.log('Found password field, entering password...');
                        passwordField.value = '{PASSWORD}';
                        passwordField.dispatchEvent(new Event('input', {{ bubbles: true }}));
                        passwordField.dispatchEvent(new Event('change', {{ bubbles: true }}));

                        // Wait a moment
                        await new Promise(r => setTimeout(r, 500));

                        // Find submit button
                        let submitBtn = document.querySelector('button[type="submit"]');
                        if (!submitBtn) {{
                            submitBtn = document.querySelector('input[type="submit"]');
                        }}
                        if (!submitBtn) {{
                            submitBtn = document.querySelector('button');
                        }}

                        if (submitBtn) {{
                            console.log('Clicking submit button...');
                            submitBtn.click();
                        }} else {{
                            // Try form submit
                            let form = passwordField.closest('form');
                            if (form) {{
                                console.log('Submitting form...');
                                form.submit();
                            }}
                        }}
                    }} else {{
                        console.log('No password field found - page might not need auth');
                    }}
                    """,
                ),
            )

            # Wait for page to process login
            print("    Waiting for authentication...")
            await asyncio.sleep(5)

            # Step 2: Extract course content
            print("[3/3] Extracting course information...")

            result = await crawler.arun(
                url=SITE_URL,
                config=CrawlerRunConfig(extraction_strategy=extraction_strategy),
            )

            if result and result.markdown:
                content_length = len(result.markdown)
                print(f"‚úÖ Extraction successful! ({content_length} characters)")
                print()

                # Save to file
                output_file = "kenkais_agency_courses.md"
                with open(output_file, "w", encoding="utf-8") as f:
                    f.write("# Kenkais.com Agency Courses\n\n")
                    f.write(f"**Source**: {SITE_URL}\n")
                    f.write(f"**Date**: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
                    f.write(f"**Model**: GLM-4-Long (ZhipuAI)\n")
                    f.write(f"**Content Length**: {content_length} characters\n\n")
                    f.write("---\n\n")
                    f.write(result.markdown)

                print(f"üíæ Saved to: {output_file}")
                print()

                # Show preview
                print("=" * 70)
                print("Content Preview (first 2000 characters)")
                print("=" * 70)
                print()
                preview = result.markdown[:2000]
                print(preview)
                if content_length > 2000:
                    print(f"\n... ({content_length - 2000} more characters)")
                    print(f"\nSee full content in: {output_file}")
                print()

                return result
            else:
                print("‚ö†Ô∏è  No content extracted")
                print()
                print("Possible issues:")
                print("- Password entry may have failed")
                print("- Page may have different structure than expected")
                print("- Site might block automated access")
                print()
                print("Try opening the site manually to verify:")
                print(f"  {SITE_URL}")
                return None

        except Exception as e:
            print(f"‚ùå Error during scraping: {e}")
            print()
            import traceback
            traceback.print_exc()
            return None


if __name__ == "__main__":
    print()
    result = asyncio.run(scrape_kenkais_automated())
    print()

    if result:
        print("=" * 70)
        print("‚úÖ SCRAPING COMPLETE!")
        print("=" * 70)
        print()
        print("Results saved to: kenkais_agency_courses.md")
        print()
    else:
        print("=" * 70)
        print("‚ùå SCRAPING FAILED")
        print("=" * 70)
        print()
        print("The automated scraping didn't work.")
        print("This could mean:")
        print("1. The site uses advanced bot detection")
        print("2. The password entry method needs adjustment")
        print("3. The page structure is different than expected")
        print()
        print("You can try manually scraping by:")
        print("1. Opening Chrome browser")
        print("2. Going to: https://www.kenkais.com/agency")
        print("3. Entering password: 9111")
        print("4. Copying the course content")
        print()
