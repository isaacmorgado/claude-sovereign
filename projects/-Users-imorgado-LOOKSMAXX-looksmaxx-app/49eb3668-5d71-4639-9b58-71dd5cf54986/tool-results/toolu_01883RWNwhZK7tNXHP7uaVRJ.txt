     1→'use client';
     2→
     3→import { useEffect, useRef, useState, useCallback } from 'react';
     4→
     5→// face-api.js types
     6→type FaceDetection = {
     7→  box: {
     8→    x: number;
     9→    y: number;
    10→    width: number;
    11→    height: number;
    12→  };
    13→};
    14→
    15→type FaceLandmarks68 = {
    16→  positions: Array<{ x: number; y: number }>;
    17→};
    18→
    19→type WithFaceLandmarks<T> = T & {
    20→  landmarks: FaceLandmarks68;
    21→  detection: FaceDetection;
    22→};
    23→
    24→export interface FaceApiLandmark {
    25→  x: number;
    26→  y: number;
    27→}
    28→
    29→export interface FaceApiDetectionResult {
    30→  landmarks: FaceApiLandmark[];
    31→  boundingBox: {
    32→    x: number;
    33→    y: number;
    34→    width: number;
    35→    height: number;
    36→  };
    37→}
    38→
    39→export interface UseFaceApiReturn {
    40→  isLoading: boolean;
    41→  isReady: boolean;
    42→  error: Error | null;
    43→  detect: (
    44→    image: HTMLImageElement | HTMLVideoElement | HTMLCanvasElement
    45→  ) => Promise<FaceApiDetectionResult | null>;
    46→}
    47→
    48→// Cache the loaded models
    49→let modelsLoaded = false;
    50→let faceapi: typeof import('face-api.js') | null = null;
    51→
    52→export function useFaceApi(): UseFaceApiReturn {
    53→  const [isLoading, setIsLoading] = useState(true);
    54→  const [isReady, setIsReady] = useState(false);
    55→  const [error, setError] = useState<Error | null>(null);
    56→  const initializingRef = useRef(false);
    57→
    58→  useEffect(() => {
    59→    if (initializingRef.current || modelsLoaded) {
    60→      if (modelsLoaded) {
    61→        setIsReady(true);
    62→        setIsLoading(false);
    63→      }
    64→      return;
    65→    }
    66→
    67→    initializingRef.current = true;
    68→
    69→    async function loadModels() {
    70→      try {
    71→        setIsLoading(true);
    72→        setError(null);
    73→
    74→        // Dynamic import of face-api.js
    75→        const faceApiModule = await import('face-api.js');
    76→        faceapi = faceApiModule;
    77→
    78→        // Load models from public/models directory
    79→        const MODEL_URL = '/models';
    80→
    81→        await Promise.all([
    82→          faceapi.nets.ssdMobilenetv1.loadFromUri(MODEL_URL),
    83→          faceapi.nets.faceLandmark68Net.loadFromUri(MODEL_URL),
    84→          faceapi.nets.tinyFaceDetector.loadFromUri(MODEL_URL),
    85→          faceapi.nets.faceLandmark68TinyNet.loadFromUri(MODEL_URL),
    86→        ]);
    87→
    88→        modelsLoaded = true;
    89→        setIsReady(true);
    90→        setIsLoading(false);
    91→      } catch (err) {
    92→        const errorMessage =
    93→          err instanceof Error ? err.message : 'Failed to load face-api.js models';
    94→        setError(new Error(errorMessage));
    95→        setIsLoading(false);
    96→        initializingRef.current = false;
    97→      }
    98→    }
    99→
   100→    loadModels();
   101→  }, []);
   102→
   103→  const detect = useCallback(
   104→    async (
   105→      image: HTMLImageElement | HTMLVideoElement | HTMLCanvasElement
   106→    ): Promise<FaceApiDetectionResult | null> => {
   107→      if (!faceapi || !isReady) {
   108→        throw new Error('face-api.js is not initialized');
   109→      }
   110→
   111→      try {
   112→        // Detect faces with landmarks using SSD MobileNet (better for side profiles)
   113→        const detectionWithLandmarks = await faceapi
   114→          .detectSingleFace(image, new faceapi.SsdMobilenetv1Options({ minConfidence: 0.3 }))
   115→          .withFaceLandmarks() as WithFaceLandmarks<FaceDetection> | undefined;
   116→
   117→        if (!detectionWithLandmarks) {
   118→          // Try with tiny face detector as fallback
   119→          const tinyDetection = await faceapi
   120→            .detectSingleFace(image, new faceapi.TinyFaceDetectorOptions({ inputSize: 512, scoreThreshold: 0.3 }))
   121→            .withFaceLandmarks(true) as WithFaceLandmarks<FaceDetection> | undefined;
   122→
   123→          if (!tinyDetection) {
   124→            return null;
   125→          }
   126→
   127→          const landmarks = tinyDetection.landmarks.positions.map((point) => ({
   128→            x: point.x,
   129→            y: point.y,
   130→          }));
   131→
   132→          return {
   133→            landmarks,
   134→            boundingBox: {
   135→              x: tinyDetection.detection.box.x,
   136→              y: tinyDetection.detection.box.y,
   137→              width: tinyDetection.detection.box.width,
   138→              height: tinyDetection.detection.box.height,
   139→            },
   140→          };
   141→        }
   142→
   143→        // Extract 68-point landmarks
   144→        const landmarks = detectionWithLandmarks.landmarks.positions.map((point) => ({
   145→          x: point.x,
   146→          y: point.y,
   147→        }));
   148→
   149→        return {
   150→          landmarks,
   151→          boundingBox: {
   152→            x: detectionWithLandmarks.detection.box.x,
   153→            y: detectionWithLandmarks.detection.box.y,
   154→            width: detectionWithLandmarks.detection.box.width,
   155→            height: detectionWithLandmarks.detection.box.height,
   156→          },
   157→        };
   158→      } catch (err) {
   159→        const errorMessage =
   160→          err instanceof Error ? err.message : 'Face detection failed';
   161→        throw new Error(errorMessage);
   162→      }
   163→    },
   164→    [isReady]
   165→  );
   166→
   167→  return {
   168→    isLoading,
   169→    isReady,
   170→    error,
   171→    detect,
   172→  };
   173→}
   174→
   175→/**
   176→ * Maps the 68-point landmarks from face-api.js to our side profile landmark structure
   177→ * The 68-point model landmarks:
   178→ * 0-16: Jaw contour
   179→ * 17-21: Left eyebrow
   180→ * 22-26: Right eyebrow
   181→ * 27-30: Nose bridge
   182→ * 31-35: Lower nose
   183→ * 36-41: Left eye
   184→ * 42-47: Right eye
   185→ * 48-59: Outer lip
   186→ * 60-67: Inner lip
   187→ */
   188→export const FACE_API_LANDMARK_MAPPING = {
   189→  // Jaw contour (for side profile)
   190→  JAW_CONTOUR: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16],
   191→  MENTON: 8, // Chin center
   192→  GNATHION: 8, // Same as menton for 68-point model
   193→
   194→  // Nose
   195→  NOSE_BRIDGE: [27, 28, 29, 30],
   196→  NASION: 27, // Bridge of nose
   197→  PRONASALE: 30, // Nose tip
   198→  NOSE_TIP: 30,
   199→  LEFT_NOSTRIL: 31,
   200→  RIGHT_NOSTRIL: 35,
   201→  NOSE_BOTTOM: [31, 32, 33, 34, 35],
   202→
   203→  // Eyes
   204→  LEFT_EYE: [36, 37, 38, 39, 40, 41],
   205→  RIGHT_EYE: [42, 43, 44, 45, 46, 47],
   206→  LEFT_EYE_INNER: 39,
   207→  LEFT_EYE_OUTER: 36,
   208→  RIGHT_EYE_INNER: 42,
   209→  RIGHT_EYE_OUTER: 45,
   210→
   211→  // Eyebrows
   212→  LEFT_EYEBROW: [17, 18, 19, 20, 21],
   213→  RIGHT_EYEBROW: [22, 23, 24, 25, 26],
   214→  LEFT_BROW_INNER: 21,
   215→  LEFT_BROW_OUTER: 17,
   216→  RIGHT_BROW_INNER: 22,
   217→  RIGHT_BROW_OUTER: 26,
   218→
   219→  // Lips
   220→  OUTER_LIP: [48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59],
   221→  INNER_LIP: [60, 61, 62, 63, 64, 65, 66, 67],
   222→  UPPER_LIP_TOP: 51,
   223→  LOWER_LIP_BOTTOM: 57,
   224→  LEFT_MOUTH_CORNER: 48,
   225→  RIGHT_MOUTH_CORNER: 54,
   226→  LIP_CENTER_TOP: 62,
   227→  LIP_CENTER_BOTTOM: 66,
   228→};
   229→
   230→/**
   231→ * Convert normalized landmarks (0-1) to pixel coordinates
   232→ */
   233→export function normalizeToPixels(
   234→  landmarks: FaceApiLandmark[],
   235→  imageWidth: number,
   236→  imageHeight: number
   237→): FaceApiLandmark[] {
   238→  return landmarks.map((lm) => ({
   239→    x: lm.x / imageWidth,
   240→    y: lm.y / imageHeight,
   241→  }));
   242→}
   243→
   244→/**
   245→ * Get specific landmark group for side profile analysis
   246→ */
   247→export function getSideProfileLandmarks(landmarks: FaceApiLandmark[]): {
   248→  jawContour: FaceApiLandmark[];
   249→  noseBridge: FaceApiLandmark[];
   250→  chin: FaceApiLandmark;
   251→  noseTip: FaceApiLandmark;
   252→  nasion: FaceApiLandmark;
   253→  upperLip: FaceApiLandmark;
   254→  lowerLip: FaceApiLandmark;
   255→} {
   256→  return {
   257→    jawContour: FACE_API_LANDMARK_MAPPING.JAW_CONTOUR.map((i) => landmarks[i]),
   258→    noseBridge: FACE_API_LANDMARK_MAPPING.NOSE_BRIDGE.map((i) => landmarks[i]),
   259→    chin: landmarks[FACE_API_LANDMARK_MAPPING.MENTON],
   260→    noseTip: landmarks[FACE_API_LANDMARK_MAPPING.PRONASALE],
   261→    nasion: landmarks[FACE_API_LANDMARK_MAPPING.NASION],
   262→    upperLip: landmarks[FACE_API_LANDMARK_MAPPING.UPPER_LIP_TOP],
   263→    lowerLip: landmarks[FACE_API_LANDMARK_MAPPING.LOWER_LIP_BOTTOM],
   264→  };
   265→}
   266→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
