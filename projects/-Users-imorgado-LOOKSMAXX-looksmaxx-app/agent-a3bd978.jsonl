{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","type":"user","message":{"role":"user","content":"Verify the coordinate normalization timing fix for FaceIQ parity.\n\nCheck the following:\n\n1. **serverSideDetection.ts** - Verify landmarks are kept in PIXEL coordinates:\n   - Line 143-155: Should NOT normalize to 0-1 immediately\n   - Should keep `x: point.x, y: point.y` (pixels)\n   - rawLandmarks should also be in pixels\n   - imageDimensions field should be added to ServerDetectionResult\n\n2. **mediapipeDetection.ts** - Verify normalization happens AFTER rotation:\n   - Line 229-259: Should apply FaceIQ correction in pixel space first\n   - Then normalize to 0-1 AFTER rotation\n   - Should also normalize frankfortPlane coordinates\n\n3. **Confirm the fix flow**:\n   - Step 1: InsightFace returns pixel coordinates\n   - Step 2: FaceIQ rotation applied in pixel space\n   - Step 3: Normalize to 0-1 for UI display\n\nReport any deviations from this expected flow."},"uuid":"c0965d45-6f4f-4a0f-9fa7-381f975490a7","timestamp":"2025-12-30T15:10:10.901Z"}
{"parentUuid":"c0965d45-6f4f-4a0f-9fa7-381f975490a7","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01NBiRC9xTFxbyhDU9RSyhwp","type":"message","role":"assistant","content":[{"type":"text","text":"I'll verify the coordinate normalization timing fix for FaceIQ parity by examining the relevant files."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":39783,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":39783,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWd5qwD9gwC552qKP4X4p","type":"assistant","uuid":"0d4b7b5d-3e40-404c-a9a4-924e62d8b2e7","timestamp":"2025-12-30T15:10:15.420Z"}
{"parentUuid":"0d4b7b5d-3e40-404c-a9a4-924e62d8b2e7","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01NBiRC9xTFxbyhDU9RSyhwp","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01S5zyhGjpciHbUDvjPArJLH","name":"Read","input":{"file_path":"/Users/imorgado/LOOKSMAXX/looksmaxx-app/src/lib/serverSideDetection.ts"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":39783,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":39783,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWd5qwD9gwC552qKP4X4p","type":"assistant","uuid":"38d8a1fd-a623-49f7-9855-f935ea978165","timestamp":"2025-12-30T15:10:16.260Z"}
{"parentUuid":"38d8a1fd-a623-49f7-9855-f935ea978165","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01NBiRC9xTFxbyhDU9RSyhwp","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_014A6UUmShMNGrPcun6LRYgo","name":"Read","input":{"file_path":"/Users/imorgado/LOOKSMAXX/looksmaxx-app/src/lib/mediapipeDetection.ts"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":39783,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":39783,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWd5qwD9gwC552qKP4X4p","type":"assistant","uuid":"4c68cd6c-95de-42f9-a3d9-4b95a07a1607","timestamp":"2025-12-30T15:10:16.742Z"}
{"parentUuid":"4c68cd6c-95de-42f9-a3d9-4b95a07a1607","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01S5zyhGjpciHbUDvjPArJLH","type":"tool_result","content":"     1→/**\n     2→ * Server-Side Detection Service for Side Profiles\n     3→ *\n     4→ * Calls the Railway-hosted InsightFace API for accurate side profile detection.\n     5→ * Returns 106 raw landmarks mapped to our 28 cephalometric landmarks.\n     6→ *\n     7→ * Supports manual overrides for estimated landmarks (porion, tragus, intertragicNotch)\n     8→ * to achieve FaceIQ-level Frankfort plane accuracy.\n     9→ */\n    10→\n    11→import { SideLandmarkResponse } from '@/app/api/side-landmarks/route';\n    12→\n    13→/**\n    14→ * Landmarks that are geometrically estimated and benefit from manual adjustment\n    15→ */\n    16→export const ESTIMATED_LANDMARK_IDS = ['porion', 'tragus', 'intertragicNotch'] as const;\n    17→export type EstimatedLandmarkId = typeof ESTIMATED_LANDMARK_IDS[number];\n    18→\n    19→export interface ServerDetectionResult {\n    20→  landmarks: Array<{ id: string; x: number; y: number }>;  // In PIXEL coordinates\n    21→  rawLandmarks?: Array<{ x: number; y: number }>;  // Raw 106 landmarks for FaceIQ-style normalization (pixels)\n    22→  confidence: number;\n    23→  faceBox: { x: number; y: number; width: number; height: number };\n    24→  direction?: 'left' | 'right';\n    25→  rotationAngle?: number;\n    26→  frankfortPlane?: {\n    27→    angle: number;\n    28→    orbitale: { x: number; y: number };\n    29→    porion: { x: number; y: number };\n    30→  };\n    31→  /** Image dimensions for normalization AFTER rotation */\n    32→  imageDimensions: { width: number; height: number };\n    33→  /** Tracks which landmarks were manually adjusted */\n    34→  manuallyAdjusted?: string[];\n    35→}\n    36→\n    37→/**\n    38→ * Manual landmark override for estimated ear landmarks\n    39→ */\n    40→export interface LandmarkOverride {\n    41→  id: string;\n    42→  x: number;\n    43→  y: number;\n    44→}\n    45→\n    46→/**\n    47→ * Convert image URL to base64\n    48→ */\n    49→async function imageUrlToBase64(imageUrl: string): Promise<string> {\n    50→  // If it's already a data URL, extract the base64 part\n    51→  if (imageUrl.startsWith('data:')) {\n    52→    return imageUrl;\n    53→  }\n    54→\n    55→  // If it's a blob URL or regular URL, fetch and convert\n    56→  const response = await fetch(imageUrl);\n    57→  const blob = await response.blob();\n    58→\n    59→  return new Promise((resolve, reject) => {\n    60→    const reader = new FileReader();\n    61→    reader.onloadend = () => {\n    62→      if (typeof reader.result === 'string') {\n    63→        resolve(reader.result);\n    64→      } else {\n    65→        reject(new Error('Failed to convert to base64'));\n    66→      }\n    67→    };\n    68→    reader.onerror = reject;\n    69→    reader.readAsDataURL(blob);\n    70→  });\n    71→}\n    72→\n    73→\n    74→/**\n    75→ * Get image dimensions\n    76→ */\n    77→function getImageDimensions(imageUrl: string): Promise<{ width: number; height: number }> {\n    78→  return new Promise((resolve, reject) => {\n    79→    const img = new Image();\n    80→    img.onload = () => resolve({ width: img.naturalWidth, height: img.naturalHeight });\n    81→    img.onerror = reject;\n    82→    img.src = imageUrl;\n    83→  });\n    84→}\n    85→\n    86→/**\n    87→ * Calculate Frankfort Horizontal Plane from orbitale and porion landmarks\n    88→ */\n    89→function calculateFrankfortPlane(\n    90→  orbitale: { x: number; y: number },\n    91→  porion: { x: number; y: number }\n    92→): { angle: number; orbitale: { x: number; y: number }; porion: { x: number; y: number } } {\n    93→  // Calculate angle from horizontal (in degrees)\n    94→  const dx = porion.x - orbitale.x;\n    95→  const dy = porion.y - orbitale.y;\n    96→  const angle = Math.atan2(dy, dx) * (180 / Math.PI);\n    97→\n    98→  return {\n    99→    angle,\n   100→    orbitale,\n   101→    porion,\n   102→  };\n   103→}\n   104→\n   105→/**\n   106→ * Detect side profile landmarks using server-side InsightFace API\n   107→ */\n   108→export async function detectSideProfileServer(\n   109→  imageUrl: string\n   110→): Promise<ServerDetectionResult | null> {\n   111→  try {\n   112→    console.log('[ServerDetection] Starting server-side detection...');\n   113→\n   114→    // Convert image to base64\n   115→    const base64Image = await imageUrlToBase64(imageUrl);\n   116→\n   117→    // Call our API proxy\n   118→    const response = await fetch('/api/side-landmarks', {\n   119→      method: 'POST',\n   120→      headers: {\n   121→        'Content-Type': 'application/json',\n   122→      },\n   123→      body: JSON.stringify({ image: base64Image }),\n   124→    });\n   125→\n   126→    if (!response.ok) {\n   127→      console.error('[ServerDetection] API error:', response.status);\n   128→      return null;\n   129→    }\n   130→\n   131→    const data: SideLandmarkResponse = await response.json();\n   132→\n   133→    if (!data.success || !data.data?.namedLandmarks) {\n   134→      console.log('[ServerDetection] Detection failed:', data.error || 'No landmarks found');\n   135→      return null;\n   136→    }\n   137→\n   138→    const resultData = data.data;\n   139→    console.log('[ServerDetection] Detection successful, count:', resultData.landmarkCount);\n   140→\n   141→    // Get image dimensions for normalization\n   142→    const { width, height } = await getImageDimensions(imageUrl);\n   143→    console.log('[ServerDetection] Image dimensions:', width, 'x', height);\n   144→\n   145→    // FaceIQ PARITY FIX #1: Keep landmarks in PIXEL coordinates (NOT normalized 0-1)\n   146→    // Normalization must happen AFTER FaceIQ rotation algorithm\n   147→    const landmarks = Object.entries(resultData.namedLandmarks).map(([id, point]) => ({\n   148→      id,\n   149→      x: point.x,  // Keep pixel coordinates\n   150→      y: point.y,  // Keep pixel coordinates\n   151→    }));\n   152→\n   153→    // Keep raw landmarks in pixel coordinates for FaceIQ rotation (indices 10, 26, 75)\n   154→    const rawLandmarks = resultData.landmarks?.map(pt => ({\n   155→      x: pt.x,  // Keep pixel coordinates\n   156→      y: pt.y,  // Keep pixel coordinates\n   157→    })) || [];\n   158→\n   159→    // Use Frankfort Plane from server if available (future proof), otherwise calculate locally\n   160→    let frankfortPlane: ServerDetectionResult['frankfortPlane'];\n   161→    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n   162→    const serverFrankfort = (resultData as any).frankfort_plane;\n   163→\n   164→    if (serverFrankfort) {\n   165→      frankfortPlane = serverFrankfort;\n   166→      console.log('[ServerDetection] Frankfort Plane angle (from server):', serverFrankfort.angle.toFixed(2), 'degrees');\n   167→    } else {\n   168→      // Find normalized orbitale and porion from our processed list\n   169→      const normOrbitale = landmarks.find(l => l.id === 'orbitale');\n   170→      const normPorion = landmarks.find(l => l.id === 'porion');\n   171→\n   172→      if (normOrbitale && normPorion) {\n   173→        frankfortPlane = calculateFrankfortPlane(normOrbitale, normPorion);\n   174→        console.log('[ServerDetection] Frankfort Plane angle (calculated):', frankfortPlane.angle.toFixed(2), 'degrees');\n   175→      }\n   176→    }\n   177→\n   178→    return {\n   179→      landmarks,  // Pixel coordinates (NOT normalized)\n   180→      rawLandmarks,  // Raw 106 landmarks in pixel coordinates for FaceIQ algorithm\n   181→      confidence: 0.95, // InsightFace is generally high confidence\n   182→      faceBox: resultData.crop || { x: 0, y: 0, width: 1, height: 1 },\n   183→      direction: resultData.direction,\n   184→      rotationAngle: resultData.rotationAngle,\n   185→      frankfortPlane,\n   186→      imageDimensions: { width, height },  // For normalization AFTER rotation\n   187→    };\n   188→  } catch (error) {\n   189→    console.error('[ServerDetection] Error:', error);\n   190→    return null;\n   191→  }\n   192→}\n   193→\n   194→/**\n   195→ * Merge manual landmark overrides with detected landmarks\n   196→ *\n   197→ * Used for applying user adjustments to estimated ear landmarks\n   198→ * (porion, tragus, intertragicNotch) for FaceIQ-level accuracy.\n   199→ *\n   200→ * @param detectedLandmarks - Landmarks from server detection\n   201→ * @param overrides - Manual adjustments from user\n   202→ * @returns Merged landmarks with manual overrides applied\n   203→ */\n   204→export function mergeWithOverrides(\n   205→  detectedLandmarks: Array<{ id: string; x: number; y: number }>,\n   206→  overrides: LandmarkOverride[]\n   207→): Array<{ id: string; x: number; y: number }> {\n   208→  if (!overrides.length) return detectedLandmarks;\n   209→\n   210→  const overrideMap = new Map(overrides.map(o => [o.id, o]));\n   211→\n   212→  return detectedLandmarks.map(landmark => {\n   213→    const override = overrideMap.get(landmark.id);\n   214→    if (override) {\n   215→      console.log(`[ServerDetection] Applied override for ${landmark.id}:`, {\n   216→        original: { x: landmark.x.toFixed(4), y: landmark.y.toFixed(4) },\n   217→        adjusted: { x: override.x.toFixed(4), y: override.y.toFixed(4) },\n   218→      });\n   219→      return { ...landmark, x: override.x, y: override.y };\n   220→    }\n   221→    return landmark;\n   222→  });\n   223→}\n   224→\n   225→/**\n   226→ * Recalculate Frankfort Horizontal Plane after manual adjustments\n   227→ *\n   228→ * Call this after applying overrides to porion or orbitale to update\n   229→ * the Frankfort plane angle used in side profile analysis.\n   230→ *\n   231→ * @param landmarks - Merged landmarks with overrides applied\n   232→ * @returns Updated Frankfort plane calculation or null if landmarks not found\n   233→ */\n   234→export function recalculateFrankfortPlane(\n   235→  landmarks: Array<{ id: string; x: number; y: number }>\n   236→): ServerDetectionResult['frankfortPlane'] | null {\n   237→  const orbitale = landmarks.find(l => l.id === 'orbitale');\n   238→  const porion = landmarks.find(l => l.id === 'porion');\n   239→\n   240→  if (!orbitale || !porion) {\n   241→    console.warn('[ServerDetection] Cannot calculate Frankfort plane: missing orbitale or porion');\n   242→    return null;\n   243→  }\n   244→\n   245→  const plane = calculateFrankfortPlane(orbitale, porion);\n   246→  console.log('[ServerDetection] Recalculated Frankfort Plane angle:', plane.angle.toFixed(2), 'degrees');\n   247→\n   248→  return plane;\n   249→}\n   250→\n   251→/**\n   252→ * Apply manual overrides to a detection result\n   253→ *\n   254→ * FaceIQ PARITY FIX #2: Manual overrides update landmark positions only.\n   255→ * The FaceIQ rotation reference (indices 10, 26, 75) is preserved from rawLandmarks.\n   256→ * Frankfort plane is updated for DISPLAY purposes only, not normalization.\n   257→ *\n   258→ * @param result - Original detection result\n   259→ * @param overrides - Manual landmark adjustments (in pixel coordinates)\n   260→ * @returns Updated result with overrides applied\n   261→ */\n   262→export function applyManualOverrides(\n   263→  result: ServerDetectionResult,\n   264→  overrides: LandmarkOverride[]\n   265→): ServerDetectionResult {\n   266→  if (!overrides.length) return result;\n   267→\n   268→  const mergedLandmarks = mergeWithOverrides(result.landmarks, overrides);\n   269→  const adjustedIds = overrides.map(o => o.id);\n   270→\n   271→  // FaceIQ PARITY: Update Frankfort plane for DISPLAY only (not normalization)\n   272→  // Normalization ALWAYS uses FaceIQ indices 10, 26, 75 from rawLandmarks\n   273→  const updatedFrankfort = recalculateFrankfortPlane(mergedLandmarks);\n   274→\n   275→  return {\n   276→    ...result,\n   277→    landmarks: mergedLandmarks,\n   278→    frankfortPlane: updatedFrankfort || result.frankfortPlane,  // Display only\n   279→    manuallyAdjusted: adjustedIds,\n   280→    // rawLandmarks preserved unchanged - critical for FaceIQ rotation\n   281→  };\n   282→}\n   283→\n   284→/**\n   285→ * Check if a landmark is estimated (vs directly detected)\n   286→ */\n   287→export function isEstimatedLandmark(landmarkId: string): boolean {\n   288→  return ESTIMATED_LANDMARK_IDS.includes(landmarkId as EstimatedLandmarkId);\n   289→}\n   290→\n\n</system-reminder>\n"}]},"uuid":"d4aea3cc-0f48-4622-9d9a-237d3838d3e4","timestamp":"2025-12-30T15:10:16.788Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/LOOKSMAXX/looksmaxx-app/src/lib/serverSideDetection.ts","content":"/**\n * Server-Side Detection Service for Side Profiles\n *\n * Calls the Railway-hosted InsightFace API for accurate side profile detection.\n * Returns 106 raw landmarks mapped to our 28 cephalometric landmarks.\n *\n * Supports manual overrides for estimated landmarks (porion, tragus, intertragicNotch)\n * to achieve FaceIQ-level Frankfort plane accuracy.\n */\n\nimport { SideLandmarkResponse } from '@/app/api/side-landmarks/route';\n\n/**\n * Landmarks that are geometrically estimated and benefit from manual adjustment\n */\nexport const ESTIMATED_LANDMARK_IDS = ['porion', 'tragus', 'intertragicNotch'] as const;\nexport type EstimatedLandmarkId = typeof ESTIMATED_LANDMARK_IDS[number];\n\nexport interface ServerDetectionResult {\n  landmarks: Array<{ id: string; x: number; y: number }>;  // In PIXEL coordinates\n  rawLandmarks?: Array<{ x: number; y: number }>;  // Raw 106 landmarks for FaceIQ-style normalization (pixels)\n  confidence: number;\n  faceBox: { x: number; y: number; width: number; height: number };\n  direction?: 'left' | 'right';\n  rotationAngle?: number;\n  frankfortPlane?: {\n    angle: number;\n    orbitale: { x: number; y: number };\n    porion: { x: number; y: number };\n  };\n  /** Image dimensions for normalization AFTER rotation */\n  imageDimensions: { width: number; height: number };\n  /** Tracks which landmarks were manually adjusted */\n  manuallyAdjusted?: string[];\n}\n\n/**\n * Manual landmark override for estimated ear landmarks\n */\nexport interface LandmarkOverride {\n  id: string;\n  x: number;\n  y: number;\n}\n\n/**\n * Convert image URL to base64\n */\nasync function imageUrlToBase64(imageUrl: string): Promise<string> {\n  // If it's already a data URL, extract the base64 part\n  if (imageUrl.startsWith('data:')) {\n    return imageUrl;\n  }\n\n  // If it's a blob URL or regular URL, fetch and convert\n  const response = await fetch(imageUrl);\n  const blob = await response.blob();\n\n  return new Promise((resolve, reject) => {\n    const reader = new FileReader();\n    reader.onloadend = () => {\n      if (typeof reader.result === 'string') {\n        resolve(reader.result);\n      } else {\n        reject(new Error('Failed to convert to base64'));\n      }\n    };\n    reader.onerror = reject;\n    reader.readAsDataURL(blob);\n  });\n}\n\n\n/**\n * Get image dimensions\n */\nfunction getImageDimensions(imageUrl: string): Promise<{ width: number; height: number }> {\n  return new Promise((resolve, reject) => {\n    const img = new Image();\n    img.onload = () => resolve({ width: img.naturalWidth, height: img.naturalHeight });\n    img.onerror = reject;\n    img.src = imageUrl;\n  });\n}\n\n/**\n * Calculate Frankfort Horizontal Plane from orbitale and porion landmarks\n */\nfunction calculateFrankfortPlane(\n  orbitale: { x: number; y: number },\n  porion: { x: number; y: number }\n): { angle: number; orbitale: { x: number; y: number }; porion: { x: number; y: number } } {\n  // Calculate angle from horizontal (in degrees)\n  const dx = porion.x - orbitale.x;\n  const dy = porion.y - orbitale.y;\n  const angle = Math.atan2(dy, dx) * (180 / Math.PI);\n\n  return {\n    angle,\n    orbitale,\n    porion,\n  };\n}\n\n/**\n * Detect side profile landmarks using server-side InsightFace API\n */\nexport async function detectSideProfileServer(\n  imageUrl: string\n): Promise<ServerDetectionResult | null> {\n  try {\n    console.log('[ServerDetection] Starting server-side detection...');\n\n    // Convert image to base64\n    const base64Image = await imageUrlToBase64(imageUrl);\n\n    // Call our API proxy\n    const response = await fetch('/api/side-landmarks', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n      },\n      body: JSON.stringify({ image: base64Image }),\n    });\n\n    if (!response.ok) {\n      console.error('[ServerDetection] API error:', response.status);\n      return null;\n    }\n\n    const data: SideLandmarkResponse = await response.json();\n\n    if (!data.success || !data.data?.namedLandmarks) {\n      console.log('[ServerDetection] Detection failed:', data.error || 'No landmarks found');\n      return null;\n    }\n\n    const resultData = data.data;\n    console.log('[ServerDetection] Detection successful, count:', resultData.landmarkCount);\n\n    // Get image dimensions for normalization\n    const { width, height } = await getImageDimensions(imageUrl);\n    console.log('[ServerDetection] Image dimensions:', width, 'x', height);\n\n    // FaceIQ PARITY FIX #1: Keep landmarks in PIXEL coordinates (NOT normalized 0-1)\n    // Normalization must happen AFTER FaceIQ rotation algorithm\n    const landmarks = Object.entries(resultData.namedLandmarks).map(([id, point]) => ({\n      id,\n      x: point.x,  // Keep pixel coordinates\n      y: point.y,  // Keep pixel coordinates\n    }));\n\n    // Keep raw landmarks in pixel coordinates for FaceIQ rotation (indices 10, 26, 75)\n    const rawLandmarks = resultData.landmarks?.map(pt => ({\n      x: pt.x,  // Keep pixel coordinates\n      y: pt.y,  // Keep pixel coordinates\n    })) || [];\n\n    // Use Frankfort Plane from server if available (future proof), otherwise calculate locally\n    let frankfortPlane: ServerDetectionResult['frankfortPlane'];\n    // eslint-disable-next-line @typescript-eslint/no-explicit-any\n    const serverFrankfort = (resultData as any).frankfort_plane;\n\n    if (serverFrankfort) {\n      frankfortPlane = serverFrankfort;\n      console.log('[ServerDetection] Frankfort Plane angle (from server):', serverFrankfort.angle.toFixed(2), 'degrees');\n    } else {\n      // Find normalized orbitale and porion from our processed list\n      const normOrbitale = landmarks.find(l => l.id === 'orbitale');\n      const normPorion = landmarks.find(l => l.id === 'porion');\n\n      if (normOrbitale && normPorion) {\n        frankfortPlane = calculateFrankfortPlane(normOrbitale, normPorion);\n        console.log('[ServerDetection] Frankfort Plane angle (calculated):', frankfortPlane.angle.toFixed(2), 'degrees');\n      }\n    }\n\n    return {\n      landmarks,  // Pixel coordinates (NOT normalized)\n      rawLandmarks,  // Raw 106 landmarks in pixel coordinates for FaceIQ algorithm\n      confidence: 0.95, // InsightFace is generally high confidence\n      faceBox: resultData.crop || { x: 0, y: 0, width: 1, height: 1 },\n      direction: resultData.direction,\n      rotationAngle: resultData.rotationAngle,\n      frankfortPlane,\n      imageDimensions: { width, height },  // For normalization AFTER rotation\n    };\n  } catch (error) {\n    console.error('[ServerDetection] Error:', error);\n    return null;\n  }\n}\n\n/**\n * Merge manual landmark overrides with detected landmarks\n *\n * Used for applying user adjustments to estimated ear landmarks\n * (porion, tragus, intertragicNotch) for FaceIQ-level accuracy.\n *\n * @param detectedLandmarks - Landmarks from server detection\n * @param overrides - Manual adjustments from user\n * @returns Merged landmarks with manual overrides applied\n */\nexport function mergeWithOverrides(\n  detectedLandmarks: Array<{ id: string; x: number; y: number }>,\n  overrides: LandmarkOverride[]\n): Array<{ id: string; x: number; y: number }> {\n  if (!overrides.length) return detectedLandmarks;\n\n  const overrideMap = new Map(overrides.map(o => [o.id, o]));\n\n  return detectedLandmarks.map(landmark => {\n    const override = overrideMap.get(landmark.id);\n    if (override) {\n      console.log(`[ServerDetection] Applied override for ${landmark.id}:`, {\n        original: { x: landmark.x.toFixed(4), y: landmark.y.toFixed(4) },\n        adjusted: { x: override.x.toFixed(4), y: override.y.toFixed(4) },\n      });\n      return { ...landmark, x: override.x, y: override.y };\n    }\n    return landmark;\n  });\n}\n\n/**\n * Recalculate Frankfort Horizontal Plane after manual adjustments\n *\n * Call this after applying overrides to porion or orbitale to update\n * the Frankfort plane angle used in side profile analysis.\n *\n * @param landmarks - Merged landmarks with overrides applied\n * @returns Updated Frankfort plane calculation or null if landmarks not found\n */\nexport function recalculateFrankfortPlane(\n  landmarks: Array<{ id: string; x: number; y: number }>\n): ServerDetectionResult['frankfortPlane'] | null {\n  const orbitale = landmarks.find(l => l.id === 'orbitale');\n  const porion = landmarks.find(l => l.id === 'porion');\n\n  if (!orbitale || !porion) {\n    console.warn('[ServerDetection] Cannot calculate Frankfort plane: missing orbitale or porion');\n    return null;\n  }\n\n  const plane = calculateFrankfortPlane(orbitale, porion);\n  console.log('[ServerDetection] Recalculated Frankfort Plane angle:', plane.angle.toFixed(2), 'degrees');\n\n  return plane;\n}\n\n/**\n * Apply manual overrides to a detection result\n *\n * FaceIQ PARITY FIX #2: Manual overrides update landmark positions only.\n * The FaceIQ rotation reference (indices 10, 26, 75) is preserved from rawLandmarks.\n * Frankfort plane is updated for DISPLAY purposes only, not normalization.\n *\n * @param result - Original detection result\n * @param overrides - Manual landmark adjustments (in pixel coordinates)\n * @returns Updated result with overrides applied\n */\nexport function applyManualOverrides(\n  result: ServerDetectionResult,\n  overrides: LandmarkOverride[]\n): ServerDetectionResult {\n  if (!overrides.length) return result;\n\n  const mergedLandmarks = mergeWithOverrides(result.landmarks, overrides);\n  const adjustedIds = overrides.map(o => o.id);\n\n  // FaceIQ PARITY: Update Frankfort plane for DISPLAY only (not normalization)\n  // Normalization ALWAYS uses FaceIQ indices 10, 26, 75 from rawLandmarks\n  const updatedFrankfort = recalculateFrankfortPlane(mergedLandmarks);\n\n  return {\n    ...result,\n    landmarks: mergedLandmarks,\n    frankfortPlane: updatedFrankfort || result.frankfortPlane,  // Display only\n    manuallyAdjusted: adjustedIds,\n    // rawLandmarks preserved unchanged - critical for FaceIQ rotation\n  };\n}\n\n/**\n * Check if a landmark is estimated (vs directly detected)\n */\nexport function isEstimatedLandmark(landmarkId: string): boolean {\n  return ESTIMATED_LANDMARK_IDS.includes(landmarkId as EstimatedLandmarkId);\n}\n","numLines":290,"startLine":1,"totalLines":290}}}
{"parentUuid":"d4aea3cc-0f48-4622-9d9a-237d3838d3e4","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_014A6UUmShMNGrPcun6LRYgo","type":"tool_result","content":"     1→/**\n     2→ * Face Landmark Detection using MediaPipe Tasks Vision\n     3→ * Maps detected landmarks to our custom facial landmarks\n     4→ *\n     5→ * Detection hierarchy:\n     6→ * - Front profile: MediaPipe Face Mesh (478 landmarks)\n     7→ * - Side profile: Server-side InsightFace (106 landmarks) → Edge-based fallback\n     8→ */\n     9→\n    10→import { faceDetectionService } from './faceDetectionService';\n    11→import { detectSideProfile } from './sideProfileDetection';\n    12→import { detectSideProfileServer } from './serverSideDetection';\n    13→import { applyFaceIQCorrection, applyFrankfortCorrection } from './sideProfileNormalization';\n    14→\n    15→// MediaPipe Face Mesh landmark indices (478 landmarks total)\n    16→// Reference: https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n    17→\n    18→// Front profile landmark mapping\n    19→// Based on the reference website's exact MediaPipe indices\n    20→// Uses SUBJECT's perspective: \"left\" = subject's left (appears on RIGHT side of image)\n    21→export const MEDIAPIPE_FRONT_MAPPING: Record<string, number> = {\n    22→  // Head\n    23→  // Note: MediaPipe 10 is the forehead/glabella, not actual hairline\n    24→  // We apply a Y offset during mapping to estimate the hairline position\n    25→  trichion: 10, // hairline (requires Y offset - see TRICHION_Y_OFFSET)\n    26→\n    27→  // Left Eye (subject's LEFT eye - appears on RIGHT of image)\n    28→  left_pupila: 468,\n    29→  left_canthus_medialis: 133,\n    30→  left_canthus_lateralis: 33,\n    31→  left_palpebra_superior: 470,\n    32→  left_palpebra_inferior: 472,\n    33→  left_sulcus_palpebralis_lateralis: 247,\n    34→  left_pretarsal_skin_crease: 470,\n    35→\n    36→  // Right Eye (subject's RIGHT eye - appears on LEFT of image)\n    37→  right_pupila: 473,\n    38→  right_canthus_medialis: 362,\n    39→  right_canthus_lateralis: 263,\n    40→  right_palpebra_superior: 475,\n    41→  right_palpebra_inferior: 374,\n    42→  right_sulcus_palpebralis_lateralis: 467,\n    43→  right_pretarsal_skin_crease: 475,\n    44→\n    45→  // Left Brow (subject's LEFT brow - appears on RIGHT of image)\n    46→  left_supercilium_medialis: 107,\n    47→  left_supercilium_medial_corner: 55,\n    48→  left_supercilium_superior: 52,\n    49→  left_supercilium_apex: 105,\n    50→  left_supercilium_lateralis: 70,\n    51→\n    52→  // Right Brow (subject's RIGHT brow - appears on LEFT of image)\n    53→  right_supercilium_medialis: 336,\n    54→  right_supercilium_medial_corner: 285,\n    55→  right_supercilium_superior: 282,\n    56→  right_supercilium_apex: 334,\n    57→  right_supercilium_lateralis: 276,\n    58→\n    59→  // Nose\n    60→  nasal_base: 290, // Base of nose (per landmarkers.js)\n    61→  left_dorsum_nasi: 174,\n    62→  right_dorsum_nasi: 399,\n    63→  left_ala_nasi: 48,\n    64→  right_ala_nasi: 278,\n    65→  subnasale: 2, // noseBottom in landmarkers.js\n    66→  nasion: 6, // Added for correct FWHR calculation\n    67→  glabella: 9, // Added for reference\n    68→\n    69→  // Mouth\n    70→  labrale_superius: 267, // cupidsBow - peak of cupid's bow\n    71→  cupids_bow: 0, // innerCupidsBow - center dip of upper lip\n    72→  mouth_middle: 14,\n    73→  labrale_inferius: 17,\n    74→  left_cheilion: 61,\n    75→  right_cheilion: 306,\n    76→\n    77→  // Jaw\n    78→  left_gonion_superior: 58,\n    79→  right_gonion_superior: 288,\n    80→  left_gonion_inferior: 172,\n    81→  right_gonion_inferior: 397,\n    82→\n    83→  // Chin\n    84→  left_mentum_lateralis: 176,\n    85→  right_mentum_lateralis: 400,\n    86→  menton: 152,\n    87→\n    88→  // Cheeks\n    89→  left_zygion: 234,\n    90→  right_zygion: 454,\n    91→  left_malar: 205,        // Cheek center point for fullness calculation\n    92→  right_malar: 425,       // Cheek center point for fullness calculation\n    93→  left_temporal: 54,\n    94→  right_temporal: 284,\n    95→\n    96→  // Ears\n    97→  left_auricular_lateral: 127,\n    98→  right_auricular_lateral: 356,\n    99→\n   100→  // Neck (same as bottom gonion indices, but apply +5% yOffset when rendering)\n   101→  // Per landmarkers.js: neckLeft: 172 with yOffset: 0.05, neckRight: 397 with yOffset: 0.05\n   102→  left_cervical_lateralis: 172,\n   103→  right_cervical_lateralis: 397,\n   104→};\n   105→\n   106→// ============================================\n   107→// DEPTH VARIANCE RESULT INTERFACE\n   108→// ============================================\n   109→\n   110→export interface DepthVarianceResult {\n   111→  isValid: boolean;\n   112→  confidence: number;\n   113→  depthSpread: number;        // Horizontal distance from nose to ear (0-1 normalized)\n   114→  nosalProjection: number;    // How much the nose projects forward\n   115→  frankfortDeviation: number; // Degrees deviation from horizontal\n   116→  issues: string[];           // List of validation issues\n   117→}\n   118→\n   119→// Side profile landmark mapping (IDs must match SIDE_PROFILE_LANDMARKS in landmarks.ts)\n   120→// Matches 28 side profile landmarks exactly\n   121→// For side profile photos, uses MediaPipe LEFT indices (appearing on left side of image)\n   122→// Note: Server uses a custom server-side model for side detection; this is our MediaPipe fallback\n   123→export const MEDIAPIPE_SIDE_MAPPING: Record<string, number> = {\n   124→  // 1. vertex - top of head\n   125→  vertex: 10,\n   126→  // 2. occiput - back of head (no good MediaPipe equivalent, use top)\n   127→  occiput: 10,\n   128→  // 3. pronasale - nose tip\n   129→  pronasale: 1,\n   130→  // 4. neckPoint - lower neck (no good equivalent, use chin)\n   131→  neckPoint: 152,\n   132→  // 5. porion - ear canal opening (use ear area)\n   133→  porion: 127,\n   134→  // 6. orbitale - lowest point of orbital rim (index 33 per cephalometric landmarkers.js)\n   135→  orbitale: 33,\n   136→  // 7. tragus - ear cartilage\n   137→  tragus: 127,\n   138→  // 8. intertragicNotch - notch in ear\n   139→  intertragicNotch: 127,\n   140→  // 9. cornealApex - forward point of cornea\n   141→  cornealApex: 468,\n   142→  // 10. cheekbone - zygomatic prominence\n   143→  cheekbone: 234,\n   144→  // 11. trichion - hairline\n   145→  trichion: 10,\n   146→  // 12. glabella - between brows\n   147→  glabella: 9,\n   148→  // 13. nasion - nasal root depression\n   149→  nasion: 6,\n   150→  // 14. rhinion - dorsal nose point\n   151→  rhinion: 4,\n   152→  // 15. supratip - above nose tip\n   153→  supratip: 4,\n   154→  // 16. infratip - below nose tip\n   155→  infratip: 2,\n   156→  // 17. columella - nasal septum\n   157→  columella: 2,\n   158→  // 18. subnasale - nose base meets upper lip\n   159→  subnasale: 2,\n   160→  // 19. subalare - nostril wing\n   161→  subalare: 129,\n   162→  // 20. labraleSuperius - upper lip\n   163→  labraleSuperius: 0,\n   164→  // 21. cheilion - mouth corner\n   165→  cheilion: 61,\n   166→  // 22. labraleInferius - lower lip\n   167→  labraleInferius: 17,\n   168→  // 23. sublabiale - below lower lip\n   169→  sublabiale: 18,\n   170→  // 24. pogonion - chin point\n   171→  pogonion: 152,\n   172→  // 25. menton - chin bottom\n   173→  menton: 152,\n   174→  // 26. cervicalPoint - high neck point\n   175→  cervicalPoint: 152,\n   176→  // 27. gonionTop - upper jaw angle\n   177→  gonionTop: 116,\n   178→  // 28. gonionBottom - lower jaw angle\n   179→  gonionBottom: 172,\n   180→};\n   181→\n   182→export interface DetectedLandmarks {\n   183→  landmarks: Array<{ id: string; x: number; y: number }>;\n   184→  confidence: number;\n   185→  faceBox: { x: number; y: number; width: number; height: number };\n   186→  frankfortPlane?: {\n   187→    angle: number;\n   188→    orbitale: { x: number; y: number };\n   189→    porion: { x: number; y: number };\n   190→  };\n   191→  depthValidation?: DepthVarianceResult;\n   192→}\n   193→\n   194→/**\n   195→ * Detect facial landmarks from an image URL\n   196→ *\n   197→ * Detection hierarchy:\n   198→ * - Front profile: MediaPipe Face Mesh (478 landmarks) - unchanged\n   199→ * - Side profile: Server-side InsightFace → Edge-based fallback → Manual placement\n   200→ */\n   201→export async function detectFromImageUrl(\n   202→  imageUrl: string,\n   203→  mode: 'front' | 'side'\n   204→): Promise<DetectedLandmarks | null> {\n   205→  try {\n   206→    // ============================================\n   207→    // SIDE PROFILE: Use server-side detection\n   208→    // ============================================\n   209→    if (mode === 'side') {\n   210→      console.log('[Detection] Side profile mode - trying server-side detection...');\n   211→\n   212→      // Try server-side InsightFace first (best accuracy)\n   213→      const serverResult = await detectSideProfileServer(imageUrl);\n   214→      if (serverResult) {\n   215→        console.log('[Detection] Server-side detection succeeded');\n   216→\n   217→        // Validate side profile depth variance\n   218→        const depthValidation = validateSideProfileDepth(\n   219→          serverResult.landmarks,\n   220→          serverResult.frankfortPlane\n   221→        );\n   222→        console.log('[Detection] Depth validation:', {\n   223→          isValid: depthValidation.isValid,\n   224→          confidence: depthValidation.confidence.toFixed(2),\n   225→          depthSpread: (depthValidation.depthSpread * 100).toFixed(1) + '%',\n   226→          issues: depthValidation.issues,\n   227→        });\n   228→\n   229→        // FaceIQ PARITY FIX: Apply rotation in pixel space FIRST, then normalize to 0-1\n   230→        // Step 1: Rotate landmarks using FaceIQ algorithm (in pixel coordinates)\n   231→        const rotatedLandmarks = applyFaceIQCorrection(\n   232→          serverResult.landmarks,\n   233→          serverResult.rawLandmarks,\n   234→          serverResult.direction\n   235→        );\n   236→        console.log('[Detection] Applied FaceIQ correction, direction:',\n   237→          serverResult.direction || 'N/A');\n   238→\n   239→        // Step 2: Normalize to 0-1 range AFTER rotation\n   240→        const { width, height } = serverResult.imageDimensions;\n   241→        const normalizedLandmarks = rotatedLandmarks.map(lm => ({\n   242→          id: lm.id,\n   243→          x: Math.max(0, Math.min(1, lm.x / width)),\n   244→          y: Math.max(0, Math.min(1, lm.y / height)),\n   245→        }));\n   246→        console.log('[Detection] Normalized landmarks to 0-1 range after rotation');\n   247→\n   248→        // Step 3: Normalize Frankfort plane coordinates (for display only)\n   249→        const normalizedFrankfortPlane = serverResult.frankfortPlane ? {\n   250→          angle: serverResult.frankfortPlane.angle,\n   251→          orbitale: {\n   252→            x: Math.max(0, Math.min(1, serverResult.frankfortPlane.orbitale.x / width)),\n   253→            y: Math.max(0, Math.min(1, serverResult.frankfortPlane.orbitale.y / height)),\n   254→          },\n   255→          porion: {\n   256→            x: Math.max(0, Math.min(1, serverResult.frankfortPlane.porion.x / width)),\n   257→            y: Math.max(0, Math.min(1, serverResult.frankfortPlane.porion.y / height)),\n   258→          },\n   259→        } : undefined;\n   260→\n   261→        return {\n   262→          landmarks: normalizedLandmarks,\n   263→          confidence: serverResult.confidence,\n   264→          faceBox: serverResult.faceBox,\n   265→          frankfortPlane: normalizedFrankfortPlane,\n   266→          depthValidation,\n   267→        };\n   268→      }\n   269→\n   270→      console.log('[Detection] Server-side failed, trying edge-based detection...');\n   271→\n   272→      // Fallback to edge-based detection\n   273→      const sideResult = await detectSideProfile(imageUrl);\n   274→      if (sideResult) {\n   275→        console.log('[Detection] Edge-based side profile detection succeeded');\n   276→\n   277→        // Validate side profile depth variance\n   278→        const depthValidation = validateSideProfileDepth(sideResult.landmarks);\n   279→        console.log('[Detection] Depth validation (edge-based):', {\n   280→          isValid: depthValidation.isValid,\n   281→          confidence: depthValidation.confidence.toFixed(2),\n   282→          issues: depthValidation.issues,\n   283→        });\n   284→\n   285→        // Apply Frankfort correction (edge-based doesn't have pre-calculated plane)\n   286→        const normalizedLandmarks = applyFrankfortCorrection(sideResult.landmarks);\n   287→        console.log('[Detection] Applied Frankfort correction to edge-based result');\n   288→\n   289→        return {\n   290→          landmarks: normalizedLandmarks,\n   291→          confidence: sideResult.confidence,\n   292→          faceBox: {\n   293→            x: 0,\n   294→            y: 0,\n   295→            width: 1,\n   296→            height: 1,\n   297→          },\n   298→          depthValidation,\n   299→        };\n   300→      }\n   301→\n   302→      console.log('[Detection] All side profile detection methods failed - manual placement required');\n   303→      return null;\n   304→    }\n   305→\n   306→    // ============================================\n   307→    // FRONT PROFILE: Use MediaPipe (UNCHANGED)\n   308→    // ============================================\n   309→    // Load the image\n   310→    const img = await loadImage(imageUrl);\n   311→\n   312→    // Use the singleton service\n   313→    const result = await faceDetectionService.detect(img);\n   314→\n   315→    // Check if MediaPipe detected a face\n   316→    const mediaPipeSucceeded = result.faceLandmarks && result.faceLandmarks.length > 0;\n   317→\n   318→    if (!mediaPipeSucceeded) {\n   319→      console.log('No faces detected');\n   320→      return null;\n   321→    }\n   322→\n   323→    const faceLandmarks = result.faceLandmarks[0];\n   324→\n   325→    // Front profile uses MediaPipe mapping\n   326→    const mapping = MEDIAPIPE_FRONT_MAPPING;\n   327→\n   328→    // Exact landmark offsets for cephalometric accuracy\n   329→    // These offsets correct MediaPipe's raw positions to match cephalometric standards\n   330→    const LANDMARK_OFFSETS: Record<string, { xOffset?: number; yOffset?: number }> = {\n   331→      // Hairline offset: MediaPipe index 10 is forehead/glabella, not actual hairline\n   332→      trichion: { yOffset: -0.06 },\n   333→\n   334→      // Pupil offsets: small adjustments for accurate pupil center\n   335→      left_pupila: { xOffset: 0.0015, yOffset: -0.005 },\n   336→      right_pupila: { xOffset: -0.0015, yOffset: -0.005 },\n   337→\n   338→      // Nose side offsets: widen nose detection slightly\n   339→      left_ala_nasi: { xOffset: 0.0025 },\n   340→      right_ala_nasi: { xOffset: -0.0025 },\n   341→\n   342→      // Temple offsets: move temples up slightly\n   343→      left_temporal: { yOffset: -0.01 },\n   344→      right_temporal: { yOffset: -0.01 },\n   345→\n   346→      // Outer ear offsets: push ears outward for accurate face width\n   347→      left_auricular_lateral: { xOffset: -0.04 },\n   348→      right_auricular_lateral: { xOffset: 0.04 },\n   349→\n   350→      // Neck offsets: move neck points down\n   351→      left_cervical_lateralis: { yOffset: 0.05 },\n   352→      right_cervical_lateralis: { yOffset: 0.05 },\n   353→\n   354→      // Upper eyelid crease offsets: move crease up slightly\n   355→      left_pretarsal_skin_crease: { yOffset: -0.015 },\n   356→      right_pretarsal_skin_crease: { yOffset: -0.015 },\n   357→    };\n   358→\n   359→    // Map keypoints to our landmarks\n   360→    const landmarks = Object.entries(mapping).map(([id, index]) => {\n   361→      // Handle index bounds\n   362→      const safeIndex = Math.min(index, faceLandmarks.length - 1);\n   363→      const landmark = faceLandmarks[safeIndex];\n   364→\n   365→      // Apply cephalometric offsets\n   366→      const offsets = LANDMARK_OFFSETS[id] || {};\n   367→      const xOffset = offsets.xOffset || 0;\n   368→      const yOffset = offsets.yOffset || 0;\n   369→\n   370→      return {\n   371→        id,\n   372→        // MediaPipe returns normalized coordinates (0-1)\n   373→        // Apply offsets and clamp to valid range\n   374→        x: landmark ? Math.max(0, Math.min(landmark.x + xOffset, 1)) : 0.5,\n   375→        y: landmark ? Math.max(0, Math.min(landmark.y + yOffset, 1)) : 0.5,\n   376→      };\n   377→    });\n   378→\n   379→    // Calculate face bounding box from all landmarks\n   380→    let minX = 1, minY = 1, maxX = 0, maxY = 0;\n   381→    faceLandmarks.forEach((lm) => {\n   382→      minX = Math.min(minX, lm.x);\n   383→      minY = Math.min(minY, lm.y);\n   384→      maxX = Math.max(maxX, lm.x);\n   385→      maxY = Math.max(maxY, lm.y);\n   386→    });\n   387→\n   388→    return {\n   389→      landmarks,\n   390→      confidence: 0.95, // MediaPipe doesn't return confidence per landmark\n   391→      faceBox: {\n   392→        x: minX,\n   393→        y: minY,\n   394→        width: maxX - minX,\n   395→        height: maxY - minY,\n   396→      },\n   397→    };\n   398→  } catch (error) {\n   399→    console.error('Face detection error:', error);\n   400→    return null;\n   401→  }\n   402→}\n   403→\n   404→/**\n   405→ * Load an image from URL\n   406→ */\n   407→function loadImage(url: string): Promise<HTMLImageElement> {\n   408→  return new Promise((resolve, reject) => {\n   409→    const img = new Image();\n   410→    img.crossOrigin = 'anonymous';\n   411→    img.onload = () => resolve(img);\n   412→    img.onerror = (e) => reject(e);\n   413→    img.src = url;\n   414→  });\n   415→}\n   416→\n   417→// ============================================\n   418→// SIDE PROFILE DEPTH VARIANCE VALIDATION\n   419→// ============================================\n   420→\n   421→/**\n   422→ * Validate 3D depth variance from side profile landmarks\n   423→ *\n   424→ * Checks that the photo shows a true side profile by analyzing:\n   425→ * 1. Depth spread: Horizontal distance from nose tip to ear landmarks\n   426→ * 2. Nasal projection: How far the nose projects forward\n   427→ * 3. Frankfort plane: Should be relatively horizontal in proper side profiles\n   428→ *\n   429→ * @param landmarks Array of detected landmarks with x, y coordinates\n   430→ * @param frankfortPlane Optional Frankfort plane data\n   431→ * @returns Validation result with confidence score and issues\n   432→ */\n   433→export function validateSideProfileDepth(\n   434→  landmarks: Array<{ id: string; x: number; y: number }>,\n   435→  frankfortPlane?: { angle: number; orbitale: { x: number; y: number }; porion: { x: number; y: number } }\n   436→): DepthVarianceResult {\n   437→  const issues: string[] = [];\n   438→  let confidence = 1.0;\n   439→\n   440→  // Get key landmarks for depth validation\n   441→  const getLandmark = (id: string) => landmarks.find(l => l.id === id);\n   442→\n   443→  const pronasale = getLandmark('pronasale');     // Nose tip\n   444→  const nasion = getLandmark('nasion');           // Nasal root\n   445→  const porion = getLandmark('porion');           // Ear opening\n   446→  const tragus = getLandmark('tragus');           // Ear cartilage\n   447→  const orbitale = getLandmark('orbitale');       // Lowest orbital point\n   448→  const pogonion = getLandmark('pogonion');       // Chin point\n   449→  const subnasale = getLandmark('subnasale');     // Nose base\n   450→\n   451→  // Calculate depth spread (nose to ear horizontal distance)\n   452→  let depthSpread = 0;\n   453→  const earLandmark = porion || tragus;\n   454→\n   455→  if (pronasale && earLandmark) {\n   456→    // In a true side profile, nose tip should be significantly forward of ear\n   457→    depthSpread = Math.abs(pronasale.x - earLandmark.x);\n   458→\n   459→    // Minimum depth spread threshold (nose should be at least 15% face width from ear)\n   460→    const MIN_DEPTH_SPREAD = 0.15;\n   461→    if (depthSpread < MIN_DEPTH_SPREAD) {\n   462→      issues.push(`Insufficient depth spread (${(depthSpread * 100).toFixed(1)}%): Photo may not be a true side profile`);\n   463→      confidence -= 0.3;\n   464→    }\n   465→  } else {\n   466→    issues.push('Missing key landmarks for depth calculation (pronasale or ear landmark)');\n   467→    confidence -= 0.4;\n   468→  }\n   469→\n   470→  // Calculate nasal projection\n   471→  let nosalProjection = 0;\n   472→  if (pronasale && subnasale) {\n   473→    // Nasal projection: horizontal distance from nose base to tip\n   474→    nosalProjection = Math.abs(pronasale.x - subnasale.x);\n   475→\n   476→    // Minimum nasal projection (nose tip should project at least 5% forward of base)\n   477→    const MIN_NASAL_PROJECTION = 0.05;\n   478→    if (nosalProjection < MIN_NASAL_PROJECTION) {\n   479→      issues.push(`Low nasal projection (${(nosalProjection * 100).toFixed(1)}%): Profile angle may be too frontal`);\n   480→      confidence -= 0.2;\n   481→    }\n   482→  }\n   483→\n   484→  // Check Frankfort plane deviation\n   485→  let frankfortDeviation = 0;\n   486→  if (frankfortPlane) {\n   487→    frankfortDeviation = Math.abs(frankfortPlane.angle);\n   488→\n   489→    // Frankfort plane should be within ±10 degrees of horizontal in ideal photos\n   490→    const MAX_FRANKFORT_DEVIATION = 15;\n   491→    if (frankfortDeviation > MAX_FRANKFORT_DEVIATION) {\n   492→      issues.push(`Head tilt detected (${frankfortDeviation.toFixed(1)}°): Consider re-taking photo with head level`);\n   493→      confidence -= 0.15;\n   494→    }\n   495→  } else if (orbitale && (porion || tragus)) {\n   496→    // Calculate from landmarks if not provided\n   497→    const ear = porion || tragus;\n   498→    if (ear && orbitale) {\n   499→      const dx = ear.x - orbitale.x;\n   500→      const dy = ear.y - orbitale.y;\n   501→      frankfortDeviation = Math.abs(Math.atan2(dy, dx) * (180 / Math.PI));\n   502→\n   503→      const MAX_FRANKFORT_DEVIATION = 15;\n   504→      if (frankfortDeviation > MAX_FRANKFORT_DEVIATION) {\n   505→        issues.push(`Head tilt detected (${frankfortDeviation.toFixed(1)}°): Consider re-taking photo with head level`);\n   506→        confidence -= 0.15;\n   507→      }\n   508→    }\n   509→  }\n   510→\n   511→  // Additional validation: Check landmark horizontal ordering\n   512→  // In a true side profile facing left, nose should be leftmost, ear rightmost\n   513→  // In a true side profile facing right, the opposite\n   514→  if (pronasale && earLandmark && nasion) {\n   515→    const noseToEar = pronasale.x - earLandmark.x;\n   516→    const nasionToEar = nasion.x - earLandmark.x;\n   517→\n   518→    // Both should have the same sign (nose and nasion on same side relative to ear)\n   519→    if (noseToEar * nasionToEar < 0) {\n   520→      issues.push('Inconsistent landmark positions: May indicate partial profile or rotation');\n   521→      confidence -= 0.2;\n   522→    }\n   523→  }\n   524→\n   525→  // Check chin position relative to profile\n   526→  if (pogonion && pronasale && earLandmark) {\n   527→    const chinDepth = Math.abs(pogonion.x - earLandmark.x);\n   528→    const noseDepth = Math.abs(pronasale.x - earLandmark.x);\n   529→\n   530→    // In a normal profile, chin should be closer to ear than nose tip\n   531→    // A severely recessed chin might have issues, but this is a soft check\n   532→    if (noseDepth > 0 && chinDepth / noseDepth < 0.3) {\n   533→      issues.push('Unusual chin position: Very recessed or photo angle issue');\n   534→      confidence -= 0.1;\n   535→    }\n   536→  }\n   537→\n   538→  // Clamp confidence between 0 and 1\n   539→  confidence = Math.max(0, Math.min(1, confidence));\n   540→\n   541→  return {\n   542→    isValid: confidence >= 0.5 && issues.length < 3,\n   543→    confidence,\n   544→    depthSpread,\n   545→    nosalProjection,\n   546→    frankfortDeviation,\n   547→    issues,\n   548→  };\n   549→}\n   550→\n   551→/**\n   552→ * Get the region to zoom into for a specific landmark\n   553→ */\n   554→export function getLandmarkZoomRegion(\n   555→  landmarkId: string\n   556→): { centerX: number; centerY: number; zoomLevel: number } {\n   557→  const regions: Record<string, { centerX: number; centerY: number; zoomLevel: number }> = {\n   558→    // Eyes - zoom in close\n   559→    left_pupila: { centerX: 0.35, centerY: 0.35, zoomLevel: 3 },\n   560→    left_canthus_medialis: { centerX: 0.4, centerY: 0.35, zoomLevel: 3 },\n   561→    left_canthus_lateralis: { centerX: 0.3, centerY: 0.35, zoomLevel: 3 },\n   562→    right_pupila: { centerX: 0.65, centerY: 0.35, zoomLevel: 3 },\n   563→    right_canthus_medialis: { centerX: 0.6, centerY: 0.35, zoomLevel: 3 },\n   564→    right_canthus_lateralis: { centerX: 0.7, centerY: 0.35, zoomLevel: 3 },\n   565→\n   566→    // Brows\n   567→    left_supercilium_apex: { centerX: 0.35, centerY: 0.28, zoomLevel: 2.5 },\n   568→    right_supercilium_apex: { centerX: 0.65, centerY: 0.28, zoomLevel: 2.5 },\n   569→\n   570→    // Nose\n   571→    nasal_base: { centerX: 0.5, centerY: 0.4, zoomLevel: 2 },\n   572→    subnasale: { centerX: 0.5, centerY: 0.55, zoomLevel: 2.5 },\n   573→\n   574→    // Mouth\n   575→    labrale_superius: { centerX: 0.5, centerY: 0.6, zoomLevel: 2.5 },\n   576→    mouth_middle: { centerX: 0.5, centerY: 0.65, zoomLevel: 2 },\n   577→\n   578→    // Chin\n   579→    menton: { centerX: 0.5, centerY: 0.85, zoomLevel: 2 },\n   580→\n   581→    // Default for other landmarks\n   582→    default: { centerX: 0.5, centerY: 0.5, zoomLevel: 1.5 },\n   583→  };\n   584→\n   585→  return regions[landmarkId] || regions.default;\n   586→}\n   587→\n\n</system-reminder>\n"}]},"uuid":"d5efe7ca-5349-492a-8943-f16692cd9e2e","timestamp":"2025-12-30T15:10:16.789Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/LOOKSMAXX/looksmaxx-app/src/lib/mediapipeDetection.ts","content":"/**\n * Face Landmark Detection using MediaPipe Tasks Vision\n * Maps detected landmarks to our custom facial landmarks\n *\n * Detection hierarchy:\n * - Front profile: MediaPipe Face Mesh (478 landmarks)\n * - Side profile: Server-side InsightFace (106 landmarks) → Edge-based fallback\n */\n\nimport { faceDetectionService } from './faceDetectionService';\nimport { detectSideProfile } from './sideProfileDetection';\nimport { detectSideProfileServer } from './serverSideDetection';\nimport { applyFaceIQCorrection, applyFrankfortCorrection } from './sideProfileNormalization';\n\n// MediaPipe Face Mesh landmark indices (478 landmarks total)\n// Reference: https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png\n\n// Front profile landmark mapping\n// Based on the reference website's exact MediaPipe indices\n// Uses SUBJECT's perspective: \"left\" = subject's left (appears on RIGHT side of image)\nexport const MEDIAPIPE_FRONT_MAPPING: Record<string, number> = {\n  // Head\n  // Note: MediaPipe 10 is the forehead/glabella, not actual hairline\n  // We apply a Y offset during mapping to estimate the hairline position\n  trichion: 10, // hairline (requires Y offset - see TRICHION_Y_OFFSET)\n\n  // Left Eye (subject's LEFT eye - appears on RIGHT of image)\n  left_pupila: 468,\n  left_canthus_medialis: 133,\n  left_canthus_lateralis: 33,\n  left_palpebra_superior: 470,\n  left_palpebra_inferior: 472,\n  left_sulcus_palpebralis_lateralis: 247,\n  left_pretarsal_skin_crease: 470,\n\n  // Right Eye (subject's RIGHT eye - appears on LEFT of image)\n  right_pupila: 473,\n  right_canthus_medialis: 362,\n  right_canthus_lateralis: 263,\n  right_palpebra_superior: 475,\n  right_palpebra_inferior: 374,\n  right_sulcus_palpebralis_lateralis: 467,\n  right_pretarsal_skin_crease: 475,\n\n  // Left Brow (subject's LEFT brow - appears on RIGHT of image)\n  left_supercilium_medialis: 107,\n  left_supercilium_medial_corner: 55,\n  left_supercilium_superior: 52,\n  left_supercilium_apex: 105,\n  left_supercilium_lateralis: 70,\n\n  // Right Brow (subject's RIGHT brow - appears on LEFT of image)\n  right_supercilium_medialis: 336,\n  right_supercilium_medial_corner: 285,\n  right_supercilium_superior: 282,\n  right_supercilium_apex: 334,\n  right_supercilium_lateralis: 276,\n\n  // Nose\n  nasal_base: 290, // Base of nose (per landmarkers.js)\n  left_dorsum_nasi: 174,\n  right_dorsum_nasi: 399,\n  left_ala_nasi: 48,\n  right_ala_nasi: 278,\n  subnasale: 2, // noseBottom in landmarkers.js\n  nasion: 6, // Added for correct FWHR calculation\n  glabella: 9, // Added for reference\n\n  // Mouth\n  labrale_superius: 267, // cupidsBow - peak of cupid's bow\n  cupids_bow: 0, // innerCupidsBow - center dip of upper lip\n  mouth_middle: 14,\n  labrale_inferius: 17,\n  left_cheilion: 61,\n  right_cheilion: 306,\n\n  // Jaw\n  left_gonion_superior: 58,\n  right_gonion_superior: 288,\n  left_gonion_inferior: 172,\n  right_gonion_inferior: 397,\n\n  // Chin\n  left_mentum_lateralis: 176,\n  right_mentum_lateralis: 400,\n  menton: 152,\n\n  // Cheeks\n  left_zygion: 234,\n  right_zygion: 454,\n  left_malar: 205,        // Cheek center point for fullness calculation\n  right_malar: 425,       // Cheek center point for fullness calculation\n  left_temporal: 54,\n  right_temporal: 284,\n\n  // Ears\n  left_auricular_lateral: 127,\n  right_auricular_lateral: 356,\n\n  // Neck (same as bottom gonion indices, but apply +5% yOffset when rendering)\n  // Per landmarkers.js: neckLeft: 172 with yOffset: 0.05, neckRight: 397 with yOffset: 0.05\n  left_cervical_lateralis: 172,\n  right_cervical_lateralis: 397,\n};\n\n// ============================================\n// DEPTH VARIANCE RESULT INTERFACE\n// ============================================\n\nexport interface DepthVarianceResult {\n  isValid: boolean;\n  confidence: number;\n  depthSpread: number;        // Horizontal distance from nose to ear (0-1 normalized)\n  nosalProjection: number;    // How much the nose projects forward\n  frankfortDeviation: number; // Degrees deviation from horizontal\n  issues: string[];           // List of validation issues\n}\n\n// Side profile landmark mapping (IDs must match SIDE_PROFILE_LANDMARKS in landmarks.ts)\n// Matches 28 side profile landmarks exactly\n// For side profile photos, uses MediaPipe LEFT indices (appearing on left side of image)\n// Note: Server uses a custom server-side model for side detection; this is our MediaPipe fallback\nexport const MEDIAPIPE_SIDE_MAPPING: Record<string, number> = {\n  // 1. vertex - top of head\n  vertex: 10,\n  // 2. occiput - back of head (no good MediaPipe equivalent, use top)\n  occiput: 10,\n  // 3. pronasale - nose tip\n  pronasale: 1,\n  // 4. neckPoint - lower neck (no good equivalent, use chin)\n  neckPoint: 152,\n  // 5. porion - ear canal opening (use ear area)\n  porion: 127,\n  // 6. orbitale - lowest point of orbital rim (index 33 per cephalometric landmarkers.js)\n  orbitale: 33,\n  // 7. tragus - ear cartilage\n  tragus: 127,\n  // 8. intertragicNotch - notch in ear\n  intertragicNotch: 127,\n  // 9. cornealApex - forward point of cornea\n  cornealApex: 468,\n  // 10. cheekbone - zygomatic prominence\n  cheekbone: 234,\n  // 11. trichion - hairline\n  trichion: 10,\n  // 12. glabella - between brows\n  glabella: 9,\n  // 13. nasion - nasal root depression\n  nasion: 6,\n  // 14. rhinion - dorsal nose point\n  rhinion: 4,\n  // 15. supratip - above nose tip\n  supratip: 4,\n  // 16. infratip - below nose tip\n  infratip: 2,\n  // 17. columella - nasal septum\n  columella: 2,\n  // 18. subnasale - nose base meets upper lip\n  subnasale: 2,\n  // 19. subalare - nostril wing\n  subalare: 129,\n  // 20. labraleSuperius - upper lip\n  labraleSuperius: 0,\n  // 21. cheilion - mouth corner\n  cheilion: 61,\n  // 22. labraleInferius - lower lip\n  labraleInferius: 17,\n  // 23. sublabiale - below lower lip\n  sublabiale: 18,\n  // 24. pogonion - chin point\n  pogonion: 152,\n  // 25. menton - chin bottom\n  menton: 152,\n  // 26. cervicalPoint - high neck point\n  cervicalPoint: 152,\n  // 27. gonionTop - upper jaw angle\n  gonionTop: 116,\n  // 28. gonionBottom - lower jaw angle\n  gonionBottom: 172,\n};\n\nexport interface DetectedLandmarks {\n  landmarks: Array<{ id: string; x: number; y: number }>;\n  confidence: number;\n  faceBox: { x: number; y: number; width: number; height: number };\n  frankfortPlane?: {\n    angle: number;\n    orbitale: { x: number; y: number };\n    porion: { x: number; y: number };\n  };\n  depthValidation?: DepthVarianceResult;\n}\n\n/**\n * Detect facial landmarks from an image URL\n *\n * Detection hierarchy:\n * - Front profile: MediaPipe Face Mesh (478 landmarks) - unchanged\n * - Side profile: Server-side InsightFace → Edge-based fallback → Manual placement\n */\nexport async function detectFromImageUrl(\n  imageUrl: string,\n  mode: 'front' | 'side'\n): Promise<DetectedLandmarks | null> {\n  try {\n    // ============================================\n    // SIDE PROFILE: Use server-side detection\n    // ============================================\n    if (mode === 'side') {\n      console.log('[Detection] Side profile mode - trying server-side detection...');\n\n      // Try server-side InsightFace first (best accuracy)\n      const serverResult = await detectSideProfileServer(imageUrl);\n      if (serverResult) {\n        console.log('[Detection] Server-side detection succeeded');\n\n        // Validate side profile depth variance\n        const depthValidation = validateSideProfileDepth(\n          serverResult.landmarks,\n          serverResult.frankfortPlane\n        );\n        console.log('[Detection] Depth validation:', {\n          isValid: depthValidation.isValid,\n          confidence: depthValidation.confidence.toFixed(2),\n          depthSpread: (depthValidation.depthSpread * 100).toFixed(1) + '%',\n          issues: depthValidation.issues,\n        });\n\n        // FaceIQ PARITY FIX: Apply rotation in pixel space FIRST, then normalize to 0-1\n        // Step 1: Rotate landmarks using FaceIQ algorithm (in pixel coordinates)\n        const rotatedLandmarks = applyFaceIQCorrection(\n          serverResult.landmarks,\n          serverResult.rawLandmarks,\n          serverResult.direction\n        );\n        console.log('[Detection] Applied FaceIQ correction, direction:',\n          serverResult.direction || 'N/A');\n\n        // Step 2: Normalize to 0-1 range AFTER rotation\n        const { width, height } = serverResult.imageDimensions;\n        const normalizedLandmarks = rotatedLandmarks.map(lm => ({\n          id: lm.id,\n          x: Math.max(0, Math.min(1, lm.x / width)),\n          y: Math.max(0, Math.min(1, lm.y / height)),\n        }));\n        console.log('[Detection] Normalized landmarks to 0-1 range after rotation');\n\n        // Step 3: Normalize Frankfort plane coordinates (for display only)\n        const normalizedFrankfortPlane = serverResult.frankfortPlane ? {\n          angle: serverResult.frankfortPlane.angle,\n          orbitale: {\n            x: Math.max(0, Math.min(1, serverResult.frankfortPlane.orbitale.x / width)),\n            y: Math.max(0, Math.min(1, serverResult.frankfortPlane.orbitale.y / height)),\n          },\n          porion: {\n            x: Math.max(0, Math.min(1, serverResult.frankfortPlane.porion.x / width)),\n            y: Math.max(0, Math.min(1, serverResult.frankfortPlane.porion.y / height)),\n          },\n        } : undefined;\n\n        return {\n          landmarks: normalizedLandmarks,\n          confidence: serverResult.confidence,\n          faceBox: serverResult.faceBox,\n          frankfortPlane: normalizedFrankfortPlane,\n          depthValidation,\n        };\n      }\n\n      console.log('[Detection] Server-side failed, trying edge-based detection...');\n\n      // Fallback to edge-based detection\n      const sideResult = await detectSideProfile(imageUrl);\n      if (sideResult) {\n        console.log('[Detection] Edge-based side profile detection succeeded');\n\n        // Validate side profile depth variance\n        const depthValidation = validateSideProfileDepth(sideResult.landmarks);\n        console.log('[Detection] Depth validation (edge-based):', {\n          isValid: depthValidation.isValid,\n          confidence: depthValidation.confidence.toFixed(2),\n          issues: depthValidation.issues,\n        });\n\n        // Apply Frankfort correction (edge-based doesn't have pre-calculated plane)\n        const normalizedLandmarks = applyFrankfortCorrection(sideResult.landmarks);\n        console.log('[Detection] Applied Frankfort correction to edge-based result');\n\n        return {\n          landmarks: normalizedLandmarks,\n          confidence: sideResult.confidence,\n          faceBox: {\n            x: 0,\n            y: 0,\n            width: 1,\n            height: 1,\n          },\n          depthValidation,\n        };\n      }\n\n      console.log('[Detection] All side profile detection methods failed - manual placement required');\n      return null;\n    }\n\n    // ============================================\n    // FRONT PROFILE: Use MediaPipe (UNCHANGED)\n    // ============================================\n    // Load the image\n    const img = await loadImage(imageUrl);\n\n    // Use the singleton service\n    const result = await faceDetectionService.detect(img);\n\n    // Check if MediaPipe detected a face\n    const mediaPipeSucceeded = result.faceLandmarks && result.faceLandmarks.length > 0;\n\n    if (!mediaPipeSucceeded) {\n      console.log('No faces detected');\n      return null;\n    }\n\n    const faceLandmarks = result.faceLandmarks[0];\n\n    // Front profile uses MediaPipe mapping\n    const mapping = MEDIAPIPE_FRONT_MAPPING;\n\n    // Exact landmark offsets for cephalometric accuracy\n    // These offsets correct MediaPipe's raw positions to match cephalometric standards\n    const LANDMARK_OFFSETS: Record<string, { xOffset?: number; yOffset?: number }> = {\n      // Hairline offset: MediaPipe index 10 is forehead/glabella, not actual hairline\n      trichion: { yOffset: -0.06 },\n\n      // Pupil offsets: small adjustments for accurate pupil center\n      left_pupila: { xOffset: 0.0015, yOffset: -0.005 },\n      right_pupila: { xOffset: -0.0015, yOffset: -0.005 },\n\n      // Nose side offsets: widen nose detection slightly\n      left_ala_nasi: { xOffset: 0.0025 },\n      right_ala_nasi: { xOffset: -0.0025 },\n\n      // Temple offsets: move temples up slightly\n      left_temporal: { yOffset: -0.01 },\n      right_temporal: { yOffset: -0.01 },\n\n      // Outer ear offsets: push ears outward for accurate face width\n      left_auricular_lateral: { xOffset: -0.04 },\n      right_auricular_lateral: { xOffset: 0.04 },\n\n      // Neck offsets: move neck points down\n      left_cervical_lateralis: { yOffset: 0.05 },\n      right_cervical_lateralis: { yOffset: 0.05 },\n\n      // Upper eyelid crease offsets: move crease up slightly\n      left_pretarsal_skin_crease: { yOffset: -0.015 },\n      right_pretarsal_skin_crease: { yOffset: -0.015 },\n    };\n\n    // Map keypoints to our landmarks\n    const landmarks = Object.entries(mapping).map(([id, index]) => {\n      // Handle index bounds\n      const safeIndex = Math.min(index, faceLandmarks.length - 1);\n      const landmark = faceLandmarks[safeIndex];\n\n      // Apply cephalometric offsets\n      const offsets = LANDMARK_OFFSETS[id] || {};\n      const xOffset = offsets.xOffset || 0;\n      const yOffset = offsets.yOffset || 0;\n\n      return {\n        id,\n        // MediaPipe returns normalized coordinates (0-1)\n        // Apply offsets and clamp to valid range\n        x: landmark ? Math.max(0, Math.min(landmark.x + xOffset, 1)) : 0.5,\n        y: landmark ? Math.max(0, Math.min(landmark.y + yOffset, 1)) : 0.5,\n      };\n    });\n\n    // Calculate face bounding box from all landmarks\n    let minX = 1, minY = 1, maxX = 0, maxY = 0;\n    faceLandmarks.forEach((lm) => {\n      minX = Math.min(minX, lm.x);\n      minY = Math.min(minY, lm.y);\n      maxX = Math.max(maxX, lm.x);\n      maxY = Math.max(maxY, lm.y);\n    });\n\n    return {\n      landmarks,\n      confidence: 0.95, // MediaPipe doesn't return confidence per landmark\n      faceBox: {\n        x: minX,\n        y: minY,\n        width: maxX - minX,\n        height: maxY - minY,\n      },\n    };\n  } catch (error) {\n    console.error('Face detection error:', error);\n    return null;\n  }\n}\n\n/**\n * Load an image from URL\n */\nfunction loadImage(url: string): Promise<HTMLImageElement> {\n  return new Promise((resolve, reject) => {\n    const img = new Image();\n    img.crossOrigin = 'anonymous';\n    img.onload = () => resolve(img);\n    img.onerror = (e) => reject(e);\n    img.src = url;\n  });\n}\n\n// ============================================\n// SIDE PROFILE DEPTH VARIANCE VALIDATION\n// ============================================\n\n/**\n * Validate 3D depth variance from side profile landmarks\n *\n * Checks that the photo shows a true side profile by analyzing:\n * 1. Depth spread: Horizontal distance from nose tip to ear landmarks\n * 2. Nasal projection: How far the nose projects forward\n * 3. Frankfort plane: Should be relatively horizontal in proper side profiles\n *\n * @param landmarks Array of detected landmarks with x, y coordinates\n * @param frankfortPlane Optional Frankfort plane data\n * @returns Validation result with confidence score and issues\n */\nexport function validateSideProfileDepth(\n  landmarks: Array<{ id: string; x: number; y: number }>,\n  frankfortPlane?: { angle: number; orbitale: { x: number; y: number }; porion: { x: number; y: number } }\n): DepthVarianceResult {\n  const issues: string[] = [];\n  let confidence = 1.0;\n\n  // Get key landmarks for depth validation\n  const getLandmark = (id: string) => landmarks.find(l => l.id === id);\n\n  const pronasale = getLandmark('pronasale');     // Nose tip\n  const nasion = getLandmark('nasion');           // Nasal root\n  const porion = getLandmark('porion');           // Ear opening\n  const tragus = getLandmark('tragus');           // Ear cartilage\n  const orbitale = getLandmark('orbitale');       // Lowest orbital point\n  const pogonion = getLandmark('pogonion');       // Chin point\n  const subnasale = getLandmark('subnasale');     // Nose base\n\n  // Calculate depth spread (nose to ear horizontal distance)\n  let depthSpread = 0;\n  const earLandmark = porion || tragus;\n\n  if (pronasale && earLandmark) {\n    // In a true side profile, nose tip should be significantly forward of ear\n    depthSpread = Math.abs(pronasale.x - earLandmark.x);\n\n    // Minimum depth spread threshold (nose should be at least 15% face width from ear)\n    const MIN_DEPTH_SPREAD = 0.15;\n    if (depthSpread < MIN_DEPTH_SPREAD) {\n      issues.push(`Insufficient depth spread (${(depthSpread * 100).toFixed(1)}%): Photo may not be a true side profile`);\n      confidence -= 0.3;\n    }\n  } else {\n    issues.push('Missing key landmarks for depth calculation (pronasale or ear landmark)');\n    confidence -= 0.4;\n  }\n\n  // Calculate nasal projection\n  let nosalProjection = 0;\n  if (pronasale && subnasale) {\n    // Nasal projection: horizontal distance from nose base to tip\n    nosalProjection = Math.abs(pronasale.x - subnasale.x);\n\n    // Minimum nasal projection (nose tip should project at least 5% forward of base)\n    const MIN_NASAL_PROJECTION = 0.05;\n    if (nosalProjection < MIN_NASAL_PROJECTION) {\n      issues.push(`Low nasal projection (${(nosalProjection * 100).toFixed(1)}%): Profile angle may be too frontal`);\n      confidence -= 0.2;\n    }\n  }\n\n  // Check Frankfort plane deviation\n  let frankfortDeviation = 0;\n  if (frankfortPlane) {\n    frankfortDeviation = Math.abs(frankfortPlane.angle);\n\n    // Frankfort plane should be within ±10 degrees of horizontal in ideal photos\n    const MAX_FRANKFORT_DEVIATION = 15;\n    if (frankfortDeviation > MAX_FRANKFORT_DEVIATION) {\n      issues.push(`Head tilt detected (${frankfortDeviation.toFixed(1)}°): Consider re-taking photo with head level`);\n      confidence -= 0.15;\n    }\n  } else if (orbitale && (porion || tragus)) {\n    // Calculate from landmarks if not provided\n    const ear = porion || tragus;\n    if (ear && orbitale) {\n      const dx = ear.x - orbitale.x;\n      const dy = ear.y - orbitale.y;\n      frankfortDeviation = Math.abs(Math.atan2(dy, dx) * (180 / Math.PI));\n\n      const MAX_FRANKFORT_DEVIATION = 15;\n      if (frankfortDeviation > MAX_FRANKFORT_DEVIATION) {\n        issues.push(`Head tilt detected (${frankfortDeviation.toFixed(1)}°): Consider re-taking photo with head level`);\n        confidence -= 0.15;\n      }\n    }\n  }\n\n  // Additional validation: Check landmark horizontal ordering\n  // In a true side profile facing left, nose should be leftmost, ear rightmost\n  // In a true side profile facing right, the opposite\n  if (pronasale && earLandmark && nasion) {\n    const noseToEar = pronasale.x - earLandmark.x;\n    const nasionToEar = nasion.x - earLandmark.x;\n\n    // Both should have the same sign (nose and nasion on same side relative to ear)\n    if (noseToEar * nasionToEar < 0) {\n      issues.push('Inconsistent landmark positions: May indicate partial profile or rotation');\n      confidence -= 0.2;\n    }\n  }\n\n  // Check chin position relative to profile\n  if (pogonion && pronasale && earLandmark) {\n    const chinDepth = Math.abs(pogonion.x - earLandmark.x);\n    const noseDepth = Math.abs(pronasale.x - earLandmark.x);\n\n    // In a normal profile, chin should be closer to ear than nose tip\n    // A severely recessed chin might have issues, but this is a soft check\n    if (noseDepth > 0 && chinDepth / noseDepth < 0.3) {\n      issues.push('Unusual chin position: Very recessed or photo angle issue');\n      confidence -= 0.1;\n    }\n  }\n\n  // Clamp confidence between 0 and 1\n  confidence = Math.max(0, Math.min(1, confidence));\n\n  return {\n    isValid: confidence >= 0.5 && issues.length < 3,\n    confidence,\n    depthSpread,\n    nosalProjection,\n    frankfortDeviation,\n    issues,\n  };\n}\n\n/**\n * Get the region to zoom into for a specific landmark\n */\nexport function getLandmarkZoomRegion(\n  landmarkId: string\n): { centerX: number; centerY: number; zoomLevel: number } {\n  const regions: Record<string, { centerX: number; centerY: number; zoomLevel: number }> = {\n    // Eyes - zoom in close\n    left_pupila: { centerX: 0.35, centerY: 0.35, zoomLevel: 3 },\n    left_canthus_medialis: { centerX: 0.4, centerY: 0.35, zoomLevel: 3 },\n    left_canthus_lateralis: { centerX: 0.3, centerY: 0.35, zoomLevel: 3 },\n    right_pupila: { centerX: 0.65, centerY: 0.35, zoomLevel: 3 },\n    right_canthus_medialis: { centerX: 0.6, centerY: 0.35, zoomLevel: 3 },\n    right_canthus_lateralis: { centerX: 0.7, centerY: 0.35, zoomLevel: 3 },\n\n    // Brows\n    left_supercilium_apex: { centerX: 0.35, centerY: 0.28, zoomLevel: 2.5 },\n    right_supercilium_apex: { centerX: 0.65, centerY: 0.28, zoomLevel: 2.5 },\n\n    // Nose\n    nasal_base: { centerX: 0.5, centerY: 0.4, zoomLevel: 2 },\n    subnasale: { centerX: 0.5, centerY: 0.55, zoomLevel: 2.5 },\n\n    // Mouth\n    labrale_superius: { centerX: 0.5, centerY: 0.6, zoomLevel: 2.5 },\n    mouth_middle: { centerX: 0.5, centerY: 0.65, zoomLevel: 2 },\n\n    // Chin\n    menton: { centerX: 0.5, centerY: 0.85, zoomLevel: 2 },\n\n    // Default for other landmarks\n    default: { centerX: 0.5, centerY: 0.5, zoomLevel: 1.5 },\n  };\n\n  return regions[landmarkId] || regions.default;\n}\n","numLines":587,"startLine":1,"totalLines":587}}}
{"parentUuid":"d5efe7ca-5349-492a-8943-f16692cd9e2e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01GazekhrhcnzY9dBj1UJGzF","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check the sideProfileNormalization.ts file to verify the FaceIQ correction implementation:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":13056,"cache_read_input_tokens":39783,"cache_creation":{"ephemeral_5m_input_tokens":13056,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWd5rNCKV62bCd132km1L","type":"assistant","uuid":"9025093f-d60f-4b04-8462-e3bc9b97db9e","timestamp":"2025-12-30T15:10:20.359Z"}
{"parentUuid":"9025093f-d60f-4b04-8462-e3bc9b97db9e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01GazekhrhcnzY9dBj1UJGzF","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PoEngMgj5GRhKvXKD6b4vy","name":"Read","input":{"file_path":"/Users/imorgado/LOOKSMAXX/looksmaxx-app/src/lib/sideProfileNormalization.ts"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":13056,"cache_read_input_tokens":39783,"cache_creation":{"ephemeral_5m_input_tokens":13056,"ephemeral_1h_input_tokens":0},"output_tokens":109,"service_tier":"standard"}},"requestId":"req_011CWd5rNCKV62bCd132km1L","type":"assistant","uuid":"524ad2ab-b5bf-4286-abb8-bdca3f249796","timestamp":"2025-12-30T15:10:21.478Z"}
{"parentUuid":"524ad2ab-b5bf-4286-abb8-bdca3f249796","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PoEngMgj5GRhKvXKD6b4vy","type":"tool_result","content":"     1→/**\n     2→ * Side Profile Normalization Utilities - FaceIQ Parity Implementation\n     3→ * \n     4→ * This module replicates FaceIQ's exact side profile normalization algorithm\n     5→ * for 100% parity with their landmark detection and visualization.\n     6→ * \n     7→ * FaceIQ Algorithm (reverse-engineered):\n     8→ * - Uses landmark indices 10, 26, 75 for rotation calculation\n     9→ * - Left profile: rotate based on landmarks[75] → landmarks[26]\n    10→ * - Right profile: rotate based on landmarks[10] → landmarks[26]\n    11→ * - Pivot point: landmarks[10] for right, landmarks[26] for left\n    12→ * - Crops to 1024×1024 with 1.12 scale factor and 0.65 height ratio\n    13→ */\n    14→\n    15→export interface Point2D {\n    16→    x: number;\n    17→    y: number;\n    18→}\n    19→\n    20→export interface FaceIQNormalizationResult {\n    21→    /** Rotation angle applied (in radians) */\n    22→    rotationAngle: number;\n    23→    /** Rotation angle in degrees */\n    24→    rotationAngleDeg: number;\n    25→    /** Direction the face is pointing */\n    26→    direction: 'left' | 'right';\n    27→    /** Pivot point used for rotation */\n    28→    pivot: Point2D;\n    29→    /** Rotated landmarks */\n    30→    rotatedLandmarks: Array<{ id: string; x: number; y: number }>;\n    31→    /** Rotated image as data URL (if image provided) */\n    32→    rotatedImageUrl?: string;\n    33→    /** Crop info */\n    34→    cropInfo: {\n    35→        x: number;\n    36→        y: number;\n    37→        size: number;\n    38→        scale: number;\n    39→    };\n    40→}\n    41→\n    42→/**\n    43→ * FaceIQ's landmark indices for rotation calculation\n    44→ * \n    45→ * These correspond to face contour positions in their model:\n    46→ * - Index 10: Upper face contour (forehead/temple area) - right anchor\n    47→ * - Index 26: Mid-face contour (jaw level) - common reference\n    48→ * - Index 75: Upper face contour (mirrored) - left anchor\n    49→ * \n    50→ * Our InsightFace 106 mapping:\n    51→ * - Index 10 → InsightFace contour point ~9-10 (right side)\n    52→ * - Index 26 → InsightFace contour point ~25-26 (mid contour)\n    53→ * - Index 75 → InsightFace right eye area ~71-75\n    54→ */\n    55→\n    56→/**\n    57→ * FaceIQ to our raw landmark array index mapping (documentation only)\n    58→ * These indices map to the raw landmarks array returned by the API\n    59→ *\n    60→ * @internal Used for reference - actual implementation uses direct indices\n    61→ */\n    62→// eslint-disable-next-line @typescript-eslint/no-unused-vars\n    63→const _FACEIQ_INDEX_REFERENCE = {\n    64→    10: 'faceContour_10',  // Right anchor for rotation\n    65→    26: 'faceContour_26',  // Common reference point\n    66→    75: 'rightEyeLowerInner',  // Left anchor (near eye for left-facing)\n    67→} as const;\n    68→\n    69→/**\n    70→ * Calculate rotation angle using FaceIQ's exact algorithm\n    71→ * \n    72→ * @param landmarks - Raw landmarks array from API\n    73→ * @param direction - Face direction (\"left\" or \"right\")\n    74→ * @returns Rotation angle in radians\n    75→ */\n    76→export function calculateFaceIQRotationAngle(\n    77→    landmarks: Array<{ x: number; y: number }>,\n    78→    direction: 'left' | 'right'\n    79→): number {\n    80→    // FaceIQ algorithm:\n    81→    // anchor = (direction === \"left\") ? landmarks[75] : landmarks[10];\n    82→    // reference = landmarks[26];\n    83→    // angle = Math.atan2(reference.y - anchor.y, reference.x - anchor.x);\n    84→\n    85→    const anchor = direction === 'left' ? landmarks[75] : landmarks[10];\n    86→    const reference = landmarks[26];\n    87→\n    88→    if (!anchor || !reference) {\n    89→        console.warn('[FaceIQ Normalization] Missing landmark indices 10, 26, or 75');\n    90→        return 0;\n    91→    }\n    92→\n    93→    const angle = Math.atan2(reference.y - anchor.y, reference.x - anchor.x);\n    94→    return angle;\n    95→}\n    96→\n    97→/**\n    98→ * Get the rotation pivot point using FaceIQ's logic\n    99→ * \n   100→ * @param landmarks - Raw landmarks array\n   101→ * @param direction - Face direction\n   102→ * @returns Pivot point coordinates\n   103→ */\n   104→export function getFaceIQPivot(\n   105→    landmarks: Array<{ x: number; y: number }>,\n   106→    direction: 'left' | 'right'\n   107→): Point2D {\n   108→    // FaceIQ: pivot = (direction === \"right\") ? landmarks[10] : landmarks[26]\n   109→    const pivotLandmark = direction === 'right' ? landmarks[10] : landmarks[26];\n   110→\n   111→    if (!pivotLandmark) {\n   112→        return { x: 0.5, y: 0.5 };\n   113→    }\n   114→\n   115→    return { x: pivotLandmark.x, y: pivotLandmark.y };\n   116→}\n   117→\n   118→/**\n   119→ * Rotate a point around a pivot using FaceIQ's formula\n   120→ * \n   121→ * FaceIQ's rotation function:\n   122→ * ```\n   123→ * let j = e => {\n   124→ *   let t = Math.sin(-g), i = Math.cos(-g), a = e.x - f, s = e.y - b;\n   125→ *   return { x: f + a * i - s * t, y: b + a * t + s * i };\n   126→ * };\n   127→ * ```\n   128→ */\n   129→export function rotatePointFaceIQ(\n   130→    point: Point2D,\n   131→    pivot: Point2D,\n   132→    angleRad: number\n   133→): Point2D {\n   134→    // FaceIQ rotates by NEGATIVE angle\n   135→    const negAngle = -angleRad;\n   136→    const sin = Math.sin(negAngle);\n   137→    const cos = Math.cos(negAngle);\n   138→\n   139→    const dx = point.x - pivot.x;\n   140→    const dy = point.y - pivot.y;\n   141→\n   142→    return {\n   143→        x: pivot.x + dx * cos - dy * sin,\n   144→        y: pivot.y + dx * sin + dy * cos,\n   145→    };\n   146→}\n   147→\n   148→/**\n   149→ * Calculate crop dimensions using FaceIQ's formula\n   150→ * \n   151→ * FaceIQ algorithm:\n   152→ * - croppedHeight = bboxHeight * 1.12\n   153→ * - cropSize = croppedHeight / 0.65\n   154→ * - Constrained between 32 and 2 * max(width, height)\n   155→ */\n   156→export function calculateFaceIQCropSize(\n   157→    bboxHeight: number,\n   158→    imageWidth: number,\n   159→    imageHeight: number\n   160→): number {\n   161→    const scaledHeight = bboxHeight * 1.12;\n   162→    const cropSize = scaledHeight / 0.65;\n   163→\n   164→    // FaceIQ constraints\n   165→    const minSize = 32;\n   166→    const maxSize = 2 * Math.max(imageWidth, imageHeight);\n   167→\n   168→    return Math.min(Math.max(minSize, cropSize), maxSize);\n   169→}\n   170→\n   171→/**\n   172→ * Apply FaceIQ-exact normalization to named landmarks\n   173→ * \n   174→ * This function works with our named landmark format and applies\n   175→ * the FaceIQ rotation algorithm.\n   176→ * \n   177→ * @param namedLandmarks - Landmarks as { id: string; x: number; y: number }[]\n   178→ * @param rawLandmarks - Raw landmark array (for indices 10, 26, 75)\n   179→ * @param direction - Face direction\n   180→ * @param bbox - Bounding box for crop calculation\n   181→ */\n   182→export function applyFaceIQNormalization(\n   183→    namedLandmarks: Array<{ id: string; x: number; y: number }>,\n   184→    rawLandmarks: Array<{ x: number; y: number }>,\n   185→    direction: 'left' | 'right',\n   186→    bbox?: { x: number; y: number; width: number; height: number }\n   187→): FaceIQNormalizationResult {\n   188→    // Calculate rotation angle using FaceIQ's algorithm\n   189→    const rotationAngle = calculateFaceIQRotationAngle(rawLandmarks, direction);\n   190→    const rotationAngleDeg = rotationAngle * (180 / Math.PI);\n   191→\n   192→    // Get pivot point\n   193→    const pivot = getFaceIQPivot(rawLandmarks, direction);\n   194→\n   195→    // Skip rotation if angle is very small (< 2 degrees)\n   196→    if (Math.abs(rotationAngleDeg) < 2) {\n   197→        return {\n   198→            rotationAngle: 0,\n   199→            rotationAngleDeg: 0,\n   200→            direction,\n   201→            pivot,\n   202→            rotatedLandmarks: namedLandmarks,\n   203→            cropInfo: {\n   204→                x: 0,\n   205→                y: 0,\n   206→                size: 1024,\n   207→                scale: 1,\n   208→            },\n   209→        };\n   210→    }\n   211→\n   212→    // Rotate all named landmarks\n   213→    const rotatedLandmarks = namedLandmarks.map(landmark => {\n   214→        const rotated = rotatePointFaceIQ(\n   215→            { x: landmark.x, y: landmark.y },\n   216→            pivot,\n   217→            rotationAngle\n   218→        );\n   219→        return {\n   220→            id: landmark.id,\n   221→            x: rotated.x,\n   222→            y: rotated.y,\n   223→        };\n   224→    });\n   225→\n   226→    // Calculate crop info\n   227→    const cropSize = bbox\n   228→        ? calculateFaceIQCropSize(bbox.height, 1, 1)  // Normalized\n   229→        : 1024;\n   230→\n   231→    console.log(`[FaceIQ Normalization] Direction: ${direction}, Rotation: ${rotationAngleDeg.toFixed(1)}°`);\n   232→\n   233→    return {\n   234→        rotationAngle,\n   235→        rotationAngleDeg,\n   236→        direction,\n   237→        pivot,\n   238→        rotatedLandmarks,\n   239→        cropInfo: {\n   240→            x: pivot.x - cropSize / 2,\n   241→            y: pivot.y - cropSize / 2,\n   242→            size: cropSize,\n   243→            scale: 1024 / cropSize,\n   244→        },\n   245→    };\n   246→}\n   247→\n   248→/**\n   249→ * Apply FaceIQ normalization (REQUIRED for 100% parity)\n   250→ *\n   251→ * FaceIQ PARITY FIX #3: This function REQUIRES raw landmarks and direction.\n   252→ * No fallback to Frankfort-based normalization - FaceIQ algorithm only.\n   253→ *\n   254→ * @param landmarks - Named landmarks from detection (in pixel coordinates)\n   255→ * @param rawLandmarks - Raw landmark array (REQUIRED - must have 106 landmarks)\n   256→ * @param direction - Face direction (REQUIRED - 'left' or 'right')\n   257→ * @returns Normalized landmarks (still in pixel coordinates)\n   258→ * @throws Error if raw landmarks or direction missing\n   259→ */\n   260→export function applyFaceIQCorrection(\n   261→    landmarks: Array<{ id: string; x: number; y: number }>,\n   262→    rawLandmarks?: Array<{ x: number; y: number }>,\n   263→    direction?: 'left' | 'right'\n   264→): Array<{ id: string; x: number; y: number }> {\n   265→    // FaceIQ PARITY: No fallback - raw landmarks REQUIRED\n   266→    if (!rawLandmarks || rawLandmarks.length < 76 || !direction) {\n   267→        console.error('[FaceIQ Normalization] CRITICAL: Missing raw landmarks or direction');\n   268→        console.error('[FaceIQ Normalization] rawLandmarks length:', rawLandmarks?.length);\n   269→        console.error('[FaceIQ Normalization] direction:', direction);\n   270→        throw new Error(\n   271→            'FaceIQ normalization requires raw landmarks (106) and direction. ' +\n   272→            'Ensure InsightFace detection returned rawLandmarks array.'\n   273→        );\n   274→    }\n   275→\n   276→    const result = applyFaceIQNormalization(landmarks, rawLandmarks, direction);\n   277→    return result.rotatedLandmarks;\n   278→}\n   279→\n   280→/**\n   281→ * Fallback to our original Frankfort-based correction\n   282→ * when FaceIQ indices are not available\n   283→ */\n   284→function applyFrankfortCorrectionFallback(\n   285→    landmarks: Array<{ id: string; x: number; y: number }>\n   286→): Array<{ id: string; x: number; y: number }> {\n   287→    const orbitale = landmarks.find(l => l.id === 'orbitale');\n   288→    const porion = landmarks.find(l => l.id === 'porion');\n   289→\n   290→    if (!orbitale || !porion) {\n   291→        return landmarks;\n   292→    }\n   293→\n   294→    // Calculate Frankfort angle\n   295→    const dx = porion.x - orbitale.x;\n   296→    const dy = porion.y - orbitale.y;\n   297→    const angle = Math.atan2(dy, dx);\n   298→\n   299→    if (Math.abs(angle) < 0.035) { // ~2 degrees\n   300→        return landmarks;\n   301→    }\n   302→\n   303→    const pivot: Point2D = {\n   304→        x: (orbitale.x + porion.x) / 2,\n   305→        y: (orbitale.y + porion.y) / 2,\n   306→    };\n   307→\n   308→    return landmarks.map(landmark => {\n   309→        const cos = Math.cos(-angle);\n   310→        const sin = Math.sin(-angle);\n   311→        const dx = landmark.x - pivot.x;\n   312→        const dy = landmark.y - pivot.y;\n   313→\n   314→        return {\n   315→            id: landmark.id,\n   316→            x: pivot.x + dx * cos - dy * sin,\n   317→            y: pivot.y + dx * sin + dy * cos,\n   318→        };\n   319→    });\n   320→}\n   321→\n   322→/**\n   323→ * Rotate image to match FaceIQ's 1024×1024 standardized output\n   324→ */\n   325→export async function rotateImageFaceIQ(\n   326→    imageUrl: string,\n   327→    rawLandmarks: Array<{ x: number; y: number }>,\n   328→    direction: 'left' | 'right',\n   329→    bbox: { x: number; y: number; width: number; height: number }\n   330→): Promise<string> {\n   331→    return new Promise((resolve, reject) => {\n   332→        const img = new Image();\n   333→        img.crossOrigin = 'anonymous';\n   334→\n   335→        img.onload = () => {\n   336→            const canvas = document.createElement('canvas');\n   337→            const ctx = canvas.getContext('2d');\n   338→\n   339→            if (!ctx) {\n   340→                reject(new Error('Canvas context unavailable'));\n   341→                return;\n   342→            }\n   343→\n   344→            // FaceIQ uses 1024×1024 output\n   345→            const OUTPUT_SIZE = 1024;\n   346→            canvas.width = OUTPUT_SIZE;\n   347→            canvas.height = OUTPUT_SIZE;\n   348→\n   349→            // Calculate rotation\n   350→            const rotationAngle = calculateFaceIQRotationAngle(rawLandmarks, direction);\n   351→            const pivot = getFaceIQPivot(rawLandmarks, direction);\n   352→\n   353→            // Calculate crop size using FaceIQ formula\n   354→            const cropSize = calculateFaceIQCropSize(\n   355→                bbox.height * img.height,\n   356→                img.width,\n   357→                img.height\n   358→            );\n   359→\n   360→            const scale = OUTPUT_SIZE / cropSize;\n   361→\n   362→            // Transform canvas\n   363→            ctx.translate(OUTPUT_SIZE / 2, OUTPUT_SIZE / 2);\n   364→            ctx.rotate(-rotationAngle);\n   365→            ctx.scale(scale, scale);\n   366→            ctx.translate(\n   367→                -pivot.x * img.width,\n   368→                -pivot.y * img.height\n   369→            );\n   370→\n   371→            // Draw image\n   372→            ctx.drawImage(img, 0, 0);\n   373→\n   374→            resolve(canvas.toDataURL('image/jpeg', 0.9));\n   375→        };\n   376→\n   377→        img.onerror = () => reject(new Error('Failed to load image'));\n   378→        img.src = imageUrl;\n   379→    });\n   380→}\n   381→\n   382→// ============================================\n   383→// LEGACY EXPORTS FOR BACKWARD COMPATIBILITY\n   384→// ============================================\n   385→\n   386→// Keep the old function name as an alias\n   387→export { applyFrankfortCorrectionFallback as applyFrankfortCorrection };\n   388→\n   389→// Re-export with old names for compatibility\n   390→export function normalizeLandmarksToFrankfort(\n   391→    landmarks: Array<{ id: string; x: number; y: number }>\n   392→) {\n   393→    return {\n   394→        rotationAngle: 0,\n   395→        originalFrankfortAngle: 0,\n   396→        rotatedLandmarks: applyFrankfortCorrectionFallback(landmarks),\n   397→        rotationCenter: { x: 0.5, y: 0.5 },\n   398→    };\n   399→}\n   400→\n\n</system-reminder>\n"}]},"uuid":"52b70a73-26fc-4d7a-8bbf-3f174834c002","timestamp":"2025-12-30T15:10:21.528Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/LOOKSMAXX/looksmaxx-app/src/lib/sideProfileNormalization.ts","content":"/**\n * Side Profile Normalization Utilities - FaceIQ Parity Implementation\n * \n * This module replicates FaceIQ's exact side profile normalization algorithm\n * for 100% parity with their landmark detection and visualization.\n * \n * FaceIQ Algorithm (reverse-engineered):\n * - Uses landmark indices 10, 26, 75 for rotation calculation\n * - Left profile: rotate based on landmarks[75] → landmarks[26]\n * - Right profile: rotate based on landmarks[10] → landmarks[26]\n * - Pivot point: landmarks[10] for right, landmarks[26] for left\n * - Crops to 1024×1024 with 1.12 scale factor and 0.65 height ratio\n */\n\nexport interface Point2D {\n    x: number;\n    y: number;\n}\n\nexport interface FaceIQNormalizationResult {\n    /** Rotation angle applied (in radians) */\n    rotationAngle: number;\n    /** Rotation angle in degrees */\n    rotationAngleDeg: number;\n    /** Direction the face is pointing */\n    direction: 'left' | 'right';\n    /** Pivot point used for rotation */\n    pivot: Point2D;\n    /** Rotated landmarks */\n    rotatedLandmarks: Array<{ id: string; x: number; y: number }>;\n    /** Rotated image as data URL (if image provided) */\n    rotatedImageUrl?: string;\n    /** Crop info */\n    cropInfo: {\n        x: number;\n        y: number;\n        size: number;\n        scale: number;\n    };\n}\n\n/**\n * FaceIQ's landmark indices for rotation calculation\n * \n * These correspond to face contour positions in their model:\n * - Index 10: Upper face contour (forehead/temple area) - right anchor\n * - Index 26: Mid-face contour (jaw level) - common reference\n * - Index 75: Upper face contour (mirrored) - left anchor\n * \n * Our InsightFace 106 mapping:\n * - Index 10 → InsightFace contour point ~9-10 (right side)\n * - Index 26 → InsightFace contour point ~25-26 (mid contour)\n * - Index 75 → InsightFace right eye area ~71-75\n */\n\n/**\n * FaceIQ to our raw landmark array index mapping (documentation only)\n * These indices map to the raw landmarks array returned by the API\n *\n * @internal Used for reference - actual implementation uses direct indices\n */\n// eslint-disable-next-line @typescript-eslint/no-unused-vars\nconst _FACEIQ_INDEX_REFERENCE = {\n    10: 'faceContour_10',  // Right anchor for rotation\n    26: 'faceContour_26',  // Common reference point\n    75: 'rightEyeLowerInner',  // Left anchor (near eye for left-facing)\n} as const;\n\n/**\n * Calculate rotation angle using FaceIQ's exact algorithm\n * \n * @param landmarks - Raw landmarks array from API\n * @param direction - Face direction (\"left\" or \"right\")\n * @returns Rotation angle in radians\n */\nexport function calculateFaceIQRotationAngle(\n    landmarks: Array<{ x: number; y: number }>,\n    direction: 'left' | 'right'\n): number {\n    // FaceIQ algorithm:\n    // anchor = (direction === \"left\") ? landmarks[75] : landmarks[10];\n    // reference = landmarks[26];\n    // angle = Math.atan2(reference.y - anchor.y, reference.x - anchor.x);\n\n    const anchor = direction === 'left' ? landmarks[75] : landmarks[10];\n    const reference = landmarks[26];\n\n    if (!anchor || !reference) {\n        console.warn('[FaceIQ Normalization] Missing landmark indices 10, 26, or 75');\n        return 0;\n    }\n\n    const angle = Math.atan2(reference.y - anchor.y, reference.x - anchor.x);\n    return angle;\n}\n\n/**\n * Get the rotation pivot point using FaceIQ's logic\n * \n * @param landmarks - Raw landmarks array\n * @param direction - Face direction\n * @returns Pivot point coordinates\n */\nexport function getFaceIQPivot(\n    landmarks: Array<{ x: number; y: number }>,\n    direction: 'left' | 'right'\n): Point2D {\n    // FaceIQ: pivot = (direction === \"right\") ? landmarks[10] : landmarks[26]\n    const pivotLandmark = direction === 'right' ? landmarks[10] : landmarks[26];\n\n    if (!pivotLandmark) {\n        return { x: 0.5, y: 0.5 };\n    }\n\n    return { x: pivotLandmark.x, y: pivotLandmark.y };\n}\n\n/**\n * Rotate a point around a pivot using FaceIQ's formula\n * \n * FaceIQ's rotation function:\n * ```\n * let j = e => {\n *   let t = Math.sin(-g), i = Math.cos(-g), a = e.x - f, s = e.y - b;\n *   return { x: f + a * i - s * t, y: b + a * t + s * i };\n * };\n * ```\n */\nexport function rotatePointFaceIQ(\n    point: Point2D,\n    pivot: Point2D,\n    angleRad: number\n): Point2D {\n    // FaceIQ rotates by NEGATIVE angle\n    const negAngle = -angleRad;\n    const sin = Math.sin(negAngle);\n    const cos = Math.cos(negAngle);\n\n    const dx = point.x - pivot.x;\n    const dy = point.y - pivot.y;\n\n    return {\n        x: pivot.x + dx * cos - dy * sin,\n        y: pivot.y + dx * sin + dy * cos,\n    };\n}\n\n/**\n * Calculate crop dimensions using FaceIQ's formula\n * \n * FaceIQ algorithm:\n * - croppedHeight = bboxHeight * 1.12\n * - cropSize = croppedHeight / 0.65\n * - Constrained between 32 and 2 * max(width, height)\n */\nexport function calculateFaceIQCropSize(\n    bboxHeight: number,\n    imageWidth: number,\n    imageHeight: number\n): number {\n    const scaledHeight = bboxHeight * 1.12;\n    const cropSize = scaledHeight / 0.65;\n\n    // FaceIQ constraints\n    const minSize = 32;\n    const maxSize = 2 * Math.max(imageWidth, imageHeight);\n\n    return Math.min(Math.max(minSize, cropSize), maxSize);\n}\n\n/**\n * Apply FaceIQ-exact normalization to named landmarks\n * \n * This function works with our named landmark format and applies\n * the FaceIQ rotation algorithm.\n * \n * @param namedLandmarks - Landmarks as { id: string; x: number; y: number }[]\n * @param rawLandmarks - Raw landmark array (for indices 10, 26, 75)\n * @param direction - Face direction\n * @param bbox - Bounding box for crop calculation\n */\nexport function applyFaceIQNormalization(\n    namedLandmarks: Array<{ id: string; x: number; y: number }>,\n    rawLandmarks: Array<{ x: number; y: number }>,\n    direction: 'left' | 'right',\n    bbox?: { x: number; y: number; width: number; height: number }\n): FaceIQNormalizationResult {\n    // Calculate rotation angle using FaceIQ's algorithm\n    const rotationAngle = calculateFaceIQRotationAngle(rawLandmarks, direction);\n    const rotationAngleDeg = rotationAngle * (180 / Math.PI);\n\n    // Get pivot point\n    const pivot = getFaceIQPivot(rawLandmarks, direction);\n\n    // Skip rotation if angle is very small (< 2 degrees)\n    if (Math.abs(rotationAngleDeg) < 2) {\n        return {\n            rotationAngle: 0,\n            rotationAngleDeg: 0,\n            direction,\n            pivot,\n            rotatedLandmarks: namedLandmarks,\n            cropInfo: {\n                x: 0,\n                y: 0,\n                size: 1024,\n                scale: 1,\n            },\n        };\n    }\n\n    // Rotate all named landmarks\n    const rotatedLandmarks = namedLandmarks.map(landmark => {\n        const rotated = rotatePointFaceIQ(\n            { x: landmark.x, y: landmark.y },\n            pivot,\n            rotationAngle\n        );\n        return {\n            id: landmark.id,\n            x: rotated.x,\n            y: rotated.y,\n        };\n    });\n\n    // Calculate crop info\n    const cropSize = bbox\n        ? calculateFaceIQCropSize(bbox.height, 1, 1)  // Normalized\n        : 1024;\n\n    console.log(`[FaceIQ Normalization] Direction: ${direction}, Rotation: ${rotationAngleDeg.toFixed(1)}°`);\n\n    return {\n        rotationAngle,\n        rotationAngleDeg,\n        direction,\n        pivot,\n        rotatedLandmarks,\n        cropInfo: {\n            x: pivot.x - cropSize / 2,\n            y: pivot.y - cropSize / 2,\n            size: cropSize,\n            scale: 1024 / cropSize,\n        },\n    };\n}\n\n/**\n * Apply FaceIQ normalization (REQUIRED for 100% parity)\n *\n * FaceIQ PARITY FIX #3: This function REQUIRES raw landmarks and direction.\n * No fallback to Frankfort-based normalization - FaceIQ algorithm only.\n *\n * @param landmarks - Named landmarks from detection (in pixel coordinates)\n * @param rawLandmarks - Raw landmark array (REQUIRED - must have 106 landmarks)\n * @param direction - Face direction (REQUIRED - 'left' or 'right')\n * @returns Normalized landmarks (still in pixel coordinates)\n * @throws Error if raw landmarks or direction missing\n */\nexport function applyFaceIQCorrection(\n    landmarks: Array<{ id: string; x: number; y: number }>,\n    rawLandmarks?: Array<{ x: number; y: number }>,\n    direction?: 'left' | 'right'\n): Array<{ id: string; x: number; y: number }> {\n    // FaceIQ PARITY: No fallback - raw landmarks REQUIRED\n    if (!rawLandmarks || rawLandmarks.length < 76 || !direction) {\n        console.error('[FaceIQ Normalization] CRITICAL: Missing raw landmarks or direction');\n        console.error('[FaceIQ Normalization] rawLandmarks length:', rawLandmarks?.length);\n        console.error('[FaceIQ Normalization] direction:', direction);\n        throw new Error(\n            'FaceIQ normalization requires raw landmarks (106) and direction. ' +\n            'Ensure InsightFace detection returned rawLandmarks array.'\n        );\n    }\n\n    const result = applyFaceIQNormalization(landmarks, rawLandmarks, direction);\n    return result.rotatedLandmarks;\n}\n\n/**\n * Fallback to our original Frankfort-based correction\n * when FaceIQ indices are not available\n */\nfunction applyFrankfortCorrectionFallback(\n    landmarks: Array<{ id: string; x: number; y: number }>\n): Array<{ id: string; x: number; y: number }> {\n    const orbitale = landmarks.find(l => l.id === 'orbitale');\n    const porion = landmarks.find(l => l.id === 'porion');\n\n    if (!orbitale || !porion) {\n        return landmarks;\n    }\n\n    // Calculate Frankfort angle\n    const dx = porion.x - orbitale.x;\n    const dy = porion.y - orbitale.y;\n    const angle = Math.atan2(dy, dx);\n\n    if (Math.abs(angle) < 0.035) { // ~2 degrees\n        return landmarks;\n    }\n\n    const pivot: Point2D = {\n        x: (orbitale.x + porion.x) / 2,\n        y: (orbitale.y + porion.y) / 2,\n    };\n\n    return landmarks.map(landmark => {\n        const cos = Math.cos(-angle);\n        const sin = Math.sin(-angle);\n        const dx = landmark.x - pivot.x;\n        const dy = landmark.y - pivot.y;\n\n        return {\n            id: landmark.id,\n            x: pivot.x + dx * cos - dy * sin,\n            y: pivot.y + dx * sin + dy * cos,\n        };\n    });\n}\n\n/**\n * Rotate image to match FaceIQ's 1024×1024 standardized output\n */\nexport async function rotateImageFaceIQ(\n    imageUrl: string,\n    rawLandmarks: Array<{ x: number; y: number }>,\n    direction: 'left' | 'right',\n    bbox: { x: number; y: number; width: number; height: number }\n): Promise<string> {\n    return new Promise((resolve, reject) => {\n        const img = new Image();\n        img.crossOrigin = 'anonymous';\n\n        img.onload = () => {\n            const canvas = document.createElement('canvas');\n            const ctx = canvas.getContext('2d');\n\n            if (!ctx) {\n                reject(new Error('Canvas context unavailable'));\n                return;\n            }\n\n            // FaceIQ uses 1024×1024 output\n            const OUTPUT_SIZE = 1024;\n            canvas.width = OUTPUT_SIZE;\n            canvas.height = OUTPUT_SIZE;\n\n            // Calculate rotation\n            const rotationAngle = calculateFaceIQRotationAngle(rawLandmarks, direction);\n            const pivot = getFaceIQPivot(rawLandmarks, direction);\n\n            // Calculate crop size using FaceIQ formula\n            const cropSize = calculateFaceIQCropSize(\n                bbox.height * img.height,\n                img.width,\n                img.height\n            );\n\n            const scale = OUTPUT_SIZE / cropSize;\n\n            // Transform canvas\n            ctx.translate(OUTPUT_SIZE / 2, OUTPUT_SIZE / 2);\n            ctx.rotate(-rotationAngle);\n            ctx.scale(scale, scale);\n            ctx.translate(\n                -pivot.x * img.width,\n                -pivot.y * img.height\n            );\n\n            // Draw image\n            ctx.drawImage(img, 0, 0);\n\n            resolve(canvas.toDataURL('image/jpeg', 0.9));\n        };\n\n        img.onerror = () => reject(new Error('Failed to load image'));\n        img.src = imageUrl;\n    });\n}\n\n// ============================================\n// LEGACY EXPORTS FOR BACKWARD COMPATIBILITY\n// ============================================\n\n// Keep the old function name as an alias\nexport { applyFrankfortCorrectionFallback as applyFrankfortCorrection };\n\n// Re-export with old names for compatibility\nexport function normalizeLandmarksToFrankfort(\n    landmarks: Array<{ id: string; x: number; y: number }>\n) {\n    return {\n        rotationAngle: 0,\n        originalFrankfortAngle: 0,\n        rotatedLandmarks: applyFrankfortCorrectionFallback(landmarks),\n        rotationCenter: { x: 0.5, y: 0.5 },\n    };\n}\n","numLines":400,"startLine":1,"totalLines":400}}}
{"parentUuid":"52b70a73-26fc-4d7a-8bbf-3f174834c002","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/LOOKSMAXX/looksmaxx-app","sessionId":"988cde7a-9586-4ca8-910d-50a828b2fa2d","version":"2.0.76","gitBranch":"main","agentId":"a3bd978","slug":"fizzy-singing-bird","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01BBXy9eTnYyqSP1uEiiJn6i","type":"message","role":"assistant","content":[{"type":"text","text":"Perfect! Now I have all the information needed. Let me verify the coordinate normalization timing fix.\n\n## Verification Report: FaceIQ Parity Coordinate Normalization Timing\n\n### ✅ **VERIFICATION COMPLETE - ALL CHECKS PASS**\n\n---\n\n### 1. **serverSideDetection.ts** - Pixel Coordinates Preserved ✅\n\n**Lines 145-157:** Landmarks are correctly kept in PIXEL coordinates:\n\n```typescript\n// FaceIQ PARITY FIX #1: Keep landmarks in PIXEL coordinates (NOT normalized 0-1)\n// Normalization must happen AFTER FaceIQ rotation algorithm\nconst landmarks = Object.entries(resultData.namedLandmarks).map(([id, point]) => ({\n  id,\n  x: point.x,  // Keep pixel coordinates\n  y: point.y,  // Keep pixel coordinates\n}));\n\n// Keep raw landmarks in pixel coordinates for FaceIQ rotation (indices 10, 26, 75)\nconst rawLandmarks = resultData.landmarks?.map(pt => ({\n  x: pt.x,  // Keep pixel coordinates\n  y: pt.y,  // Keep pixel coordinates\n})) || [];\n```\n\n**Line 32 & 186:** Image dimensions field added to interface and result:\n```typescript\n// Interface (line 32):\nimageDimensions: { width: number; height: number };\n\n// Result (line 186):\nimageDimensions: { width, height },  // For normalization AFTER rotation\n```\n\n**Lines 179-180:** Comments explicitly state coordinates are NOT normalized:\n```typescript\nlandmarks,  // Pixel coordinates (NOT normalized)\nrawLandmarks,  // Raw 106 landmarks in pixel coordinates for FaceIQ algorithm\n```\n\n---\n\n### 2. **mediapipeDetection.ts** - Normalization After Rotation ✅\n\n**Lines 229-259:** The fix flow is correctly implemented:\n\n**Step 1: Apply FaceIQ rotation in PIXEL space (lines 231-237):**\n```typescript\n// FaceIQ PARITY FIX: Apply rotation in pixel space FIRST, then normalize to 0-1\n// Step 1: Rotate landmarks using FaceIQ algorithm (in pixel coordinates)\nconst rotatedLandmarks = applyFaceIQCorrection(\n  serverResult.landmarks,\n  serverResult.rawLandmarks,\n  serverResult.direction\n);\n```\n\n**Step 2: Normalize to 0-1 AFTER rotation (lines 239-246):**\n```typescript\n// Step 2: Normalize to 0-1 range AFTER rotation\nconst { width, height } = serverResult.imageDimensions;\nconst normalizedLandmarks = rotatedLandmarks.map(lm => ({\n  id: lm.id,\n  x: Math.max(0, Math.min(1, lm.x / width)),\n  y: Math.max(0, Math.min(1, lm.y / height)),\n}));\nconsole.log('[Detection] Normalized landmarks to 0-1 range after rotation');\n```\n\n**Step 3: Normalize Frankfort plane (lines 248-259):**\n```typescript\n// Step 3: Normalize Frankfort plane coordinates (for display only)\nconst normalizedFrankfortPlane = serverResult.frankfortPlane ? {\n  angle: serverResult.frankfortPlane.angle,\n  orbitale: {\n    x: Math.max(0, Math.min(1, serverResult.frankfortPlane.orbitale.x / width)),\n    y: Math.max(0, Math.min(1, serverResult.frankfortPlane.orbitale.y / height)),\n  },\n  porion: {\n    x: Math.max(0, Math.min(1, serverResult.frankfortPlane.porion.x / width)),\n    y: Math.max(0, Math.min(1, serverResult.frankfortPlane.porion.y / height)),\n  },\n} : undefined;\n```\n\n---\n\n### 3. **Correct Fix Flow Confirmed** ✅\n\nThe implementation follows the exact expected flow:\n\n| Step | Location | Action | Coordinate System |\n|------|----------|--------|-------------------|\n| **1** | `serverSideDetection.ts:145-157` | InsightFace returns landmarks | **Pixel coordinates** |\n| **2** | `mediapipeDetection.ts:231-237` | Apply FaceIQ rotation (indices 10, 26, 75) | **Pixel coordinates** |\n| **3** | `mediapipeDetection.ts:239-246` | Normalize to 0-1 range | **Normalized (0-1)** |\n| **4** | `mediapipeDetection.ts:248-259` | Normalize Frankfort plane | **Normalized (0-1)** |\n\n---\n\n### 4. **Additional Verification Points** ✅\n\n**sideProfileNormalization.ts (lines 260-278):** \n- FaceIQ correction function properly validates raw landmarks and direction\n- Works in pixel space and returns pixel coordinates\n- Throws error if raw landmarks missing (enforces parity)\n\n**serverSideDetection.ts (lines 254-257):**\n- Manual overrides preserve FaceIQ rotation reference\n- Frankfort plane updated for DISPLAY only\n- Comment explicitly states: \"rawLandmarks preserved unchanged - critical for FaceIQ rotation\"\n\n---\n\n## Summary\n\n**✅ ALL VERIFICATION CHECKS PASS**\n\nThe coordinate normalization timing fix for FaceIQ parity is correctly implemented:\n\n1. ✅ Landmarks kept in pixel coordinates in `serverSideDetection.ts`\n2. ✅ Image dimensions added to `ServerDetectionResult` interface\n3. ✅ FaceIQ rotation applied in pixel space BEFORE normalization\n4. ✅ Normalization to 0-1 happens AFTER rotation\n5. ✅ Frankfort plane coordinates also normalized after rotation\n6. ✅ Manual overrides preserve raw landmarks for rotation reference\n\n**No deviations found from the expected flow.**"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":5417,"cache_read_input_tokens":52839,"cache_creation":{"ephemeral_5m_input_tokens":5417,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CWd5riSUf2iS6tRGjjvVu","type":"assistant","uuid":"44c53aec-efc3-468c-ab6f-0205fe117574","timestamp":"2025-12-30T15:10:51.415Z"}
