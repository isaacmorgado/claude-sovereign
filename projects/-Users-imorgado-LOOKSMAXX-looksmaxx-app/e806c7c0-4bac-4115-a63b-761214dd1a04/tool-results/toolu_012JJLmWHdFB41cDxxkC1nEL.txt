     1→/**
     2→ * Face Landmark Detection using MediaPipe Tasks Vision
     3→ * Maps detected landmarks to our custom facial landmarks
     4→ *
     5→ * Uses the shared singleton service for optimal performance
     6→ */
     7→
     8→import { faceDetectionService } from './faceDetectionService';
     9→
    10→// MediaPipe Face Mesh landmark indices (478 landmarks total)
    11→// Reference: https://github.com/google/mediapipe/blob/master/mediapipe/modules/face_geometry/data/canonical_face_model_uv_visualization.png
    12→
    13→// Front profile landmark mapping
    14→// Based on the reference website's exact MediaPipe indices
    15→// Uses SUBJECT's perspective: "left" = subject's left (appears on RIGHT side of image)
    16→export const MEDIAPIPE_FRONT_MAPPING: Record<string, number> = {
    17→  // Head
    18→  trichion: 10, // hairline
    19→
    20→  // Left Eye (subject's LEFT eye - appears on RIGHT of image)
    21→  left_pupila: 468,
    22→  left_canthus_medialis: 133,
    23→  left_canthus_lateralis: 33,
    24→  left_palpebra_superior: 470,
    25→  left_palpebra_inferior: 472,
    26→  left_sulcus_palpebralis_lateralis: 247,
    27→  left_pretarsal_skin_crease: 470,
    28→
    29→  // Right Eye (subject's RIGHT eye - appears on LEFT of image)
    30→  right_pupila: 473,
    31→  right_canthus_medialis: 362,
    32→  right_canthus_lateralis: 263,
    33→  right_palpebra_superior: 475,
    34→  right_palpebra_inferior: 374,
    35→  right_sulcus_palpebralis_lateralis: 467,
    36→  right_pretarsal_skin_crease: 475,
    37→
    38→  // Left Brow (subject's LEFT brow - appears on RIGHT of image)
    39→  left_supercilium_medialis: 107,
    40→  left_supercilium_medial_corner: 55,
    41→  left_supercilium_superior: 52,
    42→  left_supercilium_apex: 105,
    43→  left_supercilium_lateralis: 70,
    44→
    45→  // Right Brow (subject's RIGHT brow - appears on LEFT of image)
    46→  right_supercilium_medialis: 336,
    47→  right_supercilium_medial_corner: 285,
    48→  right_supercilium_superior: 282,
    49→  right_supercilium_apex: 334,
    50→  right_supercilium_lateralis: 276,
    51→
    52→  // Nose
    53→  nasal_base: 290, // Base of nose (per landmarkers.js)
    54→  left_dorsum_nasi: 174,
    55→  right_dorsum_nasi: 399,
    56→  left_ala_nasi: 48,
    57→  right_ala_nasi: 278,
    58→  subnasale: 2, // noseBottom in landmarkers.js
    59→
    60→  // Mouth
    61→  labrale_superius: 267, // cupidsBow - peak of cupid's bow
    62→  cupids_bow: 0, // innerCupidsBow - center dip of upper lip
    63→  mouth_middle: 14,
    64→  labrale_inferius: 17,
    65→  left_cheilion: 61,
    66→  right_cheilion: 306,
    67→
    68→  // Jaw
    69→  left_gonion_superior: 58,
    70→  right_gonion_superior: 288,
    71→  left_gonion_inferior: 172,
    72→  right_gonion_inferior: 397,
    73→
    74→  // Chin
    75→  left_mentum_lateralis: 176,
    76→  right_mentum_lateralis: 400,
    77→  menton: 152,
    78→
    79→  // Cheeks
    80→  left_zygion: 234,
    81→  right_zygion: 454,
    82→  left_temporal: 54,
    83→  right_temporal: 284,
    84→
    85→  // Ears
    86→  left_auricular_lateral: 127,
    87→  right_auricular_lateral: 356,
    88→
    89→  // Neck (same as bottom gonion indices, but apply +5% yOffset when rendering)
    90→  // Per landmarkers.js: neckLeft: 172 with yOffset: 0.05, neckRight: 397 with yOffset: 0.05
    91→  left_cervical_lateralis: 172,
    92→  right_cervical_lateralis: 397,
    93→};
    94→
    95→// Side profile landmark mapping (IDs must match SIDE_PROFILE_LANDMARKS in landmarks.ts)
    96→// For side profile photos, uses MediaPipe LEFT indices (appearing on left side of image)
    97→export const MEDIAPIPE_SIDE_MAPPING: Record<string, number> = {
    98→  // Cranium (center/top landmarks)
    99→  vertex: 10,
   100→  external_occipital_region: 10,
   101→  trichion_profile: 10,
   102→
   103→  // Forehead (center landmarks)
   104→  frontalis: 151,
   105→  glabella: 9,
   106→
   107→  // Eye Region - uses LEFT eye indices
   108→  corneal_apex: 468,
   109→  lateral_eyelid: 33,
   110→  palpebra_inferior_side: 145,
   111→  orbitale: 145,
   112→
   113→  // Nose (center/midline landmarks)
   114→  nasion: 6,
   115→  rhinion: 4,
   116→  supratip_break: 4,
   117→  pronasale: 1,
   118→  infratip_lobule: 2,
   119→  columella_nasi: 2,
   120→  subnasale_side: 2,
   121→  subalare: 129,
   122→
   123→  // Lips (mostly center landmarks)
   124→  labrale_superius_side: 0,
   125→  cheilion_side: 61,
   126→  labrale_inferius_side: 17,
   127→  sublabiale: 18,
   128→
   129→  // Chin (center landmarks)
   130→  pogonion: 152,
   131→  menton_side: 152,
   132→
   133→  // Jaw - uses LEFT jaw indices
   134→  gonion_superior_side: 116,
   135→  gonion_inferior_side: 172,
   136→
   137→  // Cheek - uses LEFT cheekbone
   138→  zygion_soft_tissue: 234,
   139→
   140→  // Ear - uses LEFT ear area
   141→  porion: 127,
   142→  tragion: 127,
   143→  incisura_intertragica: 127,
   144→
   145→  // Neck
   146→  cervicale: 152,
   147→  anterior_cervical_landmark: 152,
   148→};
   149→
   150→export interface DetectedLandmarks {
   151→  landmarks: Array<{ id: string; x: number; y: number }>;
   152→  confidence: number;
   153→  faceBox: { x: number; y: number; width: number; height: number };
   154→}
   155→
   156→/**
   157→ * Detect facial landmarks from an image URL
   158→ * Uses the singleton FaceDetectionService for optimal performance
   159→ */
   160→export async function detectFromImageUrl(
   161→  imageUrl: string,
   162→  mode: 'front' | 'side'
   163→): Promise<DetectedLandmarks | null> {
   164→  try {
   165→    // Load the image
   166→    const img = await loadImage(imageUrl);
   167→
   168→    // Use the singleton service
   169→    const result = await faceDetectionService.detect(img);
   170→
   171→    if (!result.faceLandmarks || result.faceLandmarks.length === 0) {
   172→      console.log('No faces detected');
   173→      return null;
   174→    }
   175→
   176→    const faceLandmarks = result.faceLandmarks[0];
   177→
   178→    // Select mapping based on mode
   179→    const mapping = mode === 'front' ? MEDIAPIPE_FRONT_MAPPING : MEDIAPIPE_SIDE_MAPPING;
   180→
   181→    // Neck landmarks need +5% yOffset per FaceIQ landmarkers.js
   182→    const NECK_Y_OFFSET = 0.05;
   183→    const NECK_LANDMARKS = ['left_cervical_lateralis', 'right_cervical_lateralis'];
   184→
   185→    // Map keypoints to our landmarks
   186→    const landmarks = Object.entries(mapping).map(([id, index]) => {
   187→      // Handle index bounds
   188→      const safeIndex = Math.min(index, faceLandmarks.length - 1);
   189→      const landmark = faceLandmarks[safeIndex];
   190→
   191→      // Apply yOffset for neck landmarks
   192→      const yOffset = NECK_LANDMARKS.includes(id) ? NECK_Y_OFFSET : 0;
   193→
   194→      return {
   195→        id,
   196→        // MediaPipe returns normalized coordinates (0-1)
   197→        x: landmark ? landmark.x : 0.5,
   198→        y: landmark ? Math.min(landmark.y + yOffset, 1) : 0.5,
   199→      };
   200→    });
   201→
   202→    // Calculate face bounding box from all landmarks
   203→    let minX = 1, minY = 1, maxX = 0, maxY = 0;
   204→    faceLandmarks.forEach((lm) => {
   205→      minX = Math.min(minX, lm.x);
   206→      minY = Math.min(minY, lm.y);
   207→      maxX = Math.max(maxX, lm.x);
   208→      maxY = Math.max(maxY, lm.y);
   209→    });
   210→
   211→    return {
   212→      landmarks,
   213→      confidence: 0.95, // MediaPipe doesn't return confidence per landmark
   214→      faceBox: {
   215→        x: minX,
   216→        y: minY,
   217→        width: maxX - minX,
   218→        height: maxY - minY,
   219→      },
   220→    };
   221→  } catch (error) {
   222→    console.error('Face detection error:', error);
   223→    return null;
   224→  }
   225→}
   226→
   227→/**
   228→ * Load an image from URL
   229→ */
   230→function loadImage(url: string): Promise<HTMLImageElement> {
   231→  return new Promise((resolve, reject) => {
   232→    const img = new Image();
   233→    img.crossOrigin = 'anonymous';
   234→    img.onload = () => resolve(img);
   235→    img.onerror = (e) => reject(e);
   236→    img.src = url;
   237→  });
   238→}
   239→
   240→/**
   241→ * Get the region to zoom into for a specific landmark
   242→ */
   243→export function getLandmarkZoomRegion(
   244→  landmarkId: string
   245→): { centerX: number; centerY: number; zoomLevel: number } {
   246→  const regions: Record<string, { centerX: number; centerY: number; zoomLevel: number }> = {
   247→    // Eyes - zoom in close
   248→    left_pupila: { centerX: 0.35, centerY: 0.35, zoomLevel: 3 },
   249→    left_canthus_medialis: { centerX: 0.4, centerY: 0.35, zoomLevel: 3 },
   250→    left_canthus_lateralis: { centerX: 0.3, centerY: 0.35, zoomLevel: 3 },
   251→    right_pupila: { centerX: 0.65, centerY: 0.35, zoomLevel: 3 },
   252→    right_canthus_medialis: { centerX: 0.6, centerY: 0.35, zoomLevel: 3 },
   253→    right_canthus_lateralis: { centerX: 0.7, centerY: 0.35, zoomLevel: 3 },
   254→
   255→    // Brows
   256→    left_supercilium_apex: { centerX: 0.35, centerY: 0.28, zoomLevel: 2.5 },
   257→    right_supercilium_apex: { centerX: 0.65, centerY: 0.28, zoomLevel: 2.5 },
   258→
   259→    // Nose
   260→    nasal_base: { centerX: 0.5, centerY: 0.4, zoomLevel: 2 },
   261→    subnasale: { centerX: 0.5, centerY: 0.55, zoomLevel: 2.5 },
   262→
   263→    // Mouth
   264→    labrale_superius: { centerX: 0.5, centerY: 0.6, zoomLevel: 2.5 },
   265→    mouth_middle: { centerX: 0.5, centerY: 0.65, zoomLevel: 2 },
   266→
   267→    // Chin
   268→    menton: { centerX: 0.5, centerY: 0.85, zoomLevel: 2 },
   269→
   270→    // Default for other landmarks
   271→    default: { centerX: 0.5, centerY: 0.5, zoomLevel: 1.5 },
   272→  };
   273→
   274→  return regions[landmarkId] || regions.default;
   275→}
   276→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
