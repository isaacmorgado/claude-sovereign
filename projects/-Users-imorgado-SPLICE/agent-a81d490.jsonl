{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":"Analyze the SPLICE codebase for performance bottlenecks. Focus on:\n1. Server endpoints in splice-backend/server.js - any slow operations, blocking calls, N+1 queries\n2. Plugin JS files in splice-plugin/js/ - any expensive DOM operations, memory leaks\n3. Services in splice-backend/services/ - any inefficient algorithms, missing caching\n4. Database queries in usageTracking.js - any missing indexes, slow queries\n\nReport findings with file:line references and severity (HIGH/MEDIUM/LOW)."},"uuid":"504225bb-259c-4fe1-87af-34cd4ee944e9","timestamp":"2025-12-25T22:30:14.141Z"}
{"parentUuid":"504225bb-259c-4fe1-87af-34cd4ee944e9","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01UN6PccC1AjYzBKYo6nmSrz","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017wpQhHKpNk8sujDLQXayii","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/server.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":17184,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":17184,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCMh2NZXJTxpMbEqtkn","type":"assistant","uuid":"263acd64-8ae7-41cf-9096-a99b5e5dd3c3","timestamp":"2025-12-25T22:30:20.392Z"}
{"parentUuid":"263acd64-8ae7-41cf-9096-a99b5e5dd3c3","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01UN6PccC1AjYzBKYo6nmSrz","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016KAx3tT4uUiTBHxRRVpDSX","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":17184,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":17184,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCMh2NZXJTxpMbEqtkn","type":"assistant","uuid":"11ab94d2-f08a-4196-bae4-1f032446794a","timestamp":"2025-12-25T22:30:20.801Z"}
{"parentUuid":"11ab94d2-f08a-4196-bae4-1f032446794a","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01UN6PccC1AjYzBKYo6nmSrz","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TAi2f3eNyAGY9RBfXA2wUj","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/transcription.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":17184,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":17184,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCMh2NZXJTxpMbEqtkn","type":"assistant","uuid":"9ff48b3f-6307-4560-8301-c6756d483f8a","timestamp":"2025-12-25T22:30:21.213Z"}
{"parentUuid":"9ff48b3f-6307-4560-8301-c6756d483f8a","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01UN6PccC1AjYzBKYo6nmSrz","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01ALe56tZ45sWTuhctLQrcF8","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/cutListGenerator.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":17184,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":17184,"ephemeral_1h_input_tokens":0},"output_tokens":237,"service_tier":"standard"}},"requestId":"req_011CWUCMh2NZXJTxpMbEqtkn","type":"assistant","uuid":"2e6ce780-8fd8-4dc8-9c88-8fde4358c621","timestamp":"2025-12-25T22:30:21.630Z"}
{"parentUuid":"2e6ce780-8fd8-4dc8-9c88-8fde4358c621","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016KAx3tT4uUiTBHxRRVpDSX","type":"tool_result","content":"     1→/**\n     2→ * Usage Tracking Service\n     3→ *\n     4→ * Tracks user credit balances and usage for billing.\n     5→ * Uses PostgreSQL for persistence.\n     6→ */\n     7→\n     8→const { Pool } = require('pg');\n     9→\n    10→// PostgreSQL connection pool\n    11→const pool = new Pool({\n    12→  connectionString: process.env.DATABASE_URL,\n    13→  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false\n    14→});\n    15→\n    16→// Hours per tier (regular processing)\n    17→const TIER_HOURS = {\n    18→  starter: 15,\n    19→  pro: 50,\n    20→  team: 150,\n    21→  cancelled: 0\n    22→};\n    23→\n    24→// Isolation minutes per tier (sized to maintain 80% margins)\n    25→const TIER_ISOLATION_MINUTES = {\n    26→  starter: 0,     // No isolation access\n    27→  pro: 45,        // 45 minutes included\n    28→  team: 180,      // 3 hours included\n    29→  cancelled: 0\n    30→};\n    31→\n    32→// Convert to hours for database storage\n    33→const TIER_ISOLATION_HOURS = {\n    34→  starter: 0,\n    35→  pro: TIER_ISOLATION_MINUTES.pro / 60,      // 0.75 hours\n    36→  team: TIER_ISOLATION_MINUTES.team / 60,    // 3 hours\n    37→  cancelled: 0\n    38→};\n    39→\n    40→// Overage rate for isolation beyond included minutes\n    41→const ISOLATION_OVERAGE_RATE = 0.10; // $0.10 per minute (85% margin)\n    42→\n    43→/**\n    44→ * Initialize database tables\n    45→ */\n    46→async function initDatabase() {\n    47→  const client = await pool.connect();\n    48→  try {\n    49→    await client.query(`\n    50→      CREATE TABLE IF NOT EXISTS users (\n    51→        id SERIAL PRIMARY KEY,\n    52→        stripe_customer_id VARCHAR(255) UNIQUE NOT NULL,\n    53→        email VARCHAR(255),\n    54→        tier VARCHAR(50) DEFAULT 'starter',\n    55→        hours_remaining DECIMAL(10,4) DEFAULT 15,\n    56→        hours_total DECIMAL(10,4) DEFAULT 15,\n    57→        isolation_hours_remaining DECIMAL(10,4) DEFAULT 0,\n    58→        isolation_hours_total DECIMAL(10,4) DEFAULT 0,\n    59→        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    60→        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    61→      )\n    62→    `);\n    63→\n    64→    // Add isolation columns if they don't exist (for existing databases)\n    65→    await client.query(`\n    66→      DO $$\n    67→      BEGIN\n    68→        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_remaining') THEN\n    69→          ALTER TABLE users ADD COLUMN isolation_hours_remaining DECIMAL(10,4) DEFAULT 0;\n    70→        END IF;\n    71→        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_total') THEN\n    72→          ALTER TABLE users ADD COLUMN isolation_hours_total DECIMAL(10,4) DEFAULT 0;\n    73→        END IF;\n    74→      END $$;\n    75→    `);\n    76→\n    77→    await client.query(`\n    78→      CREATE TABLE IF NOT EXISTS usage_log (\n    79→        id SERIAL PRIMARY KEY,\n    80→        user_id INTEGER REFERENCES users(id),\n    81→        stripe_customer_id VARCHAR(255),\n    82→        audio_duration_seconds DECIMAL(10,2),\n    83→        hours_used DECIMAL(10,6),\n    84→        endpoint VARCHAR(100),\n    85→        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    86→      )\n    87→    `);\n    88→\n    89→    await client.query(`\n    90→      CREATE INDEX IF NOT EXISTS idx_users_stripe_customer_id ON users(stripe_customer_id)\n    91→    `);\n    92→\n    93→    await client.query(`\n    94→      CREATE INDEX IF NOT EXISTS idx_usage_log_user_id ON usage_log(user_id)\n    95→    `);\n    96→\n    97→    // Webhook events table for idempotency\n    98→    await client.query(`\n    99→      CREATE TABLE IF NOT EXISTS webhook_events (\n   100→        id SERIAL PRIMARY KEY,\n   101→        event_id VARCHAR(255) UNIQUE NOT NULL,\n   102→        event_type VARCHAR(100),\n   103→        processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n   104→      )\n   105→    `);\n   106→\n   107→    await client.query(`\n   108→      CREATE INDEX IF NOT EXISTS idx_webhook_events_event_id ON webhook_events(event_id)\n   109→    `);\n   110→\n   111→    console.log('[UsageTracking] Database initialized');\n   112→  } finally {\n   113→    client.release();\n   114→  }\n   115→}\n   116→\n   117→/**\n   118→ * Get or create a user by Stripe customer ID\n   119→ */\n   120→async function getOrCreateUser(stripeCustomerId, email = null) {\n   121→  const client = await pool.connect();\n   122→  try {\n   123→    // Try to find existing user\n   124→    let result = await client.query(\n   125→      'SELECT * FROM users WHERE stripe_customer_id = $1',\n   126→      [stripeCustomerId]\n   127→    );\n   128→\n   129→    if (result.rows.length > 0) {\n   130→      return result.rows[0];\n   131→    }\n   132→\n   133→    // Create new user with starter tier (no isolation hours)\n   134→    result = await client.query(\n   135→      `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n   136→       VALUES ($1, $2, 'starter', $3, $3, $4, $4)\n   137→       RETURNING *`,\n   138→      [stripeCustomerId, email, TIER_HOURS.starter, TIER_ISOLATION_HOURS.starter]\n   139→    );\n   140→\n   141→    return result.rows[0];\n   142→  } finally {\n   143→    client.release();\n   144→  }\n   145→}\n   146→\n   147→/**\n   148→ * Get user's current credit balance\n   149→ */\n   150→async function getBalance(stripeCustomerId) {\n   151→  const user = await getOrCreateUser(stripeCustomerId);\n   152→  const hoursTotal = parseFloat(user.hours_total);\n   153→  const hoursRemaining = parseFloat(user.hours_remaining);\n   154→  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n   155→  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n   156→\n   157→  // Tier display names\n   158→  const tierNames = {\n   159→    starter: 'Starter',\n   160→    pro: 'Pro',\n   161→    team: 'Team',\n   162→    cancelled: 'Cancelled'\n   163→  };\n   164→\n   165→  return {\n   166→    hoursRemaining,\n   167→    hoursTotal,\n   168→    isolationHoursRemaining,\n   169→    isolationHoursTotal,\n   170→    tier: user.tier,\n   171→    tierName: tierNames[user.tier] || user.tier,\n   172→    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n   173→    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n   174→    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n   175→    isolationOverageRate: ISOLATION_OVERAGE_RATE\n   176→  };\n   177→}\n   178→\n   179→/**\n   180→ * Check if user has enough credits for estimated duration\n   181→ */\n   182→async function hasCredits(stripeCustomerId, estimatedSeconds = 0) {\n   183→  const balance = await getBalance(stripeCustomerId);\n   184→  const estimatedHours = estimatedSeconds / 3600;\n   185→  return balance.hoursRemaining >= estimatedHours;\n   186→}\n   187→\n   188→/**\n   189→ * Check if user can use vocal isolation\n   190→ * Returns: { allowed, hasIncludedHours, overageRequired, overageCost }\n   191→ */\n   192→async function checkIsolationAccess(stripeCustomerId, estimatedMinutes = 0) {\n   193→  const balance = await getBalance(stripeCustomerId);\n   194→\n   195→  // Starter tier has no isolation access\n   196→  if (!balance.hasIsolationAccess) {\n   197→    return {\n   198→      allowed: false,\n   199→      reason: 'upgrade_required',\n   200→      message: 'Vocal isolation requires Pro or Team tier',\n   201→      hasIncludedHours: false,\n   202→      overageRequired: false,\n   203→      overageCost: 0\n   204→    };\n   205→  }\n   206→\n   207→  const estimatedHours = estimatedMinutes / 60;\n   208→  const remainingIncluded = balance.isolationHoursRemaining;\n   209→\n   210→  // Check if within included hours\n   211→  if (remainingIncluded >= estimatedHours) {\n   212→    return {\n   213→      allowed: true,\n   214→      reason: 'included',\n   215→      message: `Using ${estimatedMinutes.toFixed(1)} min of included isolation time`,\n   216→      hasIncludedHours: true,\n   217→      overageRequired: false,\n   218→      overageCost: 0,\n   219→      isolationHoursRemaining: remainingIncluded - estimatedHours\n   220→    };\n   221→  }\n   222→\n   223→  // Calculate overage\n   224→  const overageMinutes = estimatedMinutes - (remainingIncluded * 60);\n   225→  const overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n   226→\n   227→  return {\n   228→    allowed: true,\n   229→    reason: 'overage',\n   230→    message: `${remainingIncluded > 0 ? `Using ${(remainingIncluded * 60).toFixed(1)} included min + ` : ''}${overageMinutes.toFixed(1)} min overage ($${overageCost.toFixed(2)})`,\n   231→    hasIncludedHours: remainingIncluded > 0,\n   232→    overageRequired: true,\n   233→    overageMinutes,\n   234→    overageCost,\n   235→    isolationHoursRemaining: 0\n   236→  };\n   237→}\n   238→\n   239→/**\n   240→ * Deduct isolation usage from user's balance\n   241→ * Returns overage cost if beyond included hours\n   242→ */\n   243→async function deductIsolationUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'isolate-vocals') {\n   244→  const durationMinutes = audioDurationSeconds / 60;\n   245→  const durationHours = audioDurationSeconds / 3600;\n   246→\n   247→  const client = await pool.connect();\n   248→  try {\n   249→    // Get user\n   250→    const userResult = await client.query(\n   251→      'SELECT * FROM users WHERE stripe_customer_id = $1',\n   252→      [stripeCustomerId]\n   253→    );\n   254→\n   255→    if (userResult.rows.length === 0) {\n   256→      throw new Error('User not found');\n   257→    }\n   258→\n   259→    const user = userResult.rows[0];\n   260→    const remainingIncluded = parseFloat(user.isolation_hours_remaining || 0);\n   261→\n   262→    let overageCost = 0;\n   263→    let hoursFromIncluded = 0;\n   264→    let overageMinutes = 0;\n   265→\n   266→    if (remainingIncluded >= durationHours) {\n   267→      // Fully covered by included hours\n   268→      hoursFromIncluded = durationHours;\n   269→    } else {\n   270→      // Partial or full overage\n   271→      hoursFromIncluded = remainingIncluded;\n   272→      overageMinutes = durationMinutes - (remainingIncluded * 60);\n   273→      overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n   274→    }\n   275→\n   276→    // Update isolation balance\n   277→    await client.query(\n   278→      `UPDATE users\n   279→       SET isolation_hours_remaining = GREATEST(0, isolation_hours_remaining - $1),\n   280→           updated_at = CURRENT_TIMESTAMP\n   281→       WHERE stripe_customer_id = $2`,\n   282→      [durationHours, stripeCustomerId]\n   283→    );\n   284→\n   285→    // Log usage (mark as isolation)\n   286→    await client.query(\n   287→      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   288→       VALUES ($1, $2, $3, $4, $5)`,\n   289→      [user.id, stripeCustomerId, audioDurationSeconds, durationHours, endpoint]\n   290→    );\n   291→\n   292→    // Return updated balance and overage info\n   293→    const newBalance = await getBalance(stripeCustomerId);\n   294→    return {\n   295→      ...newBalance,\n   296→      isolationUsed: {\n   297→        durationMinutes,\n   298→        hoursFromIncluded,\n   299→        overageMinutes,\n   300→        overageCost\n   301→      }\n   302→    };\n   303→  } finally {\n   304→    client.release();\n   305→  }\n   306→}\n   307→\n   308→/**\n   309→ * Deduct usage from user's balance\n   310→ */\n   311→async function deductUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'unknown') {\n   312→  const hoursUsed = audioDurationSeconds / 3600;\n   313→\n   314→  const client = await pool.connect();\n   315→  try {\n   316→    // Get user\n   317→    const userResult = await client.query(\n   318→      'SELECT * FROM users WHERE stripe_customer_id = $1',\n   319→      [stripeCustomerId]\n   320→    );\n   321→\n   322→    if (userResult.rows.length === 0) {\n   323→      throw new Error('User not found');\n   324→    }\n   325→\n   326→    const user = userResult.rows[0];\n   327→\n   328→    // Update balance\n   329→    await client.query(\n   330→      `UPDATE users\n   331→       SET hours_remaining = GREATEST(0, hours_remaining - $1),\n   332→           updated_at = CURRENT_TIMESTAMP\n   333→       WHERE stripe_customer_id = $2`,\n   334→      [hoursUsed, stripeCustomerId]\n   335→    );\n   336→\n   337→    // Log usage\n   338→    await client.query(\n   339→      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   340→       VALUES ($1, $2, $3, $4, $5)`,\n   341→      [user.id, stripeCustomerId, audioDurationSeconds, hoursUsed, endpoint]\n   342→    );\n   343→\n   344→    // Return updated balance\n   345→    return await getBalance(stripeCustomerId);\n   346→  } finally {\n   347→    client.release();\n   348→  }\n   349→}\n   350→\n   351→/**\n   352→ * Reset user's hours for new billing period\n   353→ */\n   354→async function resetHours(stripeCustomerId, tier) {\n   355→  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n   356→  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n   357→\n   358→  await pool.query(\n   359→    `UPDATE users\n   360→     SET hours_remaining = $1, hours_total = $1,\n   361→         isolation_hours_remaining = $2, isolation_hours_total = $2,\n   362→         tier = $3, updated_at = CURRENT_TIMESTAMP\n   363→     WHERE stripe_customer_id = $4`,\n   364→    [hours, isolationHours, tier, stripeCustomerId]\n   365→  );\n   366→\n   367→  return await getBalance(stripeCustomerId);\n   368→}\n   369→\n   370→/**\n   371→ * Update user's tier (from Stripe webhook)\n   372→ */\n   373→async function updateTier(stripeCustomerId, tier, email = null) {\n   374→  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n   375→  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n   376→\n   377→  const client = await pool.connect();\n   378→  try {\n   379→    // Check if user exists\n   380→    const result = await client.query(\n   381→      'SELECT * FROM users WHERE stripe_customer_id = $1',\n   382→      [stripeCustomerId]\n   383→    );\n   384→\n   385→    if (result.rows.length === 0) {\n   386→      // Create new user with this tier\n   387→      await client.query(\n   388→        `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n   389→         VALUES ($1, $2, $3, $4, $4, $5, $5)`,\n   390→        [stripeCustomerId, email, tier, hours, isolationHours]\n   391→      );\n   392→    } else {\n   393→      // Update existing user\n   394→      await client.query(\n   395→        `UPDATE users\n   396→         SET tier = $1, hours_remaining = $2, hours_total = $2,\n   397→             isolation_hours_remaining = $3, isolation_hours_total = $3,\n   398→             updated_at = CURRENT_TIMESTAMP\n   399→         WHERE stripe_customer_id = $4`,\n   400→        [tier, hours, isolationHours, stripeCustomerId]\n   401→      );\n   402→    }\n   403→\n   404→    return await getBalance(stripeCustomerId);\n   405→  } finally {\n   406→    client.release();\n   407→  }\n   408→}\n   409→\n   410→/**\n   411→ * Get usage history for a user\n   412→ */\n   413→async function getUsageHistory(stripeCustomerId, limit = 50) {\n   414→  const result = await pool.query(\n   415→    `SELECT * FROM usage_log\n   416→     WHERE stripe_customer_id = $1\n   417→     ORDER BY created_at DESC\n   418→     LIMIT $2`,\n   419→    [stripeCustomerId, limit]\n   420→  );\n   421→  return result.rows;\n   422→}\n   423→\n   424→/**\n   425→ * Check if webhook event has already been processed (idempotency)\n   426→ */\n   427→async function isEventProcessed(eventId) {\n   428→  const result = await pool.query(\n   429→    'SELECT id FROM webhook_events WHERE event_id = $1',\n   430→    [eventId]\n   431→  );\n   432→  return result.rows.length > 0;\n   433→}\n   434→\n   435→/**\n   436→ * Record a processed webhook event\n   437→ */\n   438→async function recordWebhookEvent(eventId, eventType) {\n   439→  try {\n   440→    await pool.query(\n   441→      'INSERT INTO webhook_events (event_id, event_type) VALUES ($1, $2) ON CONFLICT (event_id) DO NOTHING',\n   442→      [eventId, eventType]\n   443→    );\n   444→  } catch (err) {\n   445→    // Ignore duplicate key errors (race condition safety)\n   446→    if (err.code !== '23505') throw err;\n   447→  }\n   448→}\n   449→\n   450→module.exports = {\n   451→  initDatabase,\n   452→  getOrCreateUser,\n   453→  getBalance,\n   454→  hasCredits,\n   455→  checkIsolationAccess,\n   456→  deductUsage,\n   457→  deductIsolationUsage,\n   458→  resetHours,\n   459→  updateTier,\n   460→  getUsageHistory,\n   461→  isEventProcessed,\n   462→  recordWebhookEvent,\n   463→  TIER_HOURS,\n   464→  TIER_ISOLATION_MINUTES,\n   465→  TIER_ISOLATION_HOURS,\n   466→  ISOLATION_OVERAGE_RATE\n   467→};\n   468→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"f4e15a31-43b8-44dd-99e8-5c07c76fff38","timestamp":"2025-12-25T22:30:21.668Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js","content":"/**\n * Usage Tracking Service\n *\n * Tracks user credit balances and usage for billing.\n * Uses PostgreSQL for persistence.\n */\n\nconst { Pool } = require('pg');\n\n// PostgreSQL connection pool\nconst pool = new Pool({\n  connectionString: process.env.DATABASE_URL,\n  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false\n});\n\n// Hours per tier (regular processing)\nconst TIER_HOURS = {\n  starter: 15,\n  pro: 50,\n  team: 150,\n  cancelled: 0\n};\n\n// Isolation minutes per tier (sized to maintain 80% margins)\nconst TIER_ISOLATION_MINUTES = {\n  starter: 0,     // No isolation access\n  pro: 45,        // 45 minutes included\n  team: 180,      // 3 hours included\n  cancelled: 0\n};\n\n// Convert to hours for database storage\nconst TIER_ISOLATION_HOURS = {\n  starter: 0,\n  pro: TIER_ISOLATION_MINUTES.pro / 60,      // 0.75 hours\n  team: TIER_ISOLATION_MINUTES.team / 60,    // 3 hours\n  cancelled: 0\n};\n\n// Overage rate for isolation beyond included minutes\nconst ISOLATION_OVERAGE_RATE = 0.10; // $0.10 per minute (85% margin)\n\n/**\n * Initialize database tables\n */\nasync function initDatabase() {\n  const client = await pool.connect();\n  try {\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS users (\n        id SERIAL PRIMARY KEY,\n        stripe_customer_id VARCHAR(255) UNIQUE NOT NULL,\n        email VARCHAR(255),\n        tier VARCHAR(50) DEFAULT 'starter',\n        hours_remaining DECIMAL(10,4) DEFAULT 15,\n        hours_total DECIMAL(10,4) DEFAULT 15,\n        isolation_hours_remaining DECIMAL(10,4) DEFAULT 0,\n        isolation_hours_total DECIMAL(10,4) DEFAULT 0,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n\n    // Add isolation columns if they don't exist (for existing databases)\n    await client.query(`\n      DO $$\n      BEGIN\n        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_remaining') THEN\n          ALTER TABLE users ADD COLUMN isolation_hours_remaining DECIMAL(10,4) DEFAULT 0;\n        END IF;\n        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_total') THEN\n          ALTER TABLE users ADD COLUMN isolation_hours_total DECIMAL(10,4) DEFAULT 0;\n        END IF;\n      END $$;\n    `);\n\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS usage_log (\n        id SERIAL PRIMARY KEY,\n        user_id INTEGER REFERENCES users(id),\n        stripe_customer_id VARCHAR(255),\n        audio_duration_seconds DECIMAL(10,2),\n        hours_used DECIMAL(10,6),\n        endpoint VARCHAR(100),\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_users_stripe_customer_id ON users(stripe_customer_id)\n    `);\n\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_usage_log_user_id ON usage_log(user_id)\n    `);\n\n    // Webhook events table for idempotency\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS webhook_events (\n        id SERIAL PRIMARY KEY,\n        event_id VARCHAR(255) UNIQUE NOT NULL,\n        event_type VARCHAR(100),\n        processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_webhook_events_event_id ON webhook_events(event_id)\n    `);\n\n    console.log('[UsageTracking] Database initialized');\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Get or create a user by Stripe customer ID\n */\nasync function getOrCreateUser(stripeCustomerId, email = null) {\n  const client = await pool.connect();\n  try {\n    // Try to find existing user\n    let result = await client.query(\n      'SELECT * FROM users WHERE stripe_customer_id = $1',\n      [stripeCustomerId]\n    );\n\n    if (result.rows.length > 0) {\n      return result.rows[0];\n    }\n\n    // Create new user with starter tier (no isolation hours)\n    result = await client.query(\n      `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n       VALUES ($1, $2, 'starter', $3, $3, $4, $4)\n       RETURNING *`,\n      [stripeCustomerId, email, TIER_HOURS.starter, TIER_ISOLATION_HOURS.starter]\n    );\n\n    return result.rows[0];\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Get user's current credit balance\n */\nasync function getBalance(stripeCustomerId) {\n  const user = await getOrCreateUser(stripeCustomerId);\n  const hoursTotal = parseFloat(user.hours_total);\n  const hoursRemaining = parseFloat(user.hours_remaining);\n  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n\n  // Tier display names\n  const tierNames = {\n    starter: 'Starter',\n    pro: 'Pro',\n    team: 'Team',\n    cancelled: 'Cancelled'\n  };\n\n  return {\n    hoursRemaining,\n    hoursTotal,\n    isolationHoursRemaining,\n    isolationHoursTotal,\n    tier: user.tier,\n    tierName: tierNames[user.tier] || user.tier,\n    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n    isolationOverageRate: ISOLATION_OVERAGE_RATE\n  };\n}\n\n/**\n * Check if user has enough credits for estimated duration\n */\nasync function hasCredits(stripeCustomerId, estimatedSeconds = 0) {\n  const balance = await getBalance(stripeCustomerId);\n  const estimatedHours = estimatedSeconds / 3600;\n  return balance.hoursRemaining >= estimatedHours;\n}\n\n/**\n * Check if user can use vocal isolation\n * Returns: { allowed, hasIncludedHours, overageRequired, overageCost }\n */\nasync function checkIsolationAccess(stripeCustomerId, estimatedMinutes = 0) {\n  const balance = await getBalance(stripeCustomerId);\n\n  // Starter tier has no isolation access\n  if (!balance.hasIsolationAccess) {\n    return {\n      allowed: false,\n      reason: 'upgrade_required',\n      message: 'Vocal isolation requires Pro or Team tier',\n      hasIncludedHours: false,\n      overageRequired: false,\n      overageCost: 0\n    };\n  }\n\n  const estimatedHours = estimatedMinutes / 60;\n  const remainingIncluded = balance.isolationHoursRemaining;\n\n  // Check if within included hours\n  if (remainingIncluded >= estimatedHours) {\n    return {\n      allowed: true,\n      reason: 'included',\n      message: `Using ${estimatedMinutes.toFixed(1)} min of included isolation time`,\n      hasIncludedHours: true,\n      overageRequired: false,\n      overageCost: 0,\n      isolationHoursRemaining: remainingIncluded - estimatedHours\n    };\n  }\n\n  // Calculate overage\n  const overageMinutes = estimatedMinutes - (remainingIncluded * 60);\n  const overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n\n  return {\n    allowed: true,\n    reason: 'overage',\n    message: `${remainingIncluded > 0 ? `Using ${(remainingIncluded * 60).toFixed(1)} included min + ` : ''}${overageMinutes.toFixed(1)} min overage ($${overageCost.toFixed(2)})`,\n    hasIncludedHours: remainingIncluded > 0,\n    overageRequired: true,\n    overageMinutes,\n    overageCost,\n    isolationHoursRemaining: 0\n  };\n}\n\n/**\n * Deduct isolation usage from user's balance\n * Returns overage cost if beyond included hours\n */\nasync function deductIsolationUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'isolate-vocals') {\n  const durationMinutes = audioDurationSeconds / 60;\n  const durationHours = audioDurationSeconds / 3600;\n\n  const client = await pool.connect();\n  try {\n    // Get user\n    const userResult = await client.query(\n      'SELECT * FROM users WHERE stripe_customer_id = $1',\n      [stripeCustomerId]\n    );\n\n    if (userResult.rows.length === 0) {\n      throw new Error('User not found');\n    }\n\n    const user = userResult.rows[0];\n    const remainingIncluded = parseFloat(user.isolation_hours_remaining || 0);\n\n    let overageCost = 0;\n    let hoursFromIncluded = 0;\n    let overageMinutes = 0;\n\n    if (remainingIncluded >= durationHours) {\n      // Fully covered by included hours\n      hoursFromIncluded = durationHours;\n    } else {\n      // Partial or full overage\n      hoursFromIncluded = remainingIncluded;\n      overageMinutes = durationMinutes - (remainingIncluded * 60);\n      overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n    }\n\n    // Update isolation balance\n    await client.query(\n      `UPDATE users\n       SET isolation_hours_remaining = GREATEST(0, isolation_hours_remaining - $1),\n           updated_at = CURRENT_TIMESTAMP\n       WHERE stripe_customer_id = $2`,\n      [durationHours, stripeCustomerId]\n    );\n\n    // Log usage (mark as isolation)\n    await client.query(\n      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n       VALUES ($1, $2, $3, $4, $5)`,\n      [user.id, stripeCustomerId, audioDurationSeconds, durationHours, endpoint]\n    );\n\n    // Return updated balance and overage info\n    const newBalance = await getBalance(stripeCustomerId);\n    return {\n      ...newBalance,\n      isolationUsed: {\n        durationMinutes,\n        hoursFromIncluded,\n        overageMinutes,\n        overageCost\n      }\n    };\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Deduct usage from user's balance\n */\nasync function deductUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'unknown') {\n  const hoursUsed = audioDurationSeconds / 3600;\n\n  const client = await pool.connect();\n  try {\n    // Get user\n    const userResult = await client.query(\n      'SELECT * FROM users WHERE stripe_customer_id = $1',\n      [stripeCustomerId]\n    );\n\n    if (userResult.rows.length === 0) {\n      throw new Error('User not found');\n    }\n\n    const user = userResult.rows[0];\n\n    // Update balance\n    await client.query(\n      `UPDATE users\n       SET hours_remaining = GREATEST(0, hours_remaining - $1),\n           updated_at = CURRENT_TIMESTAMP\n       WHERE stripe_customer_id = $2`,\n      [hoursUsed, stripeCustomerId]\n    );\n\n    // Log usage\n    await client.query(\n      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n       VALUES ($1, $2, $3, $4, $5)`,\n      [user.id, stripeCustomerId, audioDurationSeconds, hoursUsed, endpoint]\n    );\n\n    // Return updated balance\n    return await getBalance(stripeCustomerId);\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Reset user's hours for new billing period\n */\nasync function resetHours(stripeCustomerId, tier) {\n  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n\n  await pool.query(\n    `UPDATE users\n     SET hours_remaining = $1, hours_total = $1,\n         isolation_hours_remaining = $2, isolation_hours_total = $2,\n         tier = $3, updated_at = CURRENT_TIMESTAMP\n     WHERE stripe_customer_id = $4`,\n    [hours, isolationHours, tier, stripeCustomerId]\n  );\n\n  return await getBalance(stripeCustomerId);\n}\n\n/**\n * Update user's tier (from Stripe webhook)\n */\nasync function updateTier(stripeCustomerId, tier, email = null) {\n  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n\n  const client = await pool.connect();\n  try {\n    // Check if user exists\n    const result = await client.query(\n      'SELECT * FROM users WHERE stripe_customer_id = $1',\n      [stripeCustomerId]\n    );\n\n    if (result.rows.length === 0) {\n      // Create new user with this tier\n      await client.query(\n        `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n         VALUES ($1, $2, $3, $4, $4, $5, $5)`,\n        [stripeCustomerId, email, tier, hours, isolationHours]\n      );\n    } else {\n      // Update existing user\n      await client.query(\n        `UPDATE users\n         SET tier = $1, hours_remaining = $2, hours_total = $2,\n             isolation_hours_remaining = $3, isolation_hours_total = $3,\n             updated_at = CURRENT_TIMESTAMP\n         WHERE stripe_customer_id = $4`,\n        [tier, hours, isolationHours, stripeCustomerId]\n      );\n    }\n\n    return await getBalance(stripeCustomerId);\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Get usage history for a user\n */\nasync function getUsageHistory(stripeCustomerId, limit = 50) {\n  const result = await pool.query(\n    `SELECT * FROM usage_log\n     WHERE stripe_customer_id = $1\n     ORDER BY created_at DESC\n     LIMIT $2`,\n    [stripeCustomerId, limit]\n  );\n  return result.rows;\n}\n\n/**\n * Check if webhook event has already been processed (idempotency)\n */\nasync function isEventProcessed(eventId) {\n  const result = await pool.query(\n    'SELECT id FROM webhook_events WHERE event_id = $1',\n    [eventId]\n  );\n  return result.rows.length > 0;\n}\n\n/**\n * Record a processed webhook event\n */\nasync function recordWebhookEvent(eventId, eventType) {\n  try {\n    await pool.query(\n      'INSERT INTO webhook_events (event_id, event_type) VALUES ($1, $2) ON CONFLICT (event_id) DO NOTHING',\n      [eventId, eventType]\n    );\n  } catch (err) {\n    // Ignore duplicate key errors (race condition safety)\n    if (err.code !== '23505') throw err;\n  }\n}\n\nmodule.exports = {\n  initDatabase,\n  getOrCreateUser,\n  getBalance,\n  hasCredits,\n  checkIsolationAccess,\n  deductUsage,\n  deductIsolationUsage,\n  resetHours,\n  updateTier,\n  getUsageHistory,\n  isEventProcessed,\n  recordWebhookEvent,\n  TIER_HOURS,\n  TIER_ISOLATION_MINUTES,\n  TIER_ISOLATION_HOURS,\n  ISOLATION_OVERAGE_RATE\n};\n","numLines":468,"startLine":1,"totalLines":468}}}
{"parentUuid":"f4e15a31-43b8-44dd-99e8-5c07c76fff38","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TAi2f3eNyAGY9RBfXA2wUj","type":"tool_result","content":"     1→/**\n     2→ * Slice 4: GPT-4o-mini Transcription Service\n     3→ *\n     4→ * Handles audio transcription using OpenAI's gpt-4o-mini-transcribe model.\n     5→ * 50% cheaper than Whisper ($0.003/min vs $0.006/min).\n     6→ * Returns timestamped segments for take detection.\n     7→ * Includes caching to avoid repeated API calls.\n     8→ */\n     9→\n    10→const fs = require('fs');\n    11→const OpenAI = require('openai');\n    12→\n    13→// Initialize OpenAI client\n    14→const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n    15→\n    16→// In-memory cache: { wavPath: { mtime, result } }\n    17→const transcriptCache = new Map();\n    18→\n    19→// Cache configuration\n    20→const MAX_CACHE_SIZE = 50;\n    21→\n    22→/**\n    23→ * Retry function with exponential backoff\n    24→ * @param {Function} fn - Async function to retry\n    25→ * @param {number} retries - Number of retry attempts (default: 3)\n    26→ * @param {number} baseDelay - Base delay in ms (default: 1000)\n    27→ * @returns {Promise<any>} Result of the function\n    28→ */\n    29→async function withRetry(fn, retries = 3, baseDelay = 1000) {\n    30→  let lastError;\n    31→\n    32→  for (let attempt = 0; attempt <= retries; attempt++) {\n    33→    try {\n    34→      return await fn();\n    35→    } catch (err) {\n    36→      lastError = err;\n    37→\n    38→      // Don't retry on quota exceeded - fail immediately\n    39→      if (err.code === 'insufficient_quota' || err.message?.includes('quota')) {\n    40→        throw new Error('OpenAI API quota exceeded. Please check your billing at https://platform.openai.com/account/billing');\n    41→      }\n    42→\n    43→      // Don't retry on the last attempt\n    44→      if (attempt === retries) {\n    45→        break;\n    46→      }\n    47→\n    48→      // Calculate delay with exponential backoff + jitter\n    49→      const delay = baseDelay * Math.pow(2, attempt) + Math.random() * 500;\n    50→\n    51→      // Only retry on retryable errors\n    52→      const isRetryable =\n    53→        err.status === 429 ||                          // Rate limit\n    54→        err.status === 500 ||                          // Server error\n    55→        err.status === 502 ||                          // Bad gateway\n    56→        err.status === 503 ||                          // Service unavailable\n    57→        err.code === 'ECONNRESET' ||                   // Connection reset\n    58→        err.code === 'ETIMEDOUT' ||                    // Timeout\n    59→        err.message?.includes('Connection error') ||   // Connection error\n    60→        err.message?.includes('network');              // Network error\n    61→\n    62→      if (!isRetryable) {\n    63→        throw err;\n    64→      }\n    65→\n    66→      console.log(`[SPLICE] Retry ${attempt + 1}/${retries} after ${Math.round(delay)}ms: ${err.message}`);\n    67→      await new Promise(resolve => setTimeout(resolve, delay));\n    68→    }\n    69→  }\n    70→\n    71→  throw lastError;\n    72→}\n    73→\n    74→/**\n    75→ * LRU cache set with size limit\n    76→ * Evicts oldest entries when cache exceeds MAX_CACHE_SIZE\n    77→ * @param {string} key - Cache key\n    78→ * @param {any} value - Value to cache\n    79→ */\n    80→function cacheSet(key, value) {\n    81→  // Evict oldest entries if at capacity\n    82→  while (transcriptCache.size >= MAX_CACHE_SIZE) {\n    83→    const oldestKey = transcriptCache.keys().next().value;\n    84→    transcriptCache.delete(oldestKey);\n    85→    console.log(`[SPLICE] Cache eviction: removed ${oldestKey}`);\n    86→  }\n    87→\n    88→  transcriptCache.set(key, value);\n    89→}\n    90→\n    91→/**\n    92→ * Transcribe audio file using GPT-4o-mini-transcribe (with caching)\n    93→ * @param {string} wavPath - Path to the WAV file\n    94→ * @returns {Promise<{text: string, segments: Array, language: string, duration: number}>}\n    95→ */\n    96→async function transcribeAudio(wavPath) {\n    97→  // Check cache based on file modification time\n    98→  const stats = fs.statSync(wavPath);\n    99→  const mtime = stats.mtimeMs;\n   100→\n   101→  const cached = transcriptCache.get(wavPath);\n   102→  if (cached && cached.mtime === mtime) {\n   103→    console.log('[SPLICE] Using cached transcription (file unchanged)');\n   104→    return cached.result;\n   105→  }\n   106→\n   107→  console.log('[SPLICE] Starting GPT-4o-mini transcription...');\n   108→\n   109→  // GPT-4o-mini-transcribe uses the same transcriptions API as Whisper\n   110→  // but only supports 'json' response format (not verbose_json)\n   111→  const transcription = await withRetry(async () => {\n   112→    return await openai.audio.transcriptions.create({\n   113→      file: fs.createReadStream(wavPath),\n   114→      model: 'gpt-4o-mini-transcribe',\n   115→      response_format: 'json',\n   116→      language: 'en',\n   117→    });\n   118→  });\n   119→\n   120→  console.log(`[SPLICE] Transcription complete: ${transcription.text.slice(0, 100)}...`);\n   121→\n   122→  // GPT-4o-mini-transcribe returns simpler response than verbose_json\n   123→  // We need to estimate segments from the text if not provided\n   124→  const result = {\n   125→    text: transcription.text,\n   126→    segments: transcription.segments || estimateSegments(transcription.text),\n   127→    language: transcription.language || 'en',\n   128→    duration: transcription.duration || 0\n   129→  };\n   130→\n   131→  // Cache the result (with LRU eviction)\n   132→  cacheSet(wavPath, { mtime, result });\n   133→  console.log('[SPLICE] Transcription cached');\n   134→\n   135→  return result;\n   136→}\n   137→\n   138→/**\n   139→ * Estimate segments from text when not provided by API\n   140→ * Creates rough segments based on sentence boundaries\n   141→ * @param {string} text - Full transcript text\n   142→ * @returns {Array} Estimated segments\n   143→ */\n   144→function estimateSegments(text) {\n   145→  if (!text) return [];\n   146→\n   147→  // Split by sentence boundaries\n   148→  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];\n   149→\n   150→  // Create segments with estimated timing (will be refined by take detection)\n   151→  return sentences.map((sentence, index) => ({\n   152→    id: index,\n   153→    start: 0, // Will be refined by take detection\n   154→    end: 0,\n   155→    text: sentence.trim()\n   156→  }));\n   157→}\n   158→\n   159→/**\n   160→ * Transcribe audio with word-level timestamps using Whisper\n   161→ * Required for profanity, repetition, and stutter detection endpoints.\n   162→ *\n   163→ * @param {string} wavPath - Path to the WAV file\n   164→ * @returns {Promise<{text: string, words: Array<{word: string, start: number, end: number}>}>}\n   165→ */\n   166→async function transcribeWithWords(wavPath) {\n   167→  // Check cache based on file modification time\n   168→  const stats = fs.statSync(wavPath);\n   169→  const mtime = stats.mtimeMs;\n   170→  const cacheKey = `words:${wavPath}`;\n   171→\n   172→  const cached = transcriptCache.get(cacheKey);\n   173→  if (cached && cached.mtime === mtime) {\n   174→    console.log('[SPLICE] Using cached word-level transcription (file unchanged)');\n   175→    return cached.result;\n   176→  }\n   177→\n   178→  console.log('[SPLICE] Starting Whisper transcription with word timestamps...');\n   179→\n   180→  // Use whisper-1 with word-level timestamp granularity (with retry)\n   181→  const transcription = await withRetry(async () => {\n   182→    return await openai.audio.transcriptions.create({\n   183→      file: fs.createReadStream(wavPath),\n   184→      model: 'whisper-1',\n   185→      response_format: 'verbose_json',\n   186→      timestamp_granularities: ['word'],\n   187→      language: 'en',\n   188→    });\n   189→  });\n   190→\n   191→  console.log(`[SPLICE] Word transcription complete: ${transcription.words?.length || 0} words`);\n   192→\n   193→  // Map to consistent format\n   194→  const result = {\n   195→    text: transcription.text,\n   196→    words: (transcription.words || []).map(w => ({\n   197→      word: w.word,\n   198→      start: w.start,\n   199→      end: w.end\n   200→    })),\n   201→    language: transcription.language || 'en',\n   202→    duration: transcription.duration || 0\n   203→  };\n   204→\n   205→  // Cache the result (with LRU eviction)\n   206→  cacheSet(cacheKey, { mtime, result });\n   207→  console.log(`[SPLICE] Word transcription cached (${result.words.length} words)`);\n   208→\n   209→  return result;\n   210→}\n   211→\n   212→/**\n   213→ * Clear transcription cache\n   214→ * Useful for testing or memory management\n   215→ */\n   216→function clearCache() {\n   217→  const size = transcriptCache.size;\n   218→  transcriptCache.clear();\n   219→  console.log(`[SPLICE] Cleared ${size} cached transcription(s)`);\n   220→  return size;\n   221→}\n   222→\n   223→/**\n   224→ * Get cache statistics\n   225→ */\n   226→function getCacheStats() {\n   227→  return {\n   228→    entries: transcriptCache.size,\n   229→    keys: Array.from(transcriptCache.keys())\n   230→  };\n   231→}\n   232→\n   233→module.exports = { transcribeAudio, transcribeWithWords, clearCache, getCacheStats };\n   234→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"856b8598-8520-45cb-b9ad-2a260737d3a6","timestamp":"2025-12-25T22:30:21.668Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/transcription.js","content":"/**\n * Slice 4: GPT-4o-mini Transcription Service\n *\n * Handles audio transcription using OpenAI's gpt-4o-mini-transcribe model.\n * 50% cheaper than Whisper ($0.003/min vs $0.006/min).\n * Returns timestamped segments for take detection.\n * Includes caching to avoid repeated API calls.\n */\n\nconst fs = require('fs');\nconst OpenAI = require('openai');\n\n// Initialize OpenAI client\nconst openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\n// In-memory cache: { wavPath: { mtime, result } }\nconst transcriptCache = new Map();\n\n// Cache configuration\nconst MAX_CACHE_SIZE = 50;\n\n/**\n * Retry function with exponential backoff\n * @param {Function} fn - Async function to retry\n * @param {number} retries - Number of retry attempts (default: 3)\n * @param {number} baseDelay - Base delay in ms (default: 1000)\n * @returns {Promise<any>} Result of the function\n */\nasync function withRetry(fn, retries = 3, baseDelay = 1000) {\n  let lastError;\n\n  for (let attempt = 0; attempt <= retries; attempt++) {\n    try {\n      return await fn();\n    } catch (err) {\n      lastError = err;\n\n      // Don't retry on quota exceeded - fail immediately\n      if (err.code === 'insufficient_quota' || err.message?.includes('quota')) {\n        throw new Error('OpenAI API quota exceeded. Please check your billing at https://platform.openai.com/account/billing');\n      }\n\n      // Don't retry on the last attempt\n      if (attempt === retries) {\n        break;\n      }\n\n      // Calculate delay with exponential backoff + jitter\n      const delay = baseDelay * Math.pow(2, attempt) + Math.random() * 500;\n\n      // Only retry on retryable errors\n      const isRetryable =\n        err.status === 429 ||                          // Rate limit\n        err.status === 500 ||                          // Server error\n        err.status === 502 ||                          // Bad gateway\n        err.status === 503 ||                          // Service unavailable\n        err.code === 'ECONNRESET' ||                   // Connection reset\n        err.code === 'ETIMEDOUT' ||                    // Timeout\n        err.message?.includes('Connection error') ||   // Connection error\n        err.message?.includes('network');              // Network error\n\n      if (!isRetryable) {\n        throw err;\n      }\n\n      console.log(`[SPLICE] Retry ${attempt + 1}/${retries} after ${Math.round(delay)}ms: ${err.message}`);\n      await new Promise(resolve => setTimeout(resolve, delay));\n    }\n  }\n\n  throw lastError;\n}\n\n/**\n * LRU cache set with size limit\n * Evicts oldest entries when cache exceeds MAX_CACHE_SIZE\n * @param {string} key - Cache key\n * @param {any} value - Value to cache\n */\nfunction cacheSet(key, value) {\n  // Evict oldest entries if at capacity\n  while (transcriptCache.size >= MAX_CACHE_SIZE) {\n    const oldestKey = transcriptCache.keys().next().value;\n    transcriptCache.delete(oldestKey);\n    console.log(`[SPLICE] Cache eviction: removed ${oldestKey}`);\n  }\n\n  transcriptCache.set(key, value);\n}\n\n/**\n * Transcribe audio file using GPT-4o-mini-transcribe (with caching)\n * @param {string} wavPath - Path to the WAV file\n * @returns {Promise<{text: string, segments: Array, language: string, duration: number}>}\n */\nasync function transcribeAudio(wavPath) {\n  // Check cache based on file modification time\n  const stats = fs.statSync(wavPath);\n  const mtime = stats.mtimeMs;\n\n  const cached = transcriptCache.get(wavPath);\n  if (cached && cached.mtime === mtime) {\n    console.log('[SPLICE] Using cached transcription (file unchanged)');\n    return cached.result;\n  }\n\n  console.log('[SPLICE] Starting GPT-4o-mini transcription...');\n\n  // GPT-4o-mini-transcribe uses the same transcriptions API as Whisper\n  // but only supports 'json' response format (not verbose_json)\n  const transcription = await withRetry(async () => {\n    return await openai.audio.transcriptions.create({\n      file: fs.createReadStream(wavPath),\n      model: 'gpt-4o-mini-transcribe',\n      response_format: 'json',\n      language: 'en',\n    });\n  });\n\n  console.log(`[SPLICE] Transcription complete: ${transcription.text.slice(0, 100)}...`);\n\n  // GPT-4o-mini-transcribe returns simpler response than verbose_json\n  // We need to estimate segments from the text if not provided\n  const result = {\n    text: transcription.text,\n    segments: transcription.segments || estimateSegments(transcription.text),\n    language: transcription.language || 'en',\n    duration: transcription.duration || 0\n  };\n\n  // Cache the result (with LRU eviction)\n  cacheSet(wavPath, { mtime, result });\n  console.log('[SPLICE] Transcription cached');\n\n  return result;\n}\n\n/**\n * Estimate segments from text when not provided by API\n * Creates rough segments based on sentence boundaries\n * @param {string} text - Full transcript text\n * @returns {Array} Estimated segments\n */\nfunction estimateSegments(text) {\n  if (!text) return [];\n\n  // Split by sentence boundaries\n  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];\n\n  // Create segments with estimated timing (will be refined by take detection)\n  return sentences.map((sentence, index) => ({\n    id: index,\n    start: 0, // Will be refined by take detection\n    end: 0,\n    text: sentence.trim()\n  }));\n}\n\n/**\n * Transcribe audio with word-level timestamps using Whisper\n * Required for profanity, repetition, and stutter detection endpoints.\n *\n * @param {string} wavPath - Path to the WAV file\n * @returns {Promise<{text: string, words: Array<{word: string, start: number, end: number}>}>}\n */\nasync function transcribeWithWords(wavPath) {\n  // Check cache based on file modification time\n  const stats = fs.statSync(wavPath);\n  const mtime = stats.mtimeMs;\n  const cacheKey = `words:${wavPath}`;\n\n  const cached = transcriptCache.get(cacheKey);\n  if (cached && cached.mtime === mtime) {\n    console.log('[SPLICE] Using cached word-level transcription (file unchanged)');\n    return cached.result;\n  }\n\n  console.log('[SPLICE] Starting Whisper transcription with word timestamps...');\n\n  // Use whisper-1 with word-level timestamp granularity (with retry)\n  const transcription = await withRetry(async () => {\n    return await openai.audio.transcriptions.create({\n      file: fs.createReadStream(wavPath),\n      model: 'whisper-1',\n      response_format: 'verbose_json',\n      timestamp_granularities: ['word'],\n      language: 'en',\n    });\n  });\n\n  console.log(`[SPLICE] Word transcription complete: ${transcription.words?.length || 0} words`);\n\n  // Map to consistent format\n  const result = {\n    text: transcription.text,\n    words: (transcription.words || []).map(w => ({\n      word: w.word,\n      start: w.start,\n      end: w.end\n    })),\n    language: transcription.language || 'en',\n    duration: transcription.duration || 0\n  };\n\n  // Cache the result (with LRU eviction)\n  cacheSet(cacheKey, { mtime, result });\n  console.log(`[SPLICE] Word transcription cached (${result.words.length} words)`);\n\n  return result;\n}\n\n/**\n * Clear transcription cache\n * Useful for testing or memory management\n */\nfunction clearCache() {\n  const size = transcriptCache.size;\n  transcriptCache.clear();\n  console.log(`[SPLICE] Cleared ${size} cached transcription(s)`);\n  return size;\n}\n\n/**\n * Get cache statistics\n */\nfunction getCacheStats() {\n  return {\n    entries: transcriptCache.size,\n    keys: Array.from(transcriptCache.keys())\n  };\n}\n\nmodule.exports = { transcribeAudio, transcribeWithWords, clearCache, getCacheStats };\n","numLines":234,"startLine":1,"totalLines":234}}}
{"parentUuid":"856b8598-8520-45cb-b9ad-2a260737d3a6","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01ALe56tZ45sWTuhctLQrcF8","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Cut List Generator (v3.5)\n     3→ *\n     4→ * Generates JSON cut lists for direct DOM sequence building.\n     5→ * Replaces XML processing for v3.5 architecture.\n     6→ *\n     7→ * Cut list format:\n     8→ * {\n     9→ *   version: '3.5',\n    10→ *   source: { name, path, duration },\n    11→ *   segments: [{ type, sourceName, sourcePath, inPoint, outPoint }],\n    12→ *   metadata: { silencesRemoved, takesDetected, generatedAt }\n    13→ * }\n    14→ *\n    15→ * Frame Alignment:\n    16→ * When frameRate is provided, all cut points are aligned to frame boundaries.\n    17→ * This prevents sub-frame edits that can cause playback issues.\n    18→ */\n    19→\n    20→/**\n    21→ * Align a time value to the nearest frame boundary\n    22→ * @param {number} time - Time in seconds\n    23→ * @param {number} frameRate - Frames per second (e.g., 23.976, 24, 29.97, 30, 60)\n    24→ * @returns {number} Time aligned to frame boundary\n    25→ */\n    26→function alignToFrame(time, frameRate) {\n    27→  if (!frameRate || frameRate <= 0) return time;\n    28→  const frameDuration = 1 / frameRate;\n    29→  return Math.round(time / frameDuration) * frameDuration;\n    30→}\n    31→\n    32→/**\n    33→ * Align to frame boundary, rounding down (for in-points)\n    34→ */\n    35→function alignToFrameFloor(time, frameRate) {\n    36→  if (!frameRate || frameRate <= 0) return time;\n    37→  const frameDuration = 1 / frameRate;\n    38→  return Math.floor(time / frameDuration) * frameDuration;\n    39→}\n    40→\n    41→/**\n    42→ * Align to frame boundary, rounding up (for out-points)\n    43→ */\n    44→function alignToFrameCeil(time, frameRate) {\n    45→  if (!frameRate || frameRate <= 0) return time;\n    46→  const frameDuration = 1 / frameRate;\n    47→  return Math.ceil(time / frameDuration) * frameDuration;\n    48→}\n    49→\n    50→/**\n    51→ * Generate a cut list from detected silences\n    52→ * Creates segments that represent speech (non-silence) portions\n    53→ *\n    54→ * @param {Object} options - Generation options\n    55→ * @param {string} options.sourceName - Name of the source clip\n    56→ * @param {string} options.sourcePath - Full path to the source file\n    57→ * @param {number} options.duration - Total duration of source in seconds\n    58→ * @param {Array} options.silences - Array of silence segments [{start, end, duration}]\n    59→ * @param {Array} [options.takes] - Optional array of detected takes\n    60→ * @param {Object} [options.settings] - Optional generation settings\n    61→ * @returns {Object} The generated cut list\n    62→ */\n    63→function generateCutList(options) {\n    64→  const {\n    65→    sourceName,\n    66→    sourcePath,\n    67→    duration,\n    68→    silences = [],\n    69→    takes = [],\n    70→    settings = {}\n    71→  } = options;\n    72→\n    73→  // Settings with defaults\n    74→  const {\n    75→    minSegmentDuration = 0.1,  // Minimum segment length to include\n    76→    padding = 0,               // Padding around speech (extend into silence)\n    77→    markBestTakes = true,      // Color-code best takes differently\n    78→    frameRate = 0,             // Frame rate for alignment (0 = no alignment)\n    79→    jCutOffset = 0,            // J-cut: audio starts before video (negative value)\n    80→    lCutOffset = 0             // L-cut: audio extends after video (positive value)\n    81→  } = settings;\n    82→\n    83→  // Sort silences by start time\n    84→  const sortedSilences = [...silences].sort((a, b) => a.start - b.start);\n    85→\n    86→  // Build segments from gaps between silences (speech segments)\n    87→  const segments = [];\n    88→  let lastEnd = 0;\n    89→  let silencesRemoved = 0;\n    90→\n    91→  for (const silence of sortedSilences) {\n    92→    // Calculate segment boundaries with padding and frame alignment\n    93→    let segmentStart = Math.max(0, lastEnd - padding);\n    94→    let segmentEnd = Math.min(duration, silence.start + padding);\n    95→\n    96→    // Align to frame boundaries if frameRate is specified\n    97→    if (frameRate > 0) {\n    98→      segmentStart = alignToFrameFloor(segmentStart, frameRate);\n    99→      segmentEnd = alignToFrameCeil(segmentEnd, frameRate);\n   100→    }\n   101→\n   102→    // Only add if segment meets minimum duration\n   103→    if (segmentEnd - segmentStart >= minSegmentDuration) {\n   104→      const segment = {\n   105→        type: 'speech',\n   106→        sourceName,\n   107→        sourcePath,\n   108→        inPoint: parseFloat(segmentStart.toFixed(6)),\n   109→        outPoint: parseFloat(segmentEnd.toFixed(6))\n   110→      };\n   111→\n   112→      // Add J-cut/L-cut audio offsets if specified\n   113→      if (jCutOffset !== 0 || lCutOffset !== 0) {\n   114→        // Audio in/out points differ from video\n   115→        segment.audioInOffset = jCutOffset;  // Negative = audio starts earlier (J-cut)\n   116→        segment.audioOutOffset = lCutOffset; // Positive = audio ends later (L-cut)\n   117→\n   118→        // Calculate actual audio in/out points\n   119→        segment.audioInPoint = parseFloat(Math.max(0, segmentStart + jCutOffset).toFixed(6));\n   120→        segment.audioOutPoint = parseFloat(Math.min(duration, segmentEnd + lCutOffset).toFixed(6));\n   121→      }\n   122→\n   123→      // Check if this segment contains a take\n   124→      const containingTake = findContainingTake(segmentStart, segmentEnd, takes);\n   125→      if (containingTake) {\n   126→        segment.take = {\n   127→          text: containingTake.text?.substring(0, 100),\n   128→          isBest: containingTake.isBest || false\n   129→        };\n   130→        if (containingTake.isBest && markBestTakes) {\n   131→          segment.type = 'best_take';\n   132→        }\n   133→      }\n   134→\n   135→      segments.push(segment);\n   136→    }\n   137→\n   138→    silencesRemoved++;\n   139→    lastEnd = silence.end;\n   140→  }\n   141→\n   142→  // Add final segment after last silence\n   143→  if (lastEnd < duration) {\n   144→    let segmentStart = Math.max(0, lastEnd - padding);\n   145→    let segmentEnd = duration;\n   146→\n   147→    // Align to frame boundaries if frameRate is specified\n   148→    if (frameRate > 0) {\n   149→      segmentStart = alignToFrameFloor(segmentStart, frameRate);\n   150→      segmentEnd = alignToFrameCeil(segmentEnd, frameRate);\n   151→    }\n   152→\n   153→    if (segmentEnd - segmentStart >= minSegmentDuration) {\n   154→      const segment = {\n   155→        type: 'speech',\n   156→        sourceName,\n   157→        sourcePath,\n   158→        inPoint: parseFloat(segmentStart.toFixed(6)),\n   159→        outPoint: parseFloat(segmentEnd.toFixed(6))\n   160→      };\n   161→\n   162→      // Add J-cut/L-cut audio offsets if specified\n   163→      if (jCutOffset !== 0 || lCutOffset !== 0) {\n   164→        segment.audioInOffset = jCutOffset;\n   165→        segment.audioOutOffset = lCutOffset;\n   166→        segment.audioInPoint = parseFloat(Math.max(0, segmentStart + jCutOffset).toFixed(6));\n   167→        segment.audioOutPoint = parseFloat(Math.min(duration, segmentEnd + lCutOffset).toFixed(6));\n   168→      }\n   169→\n   170→      // Check if this segment contains a take\n   171→      const containingTake = findContainingTake(segmentStart, segmentEnd, takes);\n   172→      if (containingTake) {\n   173→        segment.take = {\n   174→          text: containingTake.text?.substring(0, 100),\n   175→          isBest: containingTake.isBest || false\n   176→        };\n   177→        if (containingTake.isBest && markBestTakes) {\n   178→          segment.type = 'best_take';\n   179→        }\n   180→      }\n   181→\n   182→      segments.push(segment);\n   183→    }\n   184→  }\n   185→\n   186→  // Calculate stats\n   187→  const totalKeptDuration = segments.reduce(\n   188→    (sum, seg) => sum + (seg.outPoint - seg.inPoint),\n   189→    0\n   190→  );\n   191→  const totalRemovedDuration = duration - totalKeptDuration;\n   192→\n   193→  const cutList = {\n   194→    version: '3.5',\n   195→    source: {\n   196→      name: sourceName,\n   197→      path: sourcePath,\n   198→      duration\n   199→    },\n   200→    segments,\n   201→    metadata: {\n   202→      silencesRemoved,\n   203→      takesDetected: takes.length,\n   204→      totalKeptDuration: parseFloat(totalKeptDuration.toFixed(3)),\n   205→      totalRemovedDuration: parseFloat(totalRemovedDuration.toFixed(3)),\n   206→      segmentCount: segments.length,\n   207→      frameRate: frameRate > 0 ? frameRate : null,\n   208→      frameAligned: frameRate > 0,\n   209→      jCutOffset: jCutOffset !== 0 ? jCutOffset : null,\n   210→      lCutOffset: lCutOffset !== 0 ? lCutOffset : null,\n   211→      hasAudioOffsets: jCutOffset !== 0 || lCutOffset !== 0,\n   212→      generatedAt: new Date().toISOString()\n   213→    }\n   214→  };\n   215→\n   216→  console.log(`[SPLICE] Generated cut list: ${segments.length} segments, ${silencesRemoved} silences removed`);\n   217→\n   218→  return cutList;\n   219→}\n   220→\n   221→/**\n   222→ * Find a take that overlaps with a given time range\n   223→ * @param {number} start - Start time in seconds\n   224→ * @param {number} end - End time in seconds\n   225→ * @param {Array} takes - Array of takes\n   226→ * @returns {Object|null} The overlapping take or null\n   227→ */\n   228→function findContainingTake(start, end, takes) {\n   229→  if (!takes || takes.length === 0) return null;\n   230→\n   231→  for (const take of takes) {\n   232→    const takeStart = take.startTime || take.start;\n   233→    const takeEnd = take.endTime || take.end;\n   234→\n   235→    // Check for overlap\n   236→    if (start < takeEnd && end > takeStart) {\n   237→      return take;\n   238→    }\n   239→  }\n   240→\n   241→  return null;\n   242→}\n   243→\n   244→/**\n   245→ * Generate a cut list that keeps only specific takes\n   246→ * Useful for \"keep best takes only\" workflow\n   247→ *\n   248→ * @param {Object} options - Generation options\n   249→ * @param {string} options.sourceName - Name of the source clip\n   250→ * @param {string} options.sourcePath - Full path to the source file\n   251→ * @param {number} options.duration - Total duration of source in seconds\n   252→ * @param {Array} options.takes - Array of takes to keep\n   253→ * @param {Object} [options.settings] - Optional generation settings\n   254→ * @returns {Object} The generated cut list\n   255→ */\n   256→function generateTakesCutList(options) {\n   257→  const {\n   258→    sourceName,\n   259→    sourcePath,\n   260→    duration,\n   261→    takes = [],\n   262→    settings = {}\n   263→  } = options;\n   264→\n   265→  const {\n   266→    padding = 0.2,          // Small padding around takes\n   267→    onlyBestTakes = false   // If true, only include takes marked as best\n   268→  } = settings;\n   269→\n   270→  // Filter to best takes if requested\n   271→  const takesToKeep = onlyBestTakes\n   272→    ? takes.filter(t => t.isBest)\n   273→    : takes;\n   274→\n   275→  // Sort by start time\n   276→  const sortedTakes = [...takesToKeep].sort((a, b) => {\n   277→    const aStart = a.startTime || a.start;\n   278→    const bStart = b.startTime || b.start;\n   279→    return aStart - bStart;\n   280→  });\n   281→\n   282→  // Build segments from takes\n   283→  const segments = sortedTakes.map(take => {\n   284→    const takeStart = take.startTime || take.start;\n   285→    const takeEnd = take.endTime || take.end;\n   286→\n   287→    return {\n   288→      type: take.isBest ? 'best_take' : 'take',\n   289→      sourceName,\n   290→      sourcePath,\n   291→      inPoint: Math.max(0, takeStart - padding),\n   292→      outPoint: Math.min(duration, takeEnd + padding),\n   293→      take: {\n   294→        text: take.text?.substring(0, 100),\n   295→        isBest: take.isBest || false\n   296→      }\n   297→    };\n   298→  });\n   299→\n   300→  const totalKeptDuration = segments.reduce(\n   301→    (sum, seg) => sum + (seg.outPoint - seg.inPoint),\n   302→    0\n   303→  );\n   304→\n   305→  const cutList = {\n   306→    version: '3.5',\n   307→    source: {\n   308→      name: sourceName,\n   309→      path: sourcePath,\n   310→      duration\n   311→    },\n   312→    segments,\n   313→    metadata: {\n   314→      takesKept: segments.length,\n   315→      takesTotal: takes.length,\n   316→      totalKeptDuration: parseFloat(totalKeptDuration.toFixed(3)),\n   317→      totalRemovedDuration: parseFloat((duration - totalKeptDuration).toFixed(3)),\n   318→      generatedAt: new Date().toISOString()\n   319→    }\n   320→  };\n   321→\n   322→  console.log(`[SPLICE] Generated takes cut list: ${segments.length} takes kept`);\n   323→\n   324→  return cutList;\n   325→}\n   326→\n   327→/**\n   328→ * Merge overlapping segments in a cut list\n   329→ * Useful when silences and takes create overlapping regions\n   330→ *\n   331→ * @param {Object} cutList - The cut list to merge\n   332→ * @returns {Object} Cut list with merged segments\n   333→ */\n   334→function mergeOverlappingSegments(cutList) {\n   335→  if (!cutList.segments || cutList.segments.length < 2) {\n   336→    return cutList;\n   337→  }\n   338→\n   339→  // Sort by start time\n   340→  const sorted = [...cutList.segments].sort((a, b) => a.inPoint - b.inPoint);\n   341→\n   342→  const merged = [];\n   343→  let current = { ...sorted[0] };\n   344→\n   345→  for (let i = 1; i < sorted.length; i++) {\n   346→    const next = sorted[i];\n   347→\n   348→    // Check for overlap\n   349→    if (next.inPoint <= current.outPoint) {\n   350→      // Merge: extend current segment\n   351→      current.outPoint = Math.max(current.outPoint, next.outPoint);\n   352→      // Keep the more specific type (best_take > take > speech)\n   353→      if (next.type === 'best_take' || (next.type === 'take' && current.type === 'speech')) {\n   354→        current.type = next.type;\n   355→        current.take = next.take;\n   356→      }\n   357→    } else {\n   358→      // No overlap: save current and start new\n   359→      merged.push(current);\n   360→      current = { ...next };\n   361→    }\n   362→  }\n   363→\n   364→  // Don't forget the last segment\n   365→  merged.push(current);\n   366→\n   367→  // Update metadata\n   368→  const newCutList = {\n   369→    ...cutList,\n   370→    segments: merged,\n   371→    metadata: {\n   372→      ...cutList.metadata,\n   373→      segmentCount: merged.length,\n   374→      mergedFrom: cutList.segments.length\n   375→    }\n   376→  };\n   377→\n   378→  console.log(`[SPLICE] Merged ${cutList.segments.length} segments into ${merged.length}`);\n   379→\n   380→  return newCutList;\n   381→}\n   382→\n   383→/**\n   384→ * Validate a cut list structure\n   385→ * @param {Object} cutList - The cut list to validate\n   386→ * @returns {{valid: boolean, errors: string[]}}\n   387→ */\n   388→function validateCutList(cutList) {\n   389→  const errors = [];\n   390→\n   391→  if (!cutList) {\n   392→    errors.push('Cut list is null or undefined');\n   393→    return { valid: false, errors };\n   394→  }\n   395→\n   396→  if (!cutList.version) {\n   397→    errors.push('Missing version field');\n   398→  }\n   399→\n   400→  if (!cutList.source) {\n   401→    errors.push('Missing source field');\n   402→  } else {\n   403→    if (!cutList.source.name && !cutList.source.path) {\n   404→      errors.push('Source must have name or path');\n   405→    }\n   406→    if (typeof cutList.source.duration !== 'number' || cutList.source.duration <= 0) {\n   407→      errors.push('Source duration must be a positive number');\n   408→    }\n   409→  }\n   410→\n   411→  if (!Array.isArray(cutList.segments)) {\n   412→    errors.push('Segments must be an array');\n   413→  } else {\n   414→    cutList.segments.forEach((seg, i) => {\n   415→      if (typeof seg.inPoint !== 'number') {\n   416→        errors.push(`Segment ${i}: inPoint must be a number`);\n   417→      }\n   418→      if (typeof seg.outPoint !== 'number') {\n   419→        errors.push(`Segment ${i}: outPoint must be a number`);\n   420→      }\n   421→      if (seg.inPoint >= seg.outPoint) {\n   422→        errors.push(`Segment ${i}: inPoint must be less than outPoint`);\n   423→      }\n   424→      if (!seg.sourceName && !seg.sourcePath) {\n   425→        errors.push(`Segment ${i}: must have sourceName or sourcePath`);\n   426→      }\n   427→    });\n   428→  }\n   429→\n   430→  return {\n   431→    valid: errors.length === 0,\n   432→    errors\n   433→  };\n   434→}\n   435→\n   436→module.exports = {\n   437→  generateCutList,\n   438→  generateTakesCutList,\n   439→  mergeOverlappingSegments,\n   440→  validateCutList,\n   441→  // Frame alignment utilities\n   442→  alignToFrame,\n   443→  alignToFrameFloor,\n   444→  alignToFrameCeil\n   445→};\n   446→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"eba49291-bb29-463f-909c-8f1e8e92b5c5","timestamp":"2025-12-25T22:30:21.668Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/cutListGenerator.js","content":"/**\n * SPLICE Cut List Generator (v3.5)\n *\n * Generates JSON cut lists for direct DOM sequence building.\n * Replaces XML processing for v3.5 architecture.\n *\n * Cut list format:\n * {\n *   version: '3.5',\n *   source: { name, path, duration },\n *   segments: [{ type, sourceName, sourcePath, inPoint, outPoint }],\n *   metadata: { silencesRemoved, takesDetected, generatedAt }\n * }\n *\n * Frame Alignment:\n * When frameRate is provided, all cut points are aligned to frame boundaries.\n * This prevents sub-frame edits that can cause playback issues.\n */\n\n/**\n * Align a time value to the nearest frame boundary\n * @param {number} time - Time in seconds\n * @param {number} frameRate - Frames per second (e.g., 23.976, 24, 29.97, 30, 60)\n * @returns {number} Time aligned to frame boundary\n */\nfunction alignToFrame(time, frameRate) {\n  if (!frameRate || frameRate <= 0) return time;\n  const frameDuration = 1 / frameRate;\n  return Math.round(time / frameDuration) * frameDuration;\n}\n\n/**\n * Align to frame boundary, rounding down (for in-points)\n */\nfunction alignToFrameFloor(time, frameRate) {\n  if (!frameRate || frameRate <= 0) return time;\n  const frameDuration = 1 / frameRate;\n  return Math.floor(time / frameDuration) * frameDuration;\n}\n\n/**\n * Align to frame boundary, rounding up (for out-points)\n */\nfunction alignToFrameCeil(time, frameRate) {\n  if (!frameRate || frameRate <= 0) return time;\n  const frameDuration = 1 / frameRate;\n  return Math.ceil(time / frameDuration) * frameDuration;\n}\n\n/**\n * Generate a cut list from detected silences\n * Creates segments that represent speech (non-silence) portions\n *\n * @param {Object} options - Generation options\n * @param {string} options.sourceName - Name of the source clip\n * @param {string} options.sourcePath - Full path to the source file\n * @param {number} options.duration - Total duration of source in seconds\n * @param {Array} options.silences - Array of silence segments [{start, end, duration}]\n * @param {Array} [options.takes] - Optional array of detected takes\n * @param {Object} [options.settings] - Optional generation settings\n * @returns {Object} The generated cut list\n */\nfunction generateCutList(options) {\n  const {\n    sourceName,\n    sourcePath,\n    duration,\n    silences = [],\n    takes = [],\n    settings = {}\n  } = options;\n\n  // Settings with defaults\n  const {\n    minSegmentDuration = 0.1,  // Minimum segment length to include\n    padding = 0,               // Padding around speech (extend into silence)\n    markBestTakes = true,      // Color-code best takes differently\n    frameRate = 0,             // Frame rate for alignment (0 = no alignment)\n    jCutOffset = 0,            // J-cut: audio starts before video (negative value)\n    lCutOffset = 0             // L-cut: audio extends after video (positive value)\n  } = settings;\n\n  // Sort silences by start time\n  const sortedSilences = [...silences].sort((a, b) => a.start - b.start);\n\n  // Build segments from gaps between silences (speech segments)\n  const segments = [];\n  let lastEnd = 0;\n  let silencesRemoved = 0;\n\n  for (const silence of sortedSilences) {\n    // Calculate segment boundaries with padding and frame alignment\n    let segmentStart = Math.max(0, lastEnd - padding);\n    let segmentEnd = Math.min(duration, silence.start + padding);\n\n    // Align to frame boundaries if frameRate is specified\n    if (frameRate > 0) {\n      segmentStart = alignToFrameFloor(segmentStart, frameRate);\n      segmentEnd = alignToFrameCeil(segmentEnd, frameRate);\n    }\n\n    // Only add if segment meets minimum duration\n    if (segmentEnd - segmentStart >= minSegmentDuration) {\n      const segment = {\n        type: 'speech',\n        sourceName,\n        sourcePath,\n        inPoint: parseFloat(segmentStart.toFixed(6)),\n        outPoint: parseFloat(segmentEnd.toFixed(6))\n      };\n\n      // Add J-cut/L-cut audio offsets if specified\n      if (jCutOffset !== 0 || lCutOffset !== 0) {\n        // Audio in/out points differ from video\n        segment.audioInOffset = jCutOffset;  // Negative = audio starts earlier (J-cut)\n        segment.audioOutOffset = lCutOffset; // Positive = audio ends later (L-cut)\n\n        // Calculate actual audio in/out points\n        segment.audioInPoint = parseFloat(Math.max(0, segmentStart + jCutOffset).toFixed(6));\n        segment.audioOutPoint = parseFloat(Math.min(duration, segmentEnd + lCutOffset).toFixed(6));\n      }\n\n      // Check if this segment contains a take\n      const containingTake = findContainingTake(segmentStart, segmentEnd, takes);\n      if (containingTake) {\n        segment.take = {\n          text: containingTake.text?.substring(0, 100),\n          isBest: containingTake.isBest || false\n        };\n        if (containingTake.isBest && markBestTakes) {\n          segment.type = 'best_take';\n        }\n      }\n\n      segments.push(segment);\n    }\n\n    silencesRemoved++;\n    lastEnd = silence.end;\n  }\n\n  // Add final segment after last silence\n  if (lastEnd < duration) {\n    let segmentStart = Math.max(0, lastEnd - padding);\n    let segmentEnd = duration;\n\n    // Align to frame boundaries if frameRate is specified\n    if (frameRate > 0) {\n      segmentStart = alignToFrameFloor(segmentStart, frameRate);\n      segmentEnd = alignToFrameCeil(segmentEnd, frameRate);\n    }\n\n    if (segmentEnd - segmentStart >= minSegmentDuration) {\n      const segment = {\n        type: 'speech',\n        sourceName,\n        sourcePath,\n        inPoint: parseFloat(segmentStart.toFixed(6)),\n        outPoint: parseFloat(segmentEnd.toFixed(6))\n      };\n\n      // Add J-cut/L-cut audio offsets if specified\n      if (jCutOffset !== 0 || lCutOffset !== 0) {\n        segment.audioInOffset = jCutOffset;\n        segment.audioOutOffset = lCutOffset;\n        segment.audioInPoint = parseFloat(Math.max(0, segmentStart + jCutOffset).toFixed(6));\n        segment.audioOutPoint = parseFloat(Math.min(duration, segmentEnd + lCutOffset).toFixed(6));\n      }\n\n      // Check if this segment contains a take\n      const containingTake = findContainingTake(segmentStart, segmentEnd, takes);\n      if (containingTake) {\n        segment.take = {\n          text: containingTake.text?.substring(0, 100),\n          isBest: containingTake.isBest || false\n        };\n        if (containingTake.isBest && markBestTakes) {\n          segment.type = 'best_take';\n        }\n      }\n\n      segments.push(segment);\n    }\n  }\n\n  // Calculate stats\n  const totalKeptDuration = segments.reduce(\n    (sum, seg) => sum + (seg.outPoint - seg.inPoint),\n    0\n  );\n  const totalRemovedDuration = duration - totalKeptDuration;\n\n  const cutList = {\n    version: '3.5',\n    source: {\n      name: sourceName,\n      path: sourcePath,\n      duration\n    },\n    segments,\n    metadata: {\n      silencesRemoved,\n      takesDetected: takes.length,\n      totalKeptDuration: parseFloat(totalKeptDuration.toFixed(3)),\n      totalRemovedDuration: parseFloat(totalRemovedDuration.toFixed(3)),\n      segmentCount: segments.length,\n      frameRate: frameRate > 0 ? frameRate : null,\n      frameAligned: frameRate > 0,\n      jCutOffset: jCutOffset !== 0 ? jCutOffset : null,\n      lCutOffset: lCutOffset !== 0 ? lCutOffset : null,\n      hasAudioOffsets: jCutOffset !== 0 || lCutOffset !== 0,\n      generatedAt: new Date().toISOString()\n    }\n  };\n\n  console.log(`[SPLICE] Generated cut list: ${segments.length} segments, ${silencesRemoved} silences removed`);\n\n  return cutList;\n}\n\n/**\n * Find a take that overlaps with a given time range\n * @param {number} start - Start time in seconds\n * @param {number} end - End time in seconds\n * @param {Array} takes - Array of takes\n * @returns {Object|null} The overlapping take or null\n */\nfunction findContainingTake(start, end, takes) {\n  if (!takes || takes.length === 0) return null;\n\n  for (const take of takes) {\n    const takeStart = take.startTime || take.start;\n    const takeEnd = take.endTime || take.end;\n\n    // Check for overlap\n    if (start < takeEnd && end > takeStart) {\n      return take;\n    }\n  }\n\n  return null;\n}\n\n/**\n * Generate a cut list that keeps only specific takes\n * Useful for \"keep best takes only\" workflow\n *\n * @param {Object} options - Generation options\n * @param {string} options.sourceName - Name of the source clip\n * @param {string} options.sourcePath - Full path to the source file\n * @param {number} options.duration - Total duration of source in seconds\n * @param {Array} options.takes - Array of takes to keep\n * @param {Object} [options.settings] - Optional generation settings\n * @returns {Object} The generated cut list\n */\nfunction generateTakesCutList(options) {\n  const {\n    sourceName,\n    sourcePath,\n    duration,\n    takes = [],\n    settings = {}\n  } = options;\n\n  const {\n    padding = 0.2,          // Small padding around takes\n    onlyBestTakes = false   // If true, only include takes marked as best\n  } = settings;\n\n  // Filter to best takes if requested\n  const takesToKeep = onlyBestTakes\n    ? takes.filter(t => t.isBest)\n    : takes;\n\n  // Sort by start time\n  const sortedTakes = [...takesToKeep].sort((a, b) => {\n    const aStart = a.startTime || a.start;\n    const bStart = b.startTime || b.start;\n    return aStart - bStart;\n  });\n\n  // Build segments from takes\n  const segments = sortedTakes.map(take => {\n    const takeStart = take.startTime || take.start;\n    const takeEnd = take.endTime || take.end;\n\n    return {\n      type: take.isBest ? 'best_take' : 'take',\n      sourceName,\n      sourcePath,\n      inPoint: Math.max(0, takeStart - padding),\n      outPoint: Math.min(duration, takeEnd + padding),\n      take: {\n        text: take.text?.substring(0, 100),\n        isBest: take.isBest || false\n      }\n    };\n  });\n\n  const totalKeptDuration = segments.reduce(\n    (sum, seg) => sum + (seg.outPoint - seg.inPoint),\n    0\n  );\n\n  const cutList = {\n    version: '3.5',\n    source: {\n      name: sourceName,\n      path: sourcePath,\n      duration\n    },\n    segments,\n    metadata: {\n      takesKept: segments.length,\n      takesTotal: takes.length,\n      totalKeptDuration: parseFloat(totalKeptDuration.toFixed(3)),\n      totalRemovedDuration: parseFloat((duration - totalKeptDuration).toFixed(3)),\n      generatedAt: new Date().toISOString()\n    }\n  };\n\n  console.log(`[SPLICE] Generated takes cut list: ${segments.length} takes kept`);\n\n  return cutList;\n}\n\n/**\n * Merge overlapping segments in a cut list\n * Useful when silences and takes create overlapping regions\n *\n * @param {Object} cutList - The cut list to merge\n * @returns {Object} Cut list with merged segments\n */\nfunction mergeOverlappingSegments(cutList) {\n  if (!cutList.segments || cutList.segments.length < 2) {\n    return cutList;\n  }\n\n  // Sort by start time\n  const sorted = [...cutList.segments].sort((a, b) => a.inPoint - b.inPoint);\n\n  const merged = [];\n  let current = { ...sorted[0] };\n\n  for (let i = 1; i < sorted.length; i++) {\n    const next = sorted[i];\n\n    // Check for overlap\n    if (next.inPoint <= current.outPoint) {\n      // Merge: extend current segment\n      current.outPoint = Math.max(current.outPoint, next.outPoint);\n      // Keep the more specific type (best_take > take > speech)\n      if (next.type === 'best_take' || (next.type === 'take' && current.type === 'speech')) {\n        current.type = next.type;\n        current.take = next.take;\n      }\n    } else {\n      // No overlap: save current and start new\n      merged.push(current);\n      current = { ...next };\n    }\n  }\n\n  // Don't forget the last segment\n  merged.push(current);\n\n  // Update metadata\n  const newCutList = {\n    ...cutList,\n    segments: merged,\n    metadata: {\n      ...cutList.metadata,\n      segmentCount: merged.length,\n      mergedFrom: cutList.segments.length\n    }\n  };\n\n  console.log(`[SPLICE] Merged ${cutList.segments.length} segments into ${merged.length}`);\n\n  return newCutList;\n}\n\n/**\n * Validate a cut list structure\n * @param {Object} cutList - The cut list to validate\n * @returns {{valid: boolean, errors: string[]}}\n */\nfunction validateCutList(cutList) {\n  const errors = [];\n\n  if (!cutList) {\n    errors.push('Cut list is null or undefined');\n    return { valid: false, errors };\n  }\n\n  if (!cutList.version) {\n    errors.push('Missing version field');\n  }\n\n  if (!cutList.source) {\n    errors.push('Missing source field');\n  } else {\n    if (!cutList.source.name && !cutList.source.path) {\n      errors.push('Source must have name or path');\n    }\n    if (typeof cutList.source.duration !== 'number' || cutList.source.duration <= 0) {\n      errors.push('Source duration must be a positive number');\n    }\n  }\n\n  if (!Array.isArray(cutList.segments)) {\n    errors.push('Segments must be an array');\n  } else {\n    cutList.segments.forEach((seg, i) => {\n      if (typeof seg.inPoint !== 'number') {\n        errors.push(`Segment ${i}: inPoint must be a number`);\n      }\n      if (typeof seg.outPoint !== 'number') {\n        errors.push(`Segment ${i}: outPoint must be a number`);\n      }\n      if (seg.inPoint >= seg.outPoint) {\n        errors.push(`Segment ${i}: inPoint must be less than outPoint`);\n      }\n      if (!seg.sourceName && !seg.sourcePath) {\n        errors.push(`Segment ${i}: must have sourceName or sourcePath`);\n      }\n    });\n  }\n\n  return {\n    valid: errors.length === 0,\n    errors\n  };\n}\n\nmodule.exports = {\n  generateCutList,\n  generateTakesCutList,\n  mergeOverlappingSegments,\n  validateCutList,\n  // Frame alignment utilities\n  alignToFrame,\n  alignToFrameFloor,\n  alignToFrameCeil\n};\n","numLines":446,"startLine":1,"totalLines":446}}}
{"parentUuid":"eba49291-bb29-463f-909c-8f1e8e92b5c5","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017wpQhHKpNk8sujDLQXayii","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Backend Server\n     3→ *\n     4→ * Main entry point for the SPLICE backend API.\n     5→ * Orchestrates the audio analysis pipeline.\n     6→ *\n     7→ * Slices:\n     8→ * - Slice 4: Transcription (services/transcription.js)\n     9→ * - Slice 5: Take Detection (services/takeDetection.js)\n    10→ */\n    11→\n    12→require('dotenv').config();\n    13→\n    14→const express = require('express');\n    15→const cors = require('cors');\n    16→const fs = require('fs');\n    17→const https = require('https');\n    18→const http = require('http');\n    19→const path = require('path');\n    20→\n    21→// Check if running in production (Railway injects RAILWAY_ENVIRONMENT)\n    22→const isProduction = process.env.NODE_ENV === 'production' || process.env.RAILWAY_ENVIRONMENT;\n    23→\n    24→// Import slice services\n    25→const { transcribeAudio, transcribeWithWords } = require('./services/transcription');\n    26→const { detectTakes } = require('./services/takeDetection');\n    27→const { detectSilences } = require('./services/silenceDetection');\n    28→const { detectAudioSilences, isFFprobeInstalled, getAudioDuration } = require('./services/ffprobeSilence');\n    29→const { detectSilencesRMS, sensitivityToParams } = require('./services/rmsSilenceDetection');\n    30→const {\n    31→  detectProfanity,\n    32→  getProfanityList,\n    33→  getSupportedLanguages,\n    34→  getAvailableBleepSounds,\n    35→  parseWordList\n    36→} = require('./services/profanityDetection');\n    37→const {\n    38→  detectRepetitionsBasic,\n    39→  detectRepetitionsAdvanced,\n    40→  detectStutters,\n    41→  detectAllRepetitions\n    42→} = require('./services/repetitionDetection');\n    43→const {\n    44→  analyzeMultitrack,\n    45→  autoBalanceMultitrack\n    46→} = require('./services/multitrackAnalysis');\n    47→const {\n    48→  toSRT,\n    49→  toVTT,\n    50→  toPlainText,\n    51→  toJSON,\n    52→  exportToFile,\n    53→  getSupportedFormats\n    54→} = require('./services/captionExporter');\n    55→const { processXMLFile } = require('./services/xmlProcessor');\n    56→const { isolateVocals, isReplicateConfigured } = require('./services/vocalIsolation');\n    57→const { generateCutList, generateTakesCutList, validateCutList } = require('./services/cutListGenerator');\n    58→\n    59→// Usage tracking and billing\n    60→const usageTracking = require('./services/usageTracking');\n    61→// Rate limiter for usage-based endpoints\n    62→const { requireCredits } = require('./middleware/rateLimiter');\n    63→\n    64→// Maximum file size for audio processing (500MB)\n    65→const MAX_FILE_SIZE_BYTES = 500 * 1024 * 1024;\n    66→\n    67→/**\n    68→ * Validate file size to prevent OOM crashes\n    69→ * @param {string} filePath - Path to file\n    70→ * @returns {Promise<{valid: boolean, size?: number, error?: string}>}\n    71→ */\n    72→async function validateFileSize(filePath) {\n    73→  try {\n    74→    const stats = await require('fs').promises.stat(filePath);\n    75→    if (stats.size > MAX_FILE_SIZE_BYTES) {\n    76→      return {\n    77→        valid: false,\n    78→        size: stats.size,\n    79→        error: `File too large (${(stats.size / 1024 / 1024).toFixed(1)}MB). Maximum allowed: ${MAX_FILE_SIZE_BYTES / 1024 / 1024}MB`\n    80→      };\n    81→    }\n    82→    return { valid: true, size: stats.size };\n    83→  } catch (err) {\n    84→    return { valid: false, error: `Cannot access file: ${err.message}` };\n    85→  }\n    86→}\n    87→\n    88→// Stripe for webhooks\n    89→const Stripe = require('stripe');\n    90→const stripe = new Stripe(process.env.STRIPE_SECRET_KEY);\n    91→\n    92→// =============================================================================\n    93→// Server Configuration\n    94→// =============================================================================\n    95→\n    96→const app = express();\n    97→const PORT = process.env.PORT || 3847;\n    98→\n    99→// HTTPS certificates (generated by mkcert) - only for local development\n   100→let httpsOptions = null;\n   101→if (!isProduction) {\n   102→  const keyPath = path.join(__dirname, 'localhost+1-key.pem');\n   103→  const certPath = path.join(__dirname, 'localhost+1.pem');\n   104→  if (fs.existsSync(keyPath) && fs.existsSync(certPath)) {\n   105→    httpsOptions = {\n   106→      key: fs.readFileSync(keyPath),\n   107→      cert: fs.readFileSync(certPath)\n   108→    };\n   109→  }\n   110→}\n   111→\n   112→app.use(cors());\n   113→\n   114→// Helper to determine tier from price ID with logging\n   115→function getTierFromPriceId(priceId) {\n   116→  if (priceId === process.env.STRIPE_PRICE_STARTER) return 'starter';\n   117→  if (priceId === process.env.STRIPE_PRICE_PRO) return 'pro';\n   118→  if (priceId === process.env.STRIPE_PRICE_TEAM) return 'team';\n   119→\n   120→  // Log unknown price ID for debugging\n   121→  console.warn(`[SPLICE] Unknown price ID: ${priceId} - defaulting to starter tier`);\n   122→  return 'starter';\n   123→}\n   124→\n   125→// Stripe webhook needs raw body - must be before express.json()\n   126→app.post('/webhooks/stripe', express.raw({ type: 'application/json' }), async (req, res) => {\n   127→  const sig = req.headers['stripe-signature'];\n   128→  const webhookSecret = process.env.STRIPE_WEBHOOK_SECRET;\n   129→\n   130→  let event;\n   131→\n   132→  try {\n   133→    if (webhookSecret) {\n   134→      event = stripe.webhooks.constructEvent(req.body, sig, webhookSecret);\n   135→    } else if (isProduction) {\n   136→      // SECURITY: Reject unsigned webhooks in production\n   137→      console.error('[SPLICE] CRITICAL: STRIPE_WEBHOOK_SECRET not set in production');\n   138→      return res.status(500).json({ error: 'Webhook configuration error: secret not configured' });\n   139→    } else {\n   140→      // For local development testing only\n   141→      event = JSON.parse(req.body);\n   142→      console.warn('[SPLICE] Warning: Processing webhook without signature verification (dev only)');\n   143→    }\n   144→  } catch (err) {\n   145→    console.error('[SPLICE] Webhook signature verification failed:', err.message);\n   146→    return res.status(400).json({ error: 'Webhook signature verification failed' });\n   147→  }\n   148→\n   149→  console.log(`[SPLICE] Webhook received: ${event.type} (${event.id})`);\n   150→\n   151→  // Idempotency check - skip if already processed\n   152→  if (await usageTracking.isEventProcessed(event.id)) {\n   153→    console.log(`[SPLICE] Event ${event.id} already processed, skipping`);\n   154→    return res.json({ received: true, skipped: true });\n   155→  }\n   156→\n   157→  try {\n   158→    switch (event.type) {\n   159→      case 'customer.subscription.created':\n   160→      case 'customer.subscription.updated': {\n   161→        const subscription = event.data.object;\n   162→        const customerId = subscription.customer;\n   163→\n   164→        // Validate customerId\n   165→        if (!customerId) {\n   166→          console.error('[SPLICE] Missing customer ID in subscription event');\n   167→          return res.status(400).json({ error: 'Missing customer ID' });\n   168→        }\n   169→\n   170→        // Get tier from price ID\n   171→        const priceId = subscription.items?.data?.[0]?.price?.id;\n   172→        const tier = getTierFromPriceId(priceId);\n   173→\n   174→        // Update user tier and reset hours\n   175→        await usageTracking.updateTier(customerId, tier);\n   176→        console.log(`[SPLICE] Updated customer ${customerId} to tier: ${tier}`);\n   177→        break;\n   178→      }\n   179→\n   180→      case 'customer.subscription.deleted': {\n   181→        const subscription = event.data.object;\n   182→        const customerId = subscription.customer;\n   183→\n   184→        // Validate customerId\n   185→        if (!customerId) {\n   186→          console.error('[SPLICE] Missing customer ID in subscription.deleted event');\n   187→          return res.status(400).json({ error: 'Missing customer ID' });\n   188→        }\n   189→\n   190→        // Downgrade to cancelled (0 hours)\n   191→        await usageTracking.updateTier(customerId, 'cancelled');\n   192→        console.log(`[SPLICE] Subscription cancelled for customer ${customerId}`);\n   193→        break;\n   194→      }\n   195→\n   196→      case 'invoice.payment_succeeded': {\n   197→        const invoice = event.data.object;\n   198→        const customerId = invoice.customer;\n   199→        const subscriptionId = invoice.subscription;\n   200→\n   201→        // Validate customerId\n   202→        if (!customerId) {\n   203→          console.error('[SPLICE] Missing customer ID in invoice event');\n   204→          return res.status(400).json({ error: 'Missing customer ID' });\n   205→        }\n   206→\n   207→        // Reset hours on successful payment (new billing period)\n   208→        if (subscriptionId) {\n   209→          const subscription = await stripe.subscriptions.retrieve(subscriptionId);\n   210→          const priceId = subscription.items?.data?.[0]?.price?.id;\n   211→          const tier = getTierFromPriceId(priceId);\n   212→\n   213→          await usageTracking.resetHours(customerId, tier);\n   214→          console.log(`[SPLICE] Reset hours for customer ${customerId} (tier: ${tier})`);\n   215→        }\n   216→        break;\n   217→      }\n   218→\n   219→      default:\n   220→        console.log(`[SPLICE] Unhandled event type: ${event.type}`);\n   221→    }\n   222→\n   223→    // Record event as processed (idempotency)\n   224→    await usageTracking.recordWebhookEvent(event.id, event.type);\n   225→\n   226→    res.json({ received: true });\n   227→  } catch (err) {\n   228→    console.error('[SPLICE] Webhook handler error:', err);\n   229→    res.status(500).json({ error: err.message });\n   230→  }\n   231→});\n   232→\n   233→app.use(express.json());\n   234→\n   235→// =============================================================================\n   236→// Routes\n   237→// =============================================================================\n   238→\n   239→/**\n   240→ * GET / - API information\n   241→ */\n   242→app.get('/', (req, res) => {\n   243→  res.json({\n   244→    service: 'splice-backend',\n   245→    version: '0.3.0',\n   246→    endpoints: {\n   247→      'GET /': 'This info',\n   248→      'GET /health': 'Health check',\n   249→      'GET /ffprobe-check': 'Check if FFprobe is installed',\n   250→      'GET /replicate-check': 'Check if Replicate API is configured',\n   251→      'POST /analyze': 'Analyze WAV file { wavPath }',\n   252→      'POST /silences': 'Detect silences via Whisper gaps { wavPath, threshold: 0.5 }',\n   253→      'POST /silences-audio': 'Detect silences via FFprobe { wavPath, threshold: -30, minDuration: 0.5, padding: 0.1 }',\n   254→      'POST /silences-rms': 'Detect silences via RMS analysis { wavPath, threshold: -30, minSilenceLength: 0.5, paddingStart: 0.1, paddingEnd: 0.05, autoThreshold: false, sensitivity: 50 }',\n   255→      'POST /profanity': 'Detect profanity in transcript { wavPath, language: \"en\", customBlocklist: [], customAllowlist: [] }',\n   256→      'GET /profanity/languages': 'Get supported languages for profanity detection',\n   257→      'GET /profanity/bleeps': 'Get available bleep sounds',\n   258→      'POST /repetitions': 'Detect phrase repetitions and stutters { wavPath, phraseSize: 5, tolerance: 0.7, useOpenAI: false }',\n   259→      'POST /fillers': 'Detect filler words (um, uh, like, etc.) { wavPath, customFillers: [] }',\n   260→      'POST /stutters': 'Detect single-word stutters only { wavPath, minRepeats: 2 }',\n   261→      'POST /export/captions': 'Export transcript to caption format { wavPath, format: srt|vtt|txt|json, outputPath? }',\n   262→      'GET /export/formats': 'Get supported caption export formats',\n   263→      'POST /multitrack': 'Analyze multiple audio tracks for multicam { audioPaths: [], speakerNames: [], videoTrackMapping: {} }',\n   264→      'POST /multitrack/auto-balance': 'Auto-balance speaker screentime { audioPaths: [], speakerNames: [] }',\n   265→      'POST /process-xml': 'Process FCP XML { xmlPath, silences, removeGaps: true }',\n   266→      'POST /cut-list': 'Generate JSON cut list for DOM building (v3.5) { sourceName, sourcePath, duration, silences, takes?, settings? }',\n   267→      'POST /cut-list/takes': 'Generate cut list keeping only takes { sourceName, sourcePath, duration, takes, settings? }',\n   268→      'POST /isolate-vocals': 'Isolate vocals from audio { audioPath }',\n   269→      'POST /batch/silences': 'Batch process multiple files for silence detection { files: [], options: {} }',\n   270→      'GET /batch/status/:jobId': 'Get batch job status',\n   271→      'GET /batch/results/:jobId': 'Get full batch job results',\n   272→      'GET /batch/jobs': 'List all batch jobs',\n   273→      'DELETE /batch/:jobId': 'Delete a batch job',\n   274→      'GET /credits': 'Get user credit balance (requires x-stripe-customer-id header)',\n   275→      'GET /usage-history': 'Get usage history (requires x-stripe-customer-id header)',\n   276→      'POST /webhooks/stripe': 'Stripe webhook endpoint'\n   277→    }\n   278→  });\n   279→});\n   280→\n   281→/**\n   282→ * GET /health - Health check\n   283→ */\n   284→app.get('/health', (req, res) => {\n   285→  res.json({ status: 'ok', service: 'splice-backend' });\n   286→});\n   287→\n   288→/**\n   289→ * POST /analyze - Main analysis endpoint\n   290→ *\n   291→ * Pipeline:\n   292→ * 1. Validate input (wavPath)\n   293→ * 2. Slice 4: Transcribe audio with Groq Whisper\n   294→ * 3. Slice 5: Detect takes with GPT-4o-mini\n   295→ * 4. Return combined results\n   296→ */\n   297→app.post('/analyze', requireCredits({ endpoint: 'analyze' }), async (req, res) => {\n   298→  const { wavPath } = req.body;\n   299→\n   300→  // Validate input\n   301→  if (!wavPath) {\n   302→    return res.status(400).json({ error: 'wavPath is required' });\n   303→  }\n   304→\n   305→  if (!fs.existsSync(wavPath)) {\n   306→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   307→  }\n   308→\n   309→  console.log(`[SPLICE] Analyzing: ${wavPath}`);\n   310→\n   311→  try {\n   312→    // Slice 4 - GPT-4o-mini transcription\n   313→    const transcript = await transcribeAudio(wavPath);\n   314→\n   315→    // Slice 5 - GPT-4o-mini take detection\n   316→    const takes = await detectTakes(transcript);\n   317→\n   318→    // Deduct usage based on audio duration\n   319→    const audioDuration = transcript.duration || 0;\n   320→    let balance = null;\n   321→    if (audioDuration > 0 && req.deductUsage) {\n   322→      balance = await req.deductUsage(audioDuration);\n   323→    }\n   324→\n   325→    res.json({\n   326→      success: true,\n   327→      wavPath,\n   328→      transcript,\n   329→      takes,\n   330→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   331→    });\n   332→  } catch (err) {\n   333→    console.error('[SPLICE] Error:', err);\n   334→    res.status(500).json({ error: err.message });\n   335→  }\n   336→});\n   337→\n   338→/**\n   339→ * POST /silences - Detect silent gaps in audio\n   340→ *\n   341→ * Pipeline:\n   342→ * 1. Transcribe audio with Whisper (reuses transcription)\n   343→ * 2. Analyze gaps between segments\n   344→ * 3. Return silence regions\n   345→ */\n   346→app.post('/silences', requireCredits({ endpoint: 'silences' }), async (req, res) => {\n   347→  const { wavPath, threshold = 0.5 } = req.body;\n   348→\n   349→  if (!wavPath) {\n   350→    return res.status(400).json({ error: 'wavPath is required' });\n   351→  }\n   352→\n   353→  if (!fs.existsSync(wavPath)) {\n   354→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   355→  }\n   356→\n   357→  console.log(`[SPLICE] Detecting silences: ${wavPath} (threshold: ${threshold}s)`);\n   358→\n   359→  try {\n   360→    const transcript = await transcribeAudio(wavPath);\n   361→    const silences = detectSilences(transcript.segments, threshold);\n   362→\n   363→    // Deduct usage based on audio duration\n   364→    const audioDuration = transcript.duration || 0;\n   365→    let balance = null;\n   366→    if (audioDuration > 0 && req.deductUsage) {\n   367→      balance = await req.deductUsage(audioDuration);\n   368→    }\n   369→\n   370→    res.json({\n   371→      success: true,\n   372→      wavPath,\n   373→      threshold,\n   374→      silences,\n   375→      count: silences.length,\n   376→      totalSilenceDuration: silences.reduce((sum, s) => sum + s.duration, 0).toFixed(2),\n   377→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   378→    });\n   379→  } catch (err) {\n   380→    console.error('[SPLICE] Silence detection error:', err);\n   381→    res.status(500).json({ error: err.message });\n   382→  }\n   383→});\n   384→\n   385→/**\n   386→ * POST /silences-audio - Detect silences using FFprobe audio analysis\n   387→ *\n   388→ * Uses actual audio levels (dB threshold) instead of transcript gaps.\n   389→ * More accurate for detecting silence vs background noise.\n   390→ */\n   391→app.post('/silences-audio', requireCredits({ endpoint: 'silences-audio' }), async (req, res) => {\n   392→  const {\n   393→    wavPath,\n   394→    threshold = -30,\n   395→    minDuration = 0.5,\n   396→    padding = 0.1\n   397→  } = req.body;\n   398→\n   399→  if (!wavPath) {\n   400→    return res.status(400).json({ error: 'wavPath is required' });\n   401→  }\n   402→\n   403→  if (!fs.existsSync(wavPath)) {\n   404→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   405→  }\n   406→\n   407→  // Check FFprobe availability\n   408→  const ffprobeAvailable = await isFFprobeInstalled();\n   409→  if (!ffprobeAvailable) {\n   410→    return res.status(500).json({\n   411→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n   412→    });\n   413→  }\n   414→\n   415→  console.log(`[SPLICE] FFprobe silence detection: ${wavPath} (threshold: ${threshold}dB, min: ${minDuration}s)`);\n   416→\n   417→  try {\n   418→    const silences = await detectAudioSilences(wavPath, {\n   419→      threshold,\n   420→      minDuration,\n   421→      padding\n   422→    });\n   423→\n   424→    const totalDuration = silences.reduce((sum, s) => sum + s.duration, 0);\n   425→\n   426→    // Deduct usage based on audio duration\n   427→    let balance = null;\n   428→    try {\n   429→      const audioDuration = await getAudioDuration(wavPath);\n   430→      if (audioDuration > 0 && req.deductUsage) {\n   431→        balance = await req.deductUsage(audioDuration);\n   432→      }\n   433→    } catch (durErr) {\n   434→      console.warn('[SPLICE] Could not get audio duration for billing:', durErr.message);\n   435→    }\n   436→\n   437→    res.json({\n   438→      success: true,\n   439→      wavPath,\n   440→      threshold,\n   441→      minDuration,\n   442→      padding,\n   443→      silences,\n   444→      count: silences.length,\n   445→      totalSilenceDuration: totalDuration.toFixed(2),\n   446→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   447→    });\n   448→  } catch (err) {\n   449→    console.error('[SPLICE] FFprobe silence detection error:', err);\n   450→    res.status(500).json({ error: err.message });\n   451→  }\n   452→});\n   453→\n   454→/**\n   455→ * POST /silences-rms - Detect silences using RMS audio analysis\n   456→ *\n   457→ * Advanced silence detection with:\n   458→ * - RMS (Root Mean Square) audio level analysis\n   459→ * - Auto-threshold detection from audio histogram\n   460→ * - Configurable padding (before/after cuts)\n   461→ * - Sensitivity slider mapping (0-100)\n   462→ *\n   463→ * Options:\n   464→ * - threshold: dBFS threshold (-60 to -20, default: -30)\n   465→ * - minSilenceLength: Minimum silence duration in seconds (default: 0.5)\n   466→ * - seekStep: Analysis window step in seconds (default: 0.05)\n   467→ * - paddingStart: Buffer before silence in seconds (default: 0.1)\n   468→ * - paddingEnd: Buffer after silence in seconds (default: 0.05)\n   469→ * - autoThreshold: Auto-detect optimal threshold (default: false)\n   470→ * - sensitivity: UI sensitivity 0-100 (overrides other params if provided)\n   471→ */\n   472→app.post('/silences-rms', requireCredits({ endpoint: 'silences-rms' }), async (req, res) => {\n   473→  const { wavPath, sensitivity, ...manualOptions } = req.body;\n   474→\n   475→  if (!wavPath) {\n   476→    return res.status(400).json({ error: 'wavPath is required' });\n   477→  }\n   478→\n   479→  if (!fs.existsSync(wavPath)) {\n   480→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   481→  }\n   482→\n   483→  // Validate file size to prevent OOM\n   484→  const sizeCheck = await validateFileSize(wavPath);\n   485→  if (!sizeCheck.valid) {\n   486→    return res.status(413).json({ error: sizeCheck.error });\n   487→  }\n   488→\n   489→  // Check FFprobe availability (needed for audio extraction)\n   490→  const ffprobeAvailable = await isFFprobeInstalled();\n   491→  if (!ffprobeAvailable) {\n   492→    return res.status(500).json({\n   493→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n   494→    });\n   495→  }\n   496→\n   497→  // Build options - use sensitivity if provided, otherwise use manual options\n   498→  let options = {};\n   499→  if (typeof sensitivity === 'number') {\n   500→    options = sensitivityToParams(sensitivity);\n   501→    console.log(`[SPLICE] RMS detection with sensitivity ${sensitivity}`);\n   502→  } else {\n   503→    options = {\n   504→      threshold: manualOptions.threshold ?? -30,\n   505→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n   506→      seekStep: manualOptions.seekStep ?? 0.05,\n   507→      paddingStart: manualOptions.paddingStart ?? 0.1,\n   508→      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n   509→      autoThreshold: manualOptions.autoThreshold ?? false,\n   510→      mergeDistance: manualOptions.mergeDistance ?? 0.2\n   511→    };\n   512→  }\n   513→\n   514→  console.log(`[SPLICE] RMS silence detection: ${wavPath}`);\n   515→\n   516→  try {\n   517→    const result = await detectSilencesRMS(wavPath, options);\n   518→\n   519→    // Deduct usage based on audio duration\n   520→    const audioDuration = result.metadata?.audioDuration || 0;\n   521→    let balance = null;\n   522→    if (audioDuration > 0 && req.deductUsage) {\n   523→      balance = await req.deductUsage(audioDuration);\n   524→    }\n   525→\n   526→    res.json({\n   527→      success: true,\n   528→      wavPath,\n   529→      ...result,\n   530→      count: result.silences.length,\n   531→      totalSilenceDuration: result.metadata.totalSilenceDuration.toFixed(2),\n   532→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   533→    });\n   534→  } catch (err) {\n   535→    console.error('[SPLICE] RMS silence detection error:', err);\n   536→    res.status(500).json({ error: err.message });\n   537→  }\n   538→});\n   539→\n   540→// =============================================================================\n   541→// Profanity Detection Routes\n   542→// =============================================================================\n   543→\n   544→/**\n   545→ * POST /profanity - Detect profanity in audio/transcript\n   546→ *\n   547→ * Transcribes audio (if needed) and detects profanity words.\n   548→ * Returns word-level and segment-level results for muting/bleeping.\n   549→ *\n   550→ * Options:\n   551→ * - wavPath: Path to audio file (required)\n   552→ * - transcript: Pre-existing transcript (optional, skips transcription)\n   553→ * - language: Language code (en, es, fr, de) - default: en\n   554→ * - customBlocklist: Array or comma-separated string of additional words to censor\n   555→ * - customAllowlist: Array or comma-separated string of words to allow\n   556→ * - frameRate: Frame rate for boundary alignment (default: 30)\n   557→ */\n   558→app.post('/profanity', requireCredits({ endpoint: 'profanity' }), async (req, res) => {\n   559→  const {\n   560→    wavPath,\n   561→    transcript: providedTranscript,\n   562→    language = 'en',\n   563→    customBlocklist = [],\n   564→    customAllowlist = [],\n   565→    frameRate = 30\n   566→  } = req.body;\n   567→\n   568→  if (!wavPath && !providedTranscript) {\n   569→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   570→  }\n   571→\n   572→  if (wavPath && !fs.existsSync(wavPath)) {\n   573→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   574→  }\n   575→\n   576→  console.log(`[SPLICE] Profanity detection: ${wavPath || 'provided transcript'} (language: ${language})`);\n   577→\n   578→  try {\n   579→    // Get or create transcript with word-level timestamps\n   580→    let transcript = providedTranscript;\n   581→    if (!transcript && wavPath) {\n   582→      // Use transcribeWithWords for word-level timestamps required by profanity detection\n   583→      transcript = await transcribeWithWords(wavPath);\n   584→    }\n   585→\n   586→    // Validate transcript has words\n   587→    if (!transcript || !transcript.words || transcript.words.length === 0) {\n   588→      return res.status(400).json({\n   589→        error: 'Transcript must contain word-level timing data',\n   590→        hint: 'Ensure transcription returns words array with start/end times'\n   591→      });\n   592→    }\n   593→\n   594→    // Parse custom lists\n   595→    const blocklist = parseWordList(customBlocklist);\n   596→    const allowlist = parseWordList(customAllowlist);\n   597→\n   598→    // Detect profanity\n   599→    const result = detectProfanity(transcript, {\n   600→      language,\n   601→      customBlocklist: blocklist,\n   602→      customAllowlist: allowlist,\n   603→      frameRate\n   604→    });\n   605→\n   606→    // Deduct usage based on audio duration\n   607→    const audioDuration = transcript.duration || 0;\n   608→    let balance = null;\n   609→    if (audioDuration > 0 && req.deductUsage) {\n   610→      balance = await req.deductUsage(audioDuration);\n   611→    }\n   612→\n   613→    res.json({\n   614→      success: true,\n   615→      wavPath,\n   616→      ...result,\n   617→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   618→    });\n   619→  } catch (err) {\n   620→    console.error('[SPLICE] Profanity detection error:', err);\n   621→    res.status(500).json({ error: err.message });\n   622→  }\n   623→});\n   624→\n   625→/**\n   626→ * GET /profanity/languages - Get supported languages\n   627→ */\n   628→app.get('/profanity/languages', (req, res) => {\n   629→  res.json({\n   630→    success: true,\n   631→    languages: getSupportedLanguages()\n   632→  });\n   633→});\n   634→\n   635→/**\n   636→ * GET /profanity/bleeps - Get available bleep sounds\n   637→ */\n   638→app.get('/profanity/bleeps', (req, res) => {\n   639→  res.json({\n   640→    success: true,\n   641→    sounds: getAvailableBleepSounds()\n   642→  });\n   643→});\n   644→\n   645→/**\n   646→ * GET /profanity/list/:language - Get default profanity list for a language\n   647→ */\n   648→app.get('/profanity/list/:language', (req, res) => {\n   649→  const { language } = req.params;\n   650→  const list = getProfanityList(language);\n   651→\n   652→  res.json({\n   653→    success: true,\n   654→    language,\n   655→    wordCount: list.length,\n   656→    // Return first 50 words as sample (full list is large)\n   657→    sample: list.slice(0, 50),\n   658→    note: 'Full list available but truncated for response size'\n   659→  });\n   660→});\n   661→\n   662→// =============================================================================\n   663→// Repetition/Stutter Detection Routes\n   664→// =============================================================================\n   665→\n   666→/**\n   667→ * POST /repetitions - Detect phrase repetitions and stutters\n   668→ *\n   669→ * Analyzes transcript for repeated phrases and stutters.\n   670→ * Returns segments that can be removed to clean up the edit.\n   671→ *\n   672→ * Options:\n   673→ * - wavPath: Path to audio file (required unless transcript provided)\n   674→ * - transcript: Pre-existing transcript (optional)\n   675→ * - phraseSize: Words per comparison window (default: 5)\n   676→ * - tolerance: Similarity threshold 0-1 (default: 0.7)\n   677→ * - searchRadius: Words to search ahead (default: 100)\n   678→ * - useOpenAI: Use OpenAI for boundary refinement (default: false)\n   679→ * - includeStutters: Also detect single-word stutters (default: true)\n   680→ */\n   681→app.post('/repetitions', requireCredits({ endpoint: 'repetitions' }), async (req, res) => {\n   682→  const {\n   683→    wavPath,\n   684→    transcript: providedTranscript,\n   685→    phraseSize = 5,\n   686→    tolerance = 0.7,\n   687→    searchRadius = 100,\n   688→    useOpenAI = false,\n   689→    includeStutters = true\n   690→  } = req.body;\n   691→\n   692→  if (!wavPath && !providedTranscript) {\n   693→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   694→  }\n   695→\n   696→  if (wavPath && !fs.existsSync(wavPath)) {\n   697→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   698→  }\n   699→\n   700→  console.log(`[SPLICE] Repetition detection: ${wavPath || 'provided transcript'}`);\n   701→\n   702→  try {\n   703→    // Get or create transcript with word-level timestamps\n   704→    let transcript = providedTranscript;\n   705→    if (!transcript && wavPath) {\n   706→      // Use transcribeWithWords for word-level timestamps required by repetition detection\n   707→      transcript = await transcribeWithWords(wavPath);\n   708→    }\n   709→\n   710→    // Validate transcript has words\n   711→    if (!transcript || !transcript.words || transcript.words.length === 0) {\n   712→      return res.status(400).json({\n   713→        error: 'Transcript must contain word-level timing data'\n   714→      });\n   715→    }\n   716→\n   717→    // Detect all repetitions (phrases + stutters)\n   718→    const result = await detectAllRepetitions(transcript, {\n   719→      phraseSize,\n   720→      tolerance,\n   721→      searchRadius,\n   722→      useOpenAI\n   723→    });\n   724→\n   725→    // Optionally filter out stutters\n   726→    if (!includeStutters) {\n   727→      result.stutters = [];\n   728→      result.removalSegments = result.removalSegments.filter(s => s.type !== 'stutter');\n   729→    }\n   730→\n   731→    // Deduct usage based on audio duration\n   732→    const audioDuration = transcript.duration || 0;\n   733→    let balance = null;\n   734→    if (audioDuration > 0 && req.deductUsage) {\n   735→      balance = await req.deductUsage(audioDuration);\n   736→    }\n   737→\n   738→    res.json({\n   739→      success: true,\n   740→      wavPath,\n   741→      ...result,\n   742→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   743→    });\n   744→  } catch (err) {\n   745→    console.error('[SPLICE] Repetition detection error:', err);\n   746→    res.status(500).json({ error: err.message });\n   747→  }\n   748→});\n   749→\n   750→/**\n   751→ * POST /fillers - Detect filler words (um, uh, like, etc.)\n   752→ *\n   753→ * Transcribes audio and identifies filler words with timestamps.\n   754→ * Returns segments that can be cut or reviewed for removal.\n   755→ *\n   756→ * Options:\n   757→ * - wavPath: Path to audio file (required unless transcript provided)\n   758→ * - transcript: Pre-existing transcript with word-level timing (optional)\n   759→ * - customFillers: Additional filler words to detect (optional)\n   760→ */\n   761→app.post('/fillers', requireCredits({ endpoint: 'fillers' }), async (req, res) => {\n   762→  const {\n   763→    wavPath,\n   764→    transcript: providedTranscript,\n   765→    customFillers = []\n   766→  } = req.body;\n   767→\n   768→  if (!wavPath && !providedTranscript) {\n   769→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   770→  }\n   771→\n   772→  if (wavPath && !fs.existsSync(wavPath)) {\n   773→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   774→  }\n   775→\n   776→  console.log(`[SPLICE] Filler word detection: ${wavPath || 'provided transcript'}`);\n   777→\n   778→  try {\n   779→    // Get or create transcript with word-level timestamps\n   780→    let transcript = providedTranscript;\n   781→    if (!transcript && wavPath) {\n   782→      transcript = await transcribeWithWords(wavPath);\n   783→    }\n   784→\n   785→    // Validate transcript has words\n   786→    if (!transcript || !transcript.words || transcript.words.length === 0) {\n   787→      return res.status(400).json({\n   788→        error: 'Transcript must contain word-level timing data'\n   789→      });\n   790→    }\n   791→\n   792→    // Default filler words (common in English speech)\n   793→    const defaultFillers = [\n   794→      'um', 'uh', 'ah', 'er', 'eh',           // Hesitation sounds\n   795→      'like', 'so', 'well', 'right',           // Discourse markers\n   796→      'you know', 'i mean', 'basically',       // Filler phrases\n   797→      'actually', 'literally', 'honestly',     // Overused qualifiers\n   798→      'kind of', 'sort of', 'you see'          // Hedging phrases\n   799→    ];\n   800→\n   801→    // Combine default + custom fillers (lowercase for matching)\n   802→    const fillerSet = new Set([\n   803→      ...defaultFillers,\n   804→      ...customFillers.map(f => f.toLowerCase().trim())\n   805→    ]);\n   806→\n   807→    // Detect filler words\n   808→    const fillers = [];\n   809→    const words = transcript.words;\n   810→\n   811→    for (let i = 0; i < words.length; i++) {\n   812→      const word = words[i];\n   813→      const normalizedWord = word.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim();\n   814→\n   815→      // Check single-word fillers\n   816→      if (fillerSet.has(normalizedWord)) {\n   817→        fillers.push({\n   818→          word: word.word,\n   819→          normalizedWord,\n   820→          start: word.start,\n   821→          end: word.end,\n   822→          duration: word.end - word.start,\n   823→          index: i,\n   824→          type: 'filler'\n   825→        });\n   826→        continue;\n   827→      }\n   828→\n   829→      // Check two-word phrases (e.g., \"you know\", \"kind of\")\n   830→      if (i < words.length - 1) {\n   831→        const nextWord = words[i + 1];\n   832→        const twoWordPhrase = `${normalizedWord} ${nextWord.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim()}`;\n   833→        if (fillerSet.has(twoWordPhrase)) {\n   834→          fillers.push({\n   835→            word: `${word.word} ${nextWord.word}`,\n   836→            normalizedWord: twoWordPhrase,\n   837→            start: word.start,\n   838→            end: nextWord.end,\n   839→            duration: nextWord.end - word.start,\n   840→            index: i,\n   841→            type: 'filler_phrase'\n   842→          });\n   843→          // Skip next word since it's part of this phrase\n   844→          i++;\n   845→        }\n   846→      }\n   847→    }\n   848→\n   849→    // Calculate total filler time\n   850→    const totalFillerDuration = fillers.reduce((sum, f) => sum + f.duration, 0);\n   851→    const audioDuration = transcript.duration || (words.length > 0 ? words[words.length - 1].end : 0);\n   852→    const fillerPercentage = audioDuration > 0 ? (totalFillerDuration / audioDuration) * 100 : 0;\n   853→\n   854→    // Deduct usage based on audio duration\n   855→    let balance = null;\n   856→    if (audioDuration > 0 && req.deductUsage) {\n   857→      balance = await req.deductUsage(audioDuration);\n   858→    }\n   859→\n   860→    res.json({\n   861→      success: true,\n   862→      wavPath,\n   863→      fillers,\n   864→      metadata: {\n   865→        totalWords: words.length,\n   866→        fillerCount: fillers.length,\n   867→        totalFillerDuration: parseFloat(totalFillerDuration.toFixed(3)),\n   868→        audioDuration: parseFloat(audioDuration.toFixed(3)),\n   869→        fillerPercentage: parseFloat(fillerPercentage.toFixed(2)),\n   870→        fillersPerMinute: audioDuration > 0 ? parseFloat((fillers.length / (audioDuration / 60)).toFixed(2)) : 0\n   871→      },\n   872→      removalSegments: fillers.map(f => ({\n   873→        start: f.start,\n   874→        end: f.end,\n   875→        duration: f.duration,\n   876→        reason: `Filler: \"${f.word}\"`,\n   877→        type: f.type\n   878→      })),\n   879→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   880→    });\n   881→  } catch (err) {\n   882→    console.error('[SPLICE] Filler detection error:', err);\n   883→    res.status(500).json({ error: err.message });\n   884→  }\n   885→});\n   886→\n   887→/**\n   888→ * POST /stutters - Detect single-word stutters only\n   889→ *\n   890→ * Focused detection for word-level stutters (e.g., \"I I I think\").\n   891→ * Faster than full repetition detection.\n   892→ */\n   893→app.post('/stutters', requireCredits({ endpoint: 'stutters' }), async (req, res) => {\n   894→  const {\n   895→    wavPath,\n   896→    transcript: providedTranscript,\n   897→    options = {},\n   898→    // Support both top-level and nested options for flexibility\n   899→    minRepeats = options.minRepeats ?? 2,\n   900→    maxGapMs = options.maxGapMs ?? 500,\n   901→    ignoreFillers = options.ignoreFillers ?? true,\n   902→    minWordLength = options.minWordLength ?? 1\n   903→  } = req.body;\n   904→\n   905→  if (!wavPath && !providedTranscript) {\n   906→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   907→  }\n   908→\n   909→  if (wavPath && !fs.existsSync(wavPath)) {\n   910→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   911→  }\n   912→\n   913→  console.log(`[SPLICE] Stutter detection: ${wavPath || 'provided transcript'}`);\n   914→\n   915→  try {\n   916→    // Get or create transcript with word-level timestamps\n   917→    let transcript = providedTranscript;\n   918→    if (!transcript && wavPath) {\n   919→      // Use transcribeWithWords for word-level timestamps required by stutter detection\n   920→      transcript = await transcribeWithWords(wavPath);\n   921→    }\n   922→\n   923→    // Validate transcript exists and has words array\n   924→    if (!transcript || !transcript.words) {\n   925→      return res.status(400).json({\n   926→        error: 'Transcript must contain word-level timing data'\n   927→      });\n   928→    }\n   929→\n   930→    // Empty transcript returns empty result (not an error)\n   931→    if (transcript.words.length === 0) {\n   932→      return res.json({\n   933→        success: true,\n   934→        stutters: [],\n   935→        metadata: { type: 'stutters', totalWords: 0, stutterCount: 0, totalRepeatedWords: 0 }\n   936→      });\n   937→    }\n   938→\n   939→    // Detect stutters only\n   940→    const result = detectStutters(transcript, {\n   941→      minRepeats,\n   942→      maxGapMs,\n   943→      ignoreFillers,\n   944→      minWordLength\n   945→    });\n   946→\n   947→    // Deduct usage based on audio duration\n   948→    const audioDuration = transcript.duration || 0;\n   949→    let balance = null;\n   950→    if (audioDuration > 0 && req.deductUsage) {\n   951→      balance = await req.deductUsage(audioDuration);\n   952→    }\n   953→\n   954→    res.json({\n   955→      success: true,\n   956→      wavPath,\n   957→      ...result,\n   958→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   959→    });\n   960→  } catch (err) {\n   961→    console.error('[SPLICE] Stutter detection error:', err);\n   962→    res.status(500).json({ error: err.message });\n   963→  }\n   964→});\n   965→\n   966→// =============================================================================\n   967→// Caption Export Routes\n   968→// =============================================================================\n   969→\n   970→/**\n   971→ * POST /export/captions - Export transcript to caption format (SRT, VTT, etc.)\n   972→ *\n   973→ * Converts a transcript to the specified caption format.\n   974→ * Can optionally save to file.\n   975→ *\n   976→ * Options:\n   977→ * - wavPath: Path to audio file (to transcribe first)\n   978→ * - transcript: Pre-existing transcript with word-level timing\n   979→ * - format: Export format (srt, vtt, txt, json) - default: srt\n   980→ * - outputPath: Optional file path to save to\n   981→ * - maxWordsPerCaption: Max words per caption (default: 8)\n   982→ * - maxDuration: Max duration per caption in seconds (default: 5)\n   983→ */\n   984→app.post('/export/captions', requireCredits({ endpoint: 'export-captions' }), async (req, res) => {\n   985→  const {\n   986→    wavPath,\n   987→    transcript: providedTranscript,\n   988→    format = 'srt',\n   989→    outputPath = null,\n   990→    maxWordsPerCaption = 8,\n   991→    maxDuration = 5\n   992→  } = req.body;\n   993→\n   994→  if (!wavPath && !providedTranscript) {\n   995→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   996→  }\n   997→\n   998→  if (wavPath && !fs.existsSync(wavPath)) {\n   999→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  1000→  }\n  1001→\n  1002→  console.log(`[SPLICE] Caption export: ${wavPath || 'provided transcript'} -> ${format}`);\n  1003→\n  1004→  try {\n  1005→    // Get or create transcript with word-level timestamps\n  1006→    let transcript = providedTranscript;\n  1007→    if (!transcript && wavPath) {\n  1008→      transcript = await transcribeWithWords(wavPath);\n  1009→    }\n  1010→\n  1011→    const exportOptions = { maxWordsPerCaption, maxDuration };\n  1012→\n  1013→    // Generate caption content based on format\n  1014→    let content;\n  1015→    let mimeType;\n  1016→\n  1017→    switch (format.toLowerCase()) {\n  1018→      case 'srt':\n  1019→        content = toSRT(transcript, exportOptions);\n  1020→        mimeType = 'application/x-subrip';\n  1021→        break;\n  1022→      case 'vtt':\n  1023→      case 'webvtt':\n  1024→        content = toVTT(transcript, exportOptions);\n  1025→        mimeType = 'text/vtt';\n  1026→        break;\n  1027→      case 'txt':\n  1028→      case 'text':\n  1029→        content = toPlainText(transcript, { ...exportOptions, includeTimestamps: true });\n  1030→        mimeType = 'text/plain';\n  1031→        break;\n  1032→      case 'json':\n  1033→        content = toJSON(transcript);\n  1034→        mimeType = 'application/json';\n  1035→        break;\n  1036→      default:\n  1037→        return res.status(400).json({\n  1038→          error: `Unsupported format: ${format}`,\n  1039→          supportedFormats: getSupportedFormats()\n  1040→        });\n  1041→    }\n  1042→\n  1043→    // Save to file if outputPath provided\n  1044→    let savedPath = null;\n  1045→    if (outputPath) {\n  1046→      const result = await exportToFile(transcript, outputPath, format, exportOptions);\n  1047→      savedPath = result.path;\n  1048→      console.log(`[SPLICE] Saved captions to: ${savedPath}`);\n  1049→    }\n  1050→\n  1051→    // Deduct usage based on audio duration\n  1052→    const audioDuration = transcript.duration || 0;\n  1053→    let balance = null;\n  1054→    if (audioDuration > 0 && req.deductUsage) {\n  1055→      balance = await req.deductUsage(audioDuration);\n  1056→    }\n  1057→\n  1058→    res.json({\n  1059→      success: true,\n  1060→      format,\n  1061→      content,\n  1062→      mimeType,\n  1063→      savedPath,\n  1064→      wordCount: transcript.words?.length || 0,\n  1065→      duration: transcript.duration || 0,\n  1066→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n  1067→    });\n  1068→  } catch (err) {\n  1069→    console.error('[SPLICE] Caption export error:', err);\n  1070→    res.status(500).json({ error: err.message });\n  1071→  }\n  1072→});\n  1073→\n  1074→/**\n  1075→ * GET /export/formats - Get supported export formats\n  1076→ */\n  1077→app.get('/export/formats', (req, res) => {\n  1078→  res.json({\n  1079→    success: true,\n  1080→    formats: getSupportedFormats()\n  1081→  });\n  1082→});\n  1083→\n  1084→// =============================================================================\n  1085→// Multitrack/Multicam Analysis Routes\n  1086→// =============================================================================\n  1087→\n  1088→/**\n  1089→ * POST /multitrack - Analyze multiple audio tracks for multicam editing\n  1090→ *\n  1091→ * Analyzes audio levels across multiple tracks to determine optimal\n  1092→ * video angle selection based on who is speaking.\n  1093→ *\n  1094→ * Options:\n  1095→ * - audioPaths: Array of paths to audio files (one per speaker) - required\n  1096→ * - speakerNames: Array of speaker names (optional)\n  1097→ * - videoTrackMapping: Object mapping speaker index to video track { 0: 0, 1: 1 }\n  1098→ * - minShotDuration: Minimum seconds before next cut (default: 2.0)\n  1099→ * - switchingFrequency: How often to allow cuts 0-100 (default: 50)\n  1100→ * - wideShotEnabled: Enable wide shot detection (default: true)\n  1101→ * - wideShotPercentage: Target % of wide shots (default: 20)\n  1102→ * - wideShotTracks: Video track indices for wide shots\n  1103→ * - cutawayEnabled: Enable cutaway insertion (default: false)\n  1104→ * - cutawayTracks: Video track indices for cutaways\n  1105→ * - speakerBoosts: Per-speaker dB adjustments { \"Speaker 1\": 5 }\n  1106→ */\n  1107→app.post('/multitrack', requireCredits({ endpoint: 'multitrack' }), async (req, res) => {\n  1108→  const {\n  1109→    audioPaths,\n  1110→    speakerNames,\n  1111→    videoTrackMapping = {},\n  1112→    minShotDuration = 2.0,\n  1113→    switchingFrequency = 50,\n  1114→    wideShotEnabled = true,\n  1115→    wideShotPercentage = 20,\n  1116→    wideShotTracks = [],\n  1117→    cutawayEnabled = false,\n  1118→    cutawayTracks = [],\n  1119→    speakerBoosts = {},\n  1120→    frameRate = 30\n  1121→  } = req.body;\n  1122→\n  1123→  // Validate audioPaths\n  1124→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length === 0) {\n  1125→    return res.status(400).json({ error: 'audioPaths array is required (at least 1 path)' });\n  1126→  }\n  1127→\n  1128→  // Validate all files exist\n  1129→  for (const audioPath of audioPaths) {\n  1130→    if (!fs.existsSync(audioPath)) {\n  1131→      return res.status(404).json({ error: `File not found: ${audioPath}` });\n  1132→    }\n  1133→  }\n  1134→\n  1135→  // Check FFprobe availability\n  1136→  const ffprobeAvailable = await isFFprobeInstalled();\n  1137→  if (!ffprobeAvailable) {\n  1138→    return res.status(500).json({\n  1139→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n  1140→    });\n  1141→  }\n  1142→\n  1143→  console.log(`[SPLICE] Multitrack analysis: ${audioPaths.length} track(s)`);\n  1144→\n  1145→  try {\n  1146→    const result = await analyzeMultitrack(audioPaths, {\n  1147→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n  1148→      videoTrackMapping,\n  1149→      minShotDuration,\n  1150→      switchingFrequency,\n  1151→      wideShotEnabled,\n  1152→      wideShotPercentage,\n  1153→      wideShotTracks,\n  1154→      cutawayEnabled,\n  1155→      cutawayTracks,\n  1156→      speakerBoosts,\n  1157→      frameRate\n  1158→    });\n  1159→\n  1160→    // Deduct usage based on total duration (use longest track)\n  1161→    const audioDuration = result.metadata?.totalDuration || 0;\n  1162→    let balance = null;\n  1163→    if (audioDuration > 0 && req.deductUsage) {\n  1164→      balance = await req.deductUsage(audioDuration);\n  1165→    }\n  1166→\n  1167→    res.json({\n  1168→      success: true,\n  1169→      ...result,\n  1170→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n  1171→    });\n  1172→  } catch (err) {\n  1173→    console.error('[SPLICE] Multitrack analysis error:', err);\n  1174→    res.status(500).json({ error: err.message });\n  1175→  }\n  1176→});\n  1177→\n  1178→/**\n  1179→ * POST /multitrack/auto-balance - Auto-balance speaker screentime\n  1180→ *\n  1181→ * Automatically adjusts speaker boosts to achieve equal screentime distribution.\n  1182→ * Runs multiple iterations to find optimal parameters.\n  1183→ */\n  1184→app.post('/multitrack/auto-balance', requireCredits({ endpoint: 'multitrack-auto-balance' }), async (req, res) => {\n  1185→  const {\n  1186→    audioPaths,\n  1187→    speakerNames,\n  1188→    videoTrackMapping = {},\n  1189→    minShotDuration = 2.0,\n  1190→    switchingFrequency = 50,\n  1191→    wideShotEnabled = false, // Disable wide shots for balance calc\n  1192→    frameRate = 30\n  1193→  } = req.body;\n  1194→\n  1195→  // Validate audioPaths\n  1196→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length < 2) {\n  1197→    return res.status(400).json({ error: 'audioPaths array requires at least 2 tracks for balancing' });\n  1198→  }\n  1199→\n  1200→  // Validate all files exist\n  1201→  for (const audioPath of audioPaths) {\n  1202→    if (!fs.existsSync(audioPath)) {\n  1203→      return res.status(404).json({ error: `File not found: ${audioPath}` });\n  1204→    }\n  1205→  }\n  1206→\n  1207→  // Check FFprobe availability\n  1208→  const ffprobeAvailable = await isFFprobeInstalled();\n  1209→  if (!ffprobeAvailable) {\n  1210→    return res.status(500).json({\n  1211→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n  1212→    });\n  1213→  }\n  1214→\n  1215→  console.log(`[SPLICE] Auto-balancing multitrack: ${audioPaths.length} track(s)`);\n  1216→\n  1217→  try {\n  1218→    const result = await autoBalanceMultitrack(audioPaths, {\n  1219→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n  1220→      videoTrackMapping,\n  1221→      minShotDuration,\n  1222→      switchingFrequency,\n  1223→      wideShotEnabled,\n  1224→      frameRate\n  1225→    });\n  1226→\n  1227→    // Deduct usage based on total duration\n  1228→    const audioDuration = result.metadata?.totalDuration || 0;\n  1229→    let balance = null;\n  1230→    if (audioDuration > 0 && req.deductUsage) {\n  1231→      balance = await req.deductUsage(audioDuration);\n  1232→    }\n  1233→\n  1234→    res.json({\n  1235→      success: true,\n  1236→      ...result,\n  1237→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n  1238→    });\n  1239→  } catch (err) {\n  1240→    console.error('[SPLICE] Auto-balance error:', err);\n  1241→    res.status(500).json({ error: err.message });\n  1242→  }\n  1243→});\n  1244→\n  1245→/**\n  1246→ * POST /process-xml - Process FCP XML to split clips at silences\n  1247→ *\n  1248→ * Takes an FCP XML file and silence timestamps, splits clips\n  1249→ * at silence boundaries, and optionally removes gaps.\n  1250→ */\n  1251→app.post('/process-xml', async (req, res) => {\n  1252→  const {\n  1253→    xmlPath,\n  1254→    silences,\n  1255→    removeGaps = true,\n  1256→    outputPath = null\n  1257→  } = req.body;\n  1258→\n  1259→  if (!xmlPath) {\n  1260→    return res.status(400).json({ error: 'xmlPath is required' });\n  1261→  }\n  1262→\n  1263→  if (!silences || !Array.isArray(silences)) {\n  1264→    return res.status(400).json({ error: 'silences array is required' });\n  1265→  }\n  1266→\n  1267→  if (!fs.existsSync(xmlPath)) {\n  1268→    return res.status(404).json({ error: `XML file not found: ${xmlPath}` });\n  1269→  }\n  1270→\n  1271→  console.log(`[SPLICE] Processing XML: ${xmlPath} with ${silences.length} silence(s)`);\n  1272→\n  1273→  try {\n  1274→    const result = await processXMLFile(xmlPath, silences, {\n  1275→      outputPath,\n  1276→      removeGaps\n  1277→    });\n  1278→\n  1279→    res.json({\n  1280→      success: true,\n  1281→      inputPath: xmlPath,\n  1282→      outputPath: result.outputPath,\n  1283→      stats: result.stats\n  1284→    });\n  1285→  } catch (err) {\n  1286→    console.error('[SPLICE] XML processing error:', err);\n  1287→    res.status(500).json({ error: err.message });\n  1288→  }\n  1289→});\n  1290→\n  1291→/**\n  1292→ * POST /cut-list - Generate a JSON cut list for direct DOM building (v3.5)\n  1293→ *\n  1294→ * Takes silences and optionally takes, returns a cut list that the\n  1295→ * plugin can use to build sequences directly via UXP APIs.\n  1296→ *\n  1297→ * Body:\n  1298→ * - sourceName: Name of the source clip\n  1299→ * - sourcePath: Full path to the source file\n  1300→ * - duration: Total duration in seconds\n  1301→ * - silences: Array of silence segments [{start, end, duration}]\n  1302→ * - takes: (optional) Array of detected takes\n  1303→ * - settings: (optional) Generation settings\n  1304→ */\n  1305→app.post('/cut-list', async (req, res) => {\n  1306→  const {\n  1307→    sourceName,\n  1308→    sourcePath,\n  1309→    duration,\n  1310→    silences,\n  1311→    takes = [],\n  1312→    settings = {}\n  1313→  } = req.body;\n  1314→\n  1315→  // Validate required fields\n  1316→  if (!sourceName && !sourcePath) {\n  1317→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });\n  1318→  }\n  1319→\n  1320→  if (typeof duration !== 'number' || duration <= 0) {\n  1321→    return res.status(400).json({ error: 'duration must be a positive number' });\n  1322→  }\n  1323→\n  1324→  if (!silences || !Array.isArray(silences)) {\n  1325→    return res.status(400).json({ error: 'silences array is required' });\n  1326→  }\n  1327→\n  1328→  console.log(`[SPLICE] Generating cut list for ${sourceName || sourcePath} (${silences.length} silences)`);\n  1329→\n  1330→  try {\n  1331→    const cutList = generateCutList({\n  1332→      sourceName: sourceName || path.basename(sourcePath),\n  1333→      sourcePath,\n  1334→      duration,\n  1335→      silences,\n  1336→      takes,\n  1337→      settings\n  1338→    });\n  1339→\n  1340→    // Validate the generated cut list\n  1341→    const validation = validateCutList(cutList);\n  1342→    if (!validation.valid) {\n  1343→      return res.status(500).json({\n  1344→        error: 'Generated cut list is invalid',\n  1345→        validationErrors: validation.errors\n  1346→      });\n  1347→    }\n  1348→\n  1349→    res.json({\n  1350→      success: true,\n  1351→      cutList\n  1352→    });\n  1353→  } catch (err) {\n  1354→    console.error('[SPLICE] Cut list generation error:', err);\n  1355→    res.status(500).json({ error: err.message });\n  1356→  }\n  1357→});\n  1358→\n  1359→/**\n  1360→ * POST /cut-list/takes - Generate a cut list that keeps only takes\n  1361→ *\n  1362→ * Alternative endpoint for \"keep best takes only\" workflow.\n  1363→ */\n  1364→app.post('/cut-list/takes', async (req, res) => {\n  1365→  const {\n  1366→    sourceName,\n  1367→    sourcePath,\n  1368→    duration,\n  1369→    takes,\n  1370→    settings = {}\n  1371→  } = req.body;\n  1372→\n  1373→  // Validate required fields\n  1374→  if (!sourceName && !sourcePath) {\n  1375→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });\n  1376→  }\n  1377→\n  1378→  if (typeof duration !== 'number' || duration <= 0) {\n  1379→    return res.status(400).json({ error: 'duration must be a positive number' });\n  1380→  }\n  1381→\n  1382→  if (!takes || !Array.isArray(takes) || takes.length === 0) {\n  1383→    return res.status(400).json({ error: 'takes array is required and must not be empty' });\n  1384→  }\n  1385→\n  1386→  console.log(`[SPLICE] Generating takes cut list for ${sourceName || sourcePath} (${takes.length} takes)`);\n  1387→\n  1388→  try {\n  1389→    const cutList = generateTakesCutList({\n  1390→      sourceName: sourceName || path.basename(sourcePath),\n  1391→      sourcePath,\n  1392→      duration,\n  1393→      takes,\n  1394→      settings\n  1395→    });\n  1396→\n  1397→    res.json({\n  1398→      success: true,\n  1399→      cutList\n  1400→    });\n  1401→  } catch (err) {\n  1402→    console.error('[SPLICE] Takes cut list generation error:', err);\n  1403→    res.status(500).json({ error: err.message });\n  1404→  }\n  1405→});\n  1406→\n  1407→/**\n  1408→ * GET /ffprobe-check - Check if FFprobe is installed\n  1409→ */\n  1410→app.get('/ffprobe-check', async (req, res) => {\n  1411→  const installed = await isFFprobeInstalled();\n  1412→  res.json({\n  1413→    installed,\n  1414→    message: installed\n  1415→      ? 'FFprobe is available'\n  1416→      : 'FFprobe not found. Install with: brew install ffmpeg'\n  1417→  });\n  1418→});\n  1419→\n  1420→/**\n  1421→ * GET /replicate-check - Check if Replicate API is configured\n  1422→ */\n  1423→app.get('/replicate-check', async (req, res) => {\n  1424→  const configured = isReplicateConfigured();\n  1425→  res.json({\n  1426→    configured,\n  1427→    message: configured\n  1428→      ? 'Replicate API is configured'\n  1429→      : 'REPLICATE_API_TOKEN not set. Add to .env file.'\n  1430→  });\n  1431→});\n  1432→\n  1433→/**\n  1434→ * POST /isolate-vocals - Isolate vocals from audio using Demucs\n  1435→ *\n  1436→ * Uses Replicate's Demucs model to separate vocals from background audio.\n  1437→ * Cost: ~$0.015/min of audio\n  1438→ *\n  1439→ * Tier access:\n  1440→ * - Starter: No access (upgrade required)\n  1441→ * - Pro: 2 hours included, then $0.08/min overage\n  1442→ * - Team: 5 hours included, then $0.08/min overage\n  1443→ */\n  1444→app.post('/isolate-vocals', requireCredits({ endpoint: 'isolate-vocals' }), async (req, res) => {\n  1445→  const { audioPath, stem = 'vocals', outputDir = null } = req.body;\n  1446→  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n  1447→\n  1448→  if (!audioPath) {\n  1449→    return res.status(400).json({ error: 'audioPath is required' });\n  1450→  }\n  1451→\n  1452→  if (!fs.existsSync(audioPath)) {\n  1453→    return res.status(404).json({ error: `File not found: ${audioPath}` });\n  1454→  }\n  1455→\n  1456→  // Check Replicate configuration\n  1457→  if (!isReplicateConfigured()) {\n  1458→    return res.status(500).json({\n  1459→      error: 'Replicate API not configured. Set REPLICATE_API_TOKEN in .env'\n  1460→    });\n  1461→  }\n  1462→\n  1463→  // Get audio duration for billing\n  1464→  let audioDurationSeconds = 0;\n  1465→  try {\n  1466→    audioDurationSeconds = await getAudioDuration(audioPath);\n  1467→  } catch (err) {\n  1468→    console.warn('[SPLICE] Could not get audio duration:', err.message);\n  1469→  }\n  1470→\n  1471→  const audioDurationMinutes = audioDurationSeconds / 60;\n  1472→\n  1473→  // Check isolation access if customer ID provided\n  1474→  if (stripeCustomerId) {\n  1475→    const accessCheck = await usageTracking.checkIsolationAccess(stripeCustomerId, audioDurationMinutes);\n  1476→\n  1477→    if (!accessCheck.allowed) {\n  1478→      return res.status(403).json({\n  1479→        error: accessCheck.message,\n  1480→        reason: accessCheck.reason,\n  1481→        upgradeRequired: accessCheck.reason === 'upgrade_required'\n  1482→      });\n  1483→    }\n  1484→\n  1485→    console.log(`[SPLICE] Isolation access: ${accessCheck.message}`);\n  1486→  }\n  1487→\n  1488→  console.log(`[SPLICE] Isolating vocals: ${audioPath} (${audioDurationMinutes.toFixed(1)} min)`);\n  1489→\n  1490→  try {\n  1491→    const result = await isolateVocals(audioPath, {\n  1492→      stem,\n  1493→      outputDir: outputDir || undefined\n  1494→    });\n  1495→\n  1496→    // Deduct isolation usage if customer ID provided\n  1497→    let usageInfo = null;\n  1498→    if (stripeCustomerId) {\n  1499→      usageInfo = await usageTracking.deductIsolationUsage(\n  1500→        stripeCustomerId,\n  1501→        audioDurationSeconds,\n  1502→        'isolate-vocals'\n  1503→      );\n  1504→      console.log(`[SPLICE] Isolation usage deducted: ${audioDurationMinutes.toFixed(1)} min`);\n  1505→      if (usageInfo.isolationUsed?.overageCost > 0) {\n  1506→        console.log(`[SPLICE] Overage cost: $${usageInfo.isolationUsed.overageCost.toFixed(2)}`);\n  1507→      }\n  1508→    }\n  1509→\n  1510→    res.json({\n  1511→      success: true,\n  1512→      inputPath: audioPath,\n  1513→      outputPath: result.outputPath,\n  1514→      stem: result.stem,\n  1515→      processingTime: result.processingTime,\n  1516→      availableStems: result.allStems,\n  1517→      audioDurationMinutes,\n  1518→      usage: usageInfo ? {\n  1519→        isolationHoursRemaining: usageInfo.isolationHoursRemaining,\n  1520→        overageCost: usageInfo.isolationUsed?.overageCost || 0\n  1521→      } : null\n  1522→    });\n  1523→  } catch (err) {\n  1524→    console.error('[SPLICE] Vocal isolation error:', err);\n  1525→    res.status(500).json({ error: err.message });\n  1526→  }\n  1527→});\n  1528→\n  1529→// =============================================================================\n  1530→// Batch Processing Routes\n  1531→// =============================================================================\n  1532→\n  1533→// In-memory job queue for batch processing\n  1534→const batchJobs = new Map();\n  1535→\n  1536→// Batch job limits to prevent memory leak\n  1537→const MAX_BATCH_JOBS = 10000;\n  1538→const BATCH_JOB_MAX_AGE_MS = 24 * 60 * 60 * 1000; // 24 hours\n  1539→\n  1540→/**\n  1541→ * Generate a unique job ID\n  1542→ */\n  1543→function generateJobId() {\n  1544→  return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  1545→}\n  1546→\n  1547→/**\n  1548→ * Clean up old batch jobs to prevent memory leak\n  1549→ * Removes jobs older than 24 hours\n  1550→ */\n  1551→function cleanupOldBatchJobs() {\n  1552→  const now = Date.now();\n  1553→  let removedCount = 0;\n  1554→\n  1555→  for (const [jobId, job] of batchJobs.entries()) {\n  1556→    const createdAt = new Date(job.createdAt).getTime();\n  1557→    if (now - createdAt > BATCH_JOB_MAX_AGE_MS) {\n  1558→      batchJobs.delete(jobId);\n  1559→      removedCount++;\n  1560→    }\n  1561→  }\n  1562→\n  1563→  if (removedCount > 0) {\n  1564→    console.log(`[SPLICE] Cleaned up ${removedCount} old batch job(s)`);\n  1565→  }\n  1566→}\n  1567→\n  1568→/**\n  1569→ * Enforce max job limit by removing oldest completed jobs\n  1570→ */\n  1571→function enforceJobLimit() {\n  1572→  if (batchJobs.size < MAX_BATCH_JOBS) return;\n  1573→\n  1574→  // Get completed jobs sorted by creation date (oldest first)\n  1575→  const completedJobs = Array.from(batchJobs.entries())\n  1576→    .filter(([_, job]) => job.status !== 'processing')\n  1577→    .sort((a, b) => new Date(a[1].createdAt) - new Date(b[1].createdAt));\n  1578→\n  1579→  // Remove oldest completed jobs until under limit\n  1580→  const toRemove = batchJobs.size - MAX_BATCH_JOBS + 1;\n  1581→  for (let i = 0; i < Math.min(toRemove, completedJobs.length); i++) {\n  1582→    batchJobs.delete(completedJobs[i][0]);\n  1583→  }\n  1584→\n  1585→  console.log(`[SPLICE] Enforced job limit, removed ${Math.min(toRemove, completedJobs.length)} job(s)`);\n  1586→}\n  1587→\n  1588→// Run cleanup every hour\n  1589→setInterval(cleanupOldBatchJobs, 60 * 60 * 1000);\n  1590→\n  1591→/**\n  1592→ * POST /batch/silences - Process multiple files for silence detection\n  1593→ *\n  1594→ * Creates a batch job that processes multiple audio files.\n  1595→ * Returns a job ID for tracking progress.\n  1596→ *\n  1597→ * Body:\n  1598→ * - files: Array of file paths to process\n  1599→ * - options: Detection options (sensitivity, threshold, etc.)\n  1600→ */\n  1601→app.post('/batch/silences', requireCredits({ endpoint: 'batch-silences' }), async (req, res) => {\n  1602→  const { files, options = {} } = req.body;\n  1603→\n  1604→  if (!files || !Array.isArray(files) || files.length === 0) {\n  1605→    return res.status(400).json({ error: 'files array is required' });\n  1606→  }\n  1607→\n  1608→  // Validate all files exist\n  1609→  const missingFiles = files.filter(f => !fs.existsSync(f));\n  1610→  if (missingFiles.length > 0) {\n  1611→    return res.status(404).json({\n  1612→      error: 'Some files not found',\n  1613→      missingFiles\n  1614→    });\n  1615→  }\n  1616→\n  1617→  // Enforce job limit before creating new job\n  1618→  enforceJobLimit();\n  1619→\n  1620→  const jobId = generateJobId();\n  1621→\n  1622→  // Initialize job with customer ID for usage tracking\n  1623→  const job = {\n  1624→    id: jobId,\n  1625→    type: 'silences',\n  1626→    status: 'processing',\n  1627→    createdAt: new Date().toISOString(),\n  1628→    stripeCustomerId: req.stripeCustomerId,  // Store for usage deduction\n  1629→    files: files.map(f => ({\n  1630→      path: f,\n  1631→      status: 'pending',\n  1632→      result: null,\n  1633→      error: null\n  1634→    })),\n  1635→    options,\n  1636→    progress: {\n  1637→      total: files.length,\n  1638→      completed: 0,\n  1639→      failed: 0,\n  1640→      percentage: 0\n  1641→    },\n  1642→    results: [],\n  1643→    errors: [],\n  1644→    totalUsageDeducted: 0  // Track total seconds deducted\n  1645→  };\n  1646→\n  1647→  batchJobs.set(jobId, job);\n  1648→  console.log(`[SPLICE] Batch job ${jobId} created with ${files.length} files`);\n  1649→\n  1650→  // Start processing in background\n  1651→  processBatchJob(jobId);\n  1652→\n  1653→  res.json({\n  1654→    success: true,\n  1655→    jobId,\n  1656→    message: `Batch job created with ${files.length} files`,\n  1657→    statusUrl: `/batch/status/${jobId}`\n  1658→  });\n  1659→});\n  1660→\n  1661→/**\n  1662→ * Process a batch job (runs in background)\n  1663→ */\n  1664→async function processBatchJob(jobId) {\n  1665→  const job = batchJobs.get(jobId);\n  1666→  if (!job) return;\n  1667→\n  1668→  const { sensitivity, ...manualOptions } = job.options;\n  1669→\n  1670→  // Build detection options\n  1671→  let detectionOptions = {};\n  1672→  if (typeof sensitivity === 'number') {\n  1673→    detectionOptions = sensitivityToParams(sensitivity);\n  1674→  } else {\n  1675→    detectionOptions = {\n  1676→      threshold: manualOptions.threshold ?? -30,\n  1677→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n  1678→      paddingStart: manualOptions.paddingStart ?? 0.1,\n  1679→      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n  1680→      autoThreshold: manualOptions.autoThreshold ?? false\n  1681→    };\n  1682→  }\n  1683→\n  1684→  // Process files sequentially to avoid overwhelming the system\n  1685→  for (let i = 0; i < job.files.length; i++) {\n  1686→    const fileEntry = job.files[i];\n  1687→    fileEntry.status = 'processing';\n  1688→\n  1689→    try {\n  1690→      const result = await detectSilencesRMS(fileEntry.path, detectionOptions);\n  1691→\n  1692→      fileEntry.status = 'completed';\n  1693→      fileEntry.result = {\n  1694→        silences: result.silences,\n  1695→        count: result.silences.length,\n  1696→        totalSilenceDuration: result.metadata.totalSilenceDuration,\n  1697→        audioDuration: result.metadata.audioDuration\n  1698→      };\n  1699→\n  1700→      job.results.push({\n  1701→        file: fileEntry.path,\n  1702→        ...fileEntry.result\n  1703→      });\n  1704→\n  1705→      // Deduct usage for this file\n  1706→      const audioDuration = result.metadata?.audioDuration || 0;\n  1707→      if (audioDuration > 0 && job.stripeCustomerId) {\n  1708→        try {\n  1709→          await usageTracking.deductUsage(job.stripeCustomerId, audioDuration, 'batch-silences');\n  1710→          job.totalUsageDeducted += audioDuration;\n  1711→        } catch (usageErr) {\n  1712→          console.warn(`[SPLICE] Batch ${jobId}: Usage deduction failed:`, usageErr.message);\n  1713→        }\n  1714→      }\n  1715→\n  1716→      job.progress.completed++;\n  1717→      console.log(`[SPLICE] Batch ${jobId}: ${i + 1}/${job.files.length} completed`);\n  1718→    } catch (err) {\n  1719→      fileEntry.status = 'failed';\n  1720→      fileEntry.error = err.message;\n  1721→\n  1722→      job.errors.push({\n  1723→        file: fileEntry.path,\n  1724→        error: err.message\n  1725→      });\n  1726→\n  1727→      job.progress.failed++;\n  1728→      console.error(`[SPLICE] Batch ${jobId}: ${fileEntry.path} failed:`, err.message);\n  1729→    }\n  1730→\n  1731→    // Update progress\n  1732→    job.progress.percentage = Math.round(\n  1733→      ((job.progress.completed + job.progress.failed) / job.progress.total) * 100\n  1734→    );\n  1735→  }\n  1736→\n  1737→  // Mark job as complete\n  1738→  job.status = job.progress.failed === job.progress.total ? 'failed' :\n  1739→               job.progress.failed > 0 ? 'completed_with_errors' : 'completed';\n  1740→  job.completedAt = new Date().toISOString();\n  1741→\n  1742→  console.log(`[SPLICE] Batch job ${jobId} ${job.status}`);\n  1743→}\n  1744→\n  1745→/**\n  1746→ * GET /batch/status/:jobId - Get batch job status and results\n  1747→ */\n  1748→app.get('/batch/status/:jobId', (req, res) => {\n  1749→  const { jobId } = req.params;\n  1750→  const job = batchJobs.get(jobId);\n  1751→\n  1752→  if (!job) {\n  1753→    return res.status(404).json({ error: 'Job not found' });\n  1754→  }\n  1755→\n  1756→  res.json({\n  1757→    success: true,\n  1758→    job: {\n  1759→      id: job.id,\n  1760→      type: job.type,\n  1761→      status: job.status,\n  1762→      createdAt: job.createdAt,\n  1763→      completedAt: job.completedAt,\n  1764→      progress: job.progress,\n  1765→      files: job.files.map(f => ({\n  1766→        path: f.path,\n  1767→        status: f.status,\n  1768→        silenceCount: f.result?.count,\n  1769→        error: f.error\n  1770→      }))\n  1771→    }\n  1772→  });\n  1773→});\n  1774→\n  1775→/**\n  1776→ * GET /batch/results/:jobId - Get full results for a completed batch job\n  1777→ */\n  1778→app.get('/batch/results/:jobId', (req, res) => {\n  1779→  const { jobId } = req.params;\n  1780→  const job = batchJobs.get(jobId);\n  1781→\n  1782→  if (!job) {\n  1783→    return res.status(404).json({ error: 'Job not found' });\n  1784→  }\n  1785→\n  1786→  if (job.status === 'processing') {\n  1787→    return res.status(202).json({\n  1788→      success: false,\n  1789→      message: 'Job still processing',\n  1790→      progress: job.progress\n  1791→    });\n  1792→  }\n  1793→\n  1794→  res.json({\n  1795→    success: true,\n  1796→    jobId: job.id,\n  1797→    status: job.status,\n  1798→    progress: job.progress,\n  1799→    results: job.results,\n  1800→    errors: job.errors,\n  1801→    summary: {\n  1802→      totalFiles: job.progress.total,\n  1803→      successful: job.progress.completed,\n  1804→      failed: job.progress.failed,\n  1805→      totalSilences: job.results.reduce((sum, r) => sum + (r.count || 0), 0),\n  1806→      totalSilenceDuration: job.results.reduce((sum, r) => sum + (r.totalSilenceDuration || 0), 0)\n  1807→    }\n  1808→  });\n  1809→});\n  1810→\n  1811→/**\n  1812→ * DELETE /batch/:jobId - Cancel or delete a batch job\n  1813→ */\n  1814→app.delete('/batch/:jobId', (req, res) => {\n  1815→  const { jobId } = req.params;\n  1816→  const job = batchJobs.get(jobId);\n  1817→\n  1818→  if (!job) {\n  1819→    return res.status(404).json({ error: 'Job not found' });\n  1820→  }\n  1821→\n  1822→  // Note: This doesn't actually cancel in-progress processing\n  1823→  // but prevents the job from being queried\n  1824→  batchJobs.delete(jobId);\n  1825→\n  1826→  res.json({\n  1827→    success: true,\n  1828→    message: `Job ${jobId} deleted`\n  1829→  });\n  1830→});\n  1831→\n  1832→/**\n  1833→ * GET /batch/jobs - List all batch jobs\n  1834→ */\n  1835→app.get('/batch/jobs', (req, res) => {\n  1836→  const jobs = Array.from(batchJobs.values()).map(job => ({\n  1837→    id: job.id,\n  1838→    type: job.type,\n  1839→    status: job.status,\n  1840→    createdAt: job.createdAt,\n  1841→    completedAt: job.completedAt,\n  1842→    progress: job.progress\n  1843→  }));\n  1844→\n  1845→  // Sort by creation date (newest first)\n  1846→  jobs.sort((a, b) => new Date(b.createdAt) - new Date(a.createdAt));\n  1847→\n  1848→  res.json({\n  1849→    success: true,\n  1850→    count: jobs.length,\n  1851→    jobs\n  1852→  });\n  1853→});\n  1854→\n  1855→// =============================================================================\n  1856→// Billing & Credits Routes\n  1857→// =============================================================================\n  1858→\n  1859→/**\n  1860→ * GET /credits - Get user's credit balance\n  1861→ *\n  1862→ * Requires x-stripe-customer-id header\n  1863→ */\n  1864→app.get('/credits', async (req, res) => {\n  1865→  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n  1866→\n  1867→  if (!stripeCustomerId) {\n  1868→    return res.status(401).json({\n  1869→      error: 'Authentication required',\n  1870→      message: 'Missing x-stripe-customer-id header'\n  1871→    });\n  1872→  }\n  1873→\n  1874→  try {\n  1875→    const balance = await usageTracking.getBalance(stripeCustomerId);\n  1876→    res.json({\n  1877→      success: true,\n  1878→      ...balance\n  1879→    });\n  1880→  } catch (err) {\n  1881→    console.error('[SPLICE] Credits error:', err);\n  1882→    res.status(500).json({ error: err.message });\n  1883→  }\n  1884→});\n  1885→\n  1886→/**\n  1887→ * GET /usage-history - Get user's usage history\n  1888→ */\n  1889→app.get('/usage-history', async (req, res) => {\n  1890→  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n  1891→\n  1892→  if (!stripeCustomerId) {\n  1893→    return res.status(401).json({\n  1894→      error: 'Authentication required',\n  1895→      message: 'Missing x-stripe-customer-id header'\n  1896→    });\n  1897→  }\n  1898→\n  1899→  try {\n  1900→    const history = await usageTracking.getUsageHistory(stripeCustomerId);\n  1901→    res.json({\n  1902→      success: true,\n  1903→      history\n  1904→    });\n  1905→  } catch (err) {\n  1906→    console.error('[SPLICE] Usage history error:', err);\n  1907→    res.status(500).json({ error: err.message });\n  1908→  }\n  1909→});\n  1910→\n  1911→// =============================================================================\n  1912→// Start Server\n  1913→// =============================================================================\n  1914→\n  1915→// Initialize database and start server\n  1916→async function startServer() {\n  1917→  try {\n  1918→    await usageTracking.initDatabase();\n  1919→    console.log('[SPLICE] Database initialized');\n  1920→\n  1921→    if (isProduction || !httpsOptions) {\n  1922→      // Production: Railway provides TLS termination, use HTTP\n  1923→      http.createServer(app).listen(PORT, () => {\n  1924→        console.log(`[SPLICE] Backend running at http://0.0.0.0:${PORT} (production)`);\n  1925→      });\n  1926→    } else {\n  1927→      // Development: Use HTTPS with local certificates\n  1928→      https.createServer(httpsOptions, app).listen(PORT, () => {\n  1929→        console.log(`[SPLICE] Backend running at https://127.0.0.1:${PORT} (development)`);\n  1930→        console.log(`[SPLICE] POST /analyze with { \"wavPath\": \"/path/to/audio.wav\" }`);\n  1931→      });\n  1932→    }\n  1933→  } catch (err) {\n  1934→    console.error('[SPLICE] Failed to start server:', err);\n  1935→    process.exit(1);\n  1936→  }\n  1937→}\n  1938→\n  1939→startServer();\n  1940→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"25a5ca4d-e3ee-4435-9bd2-567cf51314ff","timestamp":"2025-12-25T22:30:22.016Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/server.js","content":"/**\n * SPLICE Backend Server\n *\n * Main entry point for the SPLICE backend API.\n * Orchestrates the audio analysis pipeline.\n *\n * Slices:\n * - Slice 4: Transcription (services/transcription.js)\n * - Slice 5: Take Detection (services/takeDetection.js)\n */\n\nrequire('dotenv').config();\n\nconst express = require('express');\nconst cors = require('cors');\nconst fs = require('fs');\nconst https = require('https');\nconst http = require('http');\nconst path = require('path');\n\n// Check if running in production (Railway injects RAILWAY_ENVIRONMENT)\nconst isProduction = process.env.NODE_ENV === 'production' || process.env.RAILWAY_ENVIRONMENT;\n\n// Import slice services\nconst { transcribeAudio, transcribeWithWords } = require('./services/transcription');\nconst { detectTakes } = require('./services/takeDetection');\nconst { detectSilences } = require('./services/silenceDetection');\nconst { detectAudioSilences, isFFprobeInstalled, getAudioDuration } = require('./services/ffprobeSilence');\nconst { detectSilencesRMS, sensitivityToParams } = require('./services/rmsSilenceDetection');\nconst {\n  detectProfanity,\n  getProfanityList,\n  getSupportedLanguages,\n  getAvailableBleepSounds,\n  parseWordList\n} = require('./services/profanityDetection');\nconst {\n  detectRepetitionsBasic,\n  detectRepetitionsAdvanced,\n  detectStutters,\n  detectAllRepetitions\n} = require('./services/repetitionDetection');\nconst {\n  analyzeMultitrack,\n  autoBalanceMultitrack\n} = require('./services/multitrackAnalysis');\nconst {\n  toSRT,\n  toVTT,\n  toPlainText,\n  toJSON,\n  exportToFile,\n  getSupportedFormats\n} = require('./services/captionExporter');\nconst { processXMLFile } = require('./services/xmlProcessor');\nconst { isolateVocals, isReplicateConfigured } = require('./services/vocalIsolation');\nconst { generateCutList, generateTakesCutList, validateCutList } = require('./services/cutListGenerator');\n\n// Usage tracking and billing\nconst usageTracking = require('./services/usageTracking');\n// Rate limiter for usage-based endpoints\nconst { requireCredits } = require('./middleware/rateLimiter');\n\n// Maximum file size for audio processing (500MB)\nconst MAX_FILE_SIZE_BYTES = 500 * 1024 * 1024;\n\n/**\n * Validate file size to prevent OOM crashes\n * @param {string} filePath - Path to file\n * @returns {Promise<{valid: boolean, size?: number, error?: string}>}\n */\nasync function validateFileSize(filePath) {\n  try {\n    const stats = await require('fs').promises.stat(filePath);\n    if (stats.size > MAX_FILE_SIZE_BYTES) {\n      return {\n        valid: false,\n        size: stats.size,\n        error: `File too large (${(stats.size / 1024 / 1024).toFixed(1)}MB). Maximum allowed: ${MAX_FILE_SIZE_BYTES / 1024 / 1024}MB`\n      };\n    }\n    return { valid: true, size: stats.size };\n  } catch (err) {\n    return { valid: false, error: `Cannot access file: ${err.message}` };\n  }\n}\n\n// Stripe for webhooks\nconst Stripe = require('stripe');\nconst stripe = new Stripe(process.env.STRIPE_SECRET_KEY);\n\n// =============================================================================\n// Server Configuration\n// =============================================================================\n\nconst app = express();\nconst PORT = process.env.PORT || 3847;\n\n// HTTPS certificates (generated by mkcert) - only for local development\nlet httpsOptions = null;\nif (!isProduction) {\n  const keyPath = path.join(__dirname, 'localhost+1-key.pem');\n  const certPath = path.join(__dirname, 'localhost+1.pem');\n  if (fs.existsSync(keyPath) && fs.existsSync(certPath)) {\n    httpsOptions = {\n      key: fs.readFileSync(keyPath),\n      cert: fs.readFileSync(certPath)\n    };\n  }\n}\n\napp.use(cors());\n\n// Helper to determine tier from price ID with logging\nfunction getTierFromPriceId(priceId) {\n  if (priceId === process.env.STRIPE_PRICE_STARTER) return 'starter';\n  if (priceId === process.env.STRIPE_PRICE_PRO) return 'pro';\n  if (priceId === process.env.STRIPE_PRICE_TEAM) return 'team';\n\n  // Log unknown price ID for debugging\n  console.warn(`[SPLICE] Unknown price ID: ${priceId} - defaulting to starter tier`);\n  return 'starter';\n}\n\n// Stripe webhook needs raw body - must be before express.json()\napp.post('/webhooks/stripe', express.raw({ type: 'application/json' }), async (req, res) => {\n  const sig = req.headers['stripe-signature'];\n  const webhookSecret = process.env.STRIPE_WEBHOOK_SECRET;\n\n  let event;\n\n  try {\n    if (webhookSecret) {\n      event = stripe.webhooks.constructEvent(req.body, sig, webhookSecret);\n    } else if (isProduction) {\n      // SECURITY: Reject unsigned webhooks in production\n      console.error('[SPLICE] CRITICAL: STRIPE_WEBHOOK_SECRET not set in production');\n      return res.status(500).json({ error: 'Webhook configuration error: secret not configured' });\n    } else {\n      // For local development testing only\n      event = JSON.parse(req.body);\n      console.warn('[SPLICE] Warning: Processing webhook without signature verification (dev only)');\n    }\n  } catch (err) {\n    console.error('[SPLICE] Webhook signature verification failed:', err.message);\n    return res.status(400).json({ error: 'Webhook signature verification failed' });\n  }\n\n  console.log(`[SPLICE] Webhook received: ${event.type} (${event.id})`);\n\n  // Idempotency check - skip if already processed\n  if (await usageTracking.isEventProcessed(event.id)) {\n    console.log(`[SPLICE] Event ${event.id} already processed, skipping`);\n    return res.json({ received: true, skipped: true });\n  }\n\n  try {\n    switch (event.type) {\n      case 'customer.subscription.created':\n      case 'customer.subscription.updated': {\n        const subscription = event.data.object;\n        const customerId = subscription.customer;\n\n        // Validate customerId\n        if (!customerId) {\n          console.error('[SPLICE] Missing customer ID in subscription event');\n          return res.status(400).json({ error: 'Missing customer ID' });\n        }\n\n        // Get tier from price ID\n        const priceId = subscription.items?.data?.[0]?.price?.id;\n        const tier = getTierFromPriceId(priceId);\n\n        // Update user tier and reset hours\n        await usageTracking.updateTier(customerId, tier);\n        console.log(`[SPLICE] Updated customer ${customerId} to tier: ${tier}`);\n        break;\n      }\n\n      case 'customer.subscription.deleted': {\n        const subscription = event.data.object;\n        const customerId = subscription.customer;\n\n        // Validate customerId\n        if (!customerId) {\n          console.error('[SPLICE] Missing customer ID in subscription.deleted event');\n          return res.status(400).json({ error: 'Missing customer ID' });\n        }\n\n        // Downgrade to cancelled (0 hours)\n        await usageTracking.updateTier(customerId, 'cancelled');\n        console.log(`[SPLICE] Subscription cancelled for customer ${customerId}`);\n        break;\n      }\n\n      case 'invoice.payment_succeeded': {\n        const invoice = event.data.object;\n        const customerId = invoice.customer;\n        const subscriptionId = invoice.subscription;\n\n        // Validate customerId\n        if (!customerId) {\n          console.error('[SPLICE] Missing customer ID in invoice event');\n          return res.status(400).json({ error: 'Missing customer ID' });\n        }\n\n        // Reset hours on successful payment (new billing period)\n        if (subscriptionId) {\n          const subscription = await stripe.subscriptions.retrieve(subscriptionId);\n          const priceId = subscription.items?.data?.[0]?.price?.id;\n          const tier = getTierFromPriceId(priceId);\n\n          await usageTracking.resetHours(customerId, tier);\n          console.log(`[SPLICE] Reset hours for customer ${customerId} (tier: ${tier})`);\n        }\n        break;\n      }\n\n      default:\n        console.log(`[SPLICE] Unhandled event type: ${event.type}`);\n    }\n\n    // Record event as processed (idempotency)\n    await usageTracking.recordWebhookEvent(event.id, event.type);\n\n    res.json({ received: true });\n  } catch (err) {\n    console.error('[SPLICE] Webhook handler error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\napp.use(express.json());\n\n// =============================================================================\n// Routes\n// =============================================================================\n\n/**\n * GET / - API information\n */\napp.get('/', (req, res) => {\n  res.json({\n    service: 'splice-backend',\n    version: '0.3.0',\n    endpoints: {\n      'GET /': 'This info',\n      'GET /health': 'Health check',\n      'GET /ffprobe-check': 'Check if FFprobe is installed',\n      'GET /replicate-check': 'Check if Replicate API is configured',\n      'POST /analyze': 'Analyze WAV file { wavPath }',\n      'POST /silences': 'Detect silences via Whisper gaps { wavPath, threshold: 0.5 }',\n      'POST /silences-audio': 'Detect silences via FFprobe { wavPath, threshold: -30, minDuration: 0.5, padding: 0.1 }',\n      'POST /silences-rms': 'Detect silences via RMS analysis { wavPath, threshold: -30, minSilenceLength: 0.5, paddingStart: 0.1, paddingEnd: 0.05, autoThreshold: false, sensitivity: 50 }',\n      'POST /profanity': 'Detect profanity in transcript { wavPath, language: \"en\", customBlocklist: [], customAllowlist: [] }',\n      'GET /profanity/languages': 'Get supported languages for profanity detection',\n      'GET /profanity/bleeps': 'Get available bleep sounds',\n      'POST /repetitions': 'Detect phrase repetitions and stutters { wavPath, phraseSize: 5, tolerance: 0.7, useOpenAI: false }',\n      'POST /fillers': 'Detect filler words (um, uh, like, etc.) { wavPath, customFillers: [] }',\n      'POST /stutters': 'Detect single-word stutters only { wavPath, minRepeats: 2 }',\n      'POST /export/captions': 'Export transcript to caption format { wavPath, format: srt|vtt|txt|json, outputPath? }',\n      'GET /export/formats': 'Get supported caption export formats',\n      'POST /multitrack': 'Analyze multiple audio tracks for multicam { audioPaths: [], speakerNames: [], videoTrackMapping: {} }',\n      'POST /multitrack/auto-balance': 'Auto-balance speaker screentime { audioPaths: [], speakerNames: [] }',\n      'POST /process-xml': 'Process FCP XML { xmlPath, silences, removeGaps: true }',\n      'POST /cut-list': 'Generate JSON cut list for DOM building (v3.5) { sourceName, sourcePath, duration, silences, takes?, settings? }',\n      'POST /cut-list/takes': 'Generate cut list keeping only takes { sourceName, sourcePath, duration, takes, settings? }',\n      'POST /isolate-vocals': 'Isolate vocals from audio { audioPath }',\n      'POST /batch/silences': 'Batch process multiple files for silence detection { files: [], options: {} }',\n      'GET /batch/status/:jobId': 'Get batch job status',\n      'GET /batch/results/:jobId': 'Get full batch job results',\n      'GET /batch/jobs': 'List all batch jobs',\n      'DELETE /batch/:jobId': 'Delete a batch job',\n      'GET /credits': 'Get user credit balance (requires x-stripe-customer-id header)',\n      'GET /usage-history': 'Get usage history (requires x-stripe-customer-id header)',\n      'POST /webhooks/stripe': 'Stripe webhook endpoint'\n    }\n  });\n});\n\n/**\n * GET /health - Health check\n */\napp.get('/health', (req, res) => {\n  res.json({ status: 'ok', service: 'splice-backend' });\n});\n\n/**\n * POST /analyze - Main analysis endpoint\n *\n * Pipeline:\n * 1. Validate input (wavPath)\n * 2. Slice 4: Transcribe audio with Groq Whisper\n * 3. Slice 5: Detect takes with GPT-4o-mini\n * 4. Return combined results\n */\napp.post('/analyze', requireCredits({ endpoint: 'analyze' }), async (req, res) => {\n  const { wavPath } = req.body;\n\n  // Validate input\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Analyzing: ${wavPath}`);\n\n  try {\n    // Slice 4 - GPT-4o-mini transcription\n    const transcript = await transcribeAudio(wavPath);\n\n    // Slice 5 - GPT-4o-mini take detection\n    const takes = await detectTakes(transcript);\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      transcript,\n      takes,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /silences - Detect silent gaps in audio\n *\n * Pipeline:\n * 1. Transcribe audio with Whisper (reuses transcription)\n * 2. Analyze gaps between segments\n * 3. Return silence regions\n */\napp.post('/silences', requireCredits({ endpoint: 'silences' }), async (req, res) => {\n  const { wavPath, threshold = 0.5 } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Detecting silences: ${wavPath} (threshold: ${threshold}s)`);\n\n  try {\n    const transcript = await transcribeAudio(wavPath);\n    const silences = detectSilences(transcript.segments, threshold);\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      threshold,\n      silences,\n      count: silences.length,\n      totalSilenceDuration: silences.reduce((sum, s) => sum + s.duration, 0).toFixed(2),\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Silence detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /silences-audio - Detect silences using FFprobe audio analysis\n *\n * Uses actual audio levels (dB threshold) instead of transcript gaps.\n * More accurate for detecting silence vs background noise.\n */\napp.post('/silences-audio', requireCredits({ endpoint: 'silences-audio' }), async (req, res) => {\n  const {\n    wavPath,\n    threshold = -30,\n    minDuration = 0.5,\n    padding = 0.1\n  } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  // Check FFprobe availability\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  console.log(`[SPLICE] FFprobe silence detection: ${wavPath} (threshold: ${threshold}dB, min: ${minDuration}s)`);\n\n  try {\n    const silences = await detectAudioSilences(wavPath, {\n      threshold,\n      minDuration,\n      padding\n    });\n\n    const totalDuration = silences.reduce((sum, s) => sum + s.duration, 0);\n\n    // Deduct usage based on audio duration\n    let balance = null;\n    try {\n      const audioDuration = await getAudioDuration(wavPath);\n      if (audioDuration > 0 && req.deductUsage) {\n        balance = await req.deductUsage(audioDuration);\n      }\n    } catch (durErr) {\n      console.warn('[SPLICE] Could not get audio duration for billing:', durErr.message);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      threshold,\n      minDuration,\n      padding,\n      silences,\n      count: silences.length,\n      totalSilenceDuration: totalDuration.toFixed(2),\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] FFprobe silence detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /silences-rms - Detect silences using RMS audio analysis\n *\n * Advanced silence detection with:\n * - RMS (Root Mean Square) audio level analysis\n * - Auto-threshold detection from audio histogram\n * - Configurable padding (before/after cuts)\n * - Sensitivity slider mapping (0-100)\n *\n * Options:\n * - threshold: dBFS threshold (-60 to -20, default: -30)\n * - minSilenceLength: Minimum silence duration in seconds (default: 0.5)\n * - seekStep: Analysis window step in seconds (default: 0.05)\n * - paddingStart: Buffer before silence in seconds (default: 0.1)\n * - paddingEnd: Buffer after silence in seconds (default: 0.05)\n * - autoThreshold: Auto-detect optimal threshold (default: false)\n * - sensitivity: UI sensitivity 0-100 (overrides other params if provided)\n */\napp.post('/silences-rms', requireCredits({ endpoint: 'silences-rms' }), async (req, res) => {\n  const { wavPath, sensitivity, ...manualOptions } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  // Validate file size to prevent OOM\n  const sizeCheck = await validateFileSize(wavPath);\n  if (!sizeCheck.valid) {\n    return res.status(413).json({ error: sizeCheck.error });\n  }\n\n  // Check FFprobe availability (needed for audio extraction)\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  // Build options - use sensitivity if provided, otherwise use manual options\n  let options = {};\n  if (typeof sensitivity === 'number') {\n    options = sensitivityToParams(sensitivity);\n    console.log(`[SPLICE] RMS detection with sensitivity ${sensitivity}`);\n  } else {\n    options = {\n      threshold: manualOptions.threshold ?? -30,\n      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n      seekStep: manualOptions.seekStep ?? 0.05,\n      paddingStart: manualOptions.paddingStart ?? 0.1,\n      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n      autoThreshold: manualOptions.autoThreshold ?? false,\n      mergeDistance: manualOptions.mergeDistance ?? 0.2\n    };\n  }\n\n  console.log(`[SPLICE] RMS silence detection: ${wavPath}`);\n\n  try {\n    const result = await detectSilencesRMS(wavPath, options);\n\n    // Deduct usage based on audio duration\n    const audioDuration = result.metadata?.audioDuration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result,\n      count: result.silences.length,\n      totalSilenceDuration: result.metadata.totalSilenceDuration.toFixed(2),\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] RMS silence detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Profanity Detection Routes\n// =============================================================================\n\n/**\n * POST /profanity - Detect profanity in audio/transcript\n *\n * Transcribes audio (if needed) and detects profanity words.\n * Returns word-level and segment-level results for muting/bleeping.\n *\n * Options:\n * - wavPath: Path to audio file (required)\n * - transcript: Pre-existing transcript (optional, skips transcription)\n * - language: Language code (en, es, fr, de) - default: en\n * - customBlocklist: Array or comma-separated string of additional words to censor\n * - customAllowlist: Array or comma-separated string of words to allow\n * - frameRate: Frame rate for boundary alignment (default: 30)\n */\napp.post('/profanity', requireCredits({ endpoint: 'profanity' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    language = 'en',\n    customBlocklist = [],\n    customAllowlist = [],\n    frameRate = 30\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Profanity detection: ${wavPath || 'provided transcript'} (language: ${language})`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      // Use transcribeWithWords for word-level timestamps required by profanity detection\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript has words\n    if (!transcript || !transcript.words || transcript.words.length === 0) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data',\n        hint: 'Ensure transcription returns words array with start/end times'\n      });\n    }\n\n    // Parse custom lists\n    const blocklist = parseWordList(customBlocklist);\n    const allowlist = parseWordList(customAllowlist);\n\n    // Detect profanity\n    const result = detectProfanity(transcript, {\n      language,\n      customBlocklist: blocklist,\n      customAllowlist: allowlist,\n      frameRate\n    });\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Profanity detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /profanity/languages - Get supported languages\n */\napp.get('/profanity/languages', (req, res) => {\n  res.json({\n    success: true,\n    languages: getSupportedLanguages()\n  });\n});\n\n/**\n * GET /profanity/bleeps - Get available bleep sounds\n */\napp.get('/profanity/bleeps', (req, res) => {\n  res.json({\n    success: true,\n    sounds: getAvailableBleepSounds()\n  });\n});\n\n/**\n * GET /profanity/list/:language - Get default profanity list for a language\n */\napp.get('/profanity/list/:language', (req, res) => {\n  const { language } = req.params;\n  const list = getProfanityList(language);\n\n  res.json({\n    success: true,\n    language,\n    wordCount: list.length,\n    // Return first 50 words as sample (full list is large)\n    sample: list.slice(0, 50),\n    note: 'Full list available but truncated for response size'\n  });\n});\n\n// =============================================================================\n// Repetition/Stutter Detection Routes\n// =============================================================================\n\n/**\n * POST /repetitions - Detect phrase repetitions and stutters\n *\n * Analyzes transcript for repeated phrases and stutters.\n * Returns segments that can be removed to clean up the edit.\n *\n * Options:\n * - wavPath: Path to audio file (required unless transcript provided)\n * - transcript: Pre-existing transcript (optional)\n * - phraseSize: Words per comparison window (default: 5)\n * - tolerance: Similarity threshold 0-1 (default: 0.7)\n * - searchRadius: Words to search ahead (default: 100)\n * - useOpenAI: Use OpenAI for boundary refinement (default: false)\n * - includeStutters: Also detect single-word stutters (default: true)\n */\napp.post('/repetitions', requireCredits({ endpoint: 'repetitions' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    phraseSize = 5,\n    tolerance = 0.7,\n    searchRadius = 100,\n    useOpenAI = false,\n    includeStutters = true\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Repetition detection: ${wavPath || 'provided transcript'}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      // Use transcribeWithWords for word-level timestamps required by repetition detection\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript has words\n    if (!transcript || !transcript.words || transcript.words.length === 0) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data'\n      });\n    }\n\n    // Detect all repetitions (phrases + stutters)\n    const result = await detectAllRepetitions(transcript, {\n      phraseSize,\n      tolerance,\n      searchRadius,\n      useOpenAI\n    });\n\n    // Optionally filter out stutters\n    if (!includeStutters) {\n      result.stutters = [];\n      result.removalSegments = result.removalSegments.filter(s => s.type !== 'stutter');\n    }\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Repetition detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /fillers - Detect filler words (um, uh, like, etc.)\n *\n * Transcribes audio and identifies filler words with timestamps.\n * Returns segments that can be cut or reviewed for removal.\n *\n * Options:\n * - wavPath: Path to audio file (required unless transcript provided)\n * - transcript: Pre-existing transcript with word-level timing (optional)\n * - customFillers: Additional filler words to detect (optional)\n */\napp.post('/fillers', requireCredits({ endpoint: 'fillers' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    customFillers = []\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Filler word detection: ${wavPath || 'provided transcript'}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript has words\n    if (!transcript || !transcript.words || transcript.words.length === 0) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data'\n      });\n    }\n\n    // Default filler words (common in English speech)\n    const defaultFillers = [\n      'um', 'uh', 'ah', 'er', 'eh',           // Hesitation sounds\n      'like', 'so', 'well', 'right',           // Discourse markers\n      'you know', 'i mean', 'basically',       // Filler phrases\n      'actually', 'literally', 'honestly',     // Overused qualifiers\n      'kind of', 'sort of', 'you see'          // Hedging phrases\n    ];\n\n    // Combine default + custom fillers (lowercase for matching)\n    const fillerSet = new Set([\n      ...defaultFillers,\n      ...customFillers.map(f => f.toLowerCase().trim())\n    ]);\n\n    // Detect filler words\n    const fillers = [];\n    const words = transcript.words;\n\n    for (let i = 0; i < words.length; i++) {\n      const word = words[i];\n      const normalizedWord = word.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim();\n\n      // Check single-word fillers\n      if (fillerSet.has(normalizedWord)) {\n        fillers.push({\n          word: word.word,\n          normalizedWord,\n          start: word.start,\n          end: word.end,\n          duration: word.end - word.start,\n          index: i,\n          type: 'filler'\n        });\n        continue;\n      }\n\n      // Check two-word phrases (e.g., \"you know\", \"kind of\")\n      if (i < words.length - 1) {\n        const nextWord = words[i + 1];\n        const twoWordPhrase = `${normalizedWord} ${nextWord.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim()}`;\n        if (fillerSet.has(twoWordPhrase)) {\n          fillers.push({\n            word: `${word.word} ${nextWord.word}`,\n            normalizedWord: twoWordPhrase,\n            start: word.start,\n            end: nextWord.end,\n            duration: nextWord.end - word.start,\n            index: i,\n            type: 'filler_phrase'\n          });\n          // Skip next word since it's part of this phrase\n          i++;\n        }\n      }\n    }\n\n    // Calculate total filler time\n    const totalFillerDuration = fillers.reduce((sum, f) => sum + f.duration, 0);\n    const audioDuration = transcript.duration || (words.length > 0 ? words[words.length - 1].end : 0);\n    const fillerPercentage = audioDuration > 0 ? (totalFillerDuration / audioDuration) * 100 : 0;\n\n    // Deduct usage based on audio duration\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      fillers,\n      metadata: {\n        totalWords: words.length,\n        fillerCount: fillers.length,\n        totalFillerDuration: parseFloat(totalFillerDuration.toFixed(3)),\n        audioDuration: parseFloat(audioDuration.toFixed(3)),\n        fillerPercentage: parseFloat(fillerPercentage.toFixed(2)),\n        fillersPerMinute: audioDuration > 0 ? parseFloat((fillers.length / (audioDuration / 60)).toFixed(2)) : 0\n      },\n      removalSegments: fillers.map(f => ({\n        start: f.start,\n        end: f.end,\n        duration: f.duration,\n        reason: `Filler: \"${f.word}\"`,\n        type: f.type\n      })),\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Filler detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /stutters - Detect single-word stutters only\n *\n * Focused detection for word-level stutters (e.g., \"I I I think\").\n * Faster than full repetition detection.\n */\napp.post('/stutters', requireCredits({ endpoint: 'stutters' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    options = {},\n    // Support both top-level and nested options for flexibility\n    minRepeats = options.minRepeats ?? 2,\n    maxGapMs = options.maxGapMs ?? 500,\n    ignoreFillers = options.ignoreFillers ?? true,\n    minWordLength = options.minWordLength ?? 1\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Stutter detection: ${wavPath || 'provided transcript'}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      // Use transcribeWithWords for word-level timestamps required by stutter detection\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript exists and has words array\n    if (!transcript || !transcript.words) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data'\n      });\n    }\n\n    // Empty transcript returns empty result (not an error)\n    if (transcript.words.length === 0) {\n      return res.json({\n        success: true,\n        stutters: [],\n        metadata: { type: 'stutters', totalWords: 0, stutterCount: 0, totalRepeatedWords: 0 }\n      });\n    }\n\n    // Detect stutters only\n    const result = detectStutters(transcript, {\n      minRepeats,\n      maxGapMs,\n      ignoreFillers,\n      minWordLength\n    });\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Stutter detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Caption Export Routes\n// =============================================================================\n\n/**\n * POST /export/captions - Export transcript to caption format (SRT, VTT, etc.)\n *\n * Converts a transcript to the specified caption format.\n * Can optionally save to file.\n *\n * Options:\n * - wavPath: Path to audio file (to transcribe first)\n * - transcript: Pre-existing transcript with word-level timing\n * - format: Export format (srt, vtt, txt, json) - default: srt\n * - outputPath: Optional file path to save to\n * - maxWordsPerCaption: Max words per caption (default: 8)\n * - maxDuration: Max duration per caption in seconds (default: 5)\n */\napp.post('/export/captions', requireCredits({ endpoint: 'export-captions' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    format = 'srt',\n    outputPath = null,\n    maxWordsPerCaption = 8,\n    maxDuration = 5\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Caption export: ${wavPath || 'provided transcript'} -> ${format}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    const exportOptions = { maxWordsPerCaption, maxDuration };\n\n    // Generate caption content based on format\n    let content;\n    let mimeType;\n\n    switch (format.toLowerCase()) {\n      case 'srt':\n        content = toSRT(transcript, exportOptions);\n        mimeType = 'application/x-subrip';\n        break;\n      case 'vtt':\n      case 'webvtt':\n        content = toVTT(transcript, exportOptions);\n        mimeType = 'text/vtt';\n        break;\n      case 'txt':\n      case 'text':\n        content = toPlainText(transcript, { ...exportOptions, includeTimestamps: true });\n        mimeType = 'text/plain';\n        break;\n      case 'json':\n        content = toJSON(transcript);\n        mimeType = 'application/json';\n        break;\n      default:\n        return res.status(400).json({\n          error: `Unsupported format: ${format}`,\n          supportedFormats: getSupportedFormats()\n        });\n    }\n\n    // Save to file if outputPath provided\n    let savedPath = null;\n    if (outputPath) {\n      const result = await exportToFile(transcript, outputPath, format, exportOptions);\n      savedPath = result.path;\n      console.log(`[SPLICE] Saved captions to: ${savedPath}`);\n    }\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      format,\n      content,\n      mimeType,\n      savedPath,\n      wordCount: transcript.words?.length || 0,\n      duration: transcript.duration || 0,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Caption export error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /export/formats - Get supported export formats\n */\napp.get('/export/formats', (req, res) => {\n  res.json({\n    success: true,\n    formats: getSupportedFormats()\n  });\n});\n\n// =============================================================================\n// Multitrack/Multicam Analysis Routes\n// =============================================================================\n\n/**\n * POST /multitrack - Analyze multiple audio tracks for multicam editing\n *\n * Analyzes audio levels across multiple tracks to determine optimal\n * video angle selection based on who is speaking.\n *\n * Options:\n * - audioPaths: Array of paths to audio files (one per speaker) - required\n * - speakerNames: Array of speaker names (optional)\n * - videoTrackMapping: Object mapping speaker index to video track { 0: 0, 1: 1 }\n * - minShotDuration: Minimum seconds before next cut (default: 2.0)\n * - switchingFrequency: How often to allow cuts 0-100 (default: 50)\n * - wideShotEnabled: Enable wide shot detection (default: true)\n * - wideShotPercentage: Target % of wide shots (default: 20)\n * - wideShotTracks: Video track indices for wide shots\n * - cutawayEnabled: Enable cutaway insertion (default: false)\n * - cutawayTracks: Video track indices for cutaways\n * - speakerBoosts: Per-speaker dB adjustments { \"Speaker 1\": 5 }\n */\napp.post('/multitrack', requireCredits({ endpoint: 'multitrack' }), async (req, res) => {\n  const {\n    audioPaths,\n    speakerNames,\n    videoTrackMapping = {},\n    minShotDuration = 2.0,\n    switchingFrequency = 50,\n    wideShotEnabled = true,\n    wideShotPercentage = 20,\n    wideShotTracks = [],\n    cutawayEnabled = false,\n    cutawayTracks = [],\n    speakerBoosts = {},\n    frameRate = 30\n  } = req.body;\n\n  // Validate audioPaths\n  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length === 0) {\n    return res.status(400).json({ error: 'audioPaths array is required (at least 1 path)' });\n  }\n\n  // Validate all files exist\n  for (const audioPath of audioPaths) {\n    if (!fs.existsSync(audioPath)) {\n      return res.status(404).json({ error: `File not found: ${audioPath}` });\n    }\n  }\n\n  // Check FFprobe availability\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  console.log(`[SPLICE] Multitrack analysis: ${audioPaths.length} track(s)`);\n\n  try {\n    const result = await analyzeMultitrack(audioPaths, {\n      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n      videoTrackMapping,\n      minShotDuration,\n      switchingFrequency,\n      wideShotEnabled,\n      wideShotPercentage,\n      wideShotTracks,\n      cutawayEnabled,\n      cutawayTracks,\n      speakerBoosts,\n      frameRate\n    });\n\n    // Deduct usage based on total duration (use longest track)\n    const audioDuration = result.metadata?.totalDuration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      ...result,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Multitrack analysis error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /multitrack/auto-balance - Auto-balance speaker screentime\n *\n * Automatically adjusts speaker boosts to achieve equal screentime distribution.\n * Runs multiple iterations to find optimal parameters.\n */\napp.post('/multitrack/auto-balance', requireCredits({ endpoint: 'multitrack-auto-balance' }), async (req, res) => {\n  const {\n    audioPaths,\n    speakerNames,\n    videoTrackMapping = {},\n    minShotDuration = 2.0,\n    switchingFrequency = 50,\n    wideShotEnabled = false, // Disable wide shots for balance calc\n    frameRate = 30\n  } = req.body;\n\n  // Validate audioPaths\n  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length < 2) {\n    return res.status(400).json({ error: 'audioPaths array requires at least 2 tracks for balancing' });\n  }\n\n  // Validate all files exist\n  for (const audioPath of audioPaths) {\n    if (!fs.existsSync(audioPath)) {\n      return res.status(404).json({ error: `File not found: ${audioPath}` });\n    }\n  }\n\n  // Check FFprobe availability\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  console.log(`[SPLICE] Auto-balancing multitrack: ${audioPaths.length} track(s)`);\n\n  try {\n    const result = await autoBalanceMultitrack(audioPaths, {\n      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n      videoTrackMapping,\n      minShotDuration,\n      switchingFrequency,\n      wideShotEnabled,\n      frameRate\n    });\n\n    // Deduct usage based on total duration\n    const audioDuration = result.metadata?.totalDuration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      ...result,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Auto-balance error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /process-xml - Process FCP XML to split clips at silences\n *\n * Takes an FCP XML file and silence timestamps, splits clips\n * at silence boundaries, and optionally removes gaps.\n */\napp.post('/process-xml', async (req, res) => {\n  const {\n    xmlPath,\n    silences,\n    removeGaps = true,\n    outputPath = null\n  } = req.body;\n\n  if (!xmlPath) {\n    return res.status(400).json({ error: 'xmlPath is required' });\n  }\n\n  if (!silences || !Array.isArray(silences)) {\n    return res.status(400).json({ error: 'silences array is required' });\n  }\n\n  if (!fs.existsSync(xmlPath)) {\n    return res.status(404).json({ error: `XML file not found: ${xmlPath}` });\n  }\n\n  console.log(`[SPLICE] Processing XML: ${xmlPath} with ${silences.length} silence(s)`);\n\n  try {\n    const result = await processXMLFile(xmlPath, silences, {\n      outputPath,\n      removeGaps\n    });\n\n    res.json({\n      success: true,\n      inputPath: xmlPath,\n      outputPath: result.outputPath,\n      stats: result.stats\n    });\n  } catch (err) {\n    console.error('[SPLICE] XML processing error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /cut-list - Generate a JSON cut list for direct DOM building (v3.5)\n *\n * Takes silences and optionally takes, returns a cut list that the\n * plugin can use to build sequences directly via UXP APIs.\n *\n * Body:\n * - sourceName: Name of the source clip\n * - sourcePath: Full path to the source file\n * - duration: Total duration in seconds\n * - silences: Array of silence segments [{start, end, duration}]\n * - takes: (optional) Array of detected takes\n * - settings: (optional) Generation settings\n */\napp.post('/cut-list', async (req, res) => {\n  const {\n    sourceName,\n    sourcePath,\n    duration,\n    silences,\n    takes = [],\n    settings = {}\n  } = req.body;\n\n  // Validate required fields\n  if (!sourceName && !sourcePath) {\n    return res.status(400).json({ error: 'sourceName or sourcePath is required' });\n  }\n\n  if (typeof duration !== 'number' || duration <= 0) {\n    return res.status(400).json({ error: 'duration must be a positive number' });\n  }\n\n  if (!silences || !Array.isArray(silences)) {\n    return res.status(400).json({ error: 'silences array is required' });\n  }\n\n  console.log(`[SPLICE] Generating cut list for ${sourceName || sourcePath} (${silences.length} silences)`);\n\n  try {\n    const cutList = generateCutList({\n      sourceName: sourceName || path.basename(sourcePath),\n      sourcePath,\n      duration,\n      silences,\n      takes,\n      settings\n    });\n\n    // Validate the generated cut list\n    const validation = validateCutList(cutList);\n    if (!validation.valid) {\n      return res.status(500).json({\n        error: 'Generated cut list is invalid',\n        validationErrors: validation.errors\n      });\n    }\n\n    res.json({\n      success: true,\n      cutList\n    });\n  } catch (err) {\n    console.error('[SPLICE] Cut list generation error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /cut-list/takes - Generate a cut list that keeps only takes\n *\n * Alternative endpoint for \"keep best takes only\" workflow.\n */\napp.post('/cut-list/takes', async (req, res) => {\n  const {\n    sourceName,\n    sourcePath,\n    duration,\n    takes,\n    settings = {}\n  } = req.body;\n\n  // Validate required fields\n  if (!sourceName && !sourcePath) {\n    return res.status(400).json({ error: 'sourceName or sourcePath is required' });\n  }\n\n  if (typeof duration !== 'number' || duration <= 0) {\n    return res.status(400).json({ error: 'duration must be a positive number' });\n  }\n\n  if (!takes || !Array.isArray(takes) || takes.length === 0) {\n    return res.status(400).json({ error: 'takes array is required and must not be empty' });\n  }\n\n  console.log(`[SPLICE] Generating takes cut list for ${sourceName || sourcePath} (${takes.length} takes)`);\n\n  try {\n    const cutList = generateTakesCutList({\n      sourceName: sourceName || path.basename(sourcePath),\n      sourcePath,\n      duration,\n      takes,\n      settings\n    });\n\n    res.json({\n      success: true,\n      cutList\n    });\n  } catch (err) {\n    console.error('[SPLICE] Takes cut list generation error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /ffprobe-check - Check if FFprobe is installed\n */\napp.get('/ffprobe-check', async (req, res) => {\n  const installed = await isFFprobeInstalled();\n  res.json({\n    installed,\n    message: installed\n      ? 'FFprobe is available'\n      : 'FFprobe not found. Install with: brew install ffmpeg'\n  });\n});\n\n/**\n * GET /replicate-check - Check if Replicate API is configured\n */\napp.get('/replicate-check', async (req, res) => {\n  const configured = isReplicateConfigured();\n  res.json({\n    configured,\n    message: configured\n      ? 'Replicate API is configured'\n      : 'REPLICATE_API_TOKEN not set. Add to .env file.'\n  });\n});\n\n/**\n * POST /isolate-vocals - Isolate vocals from audio using Demucs\n *\n * Uses Replicate's Demucs model to separate vocals from background audio.\n * Cost: ~$0.015/min of audio\n *\n * Tier access:\n * - Starter: No access (upgrade required)\n * - Pro: 2 hours included, then $0.08/min overage\n * - Team: 5 hours included, then $0.08/min overage\n */\napp.post('/isolate-vocals', requireCredits({ endpoint: 'isolate-vocals' }), async (req, res) => {\n  const { audioPath, stem = 'vocals', outputDir = null } = req.body;\n  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n\n  if (!audioPath) {\n    return res.status(400).json({ error: 'audioPath is required' });\n  }\n\n  if (!fs.existsSync(audioPath)) {\n    return res.status(404).json({ error: `File not found: ${audioPath}` });\n  }\n\n  // Check Replicate configuration\n  if (!isReplicateConfigured()) {\n    return res.status(500).json({\n      error: 'Replicate API not configured. Set REPLICATE_API_TOKEN in .env'\n    });\n  }\n\n  // Get audio duration for billing\n  let audioDurationSeconds = 0;\n  try {\n    audioDurationSeconds = await getAudioDuration(audioPath);\n  } catch (err) {\n    console.warn('[SPLICE] Could not get audio duration:', err.message);\n  }\n\n  const audioDurationMinutes = audioDurationSeconds / 60;\n\n  // Check isolation access if customer ID provided\n  if (stripeCustomerId) {\n    const accessCheck = await usageTracking.checkIsolationAccess(stripeCustomerId, audioDurationMinutes);\n\n    if (!accessCheck.allowed) {\n      return res.status(403).json({\n        error: accessCheck.message,\n        reason: accessCheck.reason,\n        upgradeRequired: accessCheck.reason === 'upgrade_required'\n      });\n    }\n\n    console.log(`[SPLICE] Isolation access: ${accessCheck.message}`);\n  }\n\n  console.log(`[SPLICE] Isolating vocals: ${audioPath} (${audioDurationMinutes.toFixed(1)} min)`);\n\n  try {\n    const result = await isolateVocals(audioPath, {\n      stem,\n      outputDir: outputDir || undefined\n    });\n\n    // Deduct isolation usage if customer ID provided\n    let usageInfo = null;\n    if (stripeCustomerId) {\n      usageInfo = await usageTracking.deductIsolationUsage(\n        stripeCustomerId,\n        audioDurationSeconds,\n        'isolate-vocals'\n      );\n      console.log(`[SPLICE] Isolation usage deducted: ${audioDurationMinutes.toFixed(1)} min`);\n      if (usageInfo.isolationUsed?.overageCost > 0) {\n        console.log(`[SPLICE] Overage cost: $${usageInfo.isolationUsed.overageCost.toFixed(2)}`);\n      }\n    }\n\n    res.json({\n      success: true,\n      inputPath: audioPath,\n      outputPath: result.outputPath,\n      stem: result.stem,\n      processingTime: result.processingTime,\n      availableStems: result.allStems,\n      audioDurationMinutes,\n      usage: usageInfo ? {\n        isolationHoursRemaining: usageInfo.isolationHoursRemaining,\n        overageCost: usageInfo.isolationUsed?.overageCost || 0\n      } : null\n    });\n  } catch (err) {\n    console.error('[SPLICE] Vocal isolation error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Batch Processing Routes\n// =============================================================================\n\n// In-memory job queue for batch processing\nconst batchJobs = new Map();\n\n// Batch job limits to prevent memory leak\nconst MAX_BATCH_JOBS = 10000;\nconst BATCH_JOB_MAX_AGE_MS = 24 * 60 * 60 * 1000; // 24 hours\n\n/**\n * Generate a unique job ID\n */\nfunction generateJobId() {\n  return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n}\n\n/**\n * Clean up old batch jobs to prevent memory leak\n * Removes jobs older than 24 hours\n */\nfunction cleanupOldBatchJobs() {\n  const now = Date.now();\n  let removedCount = 0;\n\n  for (const [jobId, job] of batchJobs.entries()) {\n    const createdAt = new Date(job.createdAt).getTime();\n    if (now - createdAt > BATCH_JOB_MAX_AGE_MS) {\n      batchJobs.delete(jobId);\n      removedCount++;\n    }\n  }\n\n  if (removedCount > 0) {\n    console.log(`[SPLICE] Cleaned up ${removedCount} old batch job(s)`);\n  }\n}\n\n/**\n * Enforce max job limit by removing oldest completed jobs\n */\nfunction enforceJobLimit() {\n  if (batchJobs.size < MAX_BATCH_JOBS) return;\n\n  // Get completed jobs sorted by creation date (oldest first)\n  const completedJobs = Array.from(batchJobs.entries())\n    .filter(([_, job]) => job.status !== 'processing')\n    .sort((a, b) => new Date(a[1].createdAt) - new Date(b[1].createdAt));\n\n  // Remove oldest completed jobs until under limit\n  const toRemove = batchJobs.size - MAX_BATCH_JOBS + 1;\n  for (let i = 0; i < Math.min(toRemove, completedJobs.length); i++) {\n    batchJobs.delete(completedJobs[i][0]);\n  }\n\n  console.log(`[SPLICE] Enforced job limit, removed ${Math.min(toRemove, completedJobs.length)} job(s)`);\n}\n\n// Run cleanup every hour\nsetInterval(cleanupOldBatchJobs, 60 * 60 * 1000);\n\n/**\n * POST /batch/silences - Process multiple files for silence detection\n *\n * Creates a batch job that processes multiple audio files.\n * Returns a job ID for tracking progress.\n *\n * Body:\n * - files: Array of file paths to process\n * - options: Detection options (sensitivity, threshold, etc.)\n */\napp.post('/batch/silences', requireCredits({ endpoint: 'batch-silences' }), async (req, res) => {\n  const { files, options = {} } = req.body;\n\n  if (!files || !Array.isArray(files) || files.length === 0) {\n    return res.status(400).json({ error: 'files array is required' });\n  }\n\n  // Validate all files exist\n  const missingFiles = files.filter(f => !fs.existsSync(f));\n  if (missingFiles.length > 0) {\n    return res.status(404).json({\n      error: 'Some files not found',\n      missingFiles\n    });\n  }\n\n  // Enforce job limit before creating new job\n  enforceJobLimit();\n\n  const jobId = generateJobId();\n\n  // Initialize job with customer ID for usage tracking\n  const job = {\n    id: jobId,\n    type: 'silences',\n    status: 'processing',\n    createdAt: new Date().toISOString(),\n    stripeCustomerId: req.stripeCustomerId,  // Store for usage deduction\n    files: files.map(f => ({\n      path: f,\n      status: 'pending',\n      result: null,\n      error: null\n    })),\n    options,\n    progress: {\n      total: files.length,\n      completed: 0,\n      failed: 0,\n      percentage: 0\n    },\n    results: [],\n    errors: [],\n    totalUsageDeducted: 0  // Track total seconds deducted\n  };\n\n  batchJobs.set(jobId, job);\n  console.log(`[SPLICE] Batch job ${jobId} created with ${files.length} files`);\n\n  // Start processing in background\n  processBatchJob(jobId);\n\n  res.json({\n    success: true,\n    jobId,\n    message: `Batch job created with ${files.length} files`,\n    statusUrl: `/batch/status/${jobId}`\n  });\n});\n\n/**\n * Process a batch job (runs in background)\n */\nasync function processBatchJob(jobId) {\n  const job = batchJobs.get(jobId);\n  if (!job) return;\n\n  const { sensitivity, ...manualOptions } = job.options;\n\n  // Build detection options\n  let detectionOptions = {};\n  if (typeof sensitivity === 'number') {\n    detectionOptions = sensitivityToParams(sensitivity);\n  } else {\n    detectionOptions = {\n      threshold: manualOptions.threshold ?? -30,\n      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n      paddingStart: manualOptions.paddingStart ?? 0.1,\n      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n      autoThreshold: manualOptions.autoThreshold ?? false\n    };\n  }\n\n  // Process files sequentially to avoid overwhelming the system\n  for (let i = 0; i < job.files.length; i++) {\n    const fileEntry = job.files[i];\n    fileEntry.status = 'processing';\n\n    try {\n      const result = await detectSilencesRMS(fileEntry.path, detectionOptions);\n\n      fileEntry.status = 'completed';\n      fileEntry.result = {\n        silences: result.silences,\n        count: result.silences.length,\n        totalSilenceDuration: result.metadata.totalSilenceDuration,\n        audioDuration: result.metadata.audioDuration\n      };\n\n      job.results.push({\n        file: fileEntry.path,\n        ...fileEntry.result\n      });\n\n      // Deduct usage for this file\n      const audioDuration = result.metadata?.audioDuration || 0;\n      if (audioDuration > 0 && job.stripeCustomerId) {\n        try {\n          await usageTracking.deductUsage(job.stripeCustomerId, audioDuration, 'batch-silences');\n          job.totalUsageDeducted += audioDuration;\n        } catch (usageErr) {\n          console.warn(`[SPLICE] Batch ${jobId}: Usage deduction failed:`, usageErr.message);\n        }\n      }\n\n      job.progress.completed++;\n      console.log(`[SPLICE] Batch ${jobId}: ${i + 1}/${job.files.length} completed`);\n    } catch (err) {\n      fileEntry.status = 'failed';\n      fileEntry.error = err.message;\n\n      job.errors.push({\n        file: fileEntry.path,\n        error: err.message\n      });\n\n      job.progress.failed++;\n      console.error(`[SPLICE] Batch ${jobId}: ${fileEntry.path} failed:`, err.message);\n    }\n\n    // Update progress\n    job.progress.percentage = Math.round(\n      ((job.progress.completed + job.progress.failed) / job.progress.total) * 100\n    );\n  }\n\n  // Mark job as complete\n  job.status = job.progress.failed === job.progress.total ? 'failed' :\n               job.progress.failed > 0 ? 'completed_with_errors' : 'completed';\n  job.completedAt = new Date().toISOString();\n\n  console.log(`[SPLICE] Batch job ${jobId} ${job.status}`);\n}\n\n/**\n * GET /batch/status/:jobId - Get batch job status and results\n */\napp.get('/batch/status/:jobId', (req, res) => {\n  const { jobId } = req.params;\n  const job = batchJobs.get(jobId);\n\n  if (!job) {\n    return res.status(404).json({ error: 'Job not found' });\n  }\n\n  res.json({\n    success: true,\n    job: {\n      id: job.id,\n      type: job.type,\n      status: job.status,\n      createdAt: job.createdAt,\n      completedAt: job.completedAt,\n      progress: job.progress,\n      files: job.files.map(f => ({\n        path: f.path,\n        status: f.status,\n        silenceCount: f.result?.count,\n        error: f.error\n      }))\n    }\n  });\n});\n\n/**\n * GET /batch/results/:jobId - Get full results for a completed batch job\n */\napp.get('/batch/results/:jobId', (req, res) => {\n  const { jobId } = req.params;\n  const job = batchJobs.get(jobId);\n\n  if (!job) {\n    return res.status(404).json({ error: 'Job not found' });\n  }\n\n  if (job.status === 'processing') {\n    return res.status(202).json({\n      success: false,\n      message: 'Job still processing',\n      progress: job.progress\n    });\n  }\n\n  res.json({\n    success: true,\n    jobId: job.id,\n    status: job.status,\n    progress: job.progress,\n    results: job.results,\n    errors: job.errors,\n    summary: {\n      totalFiles: job.progress.total,\n      successful: job.progress.completed,\n      failed: job.progress.failed,\n      totalSilences: job.results.reduce((sum, r) => sum + (r.count || 0), 0),\n      totalSilenceDuration: job.results.reduce((sum, r) => sum + (r.totalSilenceDuration || 0), 0)\n    }\n  });\n});\n\n/**\n * DELETE /batch/:jobId - Cancel or delete a batch job\n */\napp.delete('/batch/:jobId', (req, res) => {\n  const { jobId } = req.params;\n  const job = batchJobs.get(jobId);\n\n  if (!job) {\n    return res.status(404).json({ error: 'Job not found' });\n  }\n\n  // Note: This doesn't actually cancel in-progress processing\n  // but prevents the job from being queried\n  batchJobs.delete(jobId);\n\n  res.json({\n    success: true,\n    message: `Job ${jobId} deleted`\n  });\n});\n\n/**\n * GET /batch/jobs - List all batch jobs\n */\napp.get('/batch/jobs', (req, res) => {\n  const jobs = Array.from(batchJobs.values()).map(job => ({\n    id: job.id,\n    type: job.type,\n    status: job.status,\n    createdAt: job.createdAt,\n    completedAt: job.completedAt,\n    progress: job.progress\n  }));\n\n  // Sort by creation date (newest first)\n  jobs.sort((a, b) => new Date(b.createdAt) - new Date(a.createdAt));\n\n  res.json({\n    success: true,\n    count: jobs.length,\n    jobs\n  });\n});\n\n// =============================================================================\n// Billing & Credits Routes\n// =============================================================================\n\n/**\n * GET /credits - Get user's credit balance\n *\n * Requires x-stripe-customer-id header\n */\napp.get('/credits', async (req, res) => {\n  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n\n  if (!stripeCustomerId) {\n    return res.status(401).json({\n      error: 'Authentication required',\n      message: 'Missing x-stripe-customer-id header'\n    });\n  }\n\n  try {\n    const balance = await usageTracking.getBalance(stripeCustomerId);\n    res.json({\n      success: true,\n      ...balance\n    });\n  } catch (err) {\n    console.error('[SPLICE] Credits error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /usage-history - Get user's usage history\n */\napp.get('/usage-history', async (req, res) => {\n  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n\n  if (!stripeCustomerId) {\n    return res.status(401).json({\n      error: 'Authentication required',\n      message: 'Missing x-stripe-customer-id header'\n    });\n  }\n\n  try {\n    const history = await usageTracking.getUsageHistory(stripeCustomerId);\n    res.json({\n      success: true,\n      history\n    });\n  } catch (err) {\n    console.error('[SPLICE] Usage history error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Start Server\n// =============================================================================\n\n// Initialize database and start server\nasync function startServer() {\n  try {\n    await usageTracking.initDatabase();\n    console.log('[SPLICE] Database initialized');\n\n    if (isProduction || !httpsOptions) {\n      // Production: Railway provides TLS termination, use HTTP\n      http.createServer(app).listen(PORT, () => {\n        console.log(`[SPLICE] Backend running at http://0.0.0.0:${PORT} (production)`);\n      });\n    } else {\n      // Development: Use HTTPS with local certificates\n      https.createServer(httpsOptions, app).listen(PORT, () => {\n        console.log(`[SPLICE] Backend running at https://127.0.0.1:${PORT} (development)`);\n        console.log(`[SPLICE] POST /analyze with { \"wavPath\": \"/path/to/audio.wav\" }`);\n      });\n    }\n  } catch (err) {\n    console.error('[SPLICE] Failed to start server:', err);\n    process.exit(1);\n  }\n}\n\nstartServer();\n","numLines":1940,"startLine":1,"totalLines":1940}}}
{"parentUuid":"25a5ca4d-e3ee-4435-9bd2-567cf51314ff","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01DVc1oPxLeXxC2wyGgX6wNj","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me read the remaining key files for the analysis:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":14726,"cache_creation_input_tokens":26240,"cache_read_input_tokens":17184,"cache_creation":{"ephemeral_5m_input_tokens":26240,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCNGQ4f3jB5WBpRVnLX","type":"assistant","uuid":"3309593b-c90b-4a78-a6e0-ec680bbfaec9","timestamp":"2025-12-25T22:30:25.444Z"}
{"parentUuid":"3309593b-c90b-4a78-a6e0-ec680bbfaec9","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01DVc1oPxLeXxC2wyGgX6wNj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DB4vR6n6MidL5wFjgkvBGR","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":14726,"cache_creation_input_tokens":26240,"cache_read_input_tokens":17184,"cache_creation":{"ephemeral_5m_input_tokens":26240,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCNGQ4f3jB5WBpRVnLX","type":"assistant","uuid":"3ef6aa2f-40a4-4455-a646-ba6c1b564c86","timestamp":"2025-12-25T22:30:26.555Z"}
{"parentUuid":"3ef6aa2f-40a4-4455-a646-ba6c1b564c86","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01DVc1oPxLeXxC2wyGgX6wNj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VXT1DyWbviGfRTChyHe4Gv","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-plugin/js/main.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":14726,"cache_creation_input_tokens":26240,"cache_read_input_tokens":17184,"cache_creation":{"ephemeral_5m_input_tokens":26240,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCNGQ4f3jB5WBpRVnLX","type":"assistant","uuid":"1d514da5-5c96-49c2-9f4e-98fe89556369","timestamp":"2025-12-25T22:30:27.256Z"}
{"parentUuid":"1d514da5-5c96-49c2-9f4e-98fe89556369","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01DVc1oPxLeXxC2wyGgX6wNj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01LdQuiBSQXzkP42kVR9boT2","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-plugin/js/builder.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":14726,"cache_creation_input_tokens":26240,"cache_read_input_tokens":17184,"cache_creation":{"ephemeral_5m_input_tokens":26240,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCNGQ4f3jB5WBpRVnLX","type":"assistant","uuid":"cd32ec2f-6dd7-4233-8bba-38a0bf48ebe6","timestamp":"2025-12-25T22:30:27.912Z"}
{"parentUuid":"cd32ec2f-6dd7-4233-8bba-38a0bf48ebe6","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01DVc1oPxLeXxC2wyGgX6wNj","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Hk9uaiYWVUaLDhzM8BrEsa","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/repetitionDetection.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":14726,"cache_creation_input_tokens":26240,"cache_read_input_tokens":17184,"cache_creation":{"ephemeral_5m_input_tokens":26240,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCNGQ4f3jB5WBpRVnLX","type":"assistant","uuid":"519f1888-1a52-40e7-a5c5-4b49c53d71a9","timestamp":"2025-12-25T22:30:28.576Z"}
{"parentUuid":"519f1888-1a52-40e7-a5c5-4b49c53d71a9","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DB4vR6n6MidL5wFjgkvBGR","type":"tool_result","content":"     1→/**\n     2→ * RMS-Based Silence Detection Service\n     3→ *\n     4→ * Advanced silence detection using Root Mean Square (RMS) audio analysis.\n     5→ * Based on Fireside's proven approach with enhancements.\n     6→ *\n     7→ * Features:\n     8→ * - RMS calculation from raw audio data\n     9→ * - dBFS (decibels relative to full scale) conversion\n    10→ * - Auto-threshold detection from audio histogram\n    11→ * - Gaussian blur smoothing for noise reduction\n    12→ * - Local maxima detection for speech/silence peak identification\n    13→ * - Configurable padding, seek step, min duration\n    14→ * - Silence merging for adjacent regions\n    15→ */\n    16→\n    17→const { exec } = require('child_process');\n    18→const { promisify } = require('util');\n    19→const fs = require('fs');\n    20→const fsPromises = require('fs').promises;\n    21→const path = require('path');\n    22→const crypto = require('crypto');\n    23→\n    24→const execAsync = promisify(exec);\n    25→\n    26→// =============================================================================\n    27→// Configuration\n    28→// =============================================================================\n    29→\n    30→const DEFAULT_OPTIONS = {\n    31→  threshold: -30,           // dBFS threshold (-60 to -20)\n    32→  minSilenceLength: 0.5,    // Minimum silence duration in seconds\n    33→  seekStep: 0.05,           // Analysis window step in seconds (50ms)\n    34→  paddingStart: 0.1,        // Buffer before silence cut (seconds)\n    35→  paddingEnd: 0.05,         // Buffer after silence cut (seconds)\n    36→  autoThreshold: false,     // Auto-detect optimal threshold\n    37→  mergeDistance: 0.2,       // Merge silences closer than this (seconds)\n    38→  sampleRate: 48000,        // Default sample rate\n    39→  chunkDuration: 30,        // Chunk duration in seconds for long files\n    40→  chunkingEnabled: true     // Enable chunked processing for memory efficiency\n    41→};\n    42→\n    43→// =============================================================================\n    44→// Core RMS Detection\n    45→// =============================================================================\n    46→\n    47→/**\n    48→ * Detect silences using RMS-based audio analysis\n    49→ *\n    50→ * @param {string} audioPath - Path to audio file (WAV, MP3, etc.)\n    51→ * @param {Object} options - Detection options\n    52→ * @returns {Promise<Object>} Detection results with silences and metadata\n    53→ */\n    54→async function detectSilencesRMS(audioPath, options = {}) {\n    55→  const opts = { ...DEFAULT_OPTIONS, ...options };\n    56→\n    57→  console.log(`[SPLICE RMS] Analyzing: ${audioPath}`);\n    58→  console.log(`[SPLICE RMS] Options: threshold=${opts.threshold}dB, minLength=${opts.minSilenceLength}s, seekStep=${opts.seekStep}s`);\n    59→\n    60→  // Check if we should use chunked processing (for files longer than 45 seconds)\n    61→  if (opts.chunkingEnabled !== false) {\n    62→    const duration = await getAudioDuration(audioPath);\n    63→    if (duration > 45) {\n    64→      console.log(`[SPLICE RMS] Long file detected (${duration}s), using chunked processing`);\n    65→      return detectSilencesRMSChunked(audioPath, options);\n    66→    }\n    67→  }\n    68→\n    69→  // Step 1: Extract raw audio data using FFmpeg\n    70→  const audioData = await extractAudioData(audioPath);\n    71→\n    72→  // Step 2: Calculate RMS values for sliding windows\n    73→  const rmsValues = calculateRMSWindows(audioData.samples, {\n    74→    windowSize: Math.floor(opts.seekStep * audioData.sampleRate),\n    75→    hopSize: Math.floor(opts.seekStep * audioData.sampleRate / 2)\n    76→  });\n    77→\n    78→  // Step 3: Convert to dBFS\n    79→  const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n    80→\n    81→  // Step 4: Determine threshold (auto-detect or use provided)\n    82→  let threshold = opts.threshold;\n    83→  let thresholdInfo = { method: 'manual', value: threshold };\n    84→\n    85→  if (opts.autoThreshold) {\n    86→    const autoResult = autoDetectThreshold(dBFSValues);\n    87→    threshold = autoResult.threshold;\n    88→    thresholdInfo = autoResult;\n    89→    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB (${autoResult.method})`);\n    90→  }\n    91→\n    92→  // Step 5: Find silence regions\n    93→  const rawSilences = findSilenceRegions(dBFSValues, {\n    94→    threshold,\n    95→    seekStep: opts.seekStep,\n    96→    minSilenceLength: opts.minSilenceLength\n    97→  });\n    98→\n    99→  // Step 6: Merge adjacent silences\n   100→  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n   101→\n   102→  // Step 7: Apply padding\n   103→  const paddedSilences = applyPadding(mergedSilences, {\n   104→    paddingStart: opts.paddingStart,\n   105→    paddingEnd: opts.paddingEnd,\n   106→    audioDuration: audioData.duration\n   107→  });\n   108→\n   109→  // Calculate statistics\n   110→  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n   111→  const speechDuration = audioData.duration - totalSilenceDuration;\n   112→\n   113→  console.log(`[SPLICE RMS] Detected ${paddedSilences.length} silence(s), total ${totalSilenceDuration.toFixed(2)}s`);\n   114→\n   115→  return {\n   116→    silences: paddedSilences,\n   117→    metadata: {\n   118→      audioDuration: audioData.duration,\n   119→      sampleRate: audioData.sampleRate,\n   120→      channels: audioData.channels,\n   121→      silenceCount: paddedSilences.length,\n   122→      totalSilenceDuration,\n   123→      speechDuration,\n   124→      silencePercentage: ((totalSilenceDuration / audioData.duration) * 100).toFixed(1),\n   125→      threshold: thresholdInfo,\n   126→      rmsAnalysis: {\n   127→        windowCount: dBFSValues.length,\n   128→        minDB: Math.min(...dBFSValues).toFixed(1),\n   129→        maxDB: Math.max(...dBFSValues).toFixed(1),\n   130→        avgDB: (dBFSValues.reduce((a, b) => a + b, 0) / dBFSValues.length).toFixed(1)\n   131→      }\n   132→    }\n   133→  };\n   134→}\n   135→\n   136→// =============================================================================\n   137→// Chunked Audio Processing\n   138→// =============================================================================\n   139→\n   140→/**\n   141→ * Detect silences using chunked RMS analysis for long files\n   142→ * Processes audio in chunks to reduce memory usage\n   143→ *\n   144→ * @param {string} audioPath - Path to audio file\n   145→ * @param {Object} options - Detection options including chunkDuration\n   146→ * @returns {Promise<Object>} Detection results with silences and metadata\n   147→ */\n   148→async function detectSilencesRMSChunked(audioPath, options = {}) {\n   149→  const opts = { ...DEFAULT_OPTIONS, ...options };\n   150→  const chunkDuration = opts.chunkDuration || 30;\n   151→\n   152→  console.log(`[SPLICE RMS] Chunked analysis: ${audioPath}`);\n   153→\n   154→  // Get audio duration first\n   155→  const duration = await getAudioDuration(audioPath);\n   156→  const numChunks = Math.ceil(duration / chunkDuration);\n   157→\n   158→  console.log(`[SPLICE RMS] Duration: ${duration}s, processing in ${numChunks} chunk(s) of ${chunkDuration}s`);\n   159→\n   160→  // If short enough, use regular processing\n   161→  if (duration <= chunkDuration * 1.5) {\n   162→    return detectSilencesRMS(audioPath, { ...opts, chunkingEnabled: false });\n   163→  }\n   164→\n   165→  let allDBFSValues = [];\n   166→  let sampleRateUsed = 16000;\n   167→\n   168→  // Process each chunk\n   169→  for (let i = 0; i < numChunks; i++) {\n   170→    const startTime = i * chunkDuration;\n   171→    const endTime = Math.min((i + 1) * chunkDuration, duration);\n   172→\n   173→    console.log(`[SPLICE RMS] Processing chunk ${i + 1}/${numChunks} (${startTime}s - ${endTime}s)`);\n   174→\n   175→    // Extract chunk audio data\n   176→    const chunkData = await extractAudioDataChunk(audioPath, startTime, endTime - startTime);\n   177→    sampleRateUsed = chunkData.sampleRate;\n   178→\n   179→    // Calculate RMS values for this chunk\n   180→    const rmsValues = calculateRMSWindows(chunkData.samples, {\n   181→      windowSize: Math.floor(opts.seekStep * chunkData.sampleRate),\n   182→      hopSize: Math.floor(opts.seekStep * chunkData.sampleRate / 2)\n   183→    });\n   184→\n   185→    // Convert to dBFS\n   186→    const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n   187→    allDBFSValues = allDBFSValues.concat(Array.from(dBFSValues));\n   188→  }\n   189→\n   190→  // Determine threshold\n   191→  let threshold = opts.threshold;\n   192→  let thresholdInfo = { method: 'manual', value: threshold };\n   193→\n   194→  if (opts.autoThreshold) {\n   195→    const autoResult = autoDetectThreshold(allDBFSValues);\n   196→    threshold = autoResult.threshold;\n   197→    thresholdInfo = autoResult;\n   198→    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB`);\n   199→  }\n   200→\n   201→  // Find silence regions\n   202→  const rawSilences = findSilenceRegions(allDBFSValues, {\n   203→    threshold,\n   204→    seekStep: opts.seekStep,\n   205→    minSilenceLength: opts.minSilenceLength\n   206→  });\n   207→\n   208→  // Merge adjacent silences\n   209→  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n   210→\n   211→  // Apply padding\n   212→  const paddedSilences = applyPadding(mergedSilences, {\n   213→    paddingStart: opts.paddingStart,\n   214→    paddingEnd: opts.paddingEnd,\n   215→    audioDuration: duration\n   216→  });\n   217→\n   218→  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n   219→\n   220→  console.log(`[SPLICE RMS] Chunked detection found ${paddedSilences.length} silence(s)`);\n   221→\n   222→  return {\n   223→    silences: paddedSilences,\n   224→    metadata: {\n   225→      audioDuration: duration,\n   226→      sampleRate: sampleRateUsed,\n   227→      channels: 1,\n   228→      silenceCount: paddedSilences.length,\n   229→      totalSilenceDuration,\n   230→      speechDuration: duration - totalSilenceDuration,\n   231→      silencePercentage: ((totalSilenceDuration / duration) * 100).toFixed(1),\n   232→      threshold: thresholdInfo,\n   233→      chunked: true,\n   234→      chunkCount: numChunks,\n   235→      rmsAnalysis: {\n   236→        windowCount: allDBFSValues.length,\n   237→        minDB: Math.min(...allDBFSValues).toFixed(1),\n   238→        maxDB: Math.max(...allDBFSValues).toFixed(1),\n   239→        avgDB: (allDBFSValues.reduce((a, b) => a + b, 0) / allDBFSValues.length).toFixed(1)\n   240→      }\n   241→    }\n   242→  };\n   243→}\n   244→\n   245→/**\n   246→ * Get audio duration using FFprobe\n   247→ *\n   248→ * @param {string} audioPath - Path to audio file\n   249→ * @returns {Promise<number>} Duration in seconds\n   250→ */\n   251→async function getAudioDuration(audioPath) {\n   252→  try {\n   253→    const durationCmd = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"${audioPath}\"`;\n   254→    const { stdout } = await execAsync(durationCmd);\n   255→    return parseFloat(stdout.trim()) || 0;\n   256→  } catch (err) {\n   257→    console.warn('[SPLICE RMS] Could not get duration:', err.message);\n   258→    return 0;\n   259→  }\n   260→}\n   261→\n   262→/**\n   263→ * Extract a chunk of audio data starting at a specific time\n   264→ *\n   265→ * @param {string} audioPath - Path to audio file\n   266→ * @param {number} startTime - Start time in seconds\n   267→ * @param {number} duration - Duration to extract in seconds\n   268→ * @returns {Promise<Object>} Audio chunk data\n   269→ */\n   270→async function extractAudioDataChunk(audioPath, startTime, duration) {\n   271→  const uniqueId = `${Date.now()}_${crypto.randomBytes(4).toString('hex')}`;\n   272→  const tempFile = path.join('/tmp', `splice_rms_chunk_${uniqueId}.raw`);\n   273→\n   274→  try {\n   275→    // Extract chunk as mono, 16kHz, signed 16-bit PCM\n   276→    const extractCmd = `ffmpeg -y -ss ${startTime} -t ${duration} -i \"${audioPath}\" -ac 1 -ar 16000 -f s16le -acodec pcm_s16le \"${tempFile}\" 2>/dev/null`;\n   277→    await execAsync(extractCmd, { maxBuffer: 50 * 1024 * 1024 });\n   278→\n   279→    const rawData = await fsPromises.readFile(tempFile);\n   280→\n   281→    // Convert to float samples\n   282→    const samples = new Float32Array(rawData.length / 2);\n   283→    for (let i = 0; i < samples.length; i++) {\n   284→      const int16 = rawData.readInt16LE(i * 2);\n   285→      samples[i] = int16 / 32768.0;\n   286→    }\n   287→\n   288→    await fsPromises.unlink(tempFile);\n   289→\n   290→    return {\n   291→      samples,\n   292→      sampleRate: 16000,\n   293→      channels: 1,\n   294→      duration\n   295→    };\n   296→  } catch (err) {\n   297→    try {\n   298→      await fsPromises.access(tempFile);\n   299→      await fsPromises.unlink(tempFile);\n   300→    } catch {\n   301→      // File doesn't exist\n   302→    }\n   303→    throw new Error(`Failed to extract audio chunk: ${err.message}`);\n   304→  }\n   305→}\n   306→\n   307→// =============================================================================\n   308→// Audio Extraction\n   309→// =============================================================================\n   310→\n   311→/**\n   312→ * Extract raw audio samples from file using FFmpeg\n   313→ *\n   314→ * @param {string} audioPath - Path to audio file\n   315→ * @returns {Promise<Object>} Audio data with samples, sampleRate, channels, duration\n   316→ */\n   317→async function extractAudioData(audioPath) {\n   318→  // First, get audio info\n   319→  const infoCmd = `ffprobe -v error -select_streams a:0 -show_entries stream=sample_rate,channels,duration -of json \"${audioPath}\"`;\n   320→\n   321→  let sampleRate = 48000;\n   322→  let channels = 1;\n   323→  let duration = 0;\n   324→\n   325→  try {\n   326→    const { stdout } = await execAsync(infoCmd);\n   327→    const info = JSON.parse(stdout);\n   328→    if (info.streams && info.streams[0]) {\n   329→      sampleRate = parseInt(info.streams[0].sample_rate) || 48000;\n   330→      channels = parseInt(info.streams[0].channels) || 1;\n   331→      duration = parseFloat(info.streams[0].duration) || 0;\n   332→    }\n   333→  } catch (err) {\n   334→    console.warn('[SPLICE RMS] Could not get audio info, using defaults:', err.message);\n   335→  }\n   336→\n   337→  // If duration not available from stream, get from format\n   338→  if (!duration) {\n   339→    try {\n   340→      const durationCmd = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"${audioPath}\"`;\n   341→      const { stdout } = await execAsync(durationCmd);\n   342→      duration = parseFloat(stdout.trim()) || 0;\n   343→    } catch (err) {\n   344→      console.warn('[SPLICE RMS] Could not get duration:', err.message);\n   345→    }\n   346→  }\n   347→\n   348→  // Extract audio as raw PCM floating point\n   349→  // Use unique temp file name with random suffix to prevent race conditions\n   350→  const uniqueId = `${Date.now()}_${crypto.randomBytes(4).toString('hex')}`;\n   351→  const tempFile = path.join('/tmp', `splice_rms_${uniqueId}.raw`);\n   352→\n   353→  try {\n   354→    // Convert to mono, 16kHz for faster processing, output as signed 16-bit PCM\n   355→    const extractCmd = `ffmpeg -y -i \"${audioPath}\" -ac 1 -ar 16000 -f s16le -acodec pcm_s16le \"${tempFile}\" 2>/dev/null`;\n   356→    await execAsync(extractCmd, { maxBuffer: 100 * 1024 * 1024 });\n   357→\n   358→    // Read the raw PCM data (async to avoid blocking event loop)\n   359→    const rawData = await fsPromises.readFile(tempFile);\n   360→\n   361→    // Convert to float samples (-1 to 1)\n   362→    const samples = new Float32Array(rawData.length / 2);\n   363→    for (let i = 0; i < samples.length; i++) {\n   364→      // Read signed 16-bit little-endian\n   365→      const int16 = rawData.readInt16LE(i * 2);\n   366→      samples[i] = int16 / 32768.0;\n   367→    }\n   368→\n   369→    // Clean up temp file (async)\n   370→    await fsPromises.unlink(tempFile);\n   371→\n   372→    return {\n   373→      samples,\n   374→      sampleRate: 16000, // We resampled to 16kHz\n   375→      channels: 1,\n   376→      duration\n   377→    };\n   378→  } catch (err) {\n   379→    // Clean up temp file if exists\n   380→    try {\n   381→      await fsPromises.access(tempFile);\n   382→      await fsPromises.unlink(tempFile);\n   383→    } catch {\n   384→      // File doesn't exist or already deleted\n   385→    }\n   386→    throw new Error(`Failed to extract audio: ${err.message}`);\n   387→  }\n   388→}\n   389→\n   390→// =============================================================================\n   391→// RMS Calculation\n   392→// =============================================================================\n   393→\n   394→/**\n   395→ * Calculate RMS values for sliding windows across audio samples\n   396→ *\n   397→ * @param {Float32Array} samples - Audio samples (-1 to 1)\n   398→ * @param {Object} options - Window and hop size\n   399→ * @returns {Float32Array} RMS values for each window\n   400→ */\n   401→function calculateRMSWindows(samples, options = {}) {\n   402→  const { windowSize = 800, hopSize = 400 } = options; // Default ~50ms at 16kHz\n   403→\n   404→  const numWindows = Math.floor((samples.length - windowSize) / hopSize) + 1;\n   405→  const rmsValues = new Float32Array(numWindows);\n   406→\n   407→  for (let i = 0; i < numWindows; i++) {\n   408→    const start = i * hopSize;\n   409→    const end = Math.min(start + windowSize, samples.length);\n   410→\n   411→    // Calculate RMS for this window\n   412→    let sumSquares = 0;\n   413→    for (let j = start; j < end; j++) {\n   414→      sumSquares += samples[j] * samples[j];\n   415→    }\n   416→\n   417→    const rms = Math.sqrt(sumSquares / (end - start));\n   418→    rmsValues[i] = rms;\n   419→  }\n   420→\n   421→  return rmsValues;\n   422→}\n   423→\n   424→/**\n   425→ * Convert RMS value to dBFS (decibels relative to full scale)\n   426→ *\n   427→ * @param {number} rms - RMS value (0 to 1)\n   428→ * @returns {number} dBFS value (negative, -inf for silence)\n   429→ */\n   430→function rmsToDBFS(rms) {\n   431→  if (rms <= 0) return -100; // Effective silence floor\n   432→  const dbfs = 20 * Math.log10(rms);\n   433→  return Math.max(-100, dbfs); // Clamp to reasonable floor\n   434→}\n   435→\n   436→// =============================================================================\n   437→// Auto-Threshold Detection\n   438→// =============================================================================\n   439→\n   440→/**\n   441→ * Auto-detect optimal silence threshold from audio histogram\n   442→ * Uses Gaussian blur + local maxima detection\n   443→ *\n   444→ * @param {Float32Array} dBFSValues - Array of dBFS values\n   445→ * @returns {Object} Detected threshold and method info\n   446→ */\n   447→function autoDetectThreshold(dBFSValues) {\n   448→  // Build histogram of dBFS values\n   449→  const histBins = 100;\n   450→  const minDB = -80;\n   451→  const maxDB = 0;\n   452→  const binWidth = (maxDB - minDB) / histBins;\n   453→\n   454→  const histogram = new Array(histBins).fill(0);\n   455→\n   456→  for (const db of dBFSValues) {\n   457→    const binIndex = Math.min(histBins - 1, Math.max(0, Math.floor((db - minDB) / binWidth)));\n   458→    histogram[binIndex]++;\n   459→  }\n   460→\n   461→  // Apply Gaussian blur to smooth histogram\n   462→  const smoothed = gaussianBlur(histogram, 5);\n   463→\n   464→  // Find local maxima (peaks)\n   465→  const peaks = findLocalMaxima(smoothed, 3);\n   466→\n   467→  // Sort peaks by value (highest first)\n   468→  peaks.sort((a, b) => b.value - a.value);\n   469→\n   470→  if (peaks.length >= 2) {\n   471→    // Interpolate between the two highest peaks\n   472→    // Assume: higher peak = speech, lower peak = silence\n   473→    const peak1DB = minDB + peaks[0].index * binWidth;\n   474→    const peak2DB = minDB + peaks[1].index * binWidth;\n   475→\n   476→    const silencePeakDB = Math.min(peak1DB, peak2DB);\n   477→    const speechPeakDB = Math.max(peak1DB, peak2DB);\n   478→\n   479→    // Set threshold between peaks (closer to silence peak)\n   480→    const threshold = silencePeakDB + (speechPeakDB - silencePeakDB) * 0.3;\n   481→\n   482→    return {\n   483→      threshold: Math.max(-60, Math.min(-20, threshold)),\n   484→      method: 'dual-peak',\n   485→      silencePeak: silencePeakDB.toFixed(1),\n   486→      speechPeak: speechPeakDB.toFixed(1)\n   487→    };\n   488→  } else if (peaks.length === 1) {\n   489→    // Single peak - use percentile-based fallback\n   490→    const sorted = [...dBFSValues].sort((a, b) => a - b);\n   491→    const percentile25 = sorted[Math.floor(sorted.length * 0.25)];\n   492→\n   493→    return {\n   494→      threshold: Math.max(-60, Math.min(-20, percentile25)),\n   495→      method: 'percentile-25',\n   496→      value: percentile25.toFixed(1)\n   497→    };\n   498→  } else {\n   499→    // No peaks found - use conservative default\n   500→    return {\n   501→      threshold: -35,\n   502→      method: 'default-fallback'\n   503→    };\n   504→  }\n   505→}\n   506→\n   507→/**\n   508→ * Apply Gaussian blur to an array\n   509→ *\n   510→ * @param {Array<number>} arr - Input array\n   511→ * @param {number} sigma - Standard deviation\n   512→ * @returns {Array<number>} Smoothed array\n   513→ */\n   514→function gaussianBlur(arr, sigma = 5) {\n   515→  const kernelSize = Math.ceil(sigma * 3) * 2 + 1;\n   516→  const kernel = [];\n   517→  const halfSize = Math.floor(kernelSize / 2);\n   518→\n   519→  // Generate Gaussian kernel\n   520→  let sum = 0;\n   521→  for (let i = 0; i < kernelSize; i++) {\n   522→    const x = i - halfSize;\n   523→    const g = Math.exp(-(x * x) / (2 * sigma * sigma));\n   524→    kernel.push(g);\n   525→    sum += g;\n   526→  }\n   527→\n   528→  // Normalize kernel\n   529→  for (let i = 0; i < kernelSize; i++) {\n   530→    kernel[i] /= sum;\n   531→  }\n   532→\n   533→  // Apply convolution\n   534→  const result = new Array(arr.length).fill(0);\n   535→  for (let i = 0; i < arr.length; i++) {\n   536→    let value = 0;\n   537→    for (let j = 0; j < kernelSize; j++) {\n   538→      const idx = i + j - halfSize;\n   539→      if (idx >= 0 && idx < arr.length) {\n   540→        value += arr[idx] * kernel[j];\n   541→      }\n   542→    }\n   543→    result[i] = value;\n   544→  }\n   545→\n   546→  return result;\n   547→}\n   548→\n   549→/**\n   550→ * Find local maxima in an array\n   551→ *\n   552→ * @param {Array<number>} arr - Input array\n   553→ * @param {number} windowSize - Neighborhood size for local maximum detection\n   554→ * @returns {Array<{index: number, value: number}>} Array of peak positions and values\n   555→ */\n   556→function findLocalMaxima(arr, windowSize = 3) {\n   557→  const peaks = [];\n   558→  const halfWindow = Math.floor(windowSize / 2);\n   559→\n   560→  for (let i = halfWindow; i < arr.length - halfWindow; i++) {\n   561→    let isMax = true;\n   562→    for (let j = i - halfWindow; j <= i + halfWindow; j++) {\n   563→      if (j !== i && arr[j] >= arr[i]) {\n   564→        isMax = false;\n   565→        break;\n   566→      }\n   567→    }\n   568→    if (isMax && arr[i] > 0) {\n   569→      peaks.push({ index: i, value: arr[i] });\n   570→    }\n   571→  }\n   572→\n   573→  return peaks;\n   574→}\n   575→\n   576→// =============================================================================\n   577→// Silence Region Detection\n   578→// =============================================================================\n   579→\n   580→/**\n   581→ * Find silence regions based on dBFS threshold\n   582→ *\n   583→ * @param {Float32Array} dBFSValues - Array of dBFS values\n   584→ * @param {Object} options - Detection options\n   585→ * @returns {Array<{start: number, end: number, duration: number}>} Silence regions\n   586→ */\n   587→function findSilenceRegions(dBFSValues, options) {\n   588→  const { threshold, seekStep, minSilenceLength } = options;\n   589→\n   590→  const silences = [];\n   591→  let silenceStart = null;\n   592→\n   593→  for (let i = 0; i < dBFSValues.length; i++) {\n   594→    const time = i * seekStep;\n   595→    const isSilent = dBFSValues[i] < threshold;\n   596→\n   597→    if (isSilent && silenceStart === null) {\n   598→      // Start of silence\n   599→      silenceStart = time;\n   600→    } else if (!isSilent && silenceStart !== null) {\n   601→      // End of silence\n   602→      const duration = time - silenceStart;\n   603→      if (duration >= minSilenceLength) {\n   604→        silences.push({\n   605→          start: silenceStart,\n   606→          end: time,\n   607→          duration\n   608→        });\n   609→      }\n   610→      silenceStart = null;\n   611→    }\n   612→  }\n   613→\n   614→  // Handle silence at end of audio\n   615→  if (silenceStart !== null) {\n   616→    const endTime = dBFSValues.length * seekStep;\n   617→    const duration = endTime - silenceStart;\n   618→    if (duration >= minSilenceLength) {\n   619→      silences.push({\n   620→        start: silenceStart,\n   621→        end: endTime,\n   622→        duration\n   623→      });\n   624→    }\n   625→  }\n   626→\n   627→  return silences;\n   628→}\n   629→\n   630→// =============================================================================\n   631→// Post-Processing\n   632→// =============================================================================\n   633→\n   634→/**\n   635→ * Merge silences that are close together\n   636→ *\n   637→ * @param {Array} silences - Array of silence regions\n   638→ * @param {number} mergeDistance - Maximum gap to merge (seconds)\n   639→ * @returns {Array} Merged silence regions\n   640→ */\n   641→function mergeSilences(silences, mergeDistance) {\n   642→  if (silences.length <= 1) return silences;\n   643→\n   644→  const merged = [];\n   645→  let current = { ...silences[0] };\n   646→\n   647→  for (let i = 1; i < silences.length; i++) {\n   648→    const next = silences[i];\n   649→\n   650→    // If gap is small enough, merge\n   651→    if (next.start - current.end <= mergeDistance) {\n   652→      current.end = next.end;\n   653→      current.duration = current.end - current.start;\n   654→    } else {\n   655→      merged.push(current);\n   656→      current = { ...next };\n   657→    }\n   658→  }\n   659→\n   660→  merged.push(current);\n   661→  return merged;\n   662→}\n   663→\n   664→/**\n   665→ * Apply padding to silence boundaries\n   666→ *\n   667→ * @param {Array} silences - Array of silence regions\n   668→ * @param {Object} options - Padding options\n   669→ * @returns {Array} Padded silence regions\n   670→ */\n   671→function applyPadding(silences, options) {\n   672→  const { paddingStart, paddingEnd, audioDuration } = options;\n   673→\n   674→  return silences\n   675→    .map(silence => {\n   676→      const paddedStart = Math.max(0, silence.start + paddingStart);\n   677→      const paddedEnd = Math.min(audioDuration, silence.end - paddingEnd);\n   678→      const duration = paddedEnd - paddedStart;\n   679→\n   680→      // Skip if padding made the silence invalid\n   681→      if (duration <= 0) return null;\n   682→\n   683→      return {\n   684→        start: parseFloat(paddedStart.toFixed(3)),\n   685→        end: parseFloat(paddedEnd.toFixed(3)),\n   686→        duration: parseFloat(duration.toFixed(3))\n   687→      };\n   688→    })\n   689→    .filter(s => s !== null);\n   690→}\n   691→\n   692→// =============================================================================\n   693→// Sensitivity Mapping\n   694→// =============================================================================\n   695→\n   696→/**\n   697→ * Map UI sensitivity (0-100) to detection parameters\n   698→ * Based on Fireside's aggressiveness mapping\n   699→ *\n   700→ * @param {number} sensitivity - UI sensitivity value (0-100)\n   701→ * @returns {Object} Detection parameters\n   702→ */\n   703→function sensitivityToParams(sensitivity) {\n   704→  // Clamp to 0-100\n   705→  const s = Math.max(0, Math.min(100, sensitivity));\n   706→\n   707→  // Linear interpolation helper\n   708→  const lerp = (min, max, t) => min + (max - min) * (t / 100);\n   709→\n   710→  return {\n   711→    // Threshold: -50dB (conservative) to -20dB (aggressive)\n   712→    threshold: lerp(-50, -20, s),\n   713→\n   714→    // Min silence length: 2.0s (conservative) to 0.3s (aggressive)\n   715→    minSilenceLength: lerp(2.0, 0.3, s),\n   716→\n   717→    // Padding start: 0.2s (conservative) to 0.05s (aggressive)\n   718→    paddingStart: lerp(0.2, 0.05, s),\n   719→\n   720→    // Padding end: 0.15s (conservative) to 0.03s (aggressive)\n   721→    paddingEnd: lerp(0.15, 0.03, s),\n   722→\n   723→    // Seek step stays constant\n   724→    seekStep: 0.05\n   725→  };\n   726→}\n   727→\n   728→// =============================================================================\n   729→// Exports\n   730→// =============================================================================\n   731→\n   732→module.exports = {\n   733→  detectSilencesRMS,\n   734→  detectSilencesRMSChunked,\n   735→  sensitivityToParams,\n   736→  autoDetectThreshold,\n   737→  getAudioDuration,\n   738→  // Export internal functions for testing\n   739→  calculateRMSWindows,\n   740→  rmsToDBFS,\n   741→  gaussianBlur,\n   742→  findLocalMaxima,\n   743→  mergeSilences,\n   744→  applyPadding,\n   745→  extractAudioDataChunk,\n   746→  DEFAULT_OPTIONS\n   747→};\n   748→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"14ebf092-5da7-4eed-be44-2bed429fec87","timestamp":"2025-12-25T22:30:28.601Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js","content":"/**\n * RMS-Based Silence Detection Service\n *\n * Advanced silence detection using Root Mean Square (RMS) audio analysis.\n * Based on Fireside's proven approach with enhancements.\n *\n * Features:\n * - RMS calculation from raw audio data\n * - dBFS (decibels relative to full scale) conversion\n * - Auto-threshold detection from audio histogram\n * - Gaussian blur smoothing for noise reduction\n * - Local maxima detection for speech/silence peak identification\n * - Configurable padding, seek step, min duration\n * - Silence merging for adjacent regions\n */\n\nconst { exec } = require('child_process');\nconst { promisify } = require('util');\nconst fs = require('fs');\nconst fsPromises = require('fs').promises;\nconst path = require('path');\nconst crypto = require('crypto');\n\nconst execAsync = promisify(exec);\n\n// =============================================================================\n// Configuration\n// =============================================================================\n\nconst DEFAULT_OPTIONS = {\n  threshold: -30,           // dBFS threshold (-60 to -20)\n  minSilenceLength: 0.5,    // Minimum silence duration in seconds\n  seekStep: 0.05,           // Analysis window step in seconds (50ms)\n  paddingStart: 0.1,        // Buffer before silence cut (seconds)\n  paddingEnd: 0.05,         // Buffer after silence cut (seconds)\n  autoThreshold: false,     // Auto-detect optimal threshold\n  mergeDistance: 0.2,       // Merge silences closer than this (seconds)\n  sampleRate: 48000,        // Default sample rate\n  chunkDuration: 30,        // Chunk duration in seconds for long files\n  chunkingEnabled: true     // Enable chunked processing for memory efficiency\n};\n\n// =============================================================================\n// Core RMS Detection\n// =============================================================================\n\n/**\n * Detect silences using RMS-based audio analysis\n *\n * @param {string} audioPath - Path to audio file (WAV, MP3, etc.)\n * @param {Object} options - Detection options\n * @returns {Promise<Object>} Detection results with silences and metadata\n */\nasync function detectSilencesRMS(audioPath, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n\n  console.log(`[SPLICE RMS] Analyzing: ${audioPath}`);\n  console.log(`[SPLICE RMS] Options: threshold=${opts.threshold}dB, minLength=${opts.minSilenceLength}s, seekStep=${opts.seekStep}s`);\n\n  // Check if we should use chunked processing (for files longer than 45 seconds)\n  if (opts.chunkingEnabled !== false) {\n    const duration = await getAudioDuration(audioPath);\n    if (duration > 45) {\n      console.log(`[SPLICE RMS] Long file detected (${duration}s), using chunked processing`);\n      return detectSilencesRMSChunked(audioPath, options);\n    }\n  }\n\n  // Step 1: Extract raw audio data using FFmpeg\n  const audioData = await extractAudioData(audioPath);\n\n  // Step 2: Calculate RMS values for sliding windows\n  const rmsValues = calculateRMSWindows(audioData.samples, {\n    windowSize: Math.floor(opts.seekStep * audioData.sampleRate),\n    hopSize: Math.floor(opts.seekStep * audioData.sampleRate / 2)\n  });\n\n  // Step 3: Convert to dBFS\n  const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n\n  // Step 4: Determine threshold (auto-detect or use provided)\n  let threshold = opts.threshold;\n  let thresholdInfo = { method: 'manual', value: threshold };\n\n  if (opts.autoThreshold) {\n    const autoResult = autoDetectThreshold(dBFSValues);\n    threshold = autoResult.threshold;\n    thresholdInfo = autoResult;\n    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB (${autoResult.method})`);\n  }\n\n  // Step 5: Find silence regions\n  const rawSilences = findSilenceRegions(dBFSValues, {\n    threshold,\n    seekStep: opts.seekStep,\n    minSilenceLength: opts.minSilenceLength\n  });\n\n  // Step 6: Merge adjacent silences\n  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n\n  // Step 7: Apply padding\n  const paddedSilences = applyPadding(mergedSilences, {\n    paddingStart: opts.paddingStart,\n    paddingEnd: opts.paddingEnd,\n    audioDuration: audioData.duration\n  });\n\n  // Calculate statistics\n  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n  const speechDuration = audioData.duration - totalSilenceDuration;\n\n  console.log(`[SPLICE RMS] Detected ${paddedSilences.length} silence(s), total ${totalSilenceDuration.toFixed(2)}s`);\n\n  return {\n    silences: paddedSilences,\n    metadata: {\n      audioDuration: audioData.duration,\n      sampleRate: audioData.sampleRate,\n      channels: audioData.channels,\n      silenceCount: paddedSilences.length,\n      totalSilenceDuration,\n      speechDuration,\n      silencePercentage: ((totalSilenceDuration / audioData.duration) * 100).toFixed(1),\n      threshold: thresholdInfo,\n      rmsAnalysis: {\n        windowCount: dBFSValues.length,\n        minDB: Math.min(...dBFSValues).toFixed(1),\n        maxDB: Math.max(...dBFSValues).toFixed(1),\n        avgDB: (dBFSValues.reduce((a, b) => a + b, 0) / dBFSValues.length).toFixed(1)\n      }\n    }\n  };\n}\n\n// =============================================================================\n// Chunked Audio Processing\n// =============================================================================\n\n/**\n * Detect silences using chunked RMS analysis for long files\n * Processes audio in chunks to reduce memory usage\n *\n * @param {string} audioPath - Path to audio file\n * @param {Object} options - Detection options including chunkDuration\n * @returns {Promise<Object>} Detection results with silences and metadata\n */\nasync function detectSilencesRMSChunked(audioPath, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n  const chunkDuration = opts.chunkDuration || 30;\n\n  console.log(`[SPLICE RMS] Chunked analysis: ${audioPath}`);\n\n  // Get audio duration first\n  const duration = await getAudioDuration(audioPath);\n  const numChunks = Math.ceil(duration / chunkDuration);\n\n  console.log(`[SPLICE RMS] Duration: ${duration}s, processing in ${numChunks} chunk(s) of ${chunkDuration}s`);\n\n  // If short enough, use regular processing\n  if (duration <= chunkDuration * 1.5) {\n    return detectSilencesRMS(audioPath, { ...opts, chunkingEnabled: false });\n  }\n\n  let allDBFSValues = [];\n  let sampleRateUsed = 16000;\n\n  // Process each chunk\n  for (let i = 0; i < numChunks; i++) {\n    const startTime = i * chunkDuration;\n    const endTime = Math.min((i + 1) * chunkDuration, duration);\n\n    console.log(`[SPLICE RMS] Processing chunk ${i + 1}/${numChunks} (${startTime}s - ${endTime}s)`);\n\n    // Extract chunk audio data\n    const chunkData = await extractAudioDataChunk(audioPath, startTime, endTime - startTime);\n    sampleRateUsed = chunkData.sampleRate;\n\n    // Calculate RMS values for this chunk\n    const rmsValues = calculateRMSWindows(chunkData.samples, {\n      windowSize: Math.floor(opts.seekStep * chunkData.sampleRate),\n      hopSize: Math.floor(opts.seekStep * chunkData.sampleRate / 2)\n    });\n\n    // Convert to dBFS\n    const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n    allDBFSValues = allDBFSValues.concat(Array.from(dBFSValues));\n  }\n\n  // Determine threshold\n  let threshold = opts.threshold;\n  let thresholdInfo = { method: 'manual', value: threshold };\n\n  if (opts.autoThreshold) {\n    const autoResult = autoDetectThreshold(allDBFSValues);\n    threshold = autoResult.threshold;\n    thresholdInfo = autoResult;\n    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB`);\n  }\n\n  // Find silence regions\n  const rawSilences = findSilenceRegions(allDBFSValues, {\n    threshold,\n    seekStep: opts.seekStep,\n    minSilenceLength: opts.minSilenceLength\n  });\n\n  // Merge adjacent silences\n  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n\n  // Apply padding\n  const paddedSilences = applyPadding(mergedSilences, {\n    paddingStart: opts.paddingStart,\n    paddingEnd: opts.paddingEnd,\n    audioDuration: duration\n  });\n\n  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n\n  console.log(`[SPLICE RMS] Chunked detection found ${paddedSilences.length} silence(s)`);\n\n  return {\n    silences: paddedSilences,\n    metadata: {\n      audioDuration: duration,\n      sampleRate: sampleRateUsed,\n      channels: 1,\n      silenceCount: paddedSilences.length,\n      totalSilenceDuration,\n      speechDuration: duration - totalSilenceDuration,\n      silencePercentage: ((totalSilenceDuration / duration) * 100).toFixed(1),\n      threshold: thresholdInfo,\n      chunked: true,\n      chunkCount: numChunks,\n      rmsAnalysis: {\n        windowCount: allDBFSValues.length,\n        minDB: Math.min(...allDBFSValues).toFixed(1),\n        maxDB: Math.max(...allDBFSValues).toFixed(1),\n        avgDB: (allDBFSValues.reduce((a, b) => a + b, 0) / allDBFSValues.length).toFixed(1)\n      }\n    }\n  };\n}\n\n/**\n * Get audio duration using FFprobe\n *\n * @param {string} audioPath - Path to audio file\n * @returns {Promise<number>} Duration in seconds\n */\nasync function getAudioDuration(audioPath) {\n  try {\n    const durationCmd = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"${audioPath}\"`;\n    const { stdout } = await execAsync(durationCmd);\n    return parseFloat(stdout.trim()) || 0;\n  } catch (err) {\n    console.warn('[SPLICE RMS] Could not get duration:', err.message);\n    return 0;\n  }\n}\n\n/**\n * Extract a chunk of audio data starting at a specific time\n *\n * @param {string} audioPath - Path to audio file\n * @param {number} startTime - Start time in seconds\n * @param {number} duration - Duration to extract in seconds\n * @returns {Promise<Object>} Audio chunk data\n */\nasync function extractAudioDataChunk(audioPath, startTime, duration) {\n  const uniqueId = `${Date.now()}_${crypto.randomBytes(4).toString('hex')}`;\n  const tempFile = path.join('/tmp', `splice_rms_chunk_${uniqueId}.raw`);\n\n  try {\n    // Extract chunk as mono, 16kHz, signed 16-bit PCM\n    const extractCmd = `ffmpeg -y -ss ${startTime} -t ${duration} -i \"${audioPath}\" -ac 1 -ar 16000 -f s16le -acodec pcm_s16le \"${tempFile}\" 2>/dev/null`;\n    await execAsync(extractCmd, { maxBuffer: 50 * 1024 * 1024 });\n\n    const rawData = await fsPromises.readFile(tempFile);\n\n    // Convert to float samples\n    const samples = new Float32Array(rawData.length / 2);\n    for (let i = 0; i < samples.length; i++) {\n      const int16 = rawData.readInt16LE(i * 2);\n      samples[i] = int16 / 32768.0;\n    }\n\n    await fsPromises.unlink(tempFile);\n\n    return {\n      samples,\n      sampleRate: 16000,\n      channels: 1,\n      duration\n    };\n  } catch (err) {\n    try {\n      await fsPromises.access(tempFile);\n      await fsPromises.unlink(tempFile);\n    } catch {\n      // File doesn't exist\n    }\n    throw new Error(`Failed to extract audio chunk: ${err.message}`);\n  }\n}\n\n// =============================================================================\n// Audio Extraction\n// =============================================================================\n\n/**\n * Extract raw audio samples from file using FFmpeg\n *\n * @param {string} audioPath - Path to audio file\n * @returns {Promise<Object>} Audio data with samples, sampleRate, channels, duration\n */\nasync function extractAudioData(audioPath) {\n  // First, get audio info\n  const infoCmd = `ffprobe -v error -select_streams a:0 -show_entries stream=sample_rate,channels,duration -of json \"${audioPath}\"`;\n\n  let sampleRate = 48000;\n  let channels = 1;\n  let duration = 0;\n\n  try {\n    const { stdout } = await execAsync(infoCmd);\n    const info = JSON.parse(stdout);\n    if (info.streams && info.streams[0]) {\n      sampleRate = parseInt(info.streams[0].sample_rate) || 48000;\n      channels = parseInt(info.streams[0].channels) || 1;\n      duration = parseFloat(info.streams[0].duration) || 0;\n    }\n  } catch (err) {\n    console.warn('[SPLICE RMS] Could not get audio info, using defaults:', err.message);\n  }\n\n  // If duration not available from stream, get from format\n  if (!duration) {\n    try {\n      const durationCmd = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"${audioPath}\"`;\n      const { stdout } = await execAsync(durationCmd);\n      duration = parseFloat(stdout.trim()) || 0;\n    } catch (err) {\n      console.warn('[SPLICE RMS] Could not get duration:', err.message);\n    }\n  }\n\n  // Extract audio as raw PCM floating point\n  // Use unique temp file name with random suffix to prevent race conditions\n  const uniqueId = `${Date.now()}_${crypto.randomBytes(4).toString('hex')}`;\n  const tempFile = path.join('/tmp', `splice_rms_${uniqueId}.raw`);\n\n  try {\n    // Convert to mono, 16kHz for faster processing, output as signed 16-bit PCM\n    const extractCmd = `ffmpeg -y -i \"${audioPath}\" -ac 1 -ar 16000 -f s16le -acodec pcm_s16le \"${tempFile}\" 2>/dev/null`;\n    await execAsync(extractCmd, { maxBuffer: 100 * 1024 * 1024 });\n\n    // Read the raw PCM data (async to avoid blocking event loop)\n    const rawData = await fsPromises.readFile(tempFile);\n\n    // Convert to float samples (-1 to 1)\n    const samples = new Float32Array(rawData.length / 2);\n    for (let i = 0; i < samples.length; i++) {\n      // Read signed 16-bit little-endian\n      const int16 = rawData.readInt16LE(i * 2);\n      samples[i] = int16 / 32768.0;\n    }\n\n    // Clean up temp file (async)\n    await fsPromises.unlink(tempFile);\n\n    return {\n      samples,\n      sampleRate: 16000, // We resampled to 16kHz\n      channels: 1,\n      duration\n    };\n  } catch (err) {\n    // Clean up temp file if exists\n    try {\n      await fsPromises.access(tempFile);\n      await fsPromises.unlink(tempFile);\n    } catch {\n      // File doesn't exist or already deleted\n    }\n    throw new Error(`Failed to extract audio: ${err.message}`);\n  }\n}\n\n// =============================================================================\n// RMS Calculation\n// =============================================================================\n\n/**\n * Calculate RMS values for sliding windows across audio samples\n *\n * @param {Float32Array} samples - Audio samples (-1 to 1)\n * @param {Object} options - Window and hop size\n * @returns {Float32Array} RMS values for each window\n */\nfunction calculateRMSWindows(samples, options = {}) {\n  const { windowSize = 800, hopSize = 400 } = options; // Default ~50ms at 16kHz\n\n  const numWindows = Math.floor((samples.length - windowSize) / hopSize) + 1;\n  const rmsValues = new Float32Array(numWindows);\n\n  for (let i = 0; i < numWindows; i++) {\n    const start = i * hopSize;\n    const end = Math.min(start + windowSize, samples.length);\n\n    // Calculate RMS for this window\n    let sumSquares = 0;\n    for (let j = start; j < end; j++) {\n      sumSquares += samples[j] * samples[j];\n    }\n\n    const rms = Math.sqrt(sumSquares / (end - start));\n    rmsValues[i] = rms;\n  }\n\n  return rmsValues;\n}\n\n/**\n * Convert RMS value to dBFS (decibels relative to full scale)\n *\n * @param {number} rms - RMS value (0 to 1)\n * @returns {number} dBFS value (negative, -inf for silence)\n */\nfunction rmsToDBFS(rms) {\n  if (rms <= 0) return -100; // Effective silence floor\n  const dbfs = 20 * Math.log10(rms);\n  return Math.max(-100, dbfs); // Clamp to reasonable floor\n}\n\n// =============================================================================\n// Auto-Threshold Detection\n// =============================================================================\n\n/**\n * Auto-detect optimal silence threshold from audio histogram\n * Uses Gaussian blur + local maxima detection\n *\n * @param {Float32Array} dBFSValues - Array of dBFS values\n * @returns {Object} Detected threshold and method info\n */\nfunction autoDetectThreshold(dBFSValues) {\n  // Build histogram of dBFS values\n  const histBins = 100;\n  const minDB = -80;\n  const maxDB = 0;\n  const binWidth = (maxDB - minDB) / histBins;\n\n  const histogram = new Array(histBins).fill(0);\n\n  for (const db of dBFSValues) {\n    const binIndex = Math.min(histBins - 1, Math.max(0, Math.floor((db - minDB) / binWidth)));\n    histogram[binIndex]++;\n  }\n\n  // Apply Gaussian blur to smooth histogram\n  const smoothed = gaussianBlur(histogram, 5);\n\n  // Find local maxima (peaks)\n  const peaks = findLocalMaxima(smoothed, 3);\n\n  // Sort peaks by value (highest first)\n  peaks.sort((a, b) => b.value - a.value);\n\n  if (peaks.length >= 2) {\n    // Interpolate between the two highest peaks\n    // Assume: higher peak = speech, lower peak = silence\n    const peak1DB = minDB + peaks[0].index * binWidth;\n    const peak2DB = minDB + peaks[1].index * binWidth;\n\n    const silencePeakDB = Math.min(peak1DB, peak2DB);\n    const speechPeakDB = Math.max(peak1DB, peak2DB);\n\n    // Set threshold between peaks (closer to silence peak)\n    const threshold = silencePeakDB + (speechPeakDB - silencePeakDB) * 0.3;\n\n    return {\n      threshold: Math.max(-60, Math.min(-20, threshold)),\n      method: 'dual-peak',\n      silencePeak: silencePeakDB.toFixed(1),\n      speechPeak: speechPeakDB.toFixed(1)\n    };\n  } else if (peaks.length === 1) {\n    // Single peak - use percentile-based fallback\n    const sorted = [...dBFSValues].sort((a, b) => a - b);\n    const percentile25 = sorted[Math.floor(sorted.length * 0.25)];\n\n    return {\n      threshold: Math.max(-60, Math.min(-20, percentile25)),\n      method: 'percentile-25',\n      value: percentile25.toFixed(1)\n    };\n  } else {\n    // No peaks found - use conservative default\n    return {\n      threshold: -35,\n      method: 'default-fallback'\n    };\n  }\n}\n\n/**\n * Apply Gaussian blur to an array\n *\n * @param {Array<number>} arr - Input array\n * @param {number} sigma - Standard deviation\n * @returns {Array<number>} Smoothed array\n */\nfunction gaussianBlur(arr, sigma = 5) {\n  const kernelSize = Math.ceil(sigma * 3) * 2 + 1;\n  const kernel = [];\n  const halfSize = Math.floor(kernelSize / 2);\n\n  // Generate Gaussian kernel\n  let sum = 0;\n  for (let i = 0; i < kernelSize; i++) {\n    const x = i - halfSize;\n    const g = Math.exp(-(x * x) / (2 * sigma * sigma));\n    kernel.push(g);\n    sum += g;\n  }\n\n  // Normalize kernel\n  for (let i = 0; i < kernelSize; i++) {\n    kernel[i] /= sum;\n  }\n\n  // Apply convolution\n  const result = new Array(arr.length).fill(0);\n  for (let i = 0; i < arr.length; i++) {\n    let value = 0;\n    for (let j = 0; j < kernelSize; j++) {\n      const idx = i + j - halfSize;\n      if (idx >= 0 && idx < arr.length) {\n        value += arr[idx] * kernel[j];\n      }\n    }\n    result[i] = value;\n  }\n\n  return result;\n}\n\n/**\n * Find local maxima in an array\n *\n * @param {Array<number>} arr - Input array\n * @param {number} windowSize - Neighborhood size for local maximum detection\n * @returns {Array<{index: number, value: number}>} Array of peak positions and values\n */\nfunction findLocalMaxima(arr, windowSize = 3) {\n  const peaks = [];\n  const halfWindow = Math.floor(windowSize / 2);\n\n  for (let i = halfWindow; i < arr.length - halfWindow; i++) {\n    let isMax = true;\n    for (let j = i - halfWindow; j <= i + halfWindow; j++) {\n      if (j !== i && arr[j] >= arr[i]) {\n        isMax = false;\n        break;\n      }\n    }\n    if (isMax && arr[i] > 0) {\n      peaks.push({ index: i, value: arr[i] });\n    }\n  }\n\n  return peaks;\n}\n\n// =============================================================================\n// Silence Region Detection\n// =============================================================================\n\n/**\n * Find silence regions based on dBFS threshold\n *\n * @param {Float32Array} dBFSValues - Array of dBFS values\n * @param {Object} options - Detection options\n * @returns {Array<{start: number, end: number, duration: number}>} Silence regions\n */\nfunction findSilenceRegions(dBFSValues, options) {\n  const { threshold, seekStep, minSilenceLength } = options;\n\n  const silences = [];\n  let silenceStart = null;\n\n  for (let i = 0; i < dBFSValues.length; i++) {\n    const time = i * seekStep;\n    const isSilent = dBFSValues[i] < threshold;\n\n    if (isSilent && silenceStart === null) {\n      // Start of silence\n      silenceStart = time;\n    } else if (!isSilent && silenceStart !== null) {\n      // End of silence\n      const duration = time - silenceStart;\n      if (duration >= minSilenceLength) {\n        silences.push({\n          start: silenceStart,\n          end: time,\n          duration\n        });\n      }\n      silenceStart = null;\n    }\n  }\n\n  // Handle silence at end of audio\n  if (silenceStart !== null) {\n    const endTime = dBFSValues.length * seekStep;\n    const duration = endTime - silenceStart;\n    if (duration >= minSilenceLength) {\n      silences.push({\n        start: silenceStart,\n        end: endTime,\n        duration\n      });\n    }\n  }\n\n  return silences;\n}\n\n// =============================================================================\n// Post-Processing\n// =============================================================================\n\n/**\n * Merge silences that are close together\n *\n * @param {Array} silences - Array of silence regions\n * @param {number} mergeDistance - Maximum gap to merge (seconds)\n * @returns {Array} Merged silence regions\n */\nfunction mergeSilences(silences, mergeDistance) {\n  if (silences.length <= 1) return silences;\n\n  const merged = [];\n  let current = { ...silences[0] };\n\n  for (let i = 1; i < silences.length; i++) {\n    const next = silences[i];\n\n    // If gap is small enough, merge\n    if (next.start - current.end <= mergeDistance) {\n      current.end = next.end;\n      current.duration = current.end - current.start;\n    } else {\n      merged.push(current);\n      current = { ...next };\n    }\n  }\n\n  merged.push(current);\n  return merged;\n}\n\n/**\n * Apply padding to silence boundaries\n *\n * @param {Array} silences - Array of silence regions\n * @param {Object} options - Padding options\n * @returns {Array} Padded silence regions\n */\nfunction applyPadding(silences, options) {\n  const { paddingStart, paddingEnd, audioDuration } = options;\n\n  return silences\n    .map(silence => {\n      const paddedStart = Math.max(0, silence.start + paddingStart);\n      const paddedEnd = Math.min(audioDuration, silence.end - paddingEnd);\n      const duration = paddedEnd - paddedStart;\n\n      // Skip if padding made the silence invalid\n      if (duration <= 0) return null;\n\n      return {\n        start: parseFloat(paddedStart.toFixed(3)),\n        end: parseFloat(paddedEnd.toFixed(3)),\n        duration: parseFloat(duration.toFixed(3))\n      };\n    })\n    .filter(s => s !== null);\n}\n\n// =============================================================================\n// Sensitivity Mapping\n// =============================================================================\n\n/**\n * Map UI sensitivity (0-100) to detection parameters\n * Based on Fireside's aggressiveness mapping\n *\n * @param {number} sensitivity - UI sensitivity value (0-100)\n * @returns {Object} Detection parameters\n */\nfunction sensitivityToParams(sensitivity) {\n  // Clamp to 0-100\n  const s = Math.max(0, Math.min(100, sensitivity));\n\n  // Linear interpolation helper\n  const lerp = (min, max, t) => min + (max - min) * (t / 100);\n\n  return {\n    // Threshold: -50dB (conservative) to -20dB (aggressive)\n    threshold: lerp(-50, -20, s),\n\n    // Min silence length: 2.0s (conservative) to 0.3s (aggressive)\n    minSilenceLength: lerp(2.0, 0.3, s),\n\n    // Padding start: 0.2s (conservative) to 0.05s (aggressive)\n    paddingStart: lerp(0.2, 0.05, s),\n\n    // Padding end: 0.15s (conservative) to 0.03s (aggressive)\n    paddingEnd: lerp(0.15, 0.03, s),\n\n    // Seek step stays constant\n    seekStep: 0.05\n  };\n}\n\n// =============================================================================\n// Exports\n// =============================================================================\n\nmodule.exports = {\n  detectSilencesRMS,\n  detectSilencesRMSChunked,\n  sensitivityToParams,\n  autoDetectThreshold,\n  getAudioDuration,\n  // Export internal functions for testing\n  calculateRMSWindows,\n  rmsToDBFS,\n  gaussianBlur,\n  findLocalMaxima,\n  mergeSilences,\n  applyPadding,\n  extractAudioDataChunk,\n  DEFAULT_OPTIONS\n};\n","numLines":748,"startLine":1,"totalLines":748}}}
{"parentUuid":"14ebf092-5da7-4eed-be44-2bed429fec87","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01LdQuiBSQXzkP42kVR9boT2","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Direct DOM Sequence Builder (v3.5)\n     3→ *\n     4→ * Builds sequences directly using UXP DOM APIs.\n     5→ * No XML export/import required - zero manual steps.\n     6→ *\n     7→ * Key APIs:\n     8→ * - SequenceEditor.createInsertProjectItemAction() - Insert clips at timecode\n     9→ * - ProjectItem.createSetColorLabelAction(index) - Color clips (set BEFORE insert)\n    10→ * - TrackItem.createSetInPointAction/OutPointAction() - Set in/out points\n    11→ * - project.executeTransaction(actions) - Batch execute atomically\n    12→ */\n    13→\n    14→const pproBuilder = require('premierepro');\n    15→\n    16→// No TICKS_PER_SECOND constant - we use native TickTime API instead\n    17→// This avoids variable collision with config.js\n    18→\n    19→// Color label indices in Premiere Pro\n    20→const COLOR_LABELS = {\n    21→  NONE: 0,\n    22→  VIOLET: 1,    // Purple\n    23→  IRIS: 2,      // Light purple\n    24→  CARIBBEAN: 3, // Teal\n    25→  LAVENDER: 4,  // Light lavender\n    26→  CERULEAN: 5,  // Light blue\n    27→  FOREST: 6,    // Dark green\n    28→  ROSE: 7,      // Pink\n    29→  MANGO: 8,     // Orange\n    30→  PURPLE: 9,    // Dark purple\n    31→  BLUE: 10,     // Blue\n    32→  TEAL: 11,     // Teal/Cyan\n    33→  MAGENTA: 12,  // Magenta\n    34→  TAN: 13,      // Tan/Brown\n    35→  GREEN: 14,    // Green\n    36→  BROWN: 15,    // Dark brown\n    37→  YELLOW: 16    // Yellow\n    38→};\n    39→\n    40→// SPLICE color scheme for cut list clips\n    41→const SPLICE_COLORS = {\n    42→  SPEECH: COLOR_LABELS.GREEN,      // Green for speech/takes\n    43→  SILENCE: COLOR_LABELS.VIOLET,    // Purple for silence segments (if kept)\n    44→  BEST_TAKE: COLOR_LABELS.CERULEAN // Light blue for best take\n    45→};\n    46→\n    47→/**\n    48→ * Find a project item by name in the project bin\n    49→ * @param {Object} project - The active project\n    50→ * @param {string} name - Name of the clip to find\n    51→ * @returns {Promise<Object|null>} The project item or null\n    52→ */\n    53→async function findProjectItemByName(project, name) {\n    54→  const rootItem = await project.getRootItem();\n    55→  return await searchItemRecursively(rootItem, name);\n    56→}\n    57→\n    58→/**\n    59→ * Recursively search for an item in the project bin\n    60→ * @param {Object} item - Current item to search\n    61→ * @param {string} name - Name to search for\n    62→ * @returns {Promise<Object|null>} The found item or null\n    63→ */\n    64→async function searchItemRecursively(item, name) {\n    65→  if (!item || !item.children) return null;\n    66→\n    67→  if (item.children.numItems === 0) return null;\n    68→\n    69→  for (let i = 0; i < item.children.numItems; i++) {\n    70→    const child = item.children[i];\n    71→\n    72→    // Check name using property access\n    73→    if (child.name === name) {\n    74→      return child;\n    75→    }\n    76→\n    77→    // Check if it's a bin (Type 2) and search recursively\n    78→    if (child.type === pproBuilder.Constants.ProjectItemType.BIN) {\n    79→      const found = await searchItemRecursively(child, name);\n    80→      if (found) return found;\n    81→    }\n    82→  }\n    83→\n    84→  return null;\n    85→}\n    86→\n    87→/**\n    88→ * Find project item by file path\n    89→ * @param {Object} project - The active project\n    90→ * @param {string} filePath - Full path to the media file\n    91→ * @returns {Promise<Object|null>} The project item or null\n    92→ */\n    93→async function findProjectItemByPath(project, filePath) {\n    94→  const rootItem = await project.getRootItem();\n    95→  return await searchItemByPathRecursively(rootItem, filePath);\n    96→}\n    97→\n    98→/**\n    99→ * Recursively search for an item by its file path\n   100→ * @param {Object} item - Current item to search\n   101→ * @param {string} filePath - Path to search for\n   102→ * @returns {Promise<Object|null>} The found item or null\n   103→ */\n   104→async function searchItemByPathRecursively(item, filePath) {\n   105→  if (!item || !item.children) return null;\n   106→\n   107→  if (item.children.numItems === 0) return null;\n   108→\n   109→  for (let i = 0; i < item.children.numItems; i++) {\n   110→    const child = item.children[i];\n   111→\n   112→    // Check if this item has a matching path (using property access)\n   113→    if (child.treePath === filePath) {\n   114→      return child;\n   115→    }\n   116→\n   117→    // Check if it's a bin (Type 2) and search recursively\n   118→    if (child.type === pproBuilder.Constants.ProjectItemType.BIN) {\n   119→      const found = await searchItemByPathRecursively(child, filePath);\n   120→      if (found) return found;\n   121→    }\n   122→  }\n   123→\n   124→  return null;\n   125→}\n   126→\n   127→/**\n   128→ * Create a new sequence from a cut list\n   129→ * @param {Object} cutList - The cut list from backend\n   130→ * @param {Object} options - Build options\n   131→ * @returns {Promise<{success: boolean, sequence?: Object, error?: string, stats?: Object}>}\n   132→ */\n   133→async function buildSequenceFromCutList(cutList, options = {}) {\n   134→  const stats = {\n   135→    clipsInserted: 0,\n   136→    totalDuration: 0,\n   137→    errors: []\n   138→  };\n   139→\n   140→  try {\n   141→    const context = await getActiveSequence();\n   142→    if (!context) {\n   143→      return { success: false, error: 'No project or sequence open' };\n   144→    }\n   145→\n   146→    const { project, sequence } = context;\n   147→\n   148→    // Create new sequence with \"_SPLICE\" suffix by cloning the existing sequence\n   149→    const originalName = sequence.name;\n   150→    const newSequenceName = `${originalName}_SPLICE`;\n   151→\n   152→    // Clone the existing sequence to preserve settings\n   153→    let newSequence = null;\n   154→    try {\n   155→      await project.lockedAccess(async () => {\n   156→        await project.executeTransaction((compoundAction) => {\n   157→          const cloneAction = sequence.createCloneAction();\n   158→          compoundAction.addAction(cloneAction);\n   159→        }, 'SPLICE: Clone Sequence');\n   160→      });\n   161→    } catch (cloneErr) {\n   162→      console.error('[SPLICE Builder] Failed to clone sequence:', cloneErr);\n   163→      return { success: false, error: `Failed to clone sequence: ${cloneErr.message}`, stats };\n   164→    }\n   165→\n   166→    // Find the newly created sequence (it will have the same name as original)\n   167→    // We need to rename it\n   168→    const sequences = await project.getSequences();\n   169→    for (const seq of sequences) {\n   170→      const name = seq.name;\n   171→      if (name === originalName && seq !== sequence) {\n   172→        newSequence = seq;\n   173→        // Rename the cloned sequence\n   174→        try {\n   175→          await project.lockedAccess(async () => {\n   176→            await project.executeTransaction((compoundAction) => {\n   177→              const renameAction = newSequence.createSetNameAction(newSequenceName);\n   178→              compoundAction.addAction(renameAction);\n   179→            }, 'SPLICE: Rename Cloned Sequence');\n   180→          });\n   181→        } catch (renameErr) {\n   182→          console.error('[SPLICE Builder] Failed to rename sequence:', renameErr);\n   183→          // Continue anyway - sequence was cloned, just has wrong name\n   184→        }\n   185→        break;\n   186→      }\n   187→    }\n   188→\n   189→    if (!newSequence) {\n   190→      // Fallback: create a new sequence from scratch\n   191→      newSequence = await project.createSequence(newSequenceName);\n   192→      if (!newSequence) {\n   193→        return { success: false, error: 'Failed to create new sequence' };\n   194→      }\n   195→    }\n   196→\n   197→    // Get the sequence editor for inserting clips\n   198→    const editor = await pproBuilder.SequenceEditor.getEditor(newSequence);\n   199→    if (!editor) {\n   200→      return { success: false, error: 'Failed to get sequence editor' };\n   201→    }\n   202→\n   203→    // Process cut list segments\n   204→    const segments = cutList.segments || [];\n   205→    if (segments.length === 0) {\n   206→      return { success: false, error: 'Cut list has no segments' };\n   207→    }\n   208→\n   209→    // Prepare segment data for insertion\n   210→    const segmentsToInsert = [];\n   211→    let currentPositionSeconds = 0;\n   212→\n   213→    for (const segment of segments) {\n   214→      try {\n   215→        // Find the source project item\n   216→        const sourceItem = await findProjectItemByPath(project, segment.sourcePath)\n   217→          || await findProjectItemByName(project, segment.sourceName);\n   218→\n   219→        if (!sourceItem) {\n   220→          stats.errors.push(`Source not found: ${segment.sourceName || segment.sourcePath}`);\n   221→          continue;\n   222→        }\n   223→\n   224→        // Create TickTime objects directly from seconds (no TICKS_PER_SECOND needed)\n   225→        const inPoint = pproBuilder.TickTime.createWithSeconds(segment.inPoint);\n   226→        const outPoint = pproBuilder.TickTime.createWithSeconds(segment.outPoint);\n   227→        const position = pproBuilder.TickTime.createWithSeconds(currentPositionSeconds);\n   228→\n   229→        // Handle J-cut/L-cut audio offsets if present\n   230→        let audioInPoint = inPoint;\n   231→        let audioOutPoint = outPoint;\n   232→\n   233→        if (segment.audioInPoint !== undefined) {\n   234→          audioInPoint = pproBuilder.TickTime.createWithSeconds(segment.audioInPoint);\n   235→        }\n   236→        if (segment.audioOutPoint !== undefined) {\n   237→          audioOutPoint = pproBuilder.TickTime.createWithSeconds(segment.audioOutPoint);\n   238→        }\n   239→\n   240→        // Calculate duration in seconds for next position\n   241→        const durationSeconds = segment.outPoint - segment.inPoint;\n   242→\n   243→        // Determine color based on segment type\n   244→        const colorIndex = getColorForSegmentType(segment.type);\n   245→\n   246→        segmentsToInsert.push({\n   247→          sourceItem,\n   248→          positionTime: position,\n   249→          inPoint,\n   250→          outPoint,\n   251→          audioInPoint,\n   252→          audioOutPoint,\n   253→          hasAudioOffset: segment.audioInPoint !== undefined || segment.audioOutPoint !== undefined,\n   254→          colorIndex\n   255→        });\n   256→\n   257→        // Update position for next clip\n   258→        currentPositionSeconds += durationSeconds;\n   259→        stats.totalDuration += durationSeconds;\n   260→\n   261→      } catch (err) {\n   262→        stats.errors.push(`Error processing segment: ${err.message}`);\n   263→        console.error('[SPLICE Builder] Segment error:', err);\n   264→      }\n   265→    }\n   266→\n   267→    if (segmentsToInsert.length === 0) {\n   268→      return { success: false, error: 'No valid segments to insert' };\n   269→    }\n   270→\n   271→    // Clear the new sequence first (it's a clone, so it has content)\n   272→    // We'll insert clips from scratch\n   273→    await clearSequence(newSequence);\n   274→\n   275→    // Execute all insert actions in a single transaction\n   276→    try {\n   277→      await project.lockedAccess(async () => {\n   278→        await project.executeTransaction((compoundAction) => {\n   279→          for (const segData of segmentsToInsert) {\n   280→            // Set color on source item if needed\n   281→            if (segData.colorIndex !== null) {\n   282→              const colorAction = segData.sourceItem.createSetColorLabelAction(segData.colorIndex);\n   283→              compoundAction.addAction(colorAction);\n   284→            }\n   285→\n   286→            // Use the pre-created TickTime object (no conversion needed)\n   287→            const insertTime = segData.positionTime;\n   288→\n   289→            // Create insert action (must be done inside transaction)\n   290→            const insertAction = editor.createInsertProjectItemAction(\n   291→              segData.sourceItem,\n   292→              insertTime,\n   293→              0,  // Video track index\n   294→              0,  // Audio track index\n   295→              false // Don't limit shift - insert on all tracks\n   296→            );\n   297→            compoundAction.addAction(insertAction);\n   298→          }\n   299→        }, 'SPLICE: Build Sequence from Cut List');\n   300→      });\n   301→    } catch (insertErr) {\n   302→      console.error('[SPLICE Builder] Failed to insert clips:', insertErr);\n   303→      return { success: false, error: `Failed to insert clips: ${insertErr.message}`, stats };\n   304→    }\n   305→\n   306→    stats.clipsInserted = segmentsToInsert.length;\n   307→\n   308→    // Set in/out points for each track item (must be done after insert)\n   309→    await setTrackItemInOutPoints(newSequence, segmentsToInsert);\n   310→\n   311→    console.log(`[SPLICE Builder] Built sequence with ${stats.clipsInserted} clips`);\n   312→\n   313→    return {\n   314→      success: true,\n   315→      sequence: newSequence,\n   316→      sequenceName: newSequenceName,\n   317→      stats\n   318→    };\n   319→\n   320→  } catch (err) {\n   321→    console.error('[SPLICE Builder] Build error:', err);\n   322→    return { success: false, error: err.message, stats };\n   323→  }\n   324→}\n   325→\n   326→/**\n   327→ * Get color index for a segment type\n   328→ * @param {string} type - Segment type (speech, silence, best_take)\n   329→ * @returns {number|null} Color label index or null for no color\n   330→ */\n   331→function getColorForSegmentType(type) {\n   332→  switch (type) {\n   333→    case 'speech':\n   334→    case 'take':\n   335→      return SPLICE_COLORS.SPEECH;\n   336→    case 'best_take':\n   337→      return SPLICE_COLORS.BEST_TAKE;\n   338→    case 'silence':\n   339→      return SPLICE_COLORS.SILENCE;\n   340→    default:\n   341→      return null;\n   342→  }\n   343→}\n   344→\n   345→/**\n   346→ * Clear all clips from a sequence\n   347→ * @param {Object} sequence - The sequence to clear\n   348→ */\n   349→async function clearSequence(sequence) {\n   350→  try {\n   351→    const project = await pproBuilder.Project.getActiveProject();\n   352→\n   353→    // Get all video tracks\n   354→    const videoTrackCount = await sequence.getVideoTrackCount();\n   355→    const audioTrackCount = await sequence.getAudioTrackCount();\n   356→\n   357→    const itemsToDelete = [];\n   358→\n   359→    // Collect all video track items\n   360→    for (let i = 0; i < videoTrackCount; i++) {\n   361→      const track = await sequence.getVideoTrack(i);\n   362→      if (track) {\n   363→        const items = await track.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n   364→        if (items) {\n   365→          itemsToDelete.push(...items);\n   366→        }\n   367→      }\n   368→    }\n   369→\n   370→    // Collect all audio track items\n   371→    for (let i = 0; i < audioTrackCount; i++) {\n   372→      const track = await sequence.getAudioTrack(i);\n   373→      if (track) {\n   374→        const items = await track.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n   375→        if (items) {\n   376→          itemsToDelete.push(...items);\n   377→        }\n   378→      }\n   379→    }\n   380→\n   381→    // Delete all items in one transaction\n   382→    if (itemsToDelete.length > 0) {\n   383→      try {\n   384→        await project.lockedAccess(async () => {\n   385→          await project.executeTransaction((compoundAction) => {\n   386→            for (const item of itemsToDelete) {\n   387→              const deleteAction = item.createRemoveAction();\n   388→              compoundAction.addAction(deleteAction);\n   389→            }\n   390→          }, 'SPLICE: Clear Sequence');\n   391→        });\n   392→      } catch (clearErr) {\n   393→        console.error('[SPLICE Builder] Failed to clear sequence:', clearErr);\n   394→        // Non-fatal - continue with insert\n   395→      }\n   396→    }\n   397→  } catch (err) {\n   398→    console.error('[SPLICE Builder] Error clearing sequence:', err);\n   399→  }\n   400→}\n   401→\n   402→/**\n   403→ * Set in/out points for track items after insertion\n   404→ * Handles J-cut/L-cut audio offsets by setting different in/out points for audio tracks\n   405→ * @param {Object} sequence - The sequence containing the clips\n   406→ * @param {Array} segmentsToInsert - The segment data with in/out points\n   407→ */\n   408→async function setTrackItemInOutPoints(sequence, segmentsToInsert) {\n   409→  try {\n   410→    // Get video track\n   411→    const videoTrack = await sequence.getVideoTrack(0);\n   412→    if (!videoTrack) return;\n   413→\n   414→    const videoTrackItems = await videoTrack.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n   415→    if (!videoTrackItems || videoTrackItems.length === 0) return;\n   416→\n   417→    // Get audio track for J-cut/L-cut handling\n   418→    const audioTrack = await sequence.getAudioTrack(0);\n   419→    const audioTrackItems = audioTrack\n   420→      ? await audioTrack.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false)\n   421→      : null;\n   422→\n   423→    const project = await pproBuilder.Project.getActiveProject();\n   424→\n   425→    // Match track items with segments (in order)\n   426→    const itemCount = Math.min(videoTrackItems.length, segmentsToInsert.length);\n   427→\n   428→    if (itemCount > 0) {\n   429→      try {\n   430→        await project.lockedAccess(async () => {\n   431→          await project.executeTransaction((compoundAction) => {\n   432→            for (let i = 0; i < itemCount; i++) {\n   433→              const videoItem = videoTrackItems[i];\n   434→              const segData = segmentsToInsert[i];\n   435→\n   436→              // Set video in/out points\n   437→              const videoInAction = videoItem.createSetInPointAction(segData.inPoint);\n   438→              const videoOutAction = videoItem.createSetOutPointAction(segData.outPoint);\n   439→              compoundAction.addAction(videoInAction);\n   440→              compoundAction.addAction(videoOutAction);\n   441→\n   442→              // Set audio in/out points (may differ for J-cut/L-cut)\n   443→              if (audioTrackItems && audioTrackItems[i]) {\n   444→                const audioItem = audioTrackItems[i];\n   445→\n   446→                if (segData.hasAudioOffset) {\n   447→                  // J-cut/L-cut: use different audio in/out points\n   448→                  const audioInAction = audioItem.createSetInPointAction(segData.audioInPoint);\n   449→                  const audioOutAction = audioItem.createSetOutPointAction(segData.audioOutPoint);\n   450→                  compoundAction.addAction(audioInAction);\n   451→                  compoundAction.addAction(audioOutAction);\n   452→                } else {\n   453→                  // Same as video\n   454→                  const audioInAction = audioItem.createSetInPointAction(segData.inPoint);\n   455→                  const audioOutAction = audioItem.createSetOutPointAction(segData.outPoint);\n   456→                  compoundAction.addAction(audioInAction);\n   457→                  compoundAction.addAction(audioOutAction);\n   458→                }\n   459→              }\n   460→            }\n   461→          }, 'SPLICE: Set In/Out Points');\n   462→        });\n   463→      } catch (inOutErr) {\n   464→        console.error('[SPLICE Builder] Failed to set in/out points:', inOutErr);\n   465→        // Non-fatal - clips are inserted but may have wrong in/out points\n   466→      }\n   467→    }\n   468→\n   469→  } catch (err) {\n   470→    console.error('[SPLICE Builder] Error setting in/out points:', err);\n   471→  }\n   472→}\n   473→\n   474→\n   475→/**\n   476→ * Build sequence from detected silences and takes\n   477→ * Convenience wrapper that creates cut list from detection results\n   478→ * @param {Array} silences - Detected silence segments\n   479→ * @param {Array} takes - Detected speech takes\n   480→ * @param {Object} sourceClip - Source project item\n   481→ * @returns {Promise<Object>} Build result\n   482→ */\n   483→async function buildSequenceFromDetection(silences, takes, sourceClip) {\n   484→  // Create cut list from detection results\n   485→  // Keep speech segments, remove silences\n   486→  const segments = [];\n   487→\n   488→  // Get source clip info (using synchronous property access)\n   489→  const sourceName = sourceClip.name;\n   490→  const sourcePath = sourceClip.treePath;\n   491→\n   492→  // Get clip duration\n   493→  const durationObj = await sourceClip.getDuration();\n   494→  const durationSecs = durationObj ? durationObj.seconds : 0;\n   495→\n   496→  // Sort silences by start time\n   497→  const sortedSilences = [...silences].sort((a, b) => a.start - b.start);\n   498→\n   499→  // Build segments from gaps between silences (speech segments)\n   500→  let lastEnd = 0;\n   501→\n   502→  for (const silence of sortedSilences) {\n   503→    // Add speech segment before this silence\n   504→    if (silence.start > lastEnd) {\n   505→      segments.push({\n   506→        type: 'speech',\n   507→        sourceName,\n   508→        sourcePath,\n   509→        inPoint: lastEnd,\n   510→        outPoint: silence.start\n   511→      });\n   512→    }\n   513→    lastEnd = silence.end;\n   514→  }\n   515→\n   516→  // Add final segment after last silence\n   517→  if (lastEnd < durationSecs) {\n   518→    segments.push({\n   519→      type: 'speech',\n   520→      sourceName,\n   521→      sourcePath,\n   522→      inPoint: lastEnd,\n   523→      outPoint: durationSecs\n   524→    });\n   525→  }\n   526→\n   527→  // Build the cut list\n   528→  const cutList = {\n   529→    version: '3.5',\n   530→    source: sourceName,\n   531→    segments,\n   532→    metadata: {\n   533→      silencesRemoved: silences.length,\n   534→      takesDetected: takes?.length || 0,\n   535→      generatedAt: new Date().toISOString()\n   536→    }\n   537→  };\n   538→\n   539→  return await buildSequenceFromCutList(cutList);\n   540→}\n   541→\n   542→/**\n   543→ * Apply cut list to existing sequence (in-place edit)\n   544→ * Alternative to creating new sequence\n   545→ * @param {Object} cutList - The cut list from backend\n   546→ * @returns {Promise<Object>} Result object\n   547→ */\n   548→async function applyCutListToSequence(cutList) {\n   549→  // TODO: Implement in-place editing as alternative to new sequence\n   550→  // This would use razor cuts on existing clips rather than rebuilding\n   551→  return { success: false, error: 'In-place editing not yet implemented' };\n   552→}\n   553→\n   554→// Export functions for use in main.js\n   555→window.spliceBuilder = {\n   556→  buildSequenceFromCutList,\n   557→  buildSequenceFromDetection,\n   558→  applyCutListToSequence,\n   559→  findProjectItemByName,\n   560→  findProjectItemByPath,\n   561→  COLOR_LABELS,\n   562→  SPLICE_COLORS\n   563→};\n   564→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"88f09322-ce60-433b-9954-5c85ce970792","timestamp":"2025-12-25T22:30:28.601Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-plugin/js/builder.js","content":"/**\n * SPLICE Direct DOM Sequence Builder (v3.5)\n *\n * Builds sequences directly using UXP DOM APIs.\n * No XML export/import required - zero manual steps.\n *\n * Key APIs:\n * - SequenceEditor.createInsertProjectItemAction() - Insert clips at timecode\n * - ProjectItem.createSetColorLabelAction(index) - Color clips (set BEFORE insert)\n * - TrackItem.createSetInPointAction/OutPointAction() - Set in/out points\n * - project.executeTransaction(actions) - Batch execute atomically\n */\n\nconst pproBuilder = require('premierepro');\n\n// No TICKS_PER_SECOND constant - we use native TickTime API instead\n// This avoids variable collision with config.js\n\n// Color label indices in Premiere Pro\nconst COLOR_LABELS = {\n  NONE: 0,\n  VIOLET: 1,    // Purple\n  IRIS: 2,      // Light purple\n  CARIBBEAN: 3, // Teal\n  LAVENDER: 4,  // Light lavender\n  CERULEAN: 5,  // Light blue\n  FOREST: 6,    // Dark green\n  ROSE: 7,      // Pink\n  MANGO: 8,     // Orange\n  PURPLE: 9,    // Dark purple\n  BLUE: 10,     // Blue\n  TEAL: 11,     // Teal/Cyan\n  MAGENTA: 12,  // Magenta\n  TAN: 13,      // Tan/Brown\n  GREEN: 14,    // Green\n  BROWN: 15,    // Dark brown\n  YELLOW: 16    // Yellow\n};\n\n// SPLICE color scheme for cut list clips\nconst SPLICE_COLORS = {\n  SPEECH: COLOR_LABELS.GREEN,      // Green for speech/takes\n  SILENCE: COLOR_LABELS.VIOLET,    // Purple for silence segments (if kept)\n  BEST_TAKE: COLOR_LABELS.CERULEAN // Light blue for best take\n};\n\n/**\n * Find a project item by name in the project bin\n * @param {Object} project - The active project\n * @param {string} name - Name of the clip to find\n * @returns {Promise<Object|null>} The project item or null\n */\nasync function findProjectItemByName(project, name) {\n  const rootItem = await project.getRootItem();\n  return await searchItemRecursively(rootItem, name);\n}\n\n/**\n * Recursively search for an item in the project bin\n * @param {Object} item - Current item to search\n * @param {string} name - Name to search for\n * @returns {Promise<Object|null>} The found item or null\n */\nasync function searchItemRecursively(item, name) {\n  if (!item || !item.children) return null;\n\n  if (item.children.numItems === 0) return null;\n\n  for (let i = 0; i < item.children.numItems; i++) {\n    const child = item.children[i];\n\n    // Check name using property access\n    if (child.name === name) {\n      return child;\n    }\n\n    // Check if it's a bin (Type 2) and search recursively\n    if (child.type === pproBuilder.Constants.ProjectItemType.BIN) {\n      const found = await searchItemRecursively(child, name);\n      if (found) return found;\n    }\n  }\n\n  return null;\n}\n\n/**\n * Find project item by file path\n * @param {Object} project - The active project\n * @param {string} filePath - Full path to the media file\n * @returns {Promise<Object|null>} The project item or null\n */\nasync function findProjectItemByPath(project, filePath) {\n  const rootItem = await project.getRootItem();\n  return await searchItemByPathRecursively(rootItem, filePath);\n}\n\n/**\n * Recursively search for an item by its file path\n * @param {Object} item - Current item to search\n * @param {string} filePath - Path to search for\n * @returns {Promise<Object|null>} The found item or null\n */\nasync function searchItemByPathRecursively(item, filePath) {\n  if (!item || !item.children) return null;\n\n  if (item.children.numItems === 0) return null;\n\n  for (let i = 0; i < item.children.numItems; i++) {\n    const child = item.children[i];\n\n    // Check if this item has a matching path (using property access)\n    if (child.treePath === filePath) {\n      return child;\n    }\n\n    // Check if it's a bin (Type 2) and search recursively\n    if (child.type === pproBuilder.Constants.ProjectItemType.BIN) {\n      const found = await searchItemByPathRecursively(child, filePath);\n      if (found) return found;\n    }\n  }\n\n  return null;\n}\n\n/**\n * Create a new sequence from a cut list\n * @param {Object} cutList - The cut list from backend\n * @param {Object} options - Build options\n * @returns {Promise<{success: boolean, sequence?: Object, error?: string, stats?: Object}>}\n */\nasync function buildSequenceFromCutList(cutList, options = {}) {\n  const stats = {\n    clipsInserted: 0,\n    totalDuration: 0,\n    errors: []\n  };\n\n  try {\n    const context = await getActiveSequence();\n    if (!context) {\n      return { success: false, error: 'No project or sequence open' };\n    }\n\n    const { project, sequence } = context;\n\n    // Create new sequence with \"_SPLICE\" suffix by cloning the existing sequence\n    const originalName = sequence.name;\n    const newSequenceName = `${originalName}_SPLICE`;\n\n    // Clone the existing sequence to preserve settings\n    let newSequence = null;\n    try {\n      await project.lockedAccess(async () => {\n        await project.executeTransaction((compoundAction) => {\n          const cloneAction = sequence.createCloneAction();\n          compoundAction.addAction(cloneAction);\n        }, 'SPLICE: Clone Sequence');\n      });\n    } catch (cloneErr) {\n      console.error('[SPLICE Builder] Failed to clone sequence:', cloneErr);\n      return { success: false, error: `Failed to clone sequence: ${cloneErr.message}`, stats };\n    }\n\n    // Find the newly created sequence (it will have the same name as original)\n    // We need to rename it\n    const sequences = await project.getSequences();\n    for (const seq of sequences) {\n      const name = seq.name;\n      if (name === originalName && seq !== sequence) {\n        newSequence = seq;\n        // Rename the cloned sequence\n        try {\n          await project.lockedAccess(async () => {\n            await project.executeTransaction((compoundAction) => {\n              const renameAction = newSequence.createSetNameAction(newSequenceName);\n              compoundAction.addAction(renameAction);\n            }, 'SPLICE: Rename Cloned Sequence');\n          });\n        } catch (renameErr) {\n          console.error('[SPLICE Builder] Failed to rename sequence:', renameErr);\n          // Continue anyway - sequence was cloned, just has wrong name\n        }\n        break;\n      }\n    }\n\n    if (!newSequence) {\n      // Fallback: create a new sequence from scratch\n      newSequence = await project.createSequence(newSequenceName);\n      if (!newSequence) {\n        return { success: false, error: 'Failed to create new sequence' };\n      }\n    }\n\n    // Get the sequence editor for inserting clips\n    const editor = await pproBuilder.SequenceEditor.getEditor(newSequence);\n    if (!editor) {\n      return { success: false, error: 'Failed to get sequence editor' };\n    }\n\n    // Process cut list segments\n    const segments = cutList.segments || [];\n    if (segments.length === 0) {\n      return { success: false, error: 'Cut list has no segments' };\n    }\n\n    // Prepare segment data for insertion\n    const segmentsToInsert = [];\n    let currentPositionSeconds = 0;\n\n    for (const segment of segments) {\n      try {\n        // Find the source project item\n        const sourceItem = await findProjectItemByPath(project, segment.sourcePath)\n          || await findProjectItemByName(project, segment.sourceName);\n\n        if (!sourceItem) {\n          stats.errors.push(`Source not found: ${segment.sourceName || segment.sourcePath}`);\n          continue;\n        }\n\n        // Create TickTime objects directly from seconds (no TICKS_PER_SECOND needed)\n        const inPoint = pproBuilder.TickTime.createWithSeconds(segment.inPoint);\n        const outPoint = pproBuilder.TickTime.createWithSeconds(segment.outPoint);\n        const position = pproBuilder.TickTime.createWithSeconds(currentPositionSeconds);\n\n        // Handle J-cut/L-cut audio offsets if present\n        let audioInPoint = inPoint;\n        let audioOutPoint = outPoint;\n\n        if (segment.audioInPoint !== undefined) {\n          audioInPoint = pproBuilder.TickTime.createWithSeconds(segment.audioInPoint);\n        }\n        if (segment.audioOutPoint !== undefined) {\n          audioOutPoint = pproBuilder.TickTime.createWithSeconds(segment.audioOutPoint);\n        }\n\n        // Calculate duration in seconds for next position\n        const durationSeconds = segment.outPoint - segment.inPoint;\n\n        // Determine color based on segment type\n        const colorIndex = getColorForSegmentType(segment.type);\n\n        segmentsToInsert.push({\n          sourceItem,\n          positionTime: position,\n          inPoint,\n          outPoint,\n          audioInPoint,\n          audioOutPoint,\n          hasAudioOffset: segment.audioInPoint !== undefined || segment.audioOutPoint !== undefined,\n          colorIndex\n        });\n\n        // Update position for next clip\n        currentPositionSeconds += durationSeconds;\n        stats.totalDuration += durationSeconds;\n\n      } catch (err) {\n        stats.errors.push(`Error processing segment: ${err.message}`);\n        console.error('[SPLICE Builder] Segment error:', err);\n      }\n    }\n\n    if (segmentsToInsert.length === 0) {\n      return { success: false, error: 'No valid segments to insert' };\n    }\n\n    // Clear the new sequence first (it's a clone, so it has content)\n    // We'll insert clips from scratch\n    await clearSequence(newSequence);\n\n    // Execute all insert actions in a single transaction\n    try {\n      await project.lockedAccess(async () => {\n        await project.executeTransaction((compoundAction) => {\n          for (const segData of segmentsToInsert) {\n            // Set color on source item if needed\n            if (segData.colorIndex !== null) {\n              const colorAction = segData.sourceItem.createSetColorLabelAction(segData.colorIndex);\n              compoundAction.addAction(colorAction);\n            }\n\n            // Use the pre-created TickTime object (no conversion needed)\n            const insertTime = segData.positionTime;\n\n            // Create insert action (must be done inside transaction)\n            const insertAction = editor.createInsertProjectItemAction(\n              segData.sourceItem,\n              insertTime,\n              0,  // Video track index\n              0,  // Audio track index\n              false // Don't limit shift - insert on all tracks\n            );\n            compoundAction.addAction(insertAction);\n          }\n        }, 'SPLICE: Build Sequence from Cut List');\n      });\n    } catch (insertErr) {\n      console.error('[SPLICE Builder] Failed to insert clips:', insertErr);\n      return { success: false, error: `Failed to insert clips: ${insertErr.message}`, stats };\n    }\n\n    stats.clipsInserted = segmentsToInsert.length;\n\n    // Set in/out points for each track item (must be done after insert)\n    await setTrackItemInOutPoints(newSequence, segmentsToInsert);\n\n    console.log(`[SPLICE Builder] Built sequence with ${stats.clipsInserted} clips`);\n\n    return {\n      success: true,\n      sequence: newSequence,\n      sequenceName: newSequenceName,\n      stats\n    };\n\n  } catch (err) {\n    console.error('[SPLICE Builder] Build error:', err);\n    return { success: false, error: err.message, stats };\n  }\n}\n\n/**\n * Get color index for a segment type\n * @param {string} type - Segment type (speech, silence, best_take)\n * @returns {number|null} Color label index or null for no color\n */\nfunction getColorForSegmentType(type) {\n  switch (type) {\n    case 'speech':\n    case 'take':\n      return SPLICE_COLORS.SPEECH;\n    case 'best_take':\n      return SPLICE_COLORS.BEST_TAKE;\n    case 'silence':\n      return SPLICE_COLORS.SILENCE;\n    default:\n      return null;\n  }\n}\n\n/**\n * Clear all clips from a sequence\n * @param {Object} sequence - The sequence to clear\n */\nasync function clearSequence(sequence) {\n  try {\n    const project = await pproBuilder.Project.getActiveProject();\n\n    // Get all video tracks\n    const videoTrackCount = await sequence.getVideoTrackCount();\n    const audioTrackCount = await sequence.getAudioTrackCount();\n\n    const itemsToDelete = [];\n\n    // Collect all video track items\n    for (let i = 0; i < videoTrackCount; i++) {\n      const track = await sequence.getVideoTrack(i);\n      if (track) {\n        const items = await track.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n        if (items) {\n          itemsToDelete.push(...items);\n        }\n      }\n    }\n\n    // Collect all audio track items\n    for (let i = 0; i < audioTrackCount; i++) {\n      const track = await sequence.getAudioTrack(i);\n      if (track) {\n        const items = await track.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n        if (items) {\n          itemsToDelete.push(...items);\n        }\n      }\n    }\n\n    // Delete all items in one transaction\n    if (itemsToDelete.length > 0) {\n      try {\n        await project.lockedAccess(async () => {\n          await project.executeTransaction((compoundAction) => {\n            for (const item of itemsToDelete) {\n              const deleteAction = item.createRemoveAction();\n              compoundAction.addAction(deleteAction);\n            }\n          }, 'SPLICE: Clear Sequence');\n        });\n      } catch (clearErr) {\n        console.error('[SPLICE Builder] Failed to clear sequence:', clearErr);\n        // Non-fatal - continue with insert\n      }\n    }\n  } catch (err) {\n    console.error('[SPLICE Builder] Error clearing sequence:', err);\n  }\n}\n\n/**\n * Set in/out points for track items after insertion\n * Handles J-cut/L-cut audio offsets by setting different in/out points for audio tracks\n * @param {Object} sequence - The sequence containing the clips\n * @param {Array} segmentsToInsert - The segment data with in/out points\n */\nasync function setTrackItemInOutPoints(sequence, segmentsToInsert) {\n  try {\n    // Get video track\n    const videoTrack = await sequence.getVideoTrack(0);\n    if (!videoTrack) return;\n\n    const videoTrackItems = await videoTrack.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n    if (!videoTrackItems || videoTrackItems.length === 0) return;\n\n    // Get audio track for J-cut/L-cut handling\n    const audioTrack = await sequence.getAudioTrack(0);\n    const audioTrackItems = audioTrack\n      ? await audioTrack.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false)\n      : null;\n\n    const project = await pproBuilder.Project.getActiveProject();\n\n    // Match track items with segments (in order)\n    const itemCount = Math.min(videoTrackItems.length, segmentsToInsert.length);\n\n    if (itemCount > 0) {\n      try {\n        await project.lockedAccess(async () => {\n          await project.executeTransaction((compoundAction) => {\n            for (let i = 0; i < itemCount; i++) {\n              const videoItem = videoTrackItems[i];\n              const segData = segmentsToInsert[i];\n\n              // Set video in/out points\n              const videoInAction = videoItem.createSetInPointAction(segData.inPoint);\n              const videoOutAction = videoItem.createSetOutPointAction(segData.outPoint);\n              compoundAction.addAction(videoInAction);\n              compoundAction.addAction(videoOutAction);\n\n              // Set audio in/out points (may differ for J-cut/L-cut)\n              if (audioTrackItems && audioTrackItems[i]) {\n                const audioItem = audioTrackItems[i];\n\n                if (segData.hasAudioOffset) {\n                  // J-cut/L-cut: use different audio in/out points\n                  const audioInAction = audioItem.createSetInPointAction(segData.audioInPoint);\n                  const audioOutAction = audioItem.createSetOutPointAction(segData.audioOutPoint);\n                  compoundAction.addAction(audioInAction);\n                  compoundAction.addAction(audioOutAction);\n                } else {\n                  // Same as video\n                  const audioInAction = audioItem.createSetInPointAction(segData.inPoint);\n                  const audioOutAction = audioItem.createSetOutPointAction(segData.outPoint);\n                  compoundAction.addAction(audioInAction);\n                  compoundAction.addAction(audioOutAction);\n                }\n              }\n            }\n          }, 'SPLICE: Set In/Out Points');\n        });\n      } catch (inOutErr) {\n        console.error('[SPLICE Builder] Failed to set in/out points:', inOutErr);\n        // Non-fatal - clips are inserted but may have wrong in/out points\n      }\n    }\n\n  } catch (err) {\n    console.error('[SPLICE Builder] Error setting in/out points:', err);\n  }\n}\n\n\n/**\n * Build sequence from detected silences and takes\n * Convenience wrapper that creates cut list from detection results\n * @param {Array} silences - Detected silence segments\n * @param {Array} takes - Detected speech takes\n * @param {Object} sourceClip - Source project item\n * @returns {Promise<Object>} Build result\n */\nasync function buildSequenceFromDetection(silences, takes, sourceClip) {\n  // Create cut list from detection results\n  // Keep speech segments, remove silences\n  const segments = [];\n\n  // Get source clip info (using synchronous property access)\n  const sourceName = sourceClip.name;\n  const sourcePath = sourceClip.treePath;\n\n  // Get clip duration\n  const durationObj = await sourceClip.getDuration();\n  const durationSecs = durationObj ? durationObj.seconds : 0;\n\n  // Sort silences by start time\n  const sortedSilences = [...silences].sort((a, b) => a.start - b.start);\n\n  // Build segments from gaps between silences (speech segments)\n  let lastEnd = 0;\n\n  for (const silence of sortedSilences) {\n    // Add speech segment before this silence\n    if (silence.start > lastEnd) {\n      segments.push({\n        type: 'speech',\n        sourceName,\n        sourcePath,\n        inPoint: lastEnd,\n        outPoint: silence.start\n      });\n    }\n    lastEnd = silence.end;\n  }\n\n  // Add final segment after last silence\n  if (lastEnd < durationSecs) {\n    segments.push({\n      type: 'speech',\n      sourceName,\n      sourcePath,\n      inPoint: lastEnd,\n      outPoint: durationSecs\n    });\n  }\n\n  // Build the cut list\n  const cutList = {\n    version: '3.5',\n    source: sourceName,\n    segments,\n    metadata: {\n      silencesRemoved: silences.length,\n      takesDetected: takes?.length || 0,\n      generatedAt: new Date().toISOString()\n    }\n  };\n\n  return await buildSequenceFromCutList(cutList);\n}\n\n/**\n * Apply cut list to existing sequence (in-place edit)\n * Alternative to creating new sequence\n * @param {Object} cutList - The cut list from backend\n * @returns {Promise<Object>} Result object\n */\nasync function applyCutListToSequence(cutList) {\n  // TODO: Implement in-place editing as alternative to new sequence\n  // This would use razor cuts on existing clips rather than rebuilding\n  return { success: false, error: 'In-place editing not yet implemented' };\n}\n\n// Export functions for use in main.js\nwindow.spliceBuilder = {\n  buildSequenceFromCutList,\n  buildSequenceFromDetection,\n  applyCutListToSequence,\n  findProjectItemByName,\n  findProjectItemByPath,\n  COLOR_LABELS,\n  SPLICE_COLORS\n};\n","numLines":564,"startLine":1,"totalLines":564}}}
{"parentUuid":"88f09322-ce60-433b-9954-5c85ce970792","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Hk9uaiYWVUaLDhzM8BrEsa","type":"tool_result","content":"     1→/**\n     2→ * Repetition/Stutter Detection Service\n     3→ *\n     4→ * Detects repeated words, phrases, and stutters in transcripts.\n     5→ * Based on Fireside's proven approach with enhancements.\n     6→ *\n     7→ * Features:\n     8→ * - Frequency-based pattern detection\n     9→ * - Phrase similarity matching\n    10→ * - OpenAI refinement for boundary detection\n    11→ * - Configurable tolerance and phrase size\n    12→ * - Word-level result with grouping\n    13→ *\n    14→ * Two Modes:\n    15→ * - Basic: Fast local detection (no API calls)\n    16→ * - Advanced: Uses OpenAI for precise boundary detection\n    17→ */\n    18→\n    19→const OpenAI = require('openai');\n    20→\n    21→// =============================================================================\n    22→// Configuration\n    23→// =============================================================================\n    24→\n    25→const DEFAULT_OPTIONS = {\n    26→  mode: 'basic',            // 'basic' or 'advanced'\n    27→  phraseSize: 5,            // Words per comparison window\n    28→  tolerance: 0.7,           // Similarity threshold (0-1)\n    29→  searchRadius: 100,        // Words to search ahead\n    30→  minOccurrences: 2,        // Minimum occurrences to count as repetition\n    31→  extendMode: 'next-punctuation', // 'next-punctuation' or 'longest-occurrence'\n    32→  useOpenAI: false,         // Use OpenAI for boundary refinement\n    33→  maxGapWords: 20           // Max words between occurrences to group\n    34→};\n    35→\n    36→// Punctuation for boundary detection\n    37→const PUNCTUATION = /[.!?,;:]/;\n    38→\n    39→// =============================================================================\n    40→// Core Detection - Basic Mode\n    41→// =============================================================================\n    42→\n    43→/**\n    44→ * Detect repetitions in transcript using basic frequency analysis\n    45→ *\n    46→ * @param {Object} transcript - Transcript with words array\n    47→ * @param {Object} options - Detection options\n    48→ * @returns {Object} Detection results with repetitions\n    49→ */\n    50→function detectRepetitionsBasic(transcript, options = {}) {\n    51→  const opts = { ...DEFAULT_OPTIONS, ...options };\n    52→  const words = transcript.words || [];\n    53→\n    54→  if (words.length < opts.phraseSize * 2) {\n    55→    return { repetitions: [], metadata: { reason: 'Not enough words for analysis' } };\n    56→  }\n    57→\n    58→  console.log(`[SPLICE Repetition] Basic mode: ${words.length} words, phraseSize=${opts.phraseSize}, tolerance=${opts.tolerance}`);\n    59→\n    60→  // Normalize words for comparison\n    61→  const normalizedWords = words.map(w => normalizeWord(w.word || w.text || ''));\n    62→\n    63→  // Find all phrase occurrences\n    64→  const phraseGroups = findPhraseOccurrences(normalizedWords, {\n    65→    phraseSize: opts.phraseSize,\n    66→    searchRadius: opts.searchRadius,\n    67→    tolerance: opts.tolerance,\n    68→    minOccurrences: opts.minOccurrences\n    69→  });\n    70→\n    71→  // Convert to time-based repetitions\n    72→  const repetitions = phraseGroups.map((group, index) => ({\n    73→    id: index,\n    74→    occurrences: group.occurrences.map(occ => ({\n    75→      startWord: occ.startIndex,\n    76→      endWord: occ.endIndex,\n    77→      start: words[occ.startIndex].start,\n    78→      end: words[occ.endIndex].end,\n    79→      text: words.slice(occ.startIndex, occ.endIndex + 1).map(w => w.word || w.text).join(' ')\n    80→    })),\n    81→    bestOccurrence: 0, // First occurrence is usually the \"keeper\"\n    82→    similarity: group.similarity,\n    83→    wordCount: opts.phraseSize\n    84→  }));\n    85→\n    86→  // Filter overlapping repetitions\n    87→  const filtered = filterOverlappingRepetitions(repetitions);\n    88→\n    89→  console.log(`[SPLICE Repetition] Found ${filtered.length} repetition group(s)`);\n    90→\n    91→  return {\n    92→    repetitions: filtered,\n    93→    metadata: {\n    94→      mode: 'basic',\n    95→      totalWords: words.length,\n    96→      phraseSize: opts.phraseSize,\n    97→      tolerance: opts.tolerance,\n    98→      groupsFound: filtered.length,\n    99→      totalOccurrences: filtered.reduce((sum, r) => sum + r.occurrences.length, 0)\n   100→    }\n   101→  };\n   102→}\n   103→\n   104→/**\n   105→ * Find phrase occurrences using sliding window comparison\n   106→ *\n   107→ * @param {Array<string>} normalizedWords - Normalized word strings\n   108→ * @param {Object} options - Search options\n   109→ * @returns {Array} Groups of similar phrases\n   110→ */\n   111→function findPhraseOccurrences(normalizedWords, options) {\n   112→  const { phraseSize, searchRadius, tolerance, minOccurrences } = options;\n   113→  const groups = [];\n   114→  const usedIndices = new Set();\n   115→\n   116→  for (let i = 0; i <= normalizedWords.length - phraseSize; i++) {\n   117→    if (usedIndices.has(i)) continue;\n   118→\n   119→    const phrase = normalizedWords.slice(i, i + phraseSize);\n   120→    const occurrences = [{\n   121→      startIndex: i,\n   122→      endIndex: i + phraseSize - 1\n   123→    }];\n   124→\n   125→    // Search ahead for similar phrases\n   126→    const searchEnd = Math.min(i + searchRadius, normalizedWords.length - phraseSize);\n   127→\n   128→    for (let j = i + phraseSize; j <= searchEnd; j++) {\n   129→      if (usedIndices.has(j)) continue;\n   130→\n   131→      const comparePhrase = normalizedWords.slice(j, j + phraseSize);\n   132→      const similarity = calculateSimilarity(phrase, comparePhrase);\n   133→\n   134→      if (similarity >= tolerance) {\n   135→        occurrences.push({\n   136→          startIndex: j,\n   137→          endIndex: j + phraseSize - 1\n   138→        });\n   139→        // Mark these indices as used\n   140→        for (let k = j; k < j + phraseSize; k++) {\n   141→          usedIndices.add(k);\n   142→        }\n   143→      }\n   144→    }\n   145→\n   146→    if (occurrences.length >= minOccurrences) {\n   147→      // Mark original phrase indices as used\n   148→      for (let k = i; k < i + phraseSize; k++) {\n   149→        usedIndices.add(k);\n   150→      }\n   151→\n   152→      groups.push({\n   153→        occurrences,\n   154→        similarity: 1.0, // First match is reference\n   155→        phraseText: phrase.join(' ')\n   156→      });\n   157→    }\n   158→  }\n   159→\n   160→  return groups;\n   161→}\n   162→\n   163→// =============================================================================\n   164→// Core Detection - Advanced Mode (with OpenAI)\n   165→// =============================================================================\n   166→\n   167→/**\n   168→ * Detect repetitions with OpenAI boundary refinement\n   169→ *\n   170→ * @param {Object} transcript - Transcript with words array\n   171→ * @param {Object} options - Detection options\n   172→ * @returns {Promise<Object>} Detection results with refined boundaries\n   173→ */\n   174→async function detectRepetitionsAdvanced(transcript, options = {}) {\n   175→  const opts = { ...DEFAULT_OPTIONS, ...options, mode: 'advanced' };\n   176→\n   177→  // First, do basic detection\n   178→  const basicResult = detectRepetitionsBasic(transcript, opts);\n   179→\n   180→  if (basicResult.repetitions.length === 0) {\n   181→    return basicResult;\n   182→  }\n   183→\n   184→  // If OpenAI refinement is disabled, return basic results\n   185→  if (!opts.useOpenAI) {\n   186→    return {\n   187→      ...basicResult,\n   188→      metadata: { ...basicResult.metadata, mode: 'advanced-local' }\n   189→    };\n   190→  }\n   191→\n   192→  // Refine boundaries using OpenAI\n   193→  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n   194→  const refinedRepetitions = [];\n   195→\n   196→  for (const rep of basicResult.repetitions) {\n   197→    try {\n   198→      const refined = await refineRepetitionBoundaries(openai, transcript.words, rep);\n   199→      refinedRepetitions.push(refined);\n   200→    } catch (err) {\n   201→      console.warn('[SPLICE Repetition] OpenAI refinement failed, using basic:', err.message);\n   202→      refinedRepetitions.push(rep);\n   203→    }\n   204→  }\n   205→\n   206→  return {\n   207→    repetitions: refinedRepetitions,\n   208→    metadata: {\n   209→      ...basicResult.metadata,\n   210→      mode: 'advanced-openai',\n   211→      openaiRefined: true\n   212→    }\n   213→  };\n   214→}\n   215→\n   216→/**\n   217→ * Refine repetition boundaries using OpenAI\n   218→ *\n   219→ * @param {OpenAI} openai - OpenAI client\n   220→ * @param {Array} words - All transcript words\n   221→ * @param {Object} repetition - Basic repetition detection\n   222→ * @returns {Promise<Object>} Refined repetition with better boundaries\n   223→ */\n   224→async function refineRepetitionBoundaries(openai, words, repetition) {\n   225→  // Get context around the repetition\n   226→  const allOccurrences = repetition.occurrences;\n   227→  const minStart = Math.max(0, allOccurrences[0].startWord - 5);\n   228→  const maxEnd = Math.min(words.length - 1, allOccurrences[allOccurrences.length - 1].endWord + 5);\n   229→\n   230→  // Build context string with markers\n   231→  let contextText = '';\n   232→  for (let i = minStart; i <= maxEnd; i++) {\n   233→    const word = words[i].word || words[i].text;\n   234→    // Mark occurrence boundaries\n   235→    const isStart = allOccurrences.some(o => o.startWord === i);\n   236→    const isEnd = allOccurrences.some(o => o.endWord === i);\n   237→\n   238→    if (isStart) contextText += '[';\n   239→    contextText += word + ' ';\n   240→    if (isEnd) contextText += '] ';\n   241→  }\n   242→\n   243→  const prompt = `Analyze this text for repeated content. The brackets mark detected repetitions:\n   244→\n   245→\"${contextText.trim()}\"\n   246→\n   247→For each bracketed section, determine:\n   248→1. Is this truly a repetition/restart/stutter?\n   249→2. What are the exact word boundaries that should be removed (keeping only the best take)?\n   250→\n   251→Respond in JSON format:\n   252→{\n   253→  \"isRepetition\": true/false,\n   254→  \"keepOccurrence\": 0-based index of the occurrence to keep (usually the last/most complete),\n   255→  \"removeRanges\": [{\"start\": wordIndex, \"end\": wordIndex}]\n   256→}`;\n   257→\n   258→  const response = await openai.chat.completions.create({\n   259→    model: 'gpt-4o-mini',\n   260→    messages: [\n   261→      { role: 'system', content: 'You are an expert video editor identifying repeated content in speech transcripts. Be precise about boundaries.' },\n   262→      { role: 'user', content: prompt }\n   263→    ],\n   264→    temperature: 0,\n   265→    max_tokens: 500\n   266→  });\n   267→\n   268→  try {\n   269→    const result = JSON.parse(response.choices[0].message.content);\n   270→\n   271→    if (!result.isRepetition) {\n   272→      // False positive - return empty\n   273→      return { ...repetition, occurrences: [], metadata: { refined: true, isRepetition: false } };\n   274→    }\n   275→\n   276→    // Update best occurrence\n   277→    return {\n   278→      ...repetition,\n   279→      bestOccurrence: result.keepOccurrence || 0,\n   280→      metadata: { refined: true, openaiResult: result }\n   281→    };\n   282→  } catch (parseErr) {\n   283→    // Return original if parsing fails\n   284→    return repetition;\n   285→  }\n   286→}\n   287→\n   288→// =============================================================================\n   289→// Stutter Detection (Single-Word Repetitions)\n   290→// =============================================================================\n   291→\n   292→/**\n   293→ * Detect single-word stutters (e.g., \"I I I think\")\n   294→ *\n   295→ * @param {Object} transcript - Transcript with words array\n   296→ * @param {Object} options - Detection options\n   297→ * @returns {Object} Stutter detection results\n   298→ */\n   299→function detectStutters(transcript, options = {}) {\n   300→  const {\n   301→    minRepeats = 2,      // Minimum times word must repeat\n   302→    maxGapMs = 500,      // Max gap between words (milliseconds)\n   303→    ignoreFillers = true, // Ignore common filler words\n   304→    minWordLength = 1    // Minimum word length (1 allows \"I\", \"a\", etc.)\n   305→  } = options;\n   306→\n   307→  const words = transcript.words || [];\n   308→  const stutters = [];\n   309→\n   310→  // Common filler words to optionally ignore\n   311→  const fillerWords = new Set(['um', 'uh', 'ah', 'er', 'like', 'so', 'well', 'you know']);\n   312→\n   313→  let i = 0;\n   314→  while (i < words.length) {\n   315→    const word = words[i];\n   316→    const normalized = normalizeWord(word.word || word.text || '');\n   317→\n   318→    // Skip fillers if configured\n   319→    if (ignoreFillers && fillerWords.has(normalized)) {\n   320→      i++;\n   321→      continue;\n   322→    }\n   323→\n   324→    // Skip very short words (configurable, default allows single chars like \"I\")\n   325→    if (normalized.length < minWordLength) {\n   326→      i++;\n   327→      continue;\n   328→    }\n   329→\n   330→    // Look for consecutive repeats\n   331→    let repeatCount = 1;\n   332→    let j = i + 1;\n   333→\n   334→    while (j < words.length) {\n   335→      const nextWord = words[j];\n   336→      const nextNormalized = normalizeWord(nextWord.word || nextWord.text || '');\n   337→\n   338→      // Check if same word\n   339→      if (nextNormalized !== normalized) break;\n   340→\n   341→      // Check gap (convert to ms)\n   342→      const gap = (nextWord.start - words[j - 1].end) * 1000;\n   343→      if (gap > maxGapMs) break;\n   344→\n   345→      repeatCount++;\n   346→      j++;\n   347→    }\n   348→\n   349→    if (repeatCount >= minRepeats) {\n   350→      stutters.push({\n   351→        word: normalized,\n   352→        occurrences: repeatCount,\n   353→        startWord: i,\n   354→        endWord: j - 1,\n   355→        start: words[i].start,\n   356→        end: words[j - 1].end,\n   357→        duration: words[j - 1].end - words[i].start,\n   358→        // Keep first occurrence, remove rest\n   359→        keepStart: words[i].start,\n   360→        keepEnd: words[i].end,\n   361→        removeStart: words[i + 1].start,\n   362→        removeEnd: words[j - 1].end\n   363→      });\n   364→    }\n   365→\n   366→    i = j; // Skip past all repeats\n   367→  }\n   368→\n   369→  console.log(`[SPLICE Repetition] Found ${stutters.length} stutter(s)`);\n   370→\n   371→  return {\n   372→    stutters,\n   373→    metadata: {\n   374→      type: 'stutters',\n   375→      totalWords: words.length,\n   376→      stutterCount: stutters.length,\n   377→      totalRepeatedWords: stutters.reduce((sum, s) => sum + s.occurrences, 0)\n   378→    }\n   379→  };\n   380→}\n   381→\n   382→// =============================================================================\n   383→// Utility Functions\n   384→// =============================================================================\n   385→\n   386→/**\n   387→ * Normalize a word for comparison\n   388→ */\n   389→function normalizeWord(word) {\n   390→  return word\n   391→    .toLowerCase()\n   392→    .replace(/[^\\w\\s'-]/g, '') // Remove punctuation except apostrophe/hyphen\n   393→    .trim();\n   394→}\n   395→\n   396→/**\n   397→ * Calculate similarity between two word arrays\n   398→ * Uses Jaccard-like similarity with position weighting\n   399→ *\n   400→ * @param {Array<string>} arr1 - First word array\n   401→ * @param {Array<string>} arr2 - Second word array\n   402→ * @returns {number} Similarity score (0-1)\n   403→ */\n   404→function calculateSimilarity(arr1, arr2) {\n   405→  if (arr1.length !== arr2.length) return 0;\n   406→  if (arr1.length === 0) return 0;\n   407→\n   408→  let matches = 0;\n   409→  for (let i = 0; i < arr1.length; i++) {\n   410→    if (arr1[i] === arr2[i]) {\n   411→      matches++;\n   412→    } else {\n   413→      // Partial credit for similar words (edit distance)\n   414→      const editDist = levenshteinDistance(arr1[i], arr2[i]);\n   415→      const maxLen = Math.max(arr1[i].length, arr2[i].length);\n   416→      if (maxLen > 0) {\n   417→        const wordSim = 1 - (editDist / maxLen);\n   418→        if (wordSim > 0.7) matches += wordSim;\n   419→      }\n   420→    }\n   421→  }\n   422→\n   423→  return matches / arr1.length;\n   424→}\n   425→\n   426→/**\n   427→ * Levenshtein edit distance between two strings\n   428→ */\n   429→function levenshteinDistance(s1, s2) {\n   430→  const m = s1.length;\n   431→  const n = s2.length;\n   432→\n   433→  if (m === 0) return n;\n   434→  if (n === 0) return m;\n   435→\n   436→  const dp = Array(m + 1).fill(null).map(() => Array(n + 1).fill(0));\n   437→\n   438→  for (let i = 0; i <= m; i++) dp[i][0] = i;\n   439→  for (let j = 0; j <= n; j++) dp[0][j] = j;\n   440→\n   441→  for (let i = 1; i <= m; i++) {\n   442→    for (let j = 1; j <= n; j++) {\n   443→      if (s1[i - 1] === s2[j - 1]) {\n   444→        dp[i][j] = dp[i - 1][j - 1];\n   445→      } else {\n   446→        dp[i][j] = 1 + Math.min(\n   447→          dp[i - 1][j],     // deletion\n   448→          dp[i][j - 1],     // insertion\n   449→          dp[i - 1][j - 1]  // substitution\n   450→        );\n   451→      }\n   452→    }\n   453→  }\n   454→\n   455→  return dp[m][n];\n   456→}\n   457→\n   458→/**\n   459→ * Filter overlapping repetition groups\n   460→ */\n   461→function filterOverlappingRepetitions(repetitions) {\n   462→  if (repetitions.length <= 1) return repetitions;\n   463→\n   464→  // Sort by first occurrence start\n   465→  const sorted = [...repetitions].sort((a, b) =>\n   466→    a.occurrences[0].start - b.occurrences[0].start\n   467→  );\n   468→\n   469→  const result = [];\n   470→  let lastEnd = -1;\n   471→\n   472→  for (const rep of sorted) {\n   473→    const firstOcc = rep.occurrences[0];\n   474→    // Check if overlaps with previous\n   475→    if (firstOcc.start >= lastEnd) {\n   476→      result.push(rep);\n   477→      // Update lastEnd to include all occurrences\n   478→      lastEnd = Math.max(...rep.occurrences.map(o => o.end));\n   479→    }\n   480→  }\n   481→\n   482→  return result;\n   483→}\n   484→\n   485→/**\n   486→ * Convert repetitions to removal segments\n   487→ * Keeps the \"best\" occurrence, returns segments to remove\n   488→ *\n   489→ * @param {Array} repetitions - Detected repetitions\n   490→ * @returns {Array} Segments to remove from timeline\n   491→ */\n   492→function repetitionsToRemovalSegments(repetitions) {\n   493→  const segments = [];\n   494→\n   495→  for (const rep of repetitions) {\n   496→    const keepIndex = rep.bestOccurrence || 0;\n   497→\n   498→    rep.occurrences.forEach((occ, i) => {\n   499→      if (i !== keepIndex) {\n   500→        segments.push({\n   501→          start: occ.start,\n   502→          end: occ.end,\n   503→          type: 'repetition',\n   504→          text: occ.text,\n   505→          reason: `Duplicate of occurrence ${keepIndex + 1}`\n   506→        });\n   507→      }\n   508→    });\n   509→  }\n   510→\n   511→  // Sort by start time and merge adjacent\n   512→  return mergeAdjacentSegments(segments);\n   513→}\n   514→\n   515→/**\n   516→ * Convert stutters to removal segments\n   517→ *\n   518→ * @param {Array} stutters - Detected stutters\n   519→ * @returns {Array} Segments to remove\n   520→ */\n   521→function stuttersToRemovalSegments(stutters) {\n   522→  return stutters.map(s => ({\n   523→    start: s.removeStart,\n   524→    end: s.removeEnd,\n   525→    type: 'stutter',\n   526→    text: `${s.word} (x${s.occurrences})`,\n   527→    keepRange: { start: s.keepStart, end: s.keepEnd }\n   528→  }));\n   529→}\n   530→\n   531→/**\n   532→ * Merge adjacent removal segments\n   533→ */\n   534→function mergeAdjacentSegments(segments, maxGap = 0.1) {\n   535→  if (segments.length <= 1) return segments;\n   536→\n   537→  const sorted = [...segments].sort((a, b) => a.start - b.start);\n   538→  const merged = [];\n   539→  let current = { ...sorted[0] };\n   540→\n   541→  for (let i = 1; i < sorted.length; i++) {\n   542→    const next = sorted[i];\n   543→\n   544→    if (next.start - current.end <= maxGap) {\n   545→      // Merge\n   546→      current.end = next.end;\n   547→      current.text = `${current.text}, ${next.text}`;\n   548→    } else {\n   549→      merged.push(current);\n   550→      current = { ...next };\n   551→    }\n   552→  }\n   553→\n   554→  merged.push(current);\n   555→  return merged;\n   556→}\n   557→\n   558→// =============================================================================\n   559→// Combined Detection\n   560→// =============================================================================\n   561→\n   562→/**\n   563→ * Run all repetition detection (phrases + stutters)\n   564→ *\n   565→ * @param {Object} transcript - Transcript with words array\n   566→ * @param {Object} options - Detection options\n   567→ * @returns {Promise<Object>} Combined detection results\n   568→ */\n   569→async function detectAllRepetitions(transcript, options = {}) {\n   570→  const opts = { ...DEFAULT_OPTIONS, ...options };\n   571→\n   572→  // Detect phrase repetitions\n   573→  const phraseResult = opts.useOpenAI\n   574→    ? await detectRepetitionsAdvanced(transcript, opts)\n   575→    : detectRepetitionsBasic(transcript, opts);\n   576→\n   577→  // Detect stutters\n   578→  const stutterResult = detectStutters(transcript, opts);\n   579→\n   580→  // Combine removal segments\n   581→  const phraseSegments = repetitionsToRemovalSegments(phraseResult.repetitions);\n   582→  const stutterSegments = stuttersToRemovalSegments(stutterResult.stutters);\n   583→  const allSegments = mergeAdjacentSegments([...phraseSegments, ...stutterSegments]);\n   584→\n   585→  // Calculate total removable time\n   586→  const totalRemovableTime = allSegments.reduce((sum, s) => sum + (s.end - s.start), 0);\n   587→\n   588→  return {\n   589→    repetitions: phraseResult.repetitions,\n   590→    stutters: stutterResult.stutters,\n   591→    removalSegments: allSegments,\n   592→    metadata: {\n   593→      ...phraseResult.metadata,\n   594→      stutterCount: stutterResult.stutters.length,\n   595→      totalRemovalSegments: allSegments.length,\n   596→      totalRemovableTime: parseFloat(totalRemovableTime.toFixed(3))\n   597→    }\n   598→  };\n   599→}\n   600→\n   601→// =============================================================================\n   602→// Exports\n   603→// =============================================================================\n   604→\n   605→module.exports = {\n   606→  // Core detection\n   607→  detectRepetitionsBasic,\n   608→  detectRepetitionsAdvanced,\n   609→  detectStutters,\n   610→  detectAllRepetitions,\n   611→\n   612→  // Conversion utilities\n   613→  repetitionsToRemovalSegments,\n   614→  stuttersToRemovalSegments,\n   615→\n   616→  // Helper utilities\n   617→  normalizeWord,\n   618→  calculateSimilarity,\n   619→  levenshteinDistance,\n   620→  filterOverlappingRepetitions,\n   621→  mergeAdjacentSegments,\n   622→\n   623→  // Config\n   624→  DEFAULT_OPTIONS\n   625→};\n   626→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"a457a679-3e64-4919-9347-71698923880e","timestamp":"2025-12-25T22:30:28.601Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/repetitionDetection.js","content":"/**\n * Repetition/Stutter Detection Service\n *\n * Detects repeated words, phrases, and stutters in transcripts.\n * Based on Fireside's proven approach with enhancements.\n *\n * Features:\n * - Frequency-based pattern detection\n * - Phrase similarity matching\n * - OpenAI refinement for boundary detection\n * - Configurable tolerance and phrase size\n * - Word-level result with grouping\n *\n * Two Modes:\n * - Basic: Fast local detection (no API calls)\n * - Advanced: Uses OpenAI for precise boundary detection\n */\n\nconst OpenAI = require('openai');\n\n// =============================================================================\n// Configuration\n// =============================================================================\n\nconst DEFAULT_OPTIONS = {\n  mode: 'basic',            // 'basic' or 'advanced'\n  phraseSize: 5,            // Words per comparison window\n  tolerance: 0.7,           // Similarity threshold (0-1)\n  searchRadius: 100,        // Words to search ahead\n  minOccurrences: 2,        // Minimum occurrences to count as repetition\n  extendMode: 'next-punctuation', // 'next-punctuation' or 'longest-occurrence'\n  useOpenAI: false,         // Use OpenAI for boundary refinement\n  maxGapWords: 20           // Max words between occurrences to group\n};\n\n// Punctuation for boundary detection\nconst PUNCTUATION = /[.!?,;:]/;\n\n// =============================================================================\n// Core Detection - Basic Mode\n// =============================================================================\n\n/**\n * Detect repetitions in transcript using basic frequency analysis\n *\n * @param {Object} transcript - Transcript with words array\n * @param {Object} options - Detection options\n * @returns {Object} Detection results with repetitions\n */\nfunction detectRepetitionsBasic(transcript, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n  const words = transcript.words || [];\n\n  if (words.length < opts.phraseSize * 2) {\n    return { repetitions: [], metadata: { reason: 'Not enough words for analysis' } };\n  }\n\n  console.log(`[SPLICE Repetition] Basic mode: ${words.length} words, phraseSize=${opts.phraseSize}, tolerance=${opts.tolerance}`);\n\n  // Normalize words for comparison\n  const normalizedWords = words.map(w => normalizeWord(w.word || w.text || ''));\n\n  // Find all phrase occurrences\n  const phraseGroups = findPhraseOccurrences(normalizedWords, {\n    phraseSize: opts.phraseSize,\n    searchRadius: opts.searchRadius,\n    tolerance: opts.tolerance,\n    minOccurrences: opts.minOccurrences\n  });\n\n  // Convert to time-based repetitions\n  const repetitions = phraseGroups.map((group, index) => ({\n    id: index,\n    occurrences: group.occurrences.map(occ => ({\n      startWord: occ.startIndex,\n      endWord: occ.endIndex,\n      start: words[occ.startIndex].start,\n      end: words[occ.endIndex].end,\n      text: words.slice(occ.startIndex, occ.endIndex + 1).map(w => w.word || w.text).join(' ')\n    })),\n    bestOccurrence: 0, // First occurrence is usually the \"keeper\"\n    similarity: group.similarity,\n    wordCount: opts.phraseSize\n  }));\n\n  // Filter overlapping repetitions\n  const filtered = filterOverlappingRepetitions(repetitions);\n\n  console.log(`[SPLICE Repetition] Found ${filtered.length} repetition group(s)`);\n\n  return {\n    repetitions: filtered,\n    metadata: {\n      mode: 'basic',\n      totalWords: words.length,\n      phraseSize: opts.phraseSize,\n      tolerance: opts.tolerance,\n      groupsFound: filtered.length,\n      totalOccurrences: filtered.reduce((sum, r) => sum + r.occurrences.length, 0)\n    }\n  };\n}\n\n/**\n * Find phrase occurrences using sliding window comparison\n *\n * @param {Array<string>} normalizedWords - Normalized word strings\n * @param {Object} options - Search options\n * @returns {Array} Groups of similar phrases\n */\nfunction findPhraseOccurrences(normalizedWords, options) {\n  const { phraseSize, searchRadius, tolerance, minOccurrences } = options;\n  const groups = [];\n  const usedIndices = new Set();\n\n  for (let i = 0; i <= normalizedWords.length - phraseSize; i++) {\n    if (usedIndices.has(i)) continue;\n\n    const phrase = normalizedWords.slice(i, i + phraseSize);\n    const occurrences = [{\n      startIndex: i,\n      endIndex: i + phraseSize - 1\n    }];\n\n    // Search ahead for similar phrases\n    const searchEnd = Math.min(i + searchRadius, normalizedWords.length - phraseSize);\n\n    for (let j = i + phraseSize; j <= searchEnd; j++) {\n      if (usedIndices.has(j)) continue;\n\n      const comparePhrase = normalizedWords.slice(j, j + phraseSize);\n      const similarity = calculateSimilarity(phrase, comparePhrase);\n\n      if (similarity >= tolerance) {\n        occurrences.push({\n          startIndex: j,\n          endIndex: j + phraseSize - 1\n        });\n        // Mark these indices as used\n        for (let k = j; k < j + phraseSize; k++) {\n          usedIndices.add(k);\n        }\n      }\n    }\n\n    if (occurrences.length >= minOccurrences) {\n      // Mark original phrase indices as used\n      for (let k = i; k < i + phraseSize; k++) {\n        usedIndices.add(k);\n      }\n\n      groups.push({\n        occurrences,\n        similarity: 1.0, // First match is reference\n        phraseText: phrase.join(' ')\n      });\n    }\n  }\n\n  return groups;\n}\n\n// =============================================================================\n// Core Detection - Advanced Mode (with OpenAI)\n// =============================================================================\n\n/**\n * Detect repetitions with OpenAI boundary refinement\n *\n * @param {Object} transcript - Transcript with words array\n * @param {Object} options - Detection options\n * @returns {Promise<Object>} Detection results with refined boundaries\n */\nasync function detectRepetitionsAdvanced(transcript, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options, mode: 'advanced' };\n\n  // First, do basic detection\n  const basicResult = detectRepetitionsBasic(transcript, opts);\n\n  if (basicResult.repetitions.length === 0) {\n    return basicResult;\n  }\n\n  // If OpenAI refinement is disabled, return basic results\n  if (!opts.useOpenAI) {\n    return {\n      ...basicResult,\n      metadata: { ...basicResult.metadata, mode: 'advanced-local' }\n    };\n  }\n\n  // Refine boundaries using OpenAI\n  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n  const refinedRepetitions = [];\n\n  for (const rep of basicResult.repetitions) {\n    try {\n      const refined = await refineRepetitionBoundaries(openai, transcript.words, rep);\n      refinedRepetitions.push(refined);\n    } catch (err) {\n      console.warn('[SPLICE Repetition] OpenAI refinement failed, using basic:', err.message);\n      refinedRepetitions.push(rep);\n    }\n  }\n\n  return {\n    repetitions: refinedRepetitions,\n    metadata: {\n      ...basicResult.metadata,\n      mode: 'advanced-openai',\n      openaiRefined: true\n    }\n  };\n}\n\n/**\n * Refine repetition boundaries using OpenAI\n *\n * @param {OpenAI} openai - OpenAI client\n * @param {Array} words - All transcript words\n * @param {Object} repetition - Basic repetition detection\n * @returns {Promise<Object>} Refined repetition with better boundaries\n */\nasync function refineRepetitionBoundaries(openai, words, repetition) {\n  // Get context around the repetition\n  const allOccurrences = repetition.occurrences;\n  const minStart = Math.max(0, allOccurrences[0].startWord - 5);\n  const maxEnd = Math.min(words.length - 1, allOccurrences[allOccurrences.length - 1].endWord + 5);\n\n  // Build context string with markers\n  let contextText = '';\n  for (let i = minStart; i <= maxEnd; i++) {\n    const word = words[i].word || words[i].text;\n    // Mark occurrence boundaries\n    const isStart = allOccurrences.some(o => o.startWord === i);\n    const isEnd = allOccurrences.some(o => o.endWord === i);\n\n    if (isStart) contextText += '[';\n    contextText += word + ' ';\n    if (isEnd) contextText += '] ';\n  }\n\n  const prompt = `Analyze this text for repeated content. The brackets mark detected repetitions:\n\n\"${contextText.trim()}\"\n\nFor each bracketed section, determine:\n1. Is this truly a repetition/restart/stutter?\n2. What are the exact word boundaries that should be removed (keeping only the best take)?\n\nRespond in JSON format:\n{\n  \"isRepetition\": true/false,\n  \"keepOccurrence\": 0-based index of the occurrence to keep (usually the last/most complete),\n  \"removeRanges\": [{\"start\": wordIndex, \"end\": wordIndex}]\n}`;\n\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4o-mini',\n    messages: [\n      { role: 'system', content: 'You are an expert video editor identifying repeated content in speech transcripts. Be precise about boundaries.' },\n      { role: 'user', content: prompt }\n    ],\n    temperature: 0,\n    max_tokens: 500\n  });\n\n  try {\n    const result = JSON.parse(response.choices[0].message.content);\n\n    if (!result.isRepetition) {\n      // False positive - return empty\n      return { ...repetition, occurrences: [], metadata: { refined: true, isRepetition: false } };\n    }\n\n    // Update best occurrence\n    return {\n      ...repetition,\n      bestOccurrence: result.keepOccurrence || 0,\n      metadata: { refined: true, openaiResult: result }\n    };\n  } catch (parseErr) {\n    // Return original if parsing fails\n    return repetition;\n  }\n}\n\n// =============================================================================\n// Stutter Detection (Single-Word Repetitions)\n// =============================================================================\n\n/**\n * Detect single-word stutters (e.g., \"I I I think\")\n *\n * @param {Object} transcript - Transcript with words array\n * @param {Object} options - Detection options\n * @returns {Object} Stutter detection results\n */\nfunction detectStutters(transcript, options = {}) {\n  const {\n    minRepeats = 2,      // Minimum times word must repeat\n    maxGapMs = 500,      // Max gap between words (milliseconds)\n    ignoreFillers = true, // Ignore common filler words\n    minWordLength = 1    // Minimum word length (1 allows \"I\", \"a\", etc.)\n  } = options;\n\n  const words = transcript.words || [];\n  const stutters = [];\n\n  // Common filler words to optionally ignore\n  const fillerWords = new Set(['um', 'uh', 'ah', 'er', 'like', 'so', 'well', 'you know']);\n\n  let i = 0;\n  while (i < words.length) {\n    const word = words[i];\n    const normalized = normalizeWord(word.word || word.text || '');\n\n    // Skip fillers if configured\n    if (ignoreFillers && fillerWords.has(normalized)) {\n      i++;\n      continue;\n    }\n\n    // Skip very short words (configurable, default allows single chars like \"I\")\n    if (normalized.length < minWordLength) {\n      i++;\n      continue;\n    }\n\n    // Look for consecutive repeats\n    let repeatCount = 1;\n    let j = i + 1;\n\n    while (j < words.length) {\n      const nextWord = words[j];\n      const nextNormalized = normalizeWord(nextWord.word || nextWord.text || '');\n\n      // Check if same word\n      if (nextNormalized !== normalized) break;\n\n      // Check gap (convert to ms)\n      const gap = (nextWord.start - words[j - 1].end) * 1000;\n      if (gap > maxGapMs) break;\n\n      repeatCount++;\n      j++;\n    }\n\n    if (repeatCount >= minRepeats) {\n      stutters.push({\n        word: normalized,\n        occurrences: repeatCount,\n        startWord: i,\n        endWord: j - 1,\n        start: words[i].start,\n        end: words[j - 1].end,\n        duration: words[j - 1].end - words[i].start,\n        // Keep first occurrence, remove rest\n        keepStart: words[i].start,\n        keepEnd: words[i].end,\n        removeStart: words[i + 1].start,\n        removeEnd: words[j - 1].end\n      });\n    }\n\n    i = j; // Skip past all repeats\n  }\n\n  console.log(`[SPLICE Repetition] Found ${stutters.length} stutter(s)`);\n\n  return {\n    stutters,\n    metadata: {\n      type: 'stutters',\n      totalWords: words.length,\n      stutterCount: stutters.length,\n      totalRepeatedWords: stutters.reduce((sum, s) => sum + s.occurrences, 0)\n    }\n  };\n}\n\n// =============================================================================\n// Utility Functions\n// =============================================================================\n\n/**\n * Normalize a word for comparison\n */\nfunction normalizeWord(word) {\n  return word\n    .toLowerCase()\n    .replace(/[^\\w\\s'-]/g, '') // Remove punctuation except apostrophe/hyphen\n    .trim();\n}\n\n/**\n * Calculate similarity between two word arrays\n * Uses Jaccard-like similarity with position weighting\n *\n * @param {Array<string>} arr1 - First word array\n * @param {Array<string>} arr2 - Second word array\n * @returns {number} Similarity score (0-1)\n */\nfunction calculateSimilarity(arr1, arr2) {\n  if (arr1.length !== arr2.length) return 0;\n  if (arr1.length === 0) return 0;\n\n  let matches = 0;\n  for (let i = 0; i < arr1.length; i++) {\n    if (arr1[i] === arr2[i]) {\n      matches++;\n    } else {\n      // Partial credit for similar words (edit distance)\n      const editDist = levenshteinDistance(arr1[i], arr2[i]);\n      const maxLen = Math.max(arr1[i].length, arr2[i].length);\n      if (maxLen > 0) {\n        const wordSim = 1 - (editDist / maxLen);\n        if (wordSim > 0.7) matches += wordSim;\n      }\n    }\n  }\n\n  return matches / arr1.length;\n}\n\n/**\n * Levenshtein edit distance between two strings\n */\nfunction levenshteinDistance(s1, s2) {\n  const m = s1.length;\n  const n = s2.length;\n\n  if (m === 0) return n;\n  if (n === 0) return m;\n\n  const dp = Array(m + 1).fill(null).map(() => Array(n + 1).fill(0));\n\n  for (let i = 0; i <= m; i++) dp[i][0] = i;\n  for (let j = 0; j <= n; j++) dp[0][j] = j;\n\n  for (let i = 1; i <= m; i++) {\n    for (let j = 1; j <= n; j++) {\n      if (s1[i - 1] === s2[j - 1]) {\n        dp[i][j] = dp[i - 1][j - 1];\n      } else {\n        dp[i][j] = 1 + Math.min(\n          dp[i - 1][j],     // deletion\n          dp[i][j - 1],     // insertion\n          dp[i - 1][j - 1]  // substitution\n        );\n      }\n    }\n  }\n\n  return dp[m][n];\n}\n\n/**\n * Filter overlapping repetition groups\n */\nfunction filterOverlappingRepetitions(repetitions) {\n  if (repetitions.length <= 1) return repetitions;\n\n  // Sort by first occurrence start\n  const sorted = [...repetitions].sort((a, b) =>\n    a.occurrences[0].start - b.occurrences[0].start\n  );\n\n  const result = [];\n  let lastEnd = -1;\n\n  for (const rep of sorted) {\n    const firstOcc = rep.occurrences[0];\n    // Check if overlaps with previous\n    if (firstOcc.start >= lastEnd) {\n      result.push(rep);\n      // Update lastEnd to include all occurrences\n      lastEnd = Math.max(...rep.occurrences.map(o => o.end));\n    }\n  }\n\n  return result;\n}\n\n/**\n * Convert repetitions to removal segments\n * Keeps the \"best\" occurrence, returns segments to remove\n *\n * @param {Array} repetitions - Detected repetitions\n * @returns {Array} Segments to remove from timeline\n */\nfunction repetitionsToRemovalSegments(repetitions) {\n  const segments = [];\n\n  for (const rep of repetitions) {\n    const keepIndex = rep.bestOccurrence || 0;\n\n    rep.occurrences.forEach((occ, i) => {\n      if (i !== keepIndex) {\n        segments.push({\n          start: occ.start,\n          end: occ.end,\n          type: 'repetition',\n          text: occ.text,\n          reason: `Duplicate of occurrence ${keepIndex + 1}`\n        });\n      }\n    });\n  }\n\n  // Sort by start time and merge adjacent\n  return mergeAdjacentSegments(segments);\n}\n\n/**\n * Convert stutters to removal segments\n *\n * @param {Array} stutters - Detected stutters\n * @returns {Array} Segments to remove\n */\nfunction stuttersToRemovalSegments(stutters) {\n  return stutters.map(s => ({\n    start: s.removeStart,\n    end: s.removeEnd,\n    type: 'stutter',\n    text: `${s.word} (x${s.occurrences})`,\n    keepRange: { start: s.keepStart, end: s.keepEnd }\n  }));\n}\n\n/**\n * Merge adjacent removal segments\n */\nfunction mergeAdjacentSegments(segments, maxGap = 0.1) {\n  if (segments.length <= 1) return segments;\n\n  const sorted = [...segments].sort((a, b) => a.start - b.start);\n  const merged = [];\n  let current = { ...sorted[0] };\n\n  for (let i = 1; i < sorted.length; i++) {\n    const next = sorted[i];\n\n    if (next.start - current.end <= maxGap) {\n      // Merge\n      current.end = next.end;\n      current.text = `${current.text}, ${next.text}`;\n    } else {\n      merged.push(current);\n      current = { ...next };\n    }\n  }\n\n  merged.push(current);\n  return merged;\n}\n\n// =============================================================================\n// Combined Detection\n// =============================================================================\n\n/**\n * Run all repetition detection (phrases + stutters)\n *\n * @param {Object} transcript - Transcript with words array\n * @param {Object} options - Detection options\n * @returns {Promise<Object>} Combined detection results\n */\nasync function detectAllRepetitions(transcript, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n\n  // Detect phrase repetitions\n  const phraseResult = opts.useOpenAI\n    ? await detectRepetitionsAdvanced(transcript, opts)\n    : detectRepetitionsBasic(transcript, opts);\n\n  // Detect stutters\n  const stutterResult = detectStutters(transcript, opts);\n\n  // Combine removal segments\n  const phraseSegments = repetitionsToRemovalSegments(phraseResult.repetitions);\n  const stutterSegments = stuttersToRemovalSegments(stutterResult.stutters);\n  const allSegments = mergeAdjacentSegments([...phraseSegments, ...stutterSegments]);\n\n  // Calculate total removable time\n  const totalRemovableTime = allSegments.reduce((sum, s) => sum + (s.end - s.start), 0);\n\n  return {\n    repetitions: phraseResult.repetitions,\n    stutters: stutterResult.stutters,\n    removalSegments: allSegments,\n    metadata: {\n      ...phraseResult.metadata,\n      stutterCount: stutterResult.stutters.length,\n      totalRemovalSegments: allSegments.length,\n      totalRemovableTime: parseFloat(totalRemovableTime.toFixed(3))\n    }\n  };\n}\n\n// =============================================================================\n// Exports\n// =============================================================================\n\nmodule.exports = {\n  // Core detection\n  detectRepetitionsBasic,\n  detectRepetitionsAdvanced,\n  detectStutters,\n  detectAllRepetitions,\n\n  // Conversion utilities\n  repetitionsToRemovalSegments,\n  stuttersToRemovalSegments,\n\n  // Helper utilities\n  normalizeWord,\n  calculateSimilarity,\n  levenshteinDistance,\n  filterOverlappingRepetitions,\n  mergeAdjacentSegments,\n\n  // Config\n  DEFAULT_OPTIONS\n};\n","numLines":626,"startLine":1,"totalLines":626}}}
{"parentUuid":"a457a679-3e64-4919-9347-71698923880e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VXT1DyWbviGfRTChyHe4Gv","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Plugin Main Entry Point\n     3→ *\n     4→ * Initializes all functionality when the plugin loads.\n     5→ * v3.4 - Timeline Markers + Unified Take-Aware Workflow.\n     6→ */\n     7→\n     8→// Initialize Premiere Pro API\n     9→const ppro = require('premierepro');\n    10→\n    11→// ============================================================================\n    12→// CACHED DOM ELEMENTS (Performance: avoid repeated getElementById calls)\n    13→// ============================================================================\n    14→const ui = {};\n    15→\n    16→function cacheUIElements() {\n    17→  ui.status = document.getElementById('status');\n    18→\n    19→  // Main GO button (unified workflow)\n    20→  ui.goBtn = document.getElementById('goBtn');\n    21→  ui.optionsToggle = document.getElementById('optionsToggle');\n    22→  ui.optionsPanel = document.getElementById('optionsPanel');\n    23→\n    24→  // Sliders and options\n    25→  ui.sensitivitySlider = document.getElementById('sensitivitySlider');\n    26→  ui.sourceOriginal = document.getElementById('sourceOriginal');\n    27→  ui.sourceIsolated = document.getElementById('sourceIsolated');\n    28→\n    29→  // Settings checkboxes\n    30→  ui.enableTakesDetection = document.getElementById('enableTakesDetection');\n    31→  ui.autoMarkBest = document.getElementById('autoMarkBest');\n    32→\n    33→  // Progress\n    34→  ui.progressContainer = document.getElementById('progressContainer');\n    35→  ui.progressText = document.getElementById('progressText');\n    36→  ui.resultsEmpty = document.getElementById('resultsEmpty');\n    37→\n    38→  // Combined Preview\n    39→  ui.combinedPreview = document.getElementById('combinedPreview');\n    40→  ui.previewList = document.getElementById('previewList');\n    41→  ui.silenceCountDisplay = document.getElementById('silenceCount');\n    42→  ui.takeCountDisplay = document.getElementById('takeCount');\n    43→  ui.selectedCount = document.getElementById('selectedCount');\n    44→  ui.selectAllSilences = document.getElementById('selectAllSilences');\n    45→  ui.applyPreviewBtn = document.getElementById('applyPreviewBtn');\n    46→  ui.cancelPreviewBtn = document.getElementById('cancelPreviewBtn');\n    47→  ui.invertSelectionBtn = document.getElementById('invertSelectionBtn');\n    48→  ui.reRunDetectionBtn = document.getElementById('reRunDetectionBtn');\n    49→\n    50→  // Results\n    51→  ui.silenceResults = document.getElementById('silenceResults');\n    52→  ui.resultsCount = document.getElementById('resultsCount');\n    53→  ui.timeSaved = document.getElementById('timeSaved');\n    54→  ui.clipsModified = document.getElementById('clipsModified');\n    55→\n    56→  // Build Sequence button (v3.5)\n    57→  ui.buildSequenceBtn = document.getElementById('buildSequenceBtn');\n    58→\n    59→  // Buttons\n    60→  ui.undoBtn = document.getElementById('undoBtn');\n    61→  ui.advancedSection = document.getElementById('advancedSection');\n    62→\n    63→  // Settings modal\n    64→  ui.settingsBtn = document.getElementById('settingsBtn');\n    65→  ui.settingsModal = document.getElementById('settingsModal');\n    66→  ui.closeSettingsBtn = document.getElementById('closeSettingsBtn');\n    67→\n    68→  // Media folder (v3.5)\n    69→  ui.mediaFolderDisplay = document.getElementById('mediaFolderDisplay');\n    70→  ui.setMediaFolderBtn = document.getElementById('setMediaFolderBtn');\n    71→  ui.clearMediaFolderBtn = document.getElementById('clearMediaFolderBtn');\n    72→\n    73→  // Custom Presets (v3.5)\n    74→  ui.savePresetBtn = document.getElementById('savePresetBtn');\n    75→  ui.managePresetsBtn = document.getElementById('managePresetsBtn');\n    76→  ui.presetModal = document.getElementById('presetModal');\n    77→  ui.presetModalTitle = document.getElementById('presetModalTitle');\n    78→  ui.closePresetModalBtn = document.getElementById('closePresetModalBtn');\n    79→  ui.presetNameInput = document.getElementById('presetNameInput');\n    80→  ui.presetDescInput = document.getElementById('presetDescInput');\n    81→  ui.presetIconPicker = document.getElementById('presetIconPicker');\n    82→  ui.presetEditId = document.getElementById('presetEditId');\n    83→  ui.presetSelectedIcon = document.getElementById('presetSelectedIcon');\n    84→  ui.savePresetConfirmBtn = document.getElementById('savePresetConfirmBtn');\n    85→  ui.presetModalError = document.getElementById('presetModalError');\n    86→  ui.managePresetsModal = document.getElementById('managePresetsModal');\n    87→  ui.closeManagePresetsBtn = document.getElementById('closeManagePresetsBtn');\n    88→  ui.presetsList = document.getElementById('presetsList');\n    89→  ui.noCustomPresets = document.getElementById('noCustomPresets');\n    90→  ui.presetSelector = document.getElementById('presetSelector');\n    91→\n    92→  // Preset settings inputs (Phase 3)\n    93→  ui.presetSensitivity = document.getElementById('presetSensitivity');\n    94→  ui.presetSensitivityValue = document.getElementById('presetSensitivityValue');\n    95→  ui.presetThreshold = document.getElementById('presetThreshold');\n    96→\n    97→  // Export/Import buttons (Phase 3)\n    98→  ui.exportPresetsBtn = document.getElementById('exportPresetsBtn');\n    99→  ui.importPresetsBtn = document.getElementById('importPresetsBtn');\n   100→  ui.importPresetsFile = document.getElementById('importPresetsFile');\n   101→}\n   102→\n   103→// ============================================================================\n   104→// STATE MANAGEMENT\n   105→// ============================================================================\n   106→let previewSilences = [];         // All detected silences\n   107→let safeSilences = [];            // Silences that don't overlap takes (safe to remove)\n   108→let protectedSilences = [];       // Silences that overlap takes (protected)\n   109→let previewTakes = [];            // Detected takes (for display)\n   110→let selectedSilenceIndices = new Set();\n   111→let isOperationInProgress = false;\n   112→let pendingUIUpdate = null;\n   113→\n   114→// Export preview state for other modules (Razor integration)\n   115→window.splicePreviewState = {\n   116→  getSelectedSilences: () => safeSilences.filter((_, i) => selectedSilenceIndices.has(i)),\n   117→  getSelectedTakes: () => previewTakes,\n   118→  hasPreview: () => safeSilences.length > 0 || previewTakes.length > 0\n   119→};\n   120→\n   121→// ============================================================================\n   122→// INITIALIZATION\n   123→// ============================================================================\n   124→document.addEventListener('DOMContentLoaded', () => {\n   125→  // Cache all DOM elements first\n   126→  cacheUIElements();\n   127→\n   128→  // Settings & UI\n   129→  initSettingsUI();\n   130→  initSettingsModal();\n   131→  initLoginModal();\n   132→  initOptionsToggles();\n   133→  initPresetSelector();\n   134→  initHelpButton();\n   135→\n   136→  // Credits display\n   137→  initCredits();\n   138→\n   139→  // Unified workflow (silences + takes)\n   140→  initUnifiedWorkflow();\n   141→\n   142→  // Preview handlers (event delegation)\n   143→  initPreviewHandlers();\n   144→\n   145→  // Undo handlers\n   146→  initUndoHandlers();\n   147→\n   148→  // Keyboard shortcuts\n   149→  initKeyboardShortcuts();\n   150→\n   151→  // Advanced features (Razor - legacy)\n   152→  initSlice9();\n   153→\n   154→  // Batch processing\n   155→  initBatchHandlers();\n   156→\n   157→  // Media folder (v3.5)\n   158→  initMediaFolderHandlers();\n   159→\n   160→  // Custom Presets UI (v3.5)\n   161→  initCustomPresetsUI();\n   162→\n   163→  console.log('[SPLICE] Plugin initialized v3.5 (Direct DOM Reconstruction)');\n   164→});\n   165→\n   166→// ============================================================================\n   167→// DEBOUNCED UI UPDATES (Performance: batch DOM updates)\n   168→// ============================================================================\n   169→function scheduleUIUpdate(callback) {\n   170→  if (pendingUIUpdate) {\n   171→    cancelAnimationFrame(pendingUIUpdate);\n   172→  }\n   173→  pendingUIUpdate = requestAnimationFrame(() => {\n   174→    callback();\n   175→    pendingUIUpdate = null;\n   176→  });\n   177→}\n   178→\n   179→function updateSelectionCounts() {\n   180→  scheduleUIUpdate(() => {\n   181→    if (ui.selectedCount) {\n   182→      ui.selectedCount.textContent = selectedSilenceIndices.size;\n   183→    }\n   184→    updateApplyButton();\n   185→    updateSelectAllCheckbox();\n   186→  });\n   187→}\n   188→\n   189→// ============================================================================\n   190→// KEYBOARD SHORTCUTS\n   191→// ============================================================================\n   192→function initKeyboardShortcuts() {\n   193→  document.addEventListener('keydown', (e) => {\n   194→    // Only handle when combined preview is visible\n   195→    const previewVisible = ui.combinedPreview?.style.display !== 'none' &&\n   196→                           ui.combinedPreview?.style.display !== '';\n   197→\n   198→    if (!previewVisible) return;\n   199→\n   200→    const isMac = navigator.platform.toUpperCase().indexOf('MAC') >= 0;\n   201→    const cmdKey = isMac ? e.metaKey : e.ctrlKey;\n   202→\n   203→    // Enter - Apply selected\n   204→    if (e.key === 'Enter' && !e.shiftKey) {\n   205→      e.preventDefault();\n   206→      if (selectedSilenceIndices.size > 0) {\n   207→        ui.applyPreviewBtn?.click();\n   208→      }\n   209→    }\n   210→\n   211→    // Escape - Cancel\n   212→    if (e.key === 'Escape') {\n   213→      e.preventDefault();\n   214→      ui.cancelPreviewBtn?.click();\n   215→    }\n   216→\n   217→    // Cmd/Ctrl+A - Select all\n   218→    if (e.key === 'a' && cmdKey) {\n   219→      e.preventDefault();\n   220→      if (ui.selectAllSilences) {\n   221→        ui.selectAllSilences.checked = true;\n   222→        selectAllSilences(true);\n   223→      }\n   224→    }\n   225→\n   226→    // Cmd/Ctrl+D - Deselect all\n   227→    if (e.key === 'd' && cmdKey) {\n   228→      e.preventDefault();\n   229→      if (ui.selectAllSilences) {\n   230→        ui.selectAllSilences.checked = false;\n   231→        selectAllSilences(false);\n   232→      }\n   233→    }\n   234→\n   235→    // Cmd/Ctrl+I - Invert selection\n   236→    if (e.key === 'i' && cmdKey) {\n   237→      e.preventDefault();\n   238→      invertSilenceSelection();\n   239→    }\n   240→  });\n   241→}\n   242→\n   243→// ============================================================================\n   244→// UNIFIED WORKFLOW (Silence Detection + Takes Detection)\n   245→// ============================================================================\n   246→function initUnifiedWorkflow() {\n   247→  if (!ui.goBtn) return;\n   248→\n   249→  ui.goBtn.addEventListener('click', async () => {\n   250→    // Prevent double-click\n   251→    if (isOperationInProgress) {\n   252→      setStatus('Operation in progress...');\n   253→      return;\n   254→    }\n   255→\n   256→    isOperationInProgress = true;\n   257→    ui.goBtn.disabled = true;\n   258→\n   259→    try {\n   260→      showProgress('Preparing audio...');\n   261→\n   262→      // Step 1: Export audio\n   263→      const exportSuccess = await exportAudioInternal();\n   264→      if (!exportSuccess) {\n   265→        throw new Error('Audio export failed');\n   266→      }\n   267→\n   268→      // Step 2: Get settings\n   269→      const sensitivity = parseInt(ui.sensitivitySlider?.value || 50);\n   270→      const useIsolated = ui.sourceIsolated?.checked ?? false;\n   271→      const detectTakes = ui.enableTakesDetection?.checked ?? true;\n   272→\n   273→      // Step 3: Vocal isolation if needed\n   274→      let audioPath = WAV_PATH;\n   275→      if (useIsolated) {\n   276→        // Check isolation access before proceeding\n   277→        const credits = getCurrentCredits();\n   278→        if (!credits || !credits.hasIsolationAccess) {\n   279→          throw new Error('Vocal isolation requires Pro or Team tier. Upgrade to access this feature.');\n   280→        }\n   281→\n   282→        // Show overage warning if applicable\n   283→        const isolationMins = credits.isolationHoursRemaining * 60;\n   284→        if (isolationMins <= 0) {\n   285→          const overageRate = credits.isolationOverageRate || 0.08;\n   286→          showProgress(`Isolating vocals (overage: $${overageRate}/min)...`);\n   287→        } else {\n   288→          showProgress(`Isolating vocals (${isolationMins.toFixed(0)} min remaining)...`);\n   289→        }\n   290→\n   291→        const isolateResult = await isolateVocals();\n   292→        if (isolateResult.success) {\n   293→          audioPath = isolateResult.outputPath;\n   294→          // Update credits display after isolation\n   295→          await refreshCredits();\n   296→        } else {\n   297→          throw new Error(isolateResult.error || 'Vocal isolation failed');\n   298→        }\n   299→      }\n   300→\n   301→      // Step 4: Run detection (parallel if takes enabled)\n   302→      const params = mapSensitivity(sensitivity);\n   303→      let silences = [];\n   304→      let takes = [];\n   305→\n   306→      if (detectTakes) {\n   307→        // Parallel detection for performance\n   308→        showProgress('Detecting silences and analyzing takes...');\n   309→        const [silenceResult, transcriptResult] = await Promise.all([\n   310→          detectSilences(audioPath, params),\n   311→          transcribeAudio()\n   312→        ]);\n   313→\n   314→        silences = silenceResult;\n   315→\n   316→        // Extract takes from transcript result\n   317→        if (transcriptResult.success && transcriptResult.takes) {\n   318→          if (transcriptResult.takes.takes) {\n   319→            takes = transcriptResult.takes.takes;\n   320→          } else if (Array.isArray(transcriptResult.takes)) {\n   321→            takes = transcriptResult.takes;\n   322→          }\n   323→        }\n   324→\n   325→        // Store transcript for reference\n   326→        window.currentTranscript = transcriptResult.transcript;\n   327→      } else {\n   328→        // Just detect silences\n   329→        showProgress('Detecting silences...');\n   330→        silences = await detectSilences(audioPath, params);\n   331→      }\n   332→\n   333→      if (silences.length === 0) {\n   334→        showEmptyState();\n   335→        setStatus('No silences detected. Try increasing sensitivity.');\n   336→        return;\n   337→      }\n   338→\n   339→      // Step 5: Filter silences by takes (protect speech)\n   340→      const filtered = filterSilencesByTakes(silences, takes);\n   341→\n   342→      // Step 6: Add timeline markers (clear old ones first)\n   343→      showProgress('Adding timeline markers...');\n   344→      await clearSpliceMarkers();\n   345→      const markerResult = await addTimelineMarkers(filtered.safe, filtered.protected, takes);\n   346→\n   347→      // Step 7: Show combined preview\n   348→      showCombinedPreview(filtered.safe, filtered.protected, takes);\n   349→\n   350→      const markerInfo = markerResult.added > 0 ? ` (${markerResult.added} markers added)` : '';\n   351→      const statusMsg = takes.length > 0\n   352→        ? `Found ${silences.length} silences, ${filtered.protected.length} protected by ${takes.length} takes${markerInfo}`\n   353→        : `Found ${silences.length} silence(s) - review and apply${markerInfo}`;\n   354→      setStatus(statusMsg);\n   355→\n   356→    } catch (err) {\n   357→      showEmptyState();\n   358→      setStatus('Error: ' + err.message);\n   359→      console.error('[SPLICE] Unified workflow error:', err);\n   360→    } finally {\n   361→      isOperationInProgress = false;\n   362→      ui.goBtn.disabled = false;\n   363→    }\n   364→  });\n   365→}\n   366→\n   367→// ============================================================================\n   368→// TAKE-AWARE SILENCE FILTERING\n   369→// ============================================================================\n   370→/**\n   371→ * Filter silences to protect those that overlap with takes (speech segments).\n   372→ * Returns { safe: [], protected: [] }\n   373→ */\n   374→function filterSilencesByTakes(silences, takes) {\n   375→  if (!takes || takes.length === 0) {\n   376→    // No takes = all silences are safe to remove\n   377→    return { safe: silences, protected: [] };\n   378→  }\n   379→\n   380→  const safe = [];\n   381→  const protected_ = [];\n   382→\n   383→  silences.forEach(silence => {\n   384→    // Check if this silence overlaps with any take\n   385→    const overlaps = takes.some(take => {\n   386→      const takeStart = take.startTime || take.start;\n   387→      const takeEnd = take.endTime || take.end;\n   388→      // Overlap exists if silence starts before take ends AND silence ends after take starts\n   389→      return silence.start < takeEnd && silence.end > takeStart;\n   390→    });\n   391→\n   392→    if (overlaps) {\n   393→      protected_.push(silence);\n   394→    } else {\n   395→      safe.push(silence);\n   396→    }\n   397→  });\n   398→\n   399→  console.log(`[SPLICE] Filtered silences: ${safe.length} safe, ${protected_.length} protected`);\n   400→  return { safe, protected: protected_ };\n   401→}\n   402→\n   403→// ============================================================================\n   404→// TIMELINE MARKERS (Visual indicators on Premiere Pro timeline)\n   405→// ============================================================================\n   406→/**\n   407→ * Add colored markers to the timeline for silences and takes.\n   408→ * - Red markers: silences safe to remove\n   409→ * - Yellow markers: silences protected by takes\n   410→ * - Blue markers: takes (speech segments)\n   411→ *\n   412→ * @param {Array} safeToRemove - Silences safe to remove\n   413→ * @param {Array} protectedSilences - Silences overlapping speech\n   414→ * @param {Array} takes - Detected speech segments\n   415→ * @returns {Promise<{added: number, errors: number}>}\n   416→ */\n   417→async function addTimelineMarkers(safeToRemove, protectedSilences, takes) {\n   418→  const results = { added: 0, errors: 0 };\n   419→\n   420→  try {\n   421→    const context = await getActiveSequence();\n   422→    if (!context) {\n   423→      console.warn('[SPLICE] No sequence available for markers');\n   424→      return results;\n   425→    }\n   426→\n   427→    const { sequence, project } = context;\n   428→\n   429→    // Get markers object for the sequence\n   430→    const markers = await ppro.Markers.getMarkers(sequence);\n   431→    if (!markers) {\n   432→      console.warn('[SPLICE] Markers API not available');\n   433→      return results;\n   434→    }\n   435→\n   436→    // Collect all marker actions\n   437→    const markerActions = [];\n   438→\n   439→    // Red markers for safe-to-remove silences\n   440→    for (const silence of safeToRemove) {\n   441→      try {\n   442→        const duration = silence.end - silence.start;\n   443→\n   444→        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n   445→        const startTime = ppro.TickTime.createWithSeconds(silence.start);\n   446→        const durationTime = ppro.TickTime.createWithSeconds(duration);\n   447→\n   448→        const action = markers.createAddMarkerAction(\n   449→          `SPLICE: Remove (${duration.toFixed(2)}s)`,\n   450→          ppro.Marker.MARKER_TYPE_COMMENT,\n   451→          startTime,\n   452→          durationTime,\n   453→          `Silence to remove: ${formatTime(silence.start)} - ${formatTime(silence.end)}`\n   454→        );\n   455→        markerActions.push({ action, colorIndex: 1, type: 'silence' }); // Red = index 1\n   456→      } catch (err) {\n   457→        console.error('[SPLICE] Error creating silence marker:', err);\n   458→        results.errors++;\n   459→      }\n   460→    }\n   461→\n   462→    // Yellow markers for protected silences\n   463→    for (const silence of protectedSilences) {\n   464→      try {\n   465→        const duration = silence.end - silence.start;\n   466→\n   467→        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n   468→        const startTime = ppro.TickTime.createWithSeconds(silence.start);\n   469→        const durationTime = ppro.TickTime.createWithSeconds(duration);\n   470→\n   471→        const action = markers.createAddMarkerAction(\n   472→          `SPLICE: Protected (${duration.toFixed(2)}s)`,\n   473→          ppro.Marker.MARKER_TYPE_COMMENT,\n   474→          startTime,\n   475→          durationTime,\n   476→          `Protected silence (overlaps speech): ${formatTime(silence.start)} - ${formatTime(silence.end)}`\n   477→        );\n   478→        markerActions.push({ action, colorIndex: 4, type: 'protected' }); // Yellow = index 4\n   479→      } catch (err) {\n   480→        console.error('[SPLICE] Error creating protected marker:', err);\n   481→        results.errors++;\n   482→      }\n   483→    }\n   484→\n   485→    // Blue markers for takes (speech segments)\n   486→    for (const take of takes) {\n   487→      try {\n   488→        const start = take.startTime || take.start;\n   489→        const end = take.endTime || take.end;\n   490→        const duration = end - start;\n   491→        const textPreview = take.text?.substring(0, 30) || 'Speech';\n   492→\n   493→        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n   494→        const startTime = ppro.TickTime.createWithSeconds(start);\n   495→        const durationTime = ppro.TickTime.createWithSeconds(duration);\n   496→\n   497→        const action = markers.createAddMarkerAction(\n   498→          `SPLICE: Take`,\n   499→          ppro.Marker.MARKER_TYPE_COMMENT,\n   500→          startTime,\n   501→          durationTime,\n   502→          `Speech segment: ${textPreview}${take.text?.length > 30 ? '...' : ''}`\n   503→        );\n   504→        markerActions.push({ action, colorIndex: 5, type: 'take' }); // Blue = index 5\n   505→      } catch (err) {\n   506→        console.error('[SPLICE] Error creating take marker:', err);\n   507→        results.errors++;\n   508→      }\n   509→    }\n   510→\n   511→    // Execute all marker actions in a single transaction\n   512→    if (markerActions.length > 0) {\n   513→      await project.lockedAccess(async () => {\n   514→        await project.executeTransaction((compoundAction) => {\n   515→          for (const { action } of markerActions) {\n   516→            compoundAction.addAction(action);\n   517→          }\n   518→        }, 'SPLICE: Add Timeline Markers');\n   519→      });\n   520→\n   521→      results.added = markerActions.length;\n   522→      console.log(`[SPLICE] Added ${results.added} timeline markers`);\n   523→    }\n   524→\n   525→  } catch (err) {\n   526→    console.error('[SPLICE] Timeline markers error:', err);\n   527→    results.errors++;\n   528→  }\n   529→\n   530→  return results;\n   531→}\n   532→\n   533→/**\n   534→ * Read current SPLICE marker positions from timeline.\n   535→ * This allows users to manually adjust markers before applying.\n   536→ * Returns silences based on current marker positions (not original detection).\n   537→ */\n   538→async function readMarkersAssilences() {\n   539→  const silences = [];\n   540→\n   541→  try {\n   542→    const context = await getActiveSequence();\n   543→    if (!context) return silences;\n   544→\n   545→    const { sequence } = context;\n   546→    const markers = await ppro.Markers.getMarkers(sequence);\n   547→    if (!markers) return silences;\n   548→\n   549→    const allMarkers = await markers.getMarkers();\n   550→    if (!allMarkers || allMarkers.length === 0) return silences;\n   551→\n   552→    for (const marker of allMarkers) {\n   553→      const name = await marker.getName();\n   554→\n   555→      // Only read \"Remove\" markers (red ones that will be cut)\n   556→      if (name && name.startsWith('SPLICE: Remove')) {\n   557→        const startTickTime = await marker.getStart();\n   558→        const durationTickTime = await marker.getDuration();\n   559→\n   560→        // Use TickTime.seconds property to get seconds directly (no TICKS_PER_SECOND needed)\n   561→        const start = startTickTime.seconds;\n   562→        const duration = durationTickTime.seconds;\n   563→        const end = start + duration;\n   564→\n   565→        silences.push({\n   566→          start,\n   567→          end,\n   568→          duration,\n   569→          fromMarker: true  // Flag to indicate this came from marker adjustment\n   570→        });\n   571→      }\n   572→    }\n   573→\n   574→    // Sort by start time\n   575→    silences.sort((a, b) => a.start - b.start);\n   576→    console.log(`[SPLICE] Read ${silences.length} silences from adjusted markers`);\n   577→\n   578→  } catch (err) {\n   579→    console.error('[SPLICE] Error reading markers:', err);\n   580→  }\n   581→\n   582→  return silences;\n   583→}\n   584→\n   585→/**\n   586→ * Remove all SPLICE markers from the timeline.\n   587→ * Call this before adding new markers to avoid duplicates.\n   588→ */\n   589→async function clearSpliceMarkers() {\n   590→  try {\n   591→    const context = await getActiveSequence();\n   592→    if (!context) return 0;\n   593→\n   594→    const { sequence, project } = context;\n   595→    const markers = await ppro.Markers.getMarkers(sequence);\n   596→    if (!markers) return 0;\n   597→\n   598→    // Get all markers\n   599→    const allMarkers = await markers.getMarkers();\n   600→    if (!allMarkers || allMarkers.length === 0) return 0;\n   601→\n   602→    // Filter SPLICE markers\n   603→    const spliceMarkers = [];\n   604→    for (const marker of allMarkers) {\n   605→      const name = await marker.getName();\n   606→      if (name && name.startsWith('SPLICE:')) {\n   607→        spliceMarkers.push(marker);\n   608→      }\n   609→    }\n   610→\n   611→    if (spliceMarkers.length === 0) return 0;\n   612→\n   613→    // Remove all SPLICE markers in a single transaction\n   614→    await project.lockedAccess(async () => {\n   615→      await project.executeTransaction((compoundAction) => {\n   616→        for (const marker of spliceMarkers) {\n   617→          const removeAction = markers.createRemoveMarkerAction(marker);\n   618→          compoundAction.addAction(removeAction);\n   619→        }\n   620→      }, 'SPLICE: Clear Markers');\n   621→    });\n   622→\n   623→    console.log(`[SPLICE] Cleared ${spliceMarkers.length} previous markers`);\n   624→    return spliceMarkers.length;\n   625→\n   626→  } catch (err) {\n   627→    console.error('[SPLICE] Error clearing markers:', err);\n   628→    return 0;\n   629→  }\n   630→}\n   631→\n   632→// ============================================================================\n   633→// COMBINED PREVIEW (Silences + Takes with colored markers)\n   634→// ============================================================================\n   635→/**\n   636→ * Show combined preview with:\n   637→ * - Red markers: silences safe to remove\n   638→ * - Yellow markers: silences protected by takes (won't be removed)\n   639→ * - Blue markers: takes (for reference)\n   640→ */\n   641→function showCombinedPreview(safeToRemove, protectedSilences, takes) {\n   642→  // Update global state\n   643→  safeSilences = safeToRemove;\n   644→  previewSilences = [...safeToRemove, ...protectedSilences];\n   645→  previewTakes = takes;\n   646→  selectedSilenceIndices = new Set(safeToRemove.map((_, i) => i));\n   647→\n   648→  // Hide other views\n   649→  hideAllViews();\n   650→\n   651→  // Update summary counts\n   652→  if (ui.silenceCountDisplay) ui.silenceCountDisplay.textContent = safeToRemove.length;\n   653→  if (ui.takeCountDisplay) ui.takeCountDisplay.textContent = takes.length;\n   654→  if (ui.selectedCount) ui.selectedCount.textContent = safeToRemove.length;\n   655→\n   656→  // Build combined items list (sorted by time)\n   657→  const items = [\n   658→    ...safeToRemove.map((s, i) => ({ type: 'silence', index: i, start: s.start, end: s.end, data: s })),\n   659→    ...protectedSilences.map(s => ({ type: 'protected', start: s.start, end: s.end, data: s })),\n   660→    ...takes.map(t => ({ type: 'take', start: t.startTime || t.start, end: t.endTime || t.end, data: t }))\n   661→  ].sort((a, b) => a.start - b.start);\n   662→\n   663→  // Build preview list using DocumentFragment\n   664→  const fragment = document.createDocumentFragment();\n   665→\n   666→  items.forEach((item) => {\n   667→    const duration = item.end - item.start;\n   668→    const div = document.createElement('div');\n   669→\n   670→    if (item.type === 'silence') {\n   671→      // Safe to remove - red marker with checkbox\n   672→      div.className = 'preview-item silence-marker';\n   673→      div.dataset.index = item.index;\n   674→      div.innerHTML = `\n   675→        <input type=\"checkbox\" class=\"preview-item-check\" checked data-index=\"${item.index}\">\n   676→        <div class=\"preview-item-info\">\n   677→          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n   678→          <div class=\"preview-item-duration\">${duration.toFixed(2)}s silence</div>\n   679→        </div>\n   680→        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek to this silence\">&gt;</button>\n   681→      `;\n   682→    } else if (item.type === 'protected') {\n   683→      // Protected by take - yellow marker, no checkbox\n   684→      div.className = 'preview-item protected-silence';\n   685→      div.innerHTML = `\n   686→        <div style=\"width: 20px; margin-right: 10px;\"></div>\n   687→        <div class=\"preview-item-info\">\n   688→          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n   689→          <div class=\"preview-item-duration\">${duration.toFixed(2)}s (overlaps speech)</div>\n   690→        </div>\n   691→        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek\">&gt;</button>\n   692→      `;\n   693→    } else if (item.type === 'take') {\n   694→      // Take (speech segment) - blue marker, no checkbox\n   695→      div.className = 'preview-item take-marker';\n   696→      const textPreview = item.data.text?.substring(0, 40) || 'Speech segment';\n   697→      div.innerHTML = `\n   698→        <div style=\"width: 20px; margin-right: 10px;\"></div>\n   699→        <div class=\"preview-item-info\">\n   700→          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n   701→          <div class=\"preview-item-duration\">${textPreview}${item.data.text?.length > 40 ? '...' : ''}</div>\n   702→        </div>\n   703→        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek to take\">&gt;</button>\n   704→      `;\n   705→    }\n   706→\n   707→    fragment.appendChild(div);\n   708→  });\n   709→\n   710→  if (ui.previewList) {\n   711→    ui.previewList.textContent = '';\n   712→    ui.previewList.appendChild(fragment);\n   713→  }\n   714→\n   715→  // Reset UI state\n   716→  if (ui.selectAllSilences) ui.selectAllSilences.checked = true;\n   717→  updateApplyButton();\n   718→\n   719→  // Show combined preview section\n   720→  if (ui.combinedPreview) ui.combinedPreview.style.display = 'block';\n   721→}\n   722→\n   723→function initPreviewHandlers() {\n   724→  // EVENT DELEGATION: Single listener on parent instead of many on children\n   725→  if (ui.previewList) {\n   726→    // Handle checkbox changes\n   727→    ui.previewList.addEventListener('change', (e) => {\n   728→      if (e.target.classList.contains('preview-item-check')) {\n   729→        const index = parseInt(e.target.dataset.index);\n   730→        const item = e.target.closest('.preview-item');\n   731→\n   732→        if (e.target.checked) {\n   733→          selectedSilenceIndices.add(index);\n   734→          item?.classList.remove('excluded');\n   735→        } else {\n   736→          selectedSilenceIndices.delete(index);\n   737→          item?.classList.add('excluded');\n   738→        }\n   739→        updateSelectionCounts();\n   740→      }\n   741→    });\n   742→\n   743→    // Handle seek button clicks\n   744→    ui.previewList.addEventListener('click', async (e) => {\n   745→      if (e.target.classList.contains('preview-item-seek')) {\n   746→        e.stopPropagation();\n   747→        const btn = e.target;\n   748→        const time = parseFloat(btn.dataset.time);\n   749→\n   750→        // Show loading state\n   751→        btn.classList.add('loading');\n   752→        btn.textContent = '...';\n   753→\n   754→        try {\n   755→          await seekToTime(time);\n   756→        } finally {\n   757→          btn.classList.remove('loading');\n   758→          btn.textContent = '>';\n   759→        }\n   760→      }\n   761→    });\n   762→  }\n   763→\n   764→  // Apply button\n   765→  if (ui.applyPreviewBtn) {\n   766→    ui.applyPreviewBtn.addEventListener('click', async () => {\n   767→      if (isOperationInProgress) return;\n   768→      isOperationInProgress = true;\n   769→      ui.applyPreviewBtn.disabled = true;\n   770→\n   771→      const originalText = ui.applyPreviewBtn.textContent;\n   772→      ui.applyPreviewBtn.textContent = 'Applying...';\n   773→\n   774→      try {\n   775→        await applySelectedSilences();\n   776→      } catch (err) {\n   777→        setStatus('Error: ' + err.message);\n   778→        console.error('[SPLICE] Apply error:', err);\n   779→      } finally {\n   780→        ui.applyPreviewBtn.textContent = originalText;\n   781→        ui.applyPreviewBtn.disabled = false;\n   782→        isOperationInProgress = false;\n   783→      }\n   784→    });\n   785→  }\n   786→\n   787→  // Cancel button\n   788→  if (ui.cancelPreviewBtn) {\n   789→    ui.cancelPreviewBtn.addEventListener('click', async () => {\n   790→      await cancelPreview();\n   791→    });\n   792→  }\n   793→\n   794→  // Select all checkbox\n   795→  if (ui.selectAllSilences) {\n   796→    ui.selectAllSilences.addEventListener('change', (e) => {\n   797→      selectAllSilences(e.target.checked);\n   798→    });\n   799→  }\n   800→\n   801→  // Invert selection button\n   802→  if (ui.invertSelectionBtn) {\n   803→    ui.invertSelectionBtn.addEventListener('click', () => {\n   804→      invertSilenceSelection();\n   805→    });\n   806→  }\n   807→\n   808→  // Duration filter buttons\n   809→  document.querySelectorAll('.preview-filters .filter-btn[data-min]').forEach(btn => {\n   810→    btn.addEventListener('click', () => {\n   811→      const min = parseFloat(btn.dataset.min);\n   812→      const max = parseFloat(btn.dataset.max);\n   813→      selectByDuration(min, max);\n   814→    });\n   815→  });\n   816→\n   817→  // Re-run detection button\n   818→  if (ui.reRunDetectionBtn) {\n   819→    ui.reRunDetectionBtn.addEventListener('click', () => {\n   820→      // Hide preview, show options, let user adjust sensitivity\n   821→      if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n   822→      if (ui.optionsPanel) ui.optionsPanel.classList.remove('collapsed');\n   823→      if (ui.optionsToggle) ui.optionsToggle.classList.add('expanded');\n   824→      showEmptyState();\n   825→      setStatus('Adjust sensitivity and click GO again');\n   826→    });\n   827→  }\n   828→\n   829→  // Build Sequence button (v3.5 - direct DOM reconstruction)\n   830→  if (ui.buildSequenceBtn) {\n   831→    ui.buildSequenceBtn.addEventListener('click', async () => {\n   832→      if (isOperationInProgress) return;\n   833→\n   834→      // Check if we have detection data\n   835→      if (safeSilences.length === 0 && previewTakes.length === 0) {\n   836→        setStatus('No detection data. Run analysis first.');\n   837→        return;\n   838→      }\n   839→\n   840→      isOperationInProgress = true;\n   841→      ui.buildSequenceBtn.disabled = true;\n   842→      const originalText = ui.buildSequenceBtn.textContent;\n   843→      ui.buildSequenceBtn.textContent = 'Building...';\n   844→\n   845→      try {\n   846→        showProgress('Building new sequence...');\n   847→\n   848→        const result = await buildSequenceWithCutList();\n   849→\n   850→        if (result.success) {\n   851→          // Hide preview and show success\n   852→          if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n   853→          showEmptyState();\n   854→\n   855→          const metadata = result.metadata || {};\n   856→          setStatus(`New sequence created: ${metadata.silencesRemoved || 0} silences removed, ${metadata.takesDetected || 0} takes detected`);\n   857→\n   858→          // Clear markers after successful build\n   859→          await clearSpliceMarkers();\n   860→\n   861→          // Clear preview state\n   862→          previewSilences = [];\n   863→          safeSilences = [];\n   864→          protectedSilences = [];\n   865→          previewTakes = [];\n   866→          selectedSilenceIndices.clear();\n   867→        } else {\n   868→          // User-friendly error messages\n   869→          const errorMsg = result.error || 'Unknown error';\n   870→          setStatus('Build failed: ' + formatBuildError(errorMsg));\n   871→        }\n   872→\n   873→      } catch (err) {\n   874→        console.error('[SPLICE] Build sequence error:', err);\n   875→        // User-friendly error messages with recovery hints\n   876→        setStatus('Build error: ' + formatBuildError(err.message));\n   877→      } finally {\n   878→        ui.buildSequenceBtn.textContent = originalText;\n   879→        ui.buildSequenceBtn.disabled = false;\n   880→        isOperationInProgress = false;\n   881→      }\n   882→    });\n   883→  }\n   884→}\n   885→\n   886→function selectAllSilences(checked) {\n   887→  selectedSilenceIndices.clear();\n   888→\n   889→  if (checked) {\n   890→    safeSilences.forEach((_, i) => selectedSilenceIndices.add(i));\n   891→  }\n   892→\n   893→  // Batch update DOM (only silence-marker items have checkboxes)\n   894→  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n   895→    const index = parseInt(checkbox.dataset.index);\n   896→    checkbox.checked = checked;\n   897→    const item = checkbox.closest('.preview-item');\n   898→    if (checked) {\n   899→      item?.classList.remove('excluded');\n   900→    } else {\n   901→      item?.classList.add('excluded');\n   902→    }\n   903→  });\n   904→\n   905→  updateSelectionCounts();\n   906→}\n   907→\n   908→function invertSilenceSelection() {\n   909→  const allIndices = new Set(safeSilences.map((_, i) => i));\n   910→  const newSelection = new Set([...allIndices].filter(i => !selectedSilenceIndices.has(i)));\n   911→  selectedSilenceIndices = newSelection;\n   912→\n   913→  // Update DOM\n   914→  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n   915→    const index = parseInt(checkbox.dataset.index);\n   916→    const isSelected = selectedSilenceIndices.has(index);\n   917→    checkbox.checked = isSelected;\n   918→    const item = checkbox.closest('.preview-item');\n   919→    if (isSelected) {\n   920→      item?.classList.remove('excluded');\n   921→    } else {\n   922→      item?.classList.add('excluded');\n   923→    }\n   924→  });\n   925→\n   926→  updateSelectionCounts();\n   927→}\n   928→\n   929→function selectByDuration(minSec, maxSec) {\n   930→  selectedSilenceIndices.clear();\n   931→\n   932→  safeSilences.forEach((silence, index) => {\n   933→    const duration = silence.end - silence.start;\n   934→    if (duration >= minSec && duration < maxSec) {\n   935→      selectedSilenceIndices.add(index);\n   936→    }\n   937→  });\n   938→\n   939→  // Update DOM\n   940→  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n   941→    const index = parseInt(checkbox.dataset.index);\n   942→    const isSelected = selectedSilenceIndices.has(index);\n   943→    checkbox.checked = isSelected;\n   944→    const item = checkbox.closest('.preview-item');\n   945→    if (isSelected) {\n   946→      item?.classList.remove('excluded');\n   947→    } else {\n   948→      item?.classList.add('excluded');\n   949→    }\n   950→  });\n   951→\n   952→  updateSelectionCounts();\n   953→  setStatus(`Selected ${selectedSilenceIndices.size} silences (${minSec}s - ${maxSec}s)`);\n   954→}\n   955→\n   956→async function cancelPreview() {\n   957→  // Keep silences available for Razor if detected\n   958→  const hadSilences = safeSilences.length > 0;\n   959→\n   960→  previewSilences = [];\n   961→  safeSilences = [];\n   962→  protectedSilences = [];\n   963→  previewTakes = [];\n   964→  selectedSilenceIndices.clear();\n   965→  showEmptyState();\n   966→\n   967→  // Clear timeline markers\n   968→  const cleared = await clearSpliceMarkers();\n   969→  const markerInfo = cleared > 0 ? ` (${cleared} markers removed)` : '';\n   970→  setStatus('Preview cancelled' + markerInfo);\n   971→\n   972→  // Show Razor section if silences were detected\n   973→  if (hadSilences && ui.advancedSection) {\n   974→    ui.advancedSection.classList.remove('hidden');\n   975→  }\n   976→}\n   977→\n   978→async function applySelectedSilences() {\n   979→  if (selectedSilenceIndices.size === 0) {\n   980→    setStatus('No silences selected');\n   981→    return;\n   982→  }\n   983→\n   984→  showProgress('Reading marker positions...');\n   985→\n   986→  // Read current marker positions from timeline\n   987→  // This allows users to manually adjust markers before applying\n   988→  const markerSilences = await readMarkersAssilences();\n   989→\n   990→  let silencesToApply;\n   991→  let adjustedCount = 0;\n   992→\n   993→  if (markerSilences.length > 0) {\n   994→    // Use marker positions (may have been adjusted by user)\n   995→    silencesToApply = markerSilences;\n   996→    adjustedCount = markerSilences.length;\n   997→    console.log(`[SPLICE] Using ${adjustedCount} silences from adjusted markers`);\n   998→  } else {\n   999→    // Fallback to original detection data\n  1000→    silencesToApply = safeSilences.filter((_, i) => selectedSilenceIndices.has(i));\n  1001→    console.log(`[SPLICE] Using ${silencesToApply.length} silences from original detection`);\n  1002→  }\n  1003→\n  1004→  if (silencesToApply.length === 0) {\n  1005→    setStatus('No silences to apply');\n  1006→    return;\n  1007→  }\n  1008→\n  1009→  // Update global silences for the apply function\n  1010→  setCurrentSilences(silencesToApply);\n  1011→\n  1012→  showProgress('Applying changes...');\n  1013→\n  1014→  // Apply to timeline\n  1015→  const result = await removeSilencesFromTimeline();\n  1016→\n  1017→  // Clear markers after applying\n  1018→  await clearSpliceMarkers();\n  1019→\n  1020→  // Hide preview\n  1021→  if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n  1022→\n  1023→  // Show results\n  1024→  showSilenceResults({\n  1025→    count: silencesToApply.length,\n  1026→    timeSaved: silencesToApply.reduce((sum, s) => sum + (s.end - s.start), 0),\n  1027→    clipsModified: result\n  1028→  });\n  1029→\n  1030→  // Show advanced section\n  1031→  if (ui.advancedSection) ui.advancedSection.classList.remove('hidden');\n  1032→\n  1033→  const adjustedInfo = adjustedCount > 0 ? ' (from adjusted markers)' : '';\n  1034→  setStatus(`Removed ${silencesToApply.length} silence(s) - ${result} clip(s) modified${adjustedInfo}`);\n  1035→\n  1036→  // Refresh credits (await to prevent race conditions)\n  1037→  try {\n  1038→    await refreshCredits();\n  1039→  } catch (err) {\n  1040→    console.warn('[SPLICE] Credits refresh failed:', err);\n  1041→  }\n  1042→\n  1043→  // Clear preview state\n  1044→  previewSilences = [];\n  1045→  safeSilences = [];\n  1046→  protectedSilences = [];\n  1047→  previewTakes = [];\n  1048→  selectedSilenceIndices.clear();\n  1049→}\n  1050→\n  1051→// ============================================================================\n  1052→// UI HELPERS\n  1053→// ============================================================================\n  1054→function updateApplyButton() {\n  1055→  if (ui.applyPreviewBtn) {\n  1056→    const count = selectedSilenceIndices.size;\n  1057→    ui.applyPreviewBtn.disabled = count === 0;\n  1058→    ui.applyPreviewBtn.textContent = count === 0 ? 'Select silences' : `Apply ${count} Selected`;\n  1059→  }\n  1060→}\n  1061→\n  1062→function updateSelectAllCheckbox() {\n  1063→  if (ui.selectAllSilences) {\n  1064→    ui.selectAllSilences.checked = selectedSilenceIndices.size === safeSilences.length && safeSilences.length > 0;\n  1065→  }\n  1066→}\n  1067→\n  1068→function hideAllViews() {\n  1069→  if (ui.progressContainer) ui.progressContainer.style.display = 'none';\n  1070→  if (ui.resultsEmpty) ui.resultsEmpty.style.display = 'none';\n  1071→  if (ui.silenceResults) ui.silenceResults.style.display = 'none';\n  1072→  if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n  1073→}\n  1074→\n  1075→function showProgress(message) {\n  1076→  hideAllViews();\n  1077→  if (ui.progressContainer) ui.progressContainer.style.display = 'block';\n  1078→  if (ui.progressText) ui.progressText.textContent = message;\n  1079→}\n  1080→\n  1081→function showEmptyState() {\n  1082→  hideAllViews();\n  1083→  if (ui.resultsEmpty) ui.resultsEmpty.style.display = 'block';\n  1084→  if (ui.advancedSection) ui.advancedSection.classList.add('hidden');\n  1085→}\n  1086→\n  1087→function showSilenceResults({ count, timeSaved, clipsModified }) {\n  1088→  hideAllViews();\n  1089→\n  1090→  // Update the results display\n  1091→  if (ui.resultsCount) ui.resultsCount.textContent = count;\n  1092→  if (ui.timeSaved) ui.timeSaved.textContent = timeSaved.toFixed(1) + 's';\n  1093→  if (ui.clipsModified) ui.clipsModified.textContent = clipsModified;\n  1094→\n  1095→  if (ui.silenceResults) ui.silenceResults.style.display = 'block';\n  1096→}\n  1097→\n  1098→function formatTime(seconds) {\n  1099→  if (typeof seconds !== 'number' || isNaN(seconds)) return '0:00';\n  1100→  const mins = Math.floor(seconds / 60);\n  1101→  const secs = Math.floor(seconds % 60);\n  1102→  return `${mins}:${secs.toString().padStart(2, '0')}`;\n  1103→}\n  1104→\n  1105→/**\n  1106→ * Format build errors into user-friendly messages\n  1107→ * @param {string} error - Raw error message\n  1108→ * @returns {string} User-friendly error message\n  1109→ */\n  1110→function formatBuildError(error) {\n  1111→  if (!error) return 'Unknown error occurred';\n  1112→\n  1113→  // Map technical errors to user-friendly messages\n  1114→  const errorMap = {\n  1115→    'Builder not initialized': 'Builder module failed to load. Try restarting the plugin.',\n  1116→    'No active project': 'No project is open. Please open or create a project first.',\n  1117→    'No media items found': 'No media files in project. Import media before building.',\n  1118→    'No detection data': 'Run analysis first (click GO button).',\n  1119→    'Failed to generate cut list': 'Server error generating cuts. Check your connection.',\n  1120→    'Failed to build sequence': 'Premiere Pro sequence build failed. Try again.',\n  1121→    'fetch failed': 'Network error. Check if backend server is running.',\n  1122→    'NetworkError': 'Cannot connect to server. Check your internet connection.',\n  1123→    'Insufficient credits': 'Out of credits. Upgrade your plan to continue.',\n  1124→    'Authentication required': 'Login required. Click the credits badge to log in.'\n  1125→  };\n  1126→\n  1127→  // Check for matching error patterns\n  1128→  for (const [pattern, friendly] of Object.entries(errorMap)) {\n  1129→    if (error.includes(pattern)) {\n  1130→      return friendly;\n  1131→    }\n  1132→  }\n  1133→\n  1134→  // Return original if no match (already reasonably clear)\n  1135→  return error;\n  1136→}\n  1137→\n  1138→function setStatus(message) {\n  1139→  if (ui.status) ui.status.textContent = message;\n  1140→}\n  1141→\n  1142→// ============================================================================\n  1143→// UNDO HANDLERS\n  1144→// ============================================================================\n  1145→function initUndoHandlers() {\n  1146→  if (ui.undoBtn) {\n  1147→    ui.undoBtn.addEventListener('click', () => {\n  1148→      try {\n  1149→        ppro.app.executeCommand('Edit.Undo');\n  1150→        showEmptyState();\n  1151→        setStatus('Changes undone');\n  1152→      } catch (e) {\n  1153→        setStatus('Undo failed - use Cmd+Z');\n  1154→      }\n  1155→    });\n  1156→  }\n  1157→}\n  1158→\n  1159→// ============================================================================\n  1160→// MEDIA FOLDER HANDLERS (v3.5)\n  1161→// ============================================================================\n  1162→function initMediaFolderHandlers() {\n  1163→  // Update display on load\n  1164→  updateMediaFolderDisplay();\n  1165→\n  1166→  // Set media folder button\n  1167→  if (ui.setMediaFolderBtn) {\n  1168→    ui.setMediaFolderBtn.addEventListener('click', async () => {\n  1169→      ui.setMediaFolderBtn.disabled = true;\n  1170→      ui.setMediaFolderBtn.textContent = 'Selecting...';\n  1171→\n  1172→      try {\n  1173→        const result = await setupMediaFolder();\n  1174→        if (result.success) {\n  1175→          updateMediaFolderDisplay();\n  1176→          setStatus(`Media folder set: ${result.path}`);\n  1177→        } else {\n  1178→          setStatus(result.error || 'Failed to set media folder');\n  1179→        }\n  1180→      } catch (err) {\n  1181→        setStatus('Error: ' + err.message);\n  1182→      } finally {\n  1183→        ui.setMediaFolderBtn.disabled = false;\n  1184→        ui.setMediaFolderBtn.textContent = 'Set Folder';\n  1185→      }\n  1186→    });\n  1187→  }\n  1188→\n  1189→  // Clear media folder button\n  1190→  if (ui.clearMediaFolderBtn) {\n  1191→    ui.clearMediaFolderBtn.addEventListener('click', () => {\n  1192→      clearMediaFolder();\n  1193→      updateMediaFolderDisplay();\n  1194→      setStatus('Media folder cleared');\n  1195→    });\n  1196→  }\n  1197→}\n  1198→\n  1199→function updateMediaFolderDisplay() {\n  1200→  if (ui.mediaFolderDisplay) {\n  1201→    const path = getMediaFolderPath();\n  1202→    if (path) {\n  1203→      // Truncate long paths for display\n  1204→      const displayPath = path.length > 40\n  1205→        ? '...' + path.slice(-37)\n  1206→        : path;\n  1207→      ui.mediaFolderDisplay.textContent = displayPath;\n  1208→      ui.mediaFolderDisplay.title = path; // Full path on hover\n  1209→      ui.mediaFolderDisplay.style.color = '#4a9eff';\n  1210→    } else {\n  1211→      ui.mediaFolderDisplay.textContent = 'Not configured';\n  1212→      ui.mediaFolderDisplay.title = '';\n  1213→      ui.mediaFolderDisplay.style.color = '#888';\n  1214→    }\n  1215→  }\n  1216→}\n  1217→\n  1218→// ============================================================================\n  1219→// BUILD SEQUENCE (v3.5 - Direct DOM Reconstruction)\n  1220→// ============================================================================\n  1221→\n  1222→// 1. ROBUST CONSTANTS (Fixes 'undefined' error)\n  1223→// We define these integers explicitly because UXP API versions vary on where they store Enums.\n  1224→const ProjectItemType = {\n  1225→    CLIP: 1,\n  1226→    BIN: 2,\n  1227→    ROOT: 3,\n  1228→    FILE: 4\n  1229→};\n  1230→\n  1231→/**\n  1232→ * Find the first media item in the project bin (recursively)\n  1233→ * @param {Object} item - ProjectItem (usually rootItem)\n  1234→ * @returns {Promise<Object|null>} First media item found or null\n  1235→ */\n  1236→async function findFirstMediaItem(item) {\n  1237→  if (!item) return null;\n  1238→\n  1239→  // Fix: Use local Constant instead of app.Constants.ProjectItemType\n  1240→  if (item.type === ProjectItemType.CLIP) {\n  1241→    return item;\n  1242→  }\n  1243→\n  1244→  // Search Children (Bin or Root)\n  1245→  // Note: item.children is a collection, check numItems\n  1246→  if (item.children && item.children.numItems > 0) {\n  1247→    for (let i = 0; i < item.children.numItems; i++) {\n  1248→      const child = item.children[i];\n  1249→      const found = await findFirstMediaItem(child);\n  1250→      if (found) return found;\n  1251→    }\n  1252→  }\n  1253→\n  1254→  return null;\n  1255→}\n  1256→\n  1257→/**\n  1258→ * Build a sequence from detection data locally\n  1259→ * Used for testing via console: await buildSequenceV35(silences)\n  1260→ */\n  1261→async function buildSequenceV35(silences, takes = []) {\n  1262→    if (!window.spliceBuilder) {\n  1263→        throw new Error('Builder not initialized. Check if builder.js is loaded.');\n  1264→    }\n  1265→\n  1266→    const app = require('premierepro');\n  1267→    const project = await app.Project.getActiveProject();\n  1268→    if (!project) throw new Error(\"No active project found.\");\n  1269→\n  1270→    const rootItem = await project.getRootItem();\n  1271→    const sourceProjectItem = await findFirstMediaItem(rootItem);\n  1272→\n  1273→    if (!sourceProjectItem) {\n  1274→        throw new Error('No media items found in project bin.');\n  1275→    }\n  1276→\n  1277→    console.log(`[SPLICE] Found source media: ${sourceProjectItem.name}`);\n  1278→\n  1279→    return await window.spliceBuilder.buildSequenceFromDetection(silences, takes, sourceProjectItem);\n  1280→}\n  1281→\n  1282→/**\n  1283→ * Request a cut list from the backend and build the sequence.\n  1284→ * Full v3.5 workflow.\n  1285→ */\n  1286→async function buildSequenceWithCutList() {\n  1287→  if (!window.spliceBuilder) {\n  1288→    throw new Error('Builder not initialized');\n  1289→  }\n  1290→\n  1291→  // Ensure BACKEND_URL is available\n  1292→  const apiUrl = (typeof BACKEND_URL !== 'undefined') ? BACKEND_URL : 'http://localhost:3000';\n  1293→\n  1294→  if (typeof showProgress === 'function') showProgress('Getting source clip info...');\n  1295→\n  1296→  const app = require('premierepro');\n  1297→  const project = await app.Project.getActiveProject();\n  1298→  if (!project) throw new Error(\"No active project.\");\n  1299→\n  1300→  const rootItem = await project.getRootItem();\n  1301→  const sourceProjectItem = await findFirstMediaItem(rootItem);\n  1302→\n  1303→  if (!sourceProjectItem) {\n  1304→    throw new Error('No media items found in project. Please import a media file first.');\n  1305→  }\n  1306→\n  1307→  // Use properties strictly\n  1308→  const sourceName = sourceProjectItem.name;\n  1309→  const sourcePath = sourceProjectItem.treePath;\n  1310→\n  1311→  const durationObj = await sourceProjectItem.getDuration();\n  1312→  const duration = durationObj ? durationObj.seconds : 0;\n  1313→\n  1314→  // Retrieve Globals\n  1315→  const silences = window.splicePreviewState?.getSelectedSilences() || (typeof safeSilences !== 'undefined' ? safeSilences : []);\n  1316→  const takes = (typeof previewTakes !== 'undefined' ? previewTakes : []);\n  1317→\n  1318→  if (silences.length === 0 && (!takes || takes.length === 0)) {\n  1319→     throw new Error('No detection data found. Run analysis first.');\n  1320→  }\n  1321→\n  1322→  if (typeof showProgress === 'function') showProgress('Generating cut list...');\n  1323→\n  1324→  const response = await fetch(`${apiUrl}/cut-list`, {\n  1325→    method: 'POST',\n  1326→    headers: getAuthHeaders(),\n  1327→    body: JSON.stringify({\n  1328→      sourceName,\n  1329→      sourcePath,\n  1330→      duration,\n  1331→      silences,\n  1332→      takes\n  1333→    })\n  1334→  });\n  1335→\n  1336→  if (!response.ok) {\n  1337→    const err = await response.json();\n  1338→    throw new Error(err.error || 'Failed to generate cut list');\n  1339→  }\n  1340→\n  1341→  const data = await response.json();\n  1342→  const cutList = data.cutList;\n  1343→\n  1344→  if (typeof showProgress === 'function') showProgress('Building sequence...');\n  1345→\n  1346→  const result = await window.spliceBuilder.buildSequenceFromCutList(cutList, sourceProjectItem);\n  1347→\n  1348→  if (!result.success) {\n  1349→    throw new Error(result.error || 'Failed to build sequence');\n  1350→  }\n  1351→\n  1352→  return {\n  1353→    success: true,\n  1354→    clipsAdded: result.clipsAdded,\n  1355→    metadata: cutList.metadata\n  1356→  };\n  1357→}\n  1358→\n  1359→// Expose functions globally for UI and testing\n  1360→if (typeof window !== 'undefined') {\n  1361→    window.buildSequenceV35 = buildSequenceV35;\n  1362→    window.buildSequenceWithCutList = buildSequenceWithCutList;\n  1363→    window.findFirstMediaItem = findFirstMediaItem;\n  1364→}\n  1365→\n  1366→// ============================================================================\n  1367→// TIMELINE SEEK (with proper error handling)\n  1368→// ============================================================================\n  1369→async function seekToTime(seconds) {\n  1370→  // Validate input\n  1371→  if (typeof seconds !== 'number' || isNaN(seconds) || !isFinite(seconds)) {\n  1372→    setStatus('Invalid seek time');\n  1373→    return;\n  1374→  }\n  1375→\n  1376→  if (seconds < 0) {\n  1377→    seconds = 0;\n  1378→  }\n  1379→\n  1380→  try {\n  1381→    const context = await getActiveSequence();\n  1382→    if (!context) {\n  1383→      setStatus('No sequence available');\n  1384→      return;\n  1385→    }\n  1386→\n  1387→    const { sequence } = context;\n  1388→\n  1389→    // Use TickTime API to convert seconds to the format needed\n  1390→    const position = ppro.TickTime.createWithSeconds(seconds);\n  1391→\n  1392→    await sequence.setPlayerPosition(position);\n  1393→    setStatus(`Seeked to ${formatTime(seconds)}`);\n  1394→  } catch (err) {\n  1395→    console.error('[SPLICE] Seek error:', err);\n  1396→    setStatus('Seek failed: ' + (err.message || 'Unknown error'));\n  1397→  }\n  1398→}\n  1399→\n  1400→// ============================================================================\n  1401→// AUTH HEADERS HELPER\n  1402→// ============================================================================\n  1403→/**\n  1404→ * Build request headers with authentication\n  1405→ * @returns {Object} Headers object with Content-Type and x-stripe-customer-id\n  1406→ */\n  1407→function getAuthHeaders() {\n  1408→  const headers = { 'Content-Type': 'application/json' };\n  1409→  const settings = getSettings();\n  1410→  if (settings.customerId) {\n  1411→    headers['x-stripe-customer-id'] = settings.customerId;\n  1412→  }\n  1413→  return headers;\n  1414→}\n  1415→\n  1416→// ============================================================================\n  1417→// AUDIO EXPORT & BACKEND CALLS\n  1418→// ============================================================================\n  1419→async function exportAudioInternal() {\n  1420→  try {\n  1421→    const context = await getActiveSequence();\n  1422→    if (!context) {\n  1423→      throw new Error('No project or sequence open');\n  1424→    }\n  1425→\n  1426→    const { sequence } = context;\n  1427→    const encoderManager = ppro.EncoderManager.getManager();\n  1428→    if (!encoderManager) {\n  1429→      throw new Error('EncoderManager not available');\n  1430→    }\n  1431→\n  1432→    const result = await encoderManager.exportSequence(\n  1433→      sequence,\n  1434→      ppro.EncoderManager.EXPORT_IMMEDIATELY,\n  1435→      WAV_PATH,\n  1436→      WAV_PRESET_PATH,\n  1437→      true\n  1438→    );\n  1439→\n  1440→    return result;\n  1441→  } catch (err) {\n  1442→    console.error('[SPLICE] Export error:', err);\n  1443→    return false;\n  1444→  }\n  1445→}\n  1446→\n  1447→async function isolateVocals() {\n  1448→  const response = await fetch(`${BACKEND_URL}/isolate-vocals`, {\n  1449→    method: 'POST',\n  1450→    headers: getAuthHeaders(),\n  1451→    body: JSON.stringify({ audioPath: WAV_PATH })\n  1452→  });\n  1453→\n  1454→  if (!response.ok) {\n  1455→    const err = await response.json();\n  1456→    return { success: false, error: err.error || 'Isolation request failed' };\n  1457→  }\n  1458→\n  1459→  return await response.json();\n  1460→}\n  1461→\n  1462→async function detectSilences(audioPath, params) {\n  1463→  const response = await fetch(`${BACKEND_URL}/silences-audio`, {\n  1464→    method: 'POST',\n  1465→    headers: getAuthHeaders(),\n  1466→    body: JSON.stringify({\n  1467→      wavPath: audioPath,\n  1468→      threshold: params.dbThreshold,\n  1469→      minDuration: params.minDuration,\n  1470→      padding: params.padding\n  1471→    })\n  1472→  });\n  1473→\n  1474→  if (!response.ok) {\n  1475→    const err = await response.json();\n  1476→    throw new Error(err.error || 'Silence detection failed');\n  1477→  }\n  1478→\n  1479→  const data = await response.json();\n  1480→  if (!data.success) {\n  1481→    throw new Error(data.error || 'Silence detection failed');\n  1482→  }\n  1483→\n  1484→  // Store silences globally for razor workflow\n  1485→  currentSilences = data.silences;\n  1486→\n  1487→  return data.silences;\n  1488→}\n  1489→\n  1490→async function transcribeAudio() {\n  1491→  const response = await fetch(`${BACKEND_URL}/analyze`, {\n  1492→    method: 'POST',\n  1493→    headers: getAuthHeaders(),\n  1494→    body: JSON.stringify({ wavPath: WAV_PATH })\n  1495→  });\n  1496→\n  1497→  if (!response.ok) {\n  1498→    const err = await response.json();\n  1499→    return { success: false, error: err.error || 'Transcription failed' };\n  1500→  }\n  1501→\n  1502→  const data = await response.json();\n  1503→  return {\n  1504→    success: data.success,\n  1505→    transcript: data.transcript,\n  1506→    takes: data.takes,\n  1507→    error: data.error\n  1508→  };\n  1509→}\n  1510→\n  1511→// ============================================================================\n  1512→// BATCH PROCESSING\n  1513→// ============================================================================\n  1514→let batchQueue = [];\n  1515→let currentBatchJobId = null;\n  1516→\n  1517→/**\n  1518→ * Initialize batch processing handlers\n  1519→ */\n  1520→function initBatchHandlers() {\n  1521→  const addToBatchBtn = document.getElementById('addToBatchBtn');\n  1522→  const startBatchBtn = document.getElementById('startBatchBtn');\n  1523→  const clearBatchBtn = document.getElementById('clearBatchBtn');\n  1524→  const batchSection = document.getElementById('batchSection');\n  1525→\n  1526→  // Show batch section in settings or after first detection\n  1527→  if (batchSection) {\n  1528→    batchSection.classList.remove('hidden');\n  1529→  }\n  1530→\n  1531→  // Add files to batch queue\n  1532→  if (addToBatchBtn) {\n  1533→    addToBatchBtn.addEventListener('click', async () => {\n  1534→      try {\n  1535→        const uxpFs = require('uxp').storage.localFileSystem;\n  1536→        const files = await uxpFs.getFileForOpening({\n  1537→          allowMultiple: true,\n  1538→          types: ['wav', 'mp3', 'mp4', 'mov', 'aac', 'm4a']\n  1539→        });\n  1540→\n  1541→        if (files && files.length > 0) {\n  1542→          for (const file of files) {\n  1543→            const path = file.nativePath || file.name;\n  1544→            if (!batchQueue.includes(path)) {\n  1545→              batchQueue.push(path);\n  1546→            }\n  1547→          }\n  1548→          updateBatchQueueDisplay();\n  1549→          setStatus(`Added ${files.length} file(s) to batch queue`);\n  1550→        }\n  1551→      } catch (err) {\n  1552→        setStatus('Error adding files: ' + err.message);\n  1553→      }\n  1554→    });\n  1555→  }\n  1556→\n  1557→  // Start batch processing\n  1558→  if (startBatchBtn) {\n  1559→    startBatchBtn.addEventListener('click', async () => {\n  1560→      if (batchQueue.length === 0) {\n  1561→        setStatus('No files in batch queue');\n  1562→        return;\n  1563→      }\n  1564→\n  1565→      startBatchBtn.disabled = true;\n  1566→      addToBatchBtn.disabled = true;\n  1567→\n  1568→      try {\n  1569→        await runBatchProcessing();\n  1570→      } finally {\n  1571→        startBatchBtn.disabled = false;\n  1572→        addToBatchBtn.disabled = false;\n  1573→      }\n  1574→    });\n  1575→  }\n  1576→\n  1577→  // Clear batch queue\n  1578→  if (clearBatchBtn) {\n  1579→    clearBatchBtn.addEventListener('click', () => {\n  1580→      batchQueue = [];\n  1581→      currentBatchJobId = null;\n  1582→      updateBatchQueueDisplay();\n  1583→      setStatus('Batch queue cleared');\n  1584→    });\n  1585→  }\n  1586→}\n  1587→\n  1588→/**\n  1589→ * Update batch queue display\n  1590→ */\n  1591→function updateBatchQueueDisplay() {\n  1592→  const batchEmpty = document.getElementById('batchEmpty');\n  1593→  const batchList = document.getElementById('batchList');\n  1594→  const startBatchBtn = document.getElementById('startBatchBtn');\n  1595→\n  1596→  if (batchQueue.length === 0) {\n  1597→    if (batchEmpty) batchEmpty.style.display = 'block';\n  1598→    if (batchList) batchList.innerHTML = '';\n  1599→    if (startBatchBtn) startBatchBtn.disabled = true;\n  1600→    return;\n  1601→  }\n  1602→\n  1603→  if (batchEmpty) batchEmpty.style.display = 'none';\n  1604→  if (startBatchBtn) startBatchBtn.disabled = false;\n  1605→\n  1606→  if (batchList) {\n  1607→    batchList.innerHTML = batchQueue.map((path, i) => {\n  1608→      const fileName = path.split('/').pop();\n  1609→      return `<div style=\"display: flex; justify-content: space-between; align-items: center; padding: 4px 6px; background: #2a2a2a; border-radius: 3px; margin-bottom: 2px; font-size: 9px;\">\n  1610→        <span style=\"overflow: hidden; text-overflow: ellipsis; white-space: nowrap; max-width: 180px;\" title=\"${path}\">${fileName}</span>\n  1611→        <button onclick=\"removeBatchItem(${i})\" style=\"background: transparent; border: none; color: #888; cursor: pointer; font-size: 10px;\">x</button>\n  1612→      </div>`;\n  1613→    }).join('');\n  1614→  }\n  1615→}\n  1616→\n  1617→/**\n  1618→ * Remove item from batch queue\n  1619→ */\n  1620→function removeBatchItem(index) {\n  1621→  batchQueue.splice(index, 1);\n  1622→  updateBatchQueueDisplay();\n  1623→}\n  1624→\n  1625→/**\n  1626→ * Run batch processing\n  1627→ */\n  1628→async function runBatchProcessing() {\n  1629→  const batchProgress = document.getElementById('batchProgress');\n  1630→  const batchProgressBar = document.getElementById('batchProgressBar');\n  1631→  const batchProgressText = document.getElementById('batchProgressText');\n  1632→\n  1633→  if (batchProgress) batchProgress.style.display = 'block';\n  1634→\n  1635→  try {\n  1636→    // Get settings\n  1637→    const sensitivity = parseInt(document.getElementById('sensitivitySlider')?.value || 50);\n  1638→    const params = mapSensitivity(sensitivity);\n  1639→\n  1640→    // Call batch endpoint\n  1641→    const response = await fetch(`${BACKEND_URL}/batch/silences`, {\n  1642→      method: 'POST',\n  1643→      headers: getAuthHeaders(),\n  1644→      body: JSON.stringify({\n  1645→        files: batchQueue,\n  1646→        options: {\n  1647→          sensitivity,\n  1648→          threshold: params.dbThreshold,\n  1649→          minSilenceLength: params.minDuration,\n  1650→          paddingStart: params.padding,\n  1651→          paddingEnd: params.padding / 2\n  1652→        }\n  1653→      })\n  1654→    });\n  1655→\n  1656→    if (!response.ok) {\n  1657→      const err = await response.json();\n  1658→      throw new Error(err.error || 'Batch request failed');\n  1659→    }\n  1660→\n  1661→    const data = await response.json();\n  1662→    currentBatchJobId = data.jobId;\n  1663→    setStatus(`Batch job started: ${data.jobId}`);\n  1664→\n  1665→    // Poll for status\n  1666→    await pollBatchStatus(currentBatchJobId, batchProgressBar, batchProgressText);\n  1667→\n  1668→  } catch (err) {\n  1669→    setStatus('Batch error: ' + err.message);\n  1670→  } finally {\n  1671→    if (batchProgress) batchProgress.style.display = 'none';\n  1672→  }\n  1673→}\n  1674→\n  1675→/**\n  1676→ * Poll batch job status\n  1677→ */\n  1678→async function pollBatchStatus(jobId, progressBar, progressText) {\n  1679→  const pollInterval = 2000; // 2 seconds\n  1680→  const maxPolls = 300; // 10 minutes max\n  1681→  let polls = 0;\n  1682→\n  1683→  while (polls < maxPolls) {\n  1684→    try {\n  1685→      const response = await fetch(`${BACKEND_URL}/batch/status/${jobId}`, {\n  1686→        headers: getAuthHeaders()\n  1687→      });\n  1688→\n  1689→      if (!response.ok) {\n  1690→        throw new Error('Failed to get batch status');\n  1691→      }\n  1692→\n  1693→      const data = await response.json();\n  1694→      const job = data.job;\n  1695→\n  1696→      // Update progress\n  1697→      if (progressBar) progressBar.value = job.progress.percentage;\n  1698→      if (progressText) progressText.textContent = `${job.progress.completed + job.progress.failed} / ${job.progress.total} files`;\n  1699→\n  1700→      // Check if complete\n  1701→      if (job.status !== 'processing') {\n  1702→        if (job.status === 'completed') {\n  1703→          setStatus(`Batch complete: ${job.progress.completed} files processed`);\n  1704→        } else if (job.status === 'completed_with_errors') {\n  1705→          setStatus(`Batch complete with ${job.progress.failed} error(s)`);\n  1706→        } else {\n  1707→          setStatus('Batch failed');\n  1708→        }\n  1709→\n  1710→        // Clear queue on completion\n  1711→        batchQueue = [];\n  1712→        updateBatchQueueDisplay();\n  1713→        return;\n  1714→      }\n  1715→\n  1716→      await new Promise(resolve => setTimeout(resolve, pollInterval));\n  1717→      polls++;\n  1718→    } catch (err) {\n  1719→      setStatus('Polling error: ' + err.message);\n  1720→      return;\n  1721→    }\n  1722→  }\n  1723→\n  1724→  setStatus('Batch timed out');\n  1725→}\n  1726→\n  1727→// Expose for inline onclick\n  1728→if (typeof window !== 'undefined') {\n  1729→  window.removeBatchItem = removeBatchItem;\n  1730→}\n  1731→\n  1732→// ============================================================================\n  1733→// CUSTOM PRESETS UI (v3.5)\n  1734→// ============================================================================\n  1735→\n  1736→/**\n  1737→ * Icon mapping for display\n  1738→ */\n  1739→const PRESET_ICON_MAP = {\n  1740→  'settings': '*',\n  1741→  'mic': 'M',\n  1742→  'people': 'P',\n  1743→  'bolt': '!',\n  1744→  'school': 'E',\n  1745→  'videocam': 'V',\n  1746→  'star': 'S',\n  1747→  'heart': 'H'\n  1748→};\n  1749→\n  1750→/**\n  1751→ * Initialize custom presets UI handlers\n  1752→ */\n  1753→function initCustomPresetsUI() {\n  1754→  // Refresh preset dropdown on load\n  1755→  refreshPresetDropdown();\n  1756→\n  1757→  // Save Preset button (+)\n  1758→  if (ui.savePresetBtn) {\n  1759→    ui.savePresetBtn.addEventListener('click', () => {\n  1760→      showPresetModal();\n  1761→    });\n  1762→  }\n  1763→\n  1764→  // Manage Presets button (...)\n  1765→  if (ui.managePresetsBtn) {\n  1766→    ui.managePresetsBtn.addEventListener('click', () => {\n  1767→      showManagePresetsModal();\n  1768→    });\n  1769→  }\n  1770→\n  1771→  // Close Preset Modal\n  1772→  if (ui.closePresetModalBtn) {\n  1773→    ui.closePresetModalBtn.addEventListener('click', () => {\n  1774→      hidePresetModal();\n  1775→    });\n  1776→  }\n  1777→\n  1778→  // Close on backdrop click\n  1779→  if (ui.presetModal) {\n  1780→    ui.presetModal.addEventListener('click', (e) => {\n  1781→      if (e.target === ui.presetModal) {\n  1782→        hidePresetModal();\n  1783→      }\n  1784→    });\n  1785→  }\n  1786→\n  1787→  // Icon picker\n  1788→  if (ui.presetIconPicker) {\n  1789→    ui.presetIconPicker.addEventListener('click', (e) => {\n  1790→      const btn = e.target.closest('.preset-icon-option');\n  1791→      if (btn) {\n  1792→        selectPresetIcon(btn.dataset.icon);\n  1793→      }\n  1794→    });\n  1795→  }\n  1796→\n  1797→  // Sensitivity slider value display (Phase 3)\n  1798→  if (ui.presetSensitivity && ui.presetSensitivityValue) {\n  1799→    ui.presetSensitivity.addEventListener('input', () => {\n  1800→      ui.presetSensitivityValue.textContent = ui.presetSensitivity.value;\n  1801→    });\n  1802→  }\n  1803→\n  1804→  // Save Preset Confirm\n  1805→  if (ui.savePresetConfirmBtn) {\n  1806→    ui.savePresetConfirmBtn.addEventListener('click', () => {\n  1807→      savePresetFromModal();\n  1808→    });\n  1809→  }\n  1810→\n  1811→  // Close Manage Presets Modal\n  1812→  if (ui.closeManagePresetsBtn) {\n  1813→    ui.closeManagePresetsBtn.addEventListener('click', () => {\n  1814→      hideManagePresetsModal();\n  1815→    });\n  1816→  }\n  1817→\n  1818→  // Close on backdrop click\n  1819→  if (ui.managePresetsModal) {\n  1820→    ui.managePresetsModal.addEventListener('click', (e) => {\n  1821→      if (e.target === ui.managePresetsModal) {\n  1822→        hideManagePresetsModal();\n  1823→      }\n  1824→    });\n  1825→  }\n  1826→\n  1827→  // Event delegation for preset list actions\n  1828→  if (ui.presetsList) {\n  1829→    ui.presetsList.addEventListener('click', (e) => {\n  1830→      const editBtn = e.target.closest('.preset-edit-btn');\n  1831→      const deleteBtn = e.target.closest('.preset-delete-btn');\n  1832→      const duplicateBtn = e.target.closest('.preset-duplicate-btn');\n  1833→\n  1834→      if (editBtn) {\n  1835→        const presetId = editBtn.dataset.presetId;\n  1836→        editPreset(presetId);\n  1837→      }\n  1838→\n  1839→      if (deleteBtn) {\n  1840→        const presetId = deleteBtn.dataset.presetId;\n  1841→        const presetName = deleteBtn.dataset.presetName;\n  1842→        deletePresetWithConfirm(presetId, presetName);\n  1843→      }\n  1844→\n  1845→      if (duplicateBtn) {\n  1846→        const presetId = duplicateBtn.dataset.presetId;\n  1847→        const presetName = duplicateBtn.dataset.presetName;\n  1848→        duplicatePresetFromUI(presetId, presetName);\n  1849→      }\n  1850→    });\n  1851→  }\n  1852→\n  1853→  // Export presets button (Phase 3)\n  1854→  if (ui.exportPresetsBtn) {\n  1855→    ui.exportPresetsBtn.addEventListener('click', () => {\n  1856→      exportPresetsToFile();\n  1857→    });\n  1858→  }\n  1859→\n  1860→  // Import presets button (Phase 3)\n  1861→  if (ui.importPresetsBtn && ui.importPresetsFile) {\n  1862→    ui.importPresetsBtn.addEventListener('click', () => {\n  1863→      ui.importPresetsFile.click();\n  1864→    });\n  1865→\n  1866→    ui.importPresetsFile.addEventListener('change', (e) => {\n  1867→      if (e.target.files && e.target.files[0]) {\n  1868→        importPresetsFromFile(e.target.files[0]);\n  1869→        e.target.value = ''; // Reset for next import\n  1870→      }\n  1871→    });\n  1872→  }\n  1873→}\n  1874→\n  1875→/**\n  1876→ * Refresh the preset dropdown with all presets (built-in + custom)\n  1877→ */\n  1878→function refreshPresetDropdown() {\n  1879→  if (!ui.presetSelector) return;\n  1880→\n  1881→  const presets = getAllPresets();\n  1882→  const currentValue = ui.presetSelector.value;\n  1883→\n  1884→  // Clear existing options\n  1885→  ui.presetSelector.innerHTML = '';\n  1886→\n  1887→  // Add all presets\n  1888→  presets.forEach(preset => {\n  1889→    const option = document.createElement('option');\n  1890→    option.value = preset.id;\n  1891→\n  1892→    // Format: \"Name - Description\" or just \"Name\"\n  1893→    const desc = preset.description ? ` - ${preset.description}` : '';\n  1894→    option.textContent = preset.name + desc;\n  1895→\n  1896→    // Add custom marker for custom presets\n  1897→    if (!preset.isBuiltIn) {\n  1898→      option.textContent = `${preset.name}${desc}`;\n  1899→      option.style.color = '#28a745';\n  1900→    }\n  1901→\n  1902→    ui.presetSelector.appendChild(option);\n  1903→  });\n  1904→\n  1905→  // Restore selection if it still exists\n  1906→  if ([...ui.presetSelector.options].some(opt => opt.value === currentValue)) {\n  1907→    ui.presetSelector.value = currentValue;\n  1908→  } else {\n  1909→    ui.presetSelector.value = 'custom';\n  1910→  }\n  1911→}\n  1912→\n  1913→/**\n  1914→ * Show the preset modal for creating a new preset\n  1915→ */\n  1916→function showPresetModal(editId = null) {\n  1917→  if (!ui.presetModal) return;\n  1918→\n  1919→  // Reset form\n  1920→  ui.presetNameInput.value = '';\n  1921→  ui.presetDescInput.value = '';\n  1922→  ui.presetEditId.value = '';\n  1923→  ui.presetSelectedIcon.value = 'settings';\n  1924→  selectPresetIcon('settings');\n  1925→  hidePresetError();\n  1926→\n  1927→  // Get current settings as defaults\n  1928→  const currentSettings = getSettings();\n  1929→  const defaultSensitivity = currentSettings.sensitivity || 50;\n  1930→  const defaultThreshold = currentSettings.threshold || -32;\n  1931→\n  1932→  if (editId) {\n  1933→    // Edit mode\n  1934→    const preset = getPresetById(editId);\n  1935→    if (preset && !preset.isBuiltIn) {\n  1936→      ui.presetModalTitle.textContent = 'Edit Preset';\n  1937→      ui.presetNameInput.value = preset.name;\n  1938→      ui.presetDescInput.value = preset.description || '';\n  1939→      ui.presetEditId.value = editId;\n  1940→      ui.presetSelectedIcon.value = preset.icon || 'settings';\n  1941→      selectPresetIcon(preset.icon || 'settings');\n  1942→      ui.savePresetConfirmBtn.textContent = 'Update Preset';\n  1943→\n  1944→      // Populate settings from preset (Phase 3)\n  1945→      const presetSensitivity = preset.settings?.sensitivity ?? defaultSensitivity;\n  1946→      const presetThreshold = preset.settings?.threshold ?? defaultThreshold;\n  1947→      if (ui.presetSensitivity) {\n  1948→        ui.presetSensitivity.value = presetSensitivity;\n  1949→        if (ui.presetSensitivityValue) ui.presetSensitivityValue.textContent = presetSensitivity;\n  1950→      }\n  1951→      if (ui.presetThreshold) {\n  1952→        ui.presetThreshold.value = presetThreshold;\n  1953→      }\n  1954→    }\n  1955→  } else {\n  1956→    // Create mode - use current settings from main UI\n  1957→    ui.presetModalTitle.textContent = 'Save Preset';\n  1958→    ui.savePresetConfirmBtn.textContent = 'Save Preset';\n  1959→\n  1960→    // Get sensitivity from main slider if available\n  1961→    const mainSensitivity = ui.sensitivitySlider?.value ?? defaultSensitivity;\n  1962→    if (ui.presetSensitivity) {\n  1963→      ui.presetSensitivity.value = mainSensitivity;\n  1964→      if (ui.presetSensitivityValue) ui.presetSensitivityValue.textContent = mainSensitivity;\n  1965→    }\n  1966→    if (ui.presetThreshold) {\n  1967→      ui.presetThreshold.value = defaultThreshold;\n  1968→    }\n  1969→  }\n  1970→\n  1971→  ui.presetModal.classList.remove('hidden');\n  1972→  ui.presetNameInput.focus();\n  1973→}\n  1974→\n  1975→/**\n  1976→ * Hide the preset modal\n  1977→ */\n  1978→function hidePresetModal() {\n  1979→  if (ui.presetModal) {\n  1980→    ui.presetModal.classList.add('hidden');\n  1981→  }\n  1982→}\n  1983→\n  1984→/**\n  1985→ * Select an icon in the preset modal\n  1986→ */\n  1987→function selectPresetIcon(iconName) {\n  1988→  if (!ui.presetIconPicker || !ui.presetSelectedIcon) return;\n  1989→\n  1990→  // Update hidden input\n  1991→  ui.presetSelectedIcon.value = iconName;\n  1992→\n  1993→  // Update visual selection\n  1994→  ui.presetIconPicker.querySelectorAll('.preset-icon-option').forEach(btn => {\n  1995→    if (btn.dataset.icon === iconName) {\n  1996→      btn.classList.add('selected');\n  1997→    } else {\n  1998→      btn.classList.remove('selected');\n  1999→    }\n  2000→  });\n  2001→}\n  2002→\n  2003→/**\n  2004→ * Show error message in preset modal\n  2005→ */\n  2006→function showPresetError(message) {\n  2007→  if (ui.presetModalError) {\n  2008→    ui.presetModalError.textContent = message;\n  2009→    ui.presetModalError.style.display = 'block';\n  2010→  }\n  2011→}\n  2012→\n  2013→/**\n  2014→ * Hide error message in preset modal\n  2015→ */\n  2016→function hidePresetError() {\n  2017→  if (ui.presetModalError) {\n  2018→    ui.presetModalError.style.display = 'none';\n  2019→  }\n  2020→}\n  2021→\n  2022→/**\n  2023→ * Save preset from modal (create or update)\n  2024→ */\n  2025→function savePresetFromModal() {\n  2026→  const name = ui.presetNameInput?.value?.trim();\n  2027→  const description = ui.presetDescInput?.value?.trim() || '';\n  2028→  const icon = ui.presetSelectedIcon?.value || 'settings';\n  2029→  const editId = ui.presetEditId?.value;\n  2030→\n  2031→  if (!name) {\n  2032→    showPresetError('Please enter a preset name');\n  2033→    return;\n  2034→  }\n  2035→\n  2036→  // Capture settings from modal inputs (Phase 3)\n  2037→  const sensitivityRaw = parseInt(ui.presetSensitivity?.value);\n  2038→  const thresholdRaw = parseInt(ui.presetThreshold?.value);\n  2039→  const sensitivity = isNaN(sensitivityRaw) ? 50 : sensitivityRaw;\n  2040→  const threshold = isNaN(thresholdRaw) ? -32 : thresholdRaw;\n  2041→\n  2042→  // Get base settings and override with modal values\n  2043→  const baseSettings = getCurrentPresetSettings();\n  2044→  const settings = {\n  2045→    ...baseSettings,\n  2046→    sensitivity,\n  2047→    threshold\n  2048→  };\n  2049→\n  2050→  let result;\n  2051→\n  2052→  if (editId) {\n  2053→    // Update existing preset (including settings)\n  2054→    result = updateCustomPreset(editId, {\n  2055→      name,\n  2056→      description,\n  2057→      icon,\n  2058→      settings\n  2059→    });\n  2060→\n  2061→    if (result.success) {\n  2062→      setStatus(`Preset \"${name}\" updated`);\n  2063→    }\n  2064→  } else {\n  2065→    // Create new preset with modal settings\n  2066→    result = createCustomPreset({\n  2067→      name,\n  2068→      description,\n  2069→      icon,\n  2070→      settings\n  2071→    });\n  2072→\n  2073→    if (result.success) {\n  2074→      setStatus(`Preset \"${name}\" saved`);\n  2075→      // Select the newly created preset\n  2076→      refreshPresetDropdown();\n  2077→      if (ui.presetSelector) {\n  2078→        ui.presetSelector.value = result.id;\n  2079→      }\n  2080→    }\n  2081→  }\n  2082→\n  2083→  if (result.success) {\n  2084→    hidePresetModal();\n  2085→    refreshPresetDropdown();\n  2086→  } else {\n  2087→    showPresetError(result.error || 'Failed to save preset');\n  2088→  }\n  2089→}\n  2090→\n  2091→/**\n  2092→ * Show the manage presets modal\n  2093→ */\n  2094→function showManagePresetsModal() {\n  2095→  if (!ui.managePresetsModal) return;\n  2096→\n  2097→  renderPresetsList();\n  2098→  ui.managePresetsModal.classList.remove('hidden');\n  2099→}\n  2100→\n  2101→/**\n  2102→ * Hide the manage presets modal\n  2103→ */\n  2104→function hideManagePresetsModal() {\n  2105→  if (ui.managePresetsModal) {\n  2106→    ui.managePresetsModal.classList.add('hidden');\n  2107→  }\n  2108→}\n  2109→\n  2110→/**\n  2111→ * Render the presets list in the manage modal\n  2112→ */\n  2113→function renderPresetsList() {\n  2114→  if (!ui.presetsList) return;\n  2115→\n  2116→  const presets = getAllPresets();\n  2117→  const customPresets = presets.filter(p => !p.isBuiltIn);\n  2118→\n  2119→  // Show/hide empty state\n  2120→  if (ui.noCustomPresets) {\n  2121→    ui.noCustomPresets.style.display = customPresets.length === 0 ? 'block' : 'none';\n  2122→  }\n  2123→\n  2124→  // Build HTML\n  2125→  const fragment = document.createDocumentFragment();\n  2126→\n  2127→  presets.forEach(preset => {\n  2128→    const div = document.createElement('div');\n  2129→    div.className = `preset-list-item ${preset.isBuiltIn ? 'built-in' : 'custom'}`;\n  2130→\n  2131→    const iconChar = PRESET_ICON_MAP[preset.icon] || '*';\n  2132→    const desc = preset.description || (preset.isBuiltIn ? 'Built-in preset' : 'Custom preset');\n  2133→\n  2134→    if (preset.isBuiltIn) {\n  2135→      // Built-in presets: only duplicate button (Phase 3)\n  2136→      div.innerHTML = `\n  2137→        <div class=\"preset-list-item-icon\">${iconChar}</div>\n  2138→        <div class=\"preset-list-item-info\">\n  2139→          <div class=\"preset-list-item-name\">${preset.name}</div>\n  2140→          <div class=\"preset-list-item-desc\">${desc}</div>\n  2141→        </div>\n  2142→        <div class=\"preset-list-item-actions\">\n  2143→          <button class=\"preset-duplicate-btn\" data-preset-id=\"${preset.id}\" data-preset-name=\"${preset.name}\" title=\"Duplicate preset\">D</button>\n  2144→        </div>\n  2145→      `;\n  2146→    } else {\n  2147→      // Custom presets: duplicate/edit/delete buttons (Phase 3)\n  2148→      div.innerHTML = `\n  2149→        <div class=\"preset-list-item-icon\">${iconChar}</div>\n  2150→        <div class=\"preset-list-item-info\">\n  2151→          <div class=\"preset-list-item-name\">${preset.name}</div>\n  2152→          <div class=\"preset-list-item-desc\">${desc}</div>\n  2153→        </div>\n  2154→        <div class=\"preset-list-item-actions\">\n  2155→          <button class=\"preset-duplicate-btn\" data-preset-id=\"${preset.id}\" data-preset-name=\"${preset.name}\" title=\"Duplicate preset\">D</button>\n  2156→          <button class=\"preset-edit-btn\" data-preset-id=\"${preset.id}\" title=\"Edit preset\">E</button>\n  2157→          <button class=\"preset-delete-btn delete\" data-preset-id=\"${preset.id}\" data-preset-name=\"${preset.name}\" title=\"Delete preset\">X</button>\n  2158→        </div>\n  2159→      `;\n  2160→    }\n  2161→\n  2162→    fragment.appendChild(div);\n  2163→  });\n  2164→\n  2165→  ui.presetsList.innerHTML = '';\n  2166→  ui.presetsList.appendChild(fragment);\n  2167→}\n  2168→\n  2169→/**\n  2170→ * Edit a preset\n  2171→ */\n  2172→function editPreset(presetId) {\n  2173→  hideManagePresetsModal();\n  2174→  showPresetModal(presetId);\n  2175→}\n  2176→\n  2177→/**\n  2178→ * Delete a preset with confirmation\n  2179→ */\n  2180→function deletePresetWithConfirm(presetId, presetName) {\n  2181→  // Simple confirmation (UXP doesn't have native confirm)\n  2182→  const confirmed = confirm(`Delete preset \"${presetName}\"?`);\n  2183→  if (!confirmed) return;\n  2184→\n  2185→  const result = deleteCustomPreset(presetId);\n  2186→\n  2187→  if (result.success) {\n  2188→    setStatus(`Preset \"${presetName}\" deleted`);\n  2189→    renderPresetsList();\n  2190→    refreshPresetDropdown();\n  2191→  } else {\n  2192→    setStatus('Failed to delete preset: ' + (result.error || 'Unknown error'));\n  2193→  }\n  2194→}\n  2195→\n  2196→/**\n  2197→ * Duplicate a preset from the UI (Phase 3)\n  2198→ */\n  2199→function duplicatePresetFromUI(presetId, presetName) {\n  2200→  const result = duplicatePreset(presetId);\n  2201→\n  2202→  if (result.success) {\n  2203→    setStatus(`Created copy of \"${presetName}\"`);\n  2204→    renderPresetsList();\n  2205→    refreshPresetDropdown();\n  2206→\n  2207→    // Select the new copy in dropdown\n  2208→    if (ui.presetSelector && result.id) {\n  2209→      ui.presetSelector.value = result.id;\n  2210→    }\n  2211→  } else {\n  2212→    setStatus('Failed to duplicate preset: ' + (result.error || 'Unknown error'));\n  2213→  }\n  2214→}\n  2215→\n  2216→/**\n  2217→ * Export presets to a JSON file (Phase 3)\n  2218→ */\n  2219→async function exportPresetsToFile() {\n  2220→  const result = exportPresets();\n  2221→\n  2222→  if (!result.success) {\n  2223→    setStatus(result.error || 'No presets to export');\n  2224→    return;\n  2225→  }\n  2226→\n  2227→  try {\n  2228→    // Create a blob and download it\n  2229→    const blob = new Blob([result.data], { type: 'application/json' });\n  2230→    const filename = `splice-presets-${new Date().toISOString().slice(0, 10)}.json`;\n  2231→\n  2232→    // UXP approach: use file system API\n  2233→    const uxpFs = require('uxp').storage.localFileSystem;\n  2234→    const file = await uxpFs.getFileForSaving(filename, { types: ['json'] });\n  2235→\n  2236→    if (file) {\n  2237→      await file.write(result.data);\n  2238→      setStatus(`Exported ${result.count} preset(s) to ${file.name}`);\n  2239→    } else {\n  2240→      setStatus('Export cancelled');\n  2241→    }\n  2242→  } catch (err) {\n  2243→    console.error('[SPLICE] Export error:', err);\n  2244→    setStatus('Export failed: ' + err.message);\n  2245→  }\n  2246→}\n  2247→\n  2248→/**\n  2249→ * Import presets from a JSON file (Phase 3)\n  2250→ */\n  2251→async function importPresetsFromFile(file) {\n  2252→  try {\n  2253→    // Read file contents\n  2254→    const reader = new FileReader();\n  2255→\n  2256→    reader.onload = (e) => {\n  2257→      const jsonString = e.target.result;\n  2258→      const result = importPresets(jsonString, true);\n  2259→\n  2260→      if (result.success) {\n  2261→        setStatus(`Imported ${result.imported} preset(s)${result.skipped > 0 ? `, ${result.skipped} skipped` : ''}`);\n  2262→        renderPresetsList();\n  2263→        refreshPresetDropdown();\n  2264→      } else {\n  2265→        setStatus('Import failed: ' + (result.error || 'Unknown error'));\n  2266→      }\n  2267→    };\n  2268→\n  2269→    reader.onerror = () => {\n  2270→      setStatus('Failed to read file');\n  2271→    };\n  2272→\n  2273→    reader.readAsText(file);\n  2274→  } catch (err) {\n  2275→    console.error('[SPLICE] Import error:', err);\n  2276→    setStatus('Import failed: ' + err.message);\n  2277→  }\n  2278→}\n  2279→\n  2280→/**\n  2281→ * Apply a preset and update UI controls\n  2282→ * Called when user selects a preset from dropdown\n  2283→ */\n  2284→function applyPresetAndUpdateUI(presetId) {\n  2285→  const preset = getPresetById(presetId);\n  2286→  if (!preset) return;\n  2287→\n  2288→  // For custom presets, apply the settings\n  2289→  if (!preset.isBuiltIn) {\n  2290→    const settings = preset.settings;\n  2291→    if (settings) {\n  2292→      // Update sensitivity slider\n  2293→      if (ui.sensitivitySlider && settings.sensitivity !== undefined) {\n  2294→        ui.sensitivitySlider.value = settings.sensitivity;\n  2295→      }\n  2296→\n  2297→      // Update takes detection checkbox\n  2298→      if (ui.enableTakesDetection && settings.enableTakesDetection !== undefined) {\n  2299→        ui.enableTakesDetection.checked = settings.enableTakesDetection;\n  2300→      }\n  2301→\n  2302→      // Update auto-mark best checkbox\n  2303→      if (ui.autoMarkBest && settings.autoMarkBest !== undefined) {\n  2304→        ui.autoMarkBest.checked = settings.autoMarkBest;\n  2305→      }\n  2306→\n  2307→      // Save as active preset\n  2308→      saveSettings({ activePreset: presetId, ...settings });\n  2309→    }\n  2310→  } else {\n  2311→    // Built-in preset: use applyPreset from settings.js\n  2312→    const appliedSettings = applyPreset(presetId);\n  2313→\n  2314→    // Update sensitivity slider to match preset\n  2315→    if (ui.sensitivitySlider && appliedSettings.sensitivity !== undefined) {\n  2316→      ui.sensitivitySlider.value = appliedSettings.sensitivity;\n  2317→    }\n  2318→\n  2319→    // Update takes detection checkbox\n  2320→    if (ui.enableTakesDetection && appliedSettings.enableTakesDetection !== undefined) {\n  2321→      ui.enableTakesDetection.checked = appliedSettings.enableTakesDetection;\n  2322→    }\n  2323→\n  2324→    // Update auto-mark best checkbox\n  2325→    if (ui.autoMarkBest && appliedSettings.autoMarkBest !== undefined) {\n  2326→      ui.autoMarkBest.checked = appliedSettings.autoMarkBest;\n  2327→    }\n  2328→  }\n  2329→\n  2330→  setStatus(`Applied preset: ${preset.name}`);\n  2331→}\n  2332→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"311f2001-6a62-4efe-a061-81e2fb0e792f","timestamp":"2025-12-25T22:30:32.538Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-plugin/js/main.js","content":"/**\n * SPLICE Plugin Main Entry Point\n *\n * Initializes all functionality when the plugin loads.\n * v3.4 - Timeline Markers + Unified Take-Aware Workflow.\n */\n\n// Initialize Premiere Pro API\nconst ppro = require('premierepro');\n\n// ============================================================================\n// CACHED DOM ELEMENTS (Performance: avoid repeated getElementById calls)\n// ============================================================================\nconst ui = {};\n\nfunction cacheUIElements() {\n  ui.status = document.getElementById('status');\n\n  // Main GO button (unified workflow)\n  ui.goBtn = document.getElementById('goBtn');\n  ui.optionsToggle = document.getElementById('optionsToggle');\n  ui.optionsPanel = document.getElementById('optionsPanel');\n\n  // Sliders and options\n  ui.sensitivitySlider = document.getElementById('sensitivitySlider');\n  ui.sourceOriginal = document.getElementById('sourceOriginal');\n  ui.sourceIsolated = document.getElementById('sourceIsolated');\n\n  // Settings checkboxes\n  ui.enableTakesDetection = document.getElementById('enableTakesDetection');\n  ui.autoMarkBest = document.getElementById('autoMarkBest');\n\n  // Progress\n  ui.progressContainer = document.getElementById('progressContainer');\n  ui.progressText = document.getElementById('progressText');\n  ui.resultsEmpty = document.getElementById('resultsEmpty');\n\n  // Combined Preview\n  ui.combinedPreview = document.getElementById('combinedPreview');\n  ui.previewList = document.getElementById('previewList');\n  ui.silenceCountDisplay = document.getElementById('silenceCount');\n  ui.takeCountDisplay = document.getElementById('takeCount');\n  ui.selectedCount = document.getElementById('selectedCount');\n  ui.selectAllSilences = document.getElementById('selectAllSilences');\n  ui.applyPreviewBtn = document.getElementById('applyPreviewBtn');\n  ui.cancelPreviewBtn = document.getElementById('cancelPreviewBtn');\n  ui.invertSelectionBtn = document.getElementById('invertSelectionBtn');\n  ui.reRunDetectionBtn = document.getElementById('reRunDetectionBtn');\n\n  // Results\n  ui.silenceResults = document.getElementById('silenceResults');\n  ui.resultsCount = document.getElementById('resultsCount');\n  ui.timeSaved = document.getElementById('timeSaved');\n  ui.clipsModified = document.getElementById('clipsModified');\n\n  // Build Sequence button (v3.5)\n  ui.buildSequenceBtn = document.getElementById('buildSequenceBtn');\n\n  // Buttons\n  ui.undoBtn = document.getElementById('undoBtn');\n  ui.advancedSection = document.getElementById('advancedSection');\n\n  // Settings modal\n  ui.settingsBtn = document.getElementById('settingsBtn');\n  ui.settingsModal = document.getElementById('settingsModal');\n  ui.closeSettingsBtn = document.getElementById('closeSettingsBtn');\n\n  // Media folder (v3.5)\n  ui.mediaFolderDisplay = document.getElementById('mediaFolderDisplay');\n  ui.setMediaFolderBtn = document.getElementById('setMediaFolderBtn');\n  ui.clearMediaFolderBtn = document.getElementById('clearMediaFolderBtn');\n\n  // Custom Presets (v3.5)\n  ui.savePresetBtn = document.getElementById('savePresetBtn');\n  ui.managePresetsBtn = document.getElementById('managePresetsBtn');\n  ui.presetModal = document.getElementById('presetModal');\n  ui.presetModalTitle = document.getElementById('presetModalTitle');\n  ui.closePresetModalBtn = document.getElementById('closePresetModalBtn');\n  ui.presetNameInput = document.getElementById('presetNameInput');\n  ui.presetDescInput = document.getElementById('presetDescInput');\n  ui.presetIconPicker = document.getElementById('presetIconPicker');\n  ui.presetEditId = document.getElementById('presetEditId');\n  ui.presetSelectedIcon = document.getElementById('presetSelectedIcon');\n  ui.savePresetConfirmBtn = document.getElementById('savePresetConfirmBtn');\n  ui.presetModalError = document.getElementById('presetModalError');\n  ui.managePresetsModal = document.getElementById('managePresetsModal');\n  ui.closeManagePresetsBtn = document.getElementById('closeManagePresetsBtn');\n  ui.presetsList = document.getElementById('presetsList');\n  ui.noCustomPresets = document.getElementById('noCustomPresets');\n  ui.presetSelector = document.getElementById('presetSelector');\n\n  // Preset settings inputs (Phase 3)\n  ui.presetSensitivity = document.getElementById('presetSensitivity');\n  ui.presetSensitivityValue = document.getElementById('presetSensitivityValue');\n  ui.presetThreshold = document.getElementById('presetThreshold');\n\n  // Export/Import buttons (Phase 3)\n  ui.exportPresetsBtn = document.getElementById('exportPresetsBtn');\n  ui.importPresetsBtn = document.getElementById('importPresetsBtn');\n  ui.importPresetsFile = document.getElementById('importPresetsFile');\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\nlet previewSilences = [];         // All detected silences\nlet safeSilences = [];            // Silences that don't overlap takes (safe to remove)\nlet protectedSilences = [];       // Silences that overlap takes (protected)\nlet previewTakes = [];            // Detected takes (for display)\nlet selectedSilenceIndices = new Set();\nlet isOperationInProgress = false;\nlet pendingUIUpdate = null;\n\n// Export preview state for other modules (Razor integration)\nwindow.splicePreviewState = {\n  getSelectedSilences: () => safeSilences.filter((_, i) => selectedSilenceIndices.has(i)),\n  getSelectedTakes: () => previewTakes,\n  hasPreview: () => safeSilences.length > 0 || previewTakes.length > 0\n};\n\n// ============================================================================\n// INITIALIZATION\n// ============================================================================\ndocument.addEventListener('DOMContentLoaded', () => {\n  // Cache all DOM elements first\n  cacheUIElements();\n\n  // Settings & UI\n  initSettingsUI();\n  initSettingsModal();\n  initLoginModal();\n  initOptionsToggles();\n  initPresetSelector();\n  initHelpButton();\n\n  // Credits display\n  initCredits();\n\n  // Unified workflow (silences + takes)\n  initUnifiedWorkflow();\n\n  // Preview handlers (event delegation)\n  initPreviewHandlers();\n\n  // Undo handlers\n  initUndoHandlers();\n\n  // Keyboard shortcuts\n  initKeyboardShortcuts();\n\n  // Advanced features (Razor - legacy)\n  initSlice9();\n\n  // Batch processing\n  initBatchHandlers();\n\n  // Media folder (v3.5)\n  initMediaFolderHandlers();\n\n  // Custom Presets UI (v3.5)\n  initCustomPresetsUI();\n\n  console.log('[SPLICE] Plugin initialized v3.5 (Direct DOM Reconstruction)');\n});\n\n// ============================================================================\n// DEBOUNCED UI UPDATES (Performance: batch DOM updates)\n// ============================================================================\nfunction scheduleUIUpdate(callback) {\n  if (pendingUIUpdate) {\n    cancelAnimationFrame(pendingUIUpdate);\n  }\n  pendingUIUpdate = requestAnimationFrame(() => {\n    callback();\n    pendingUIUpdate = null;\n  });\n}\n\nfunction updateSelectionCounts() {\n  scheduleUIUpdate(() => {\n    if (ui.selectedCount) {\n      ui.selectedCount.textContent = selectedSilenceIndices.size;\n    }\n    updateApplyButton();\n    updateSelectAllCheckbox();\n  });\n}\n\n// ============================================================================\n// KEYBOARD SHORTCUTS\n// ============================================================================\nfunction initKeyboardShortcuts() {\n  document.addEventListener('keydown', (e) => {\n    // Only handle when combined preview is visible\n    const previewVisible = ui.combinedPreview?.style.display !== 'none' &&\n                           ui.combinedPreview?.style.display !== '';\n\n    if (!previewVisible) return;\n\n    const isMac = navigator.platform.toUpperCase().indexOf('MAC') >= 0;\n    const cmdKey = isMac ? e.metaKey : e.ctrlKey;\n\n    // Enter - Apply selected\n    if (e.key === 'Enter' && !e.shiftKey) {\n      e.preventDefault();\n      if (selectedSilenceIndices.size > 0) {\n        ui.applyPreviewBtn?.click();\n      }\n    }\n\n    // Escape - Cancel\n    if (e.key === 'Escape') {\n      e.preventDefault();\n      ui.cancelPreviewBtn?.click();\n    }\n\n    // Cmd/Ctrl+A - Select all\n    if (e.key === 'a' && cmdKey) {\n      e.preventDefault();\n      if (ui.selectAllSilences) {\n        ui.selectAllSilences.checked = true;\n        selectAllSilences(true);\n      }\n    }\n\n    // Cmd/Ctrl+D - Deselect all\n    if (e.key === 'd' && cmdKey) {\n      e.preventDefault();\n      if (ui.selectAllSilences) {\n        ui.selectAllSilences.checked = false;\n        selectAllSilences(false);\n      }\n    }\n\n    // Cmd/Ctrl+I - Invert selection\n    if (e.key === 'i' && cmdKey) {\n      e.preventDefault();\n      invertSilenceSelection();\n    }\n  });\n}\n\n// ============================================================================\n// UNIFIED WORKFLOW (Silence Detection + Takes Detection)\n// ============================================================================\nfunction initUnifiedWorkflow() {\n  if (!ui.goBtn) return;\n\n  ui.goBtn.addEventListener('click', async () => {\n    // Prevent double-click\n    if (isOperationInProgress) {\n      setStatus('Operation in progress...');\n      return;\n    }\n\n    isOperationInProgress = true;\n    ui.goBtn.disabled = true;\n\n    try {\n      showProgress('Preparing audio...');\n\n      // Step 1: Export audio\n      const exportSuccess = await exportAudioInternal();\n      if (!exportSuccess) {\n        throw new Error('Audio export failed');\n      }\n\n      // Step 2: Get settings\n      const sensitivity = parseInt(ui.sensitivitySlider?.value || 50);\n      const useIsolated = ui.sourceIsolated?.checked ?? false;\n      const detectTakes = ui.enableTakesDetection?.checked ?? true;\n\n      // Step 3: Vocal isolation if needed\n      let audioPath = WAV_PATH;\n      if (useIsolated) {\n        // Check isolation access before proceeding\n        const credits = getCurrentCredits();\n        if (!credits || !credits.hasIsolationAccess) {\n          throw new Error('Vocal isolation requires Pro or Team tier. Upgrade to access this feature.');\n        }\n\n        // Show overage warning if applicable\n        const isolationMins = credits.isolationHoursRemaining * 60;\n        if (isolationMins <= 0) {\n          const overageRate = credits.isolationOverageRate || 0.08;\n          showProgress(`Isolating vocals (overage: $${overageRate}/min)...`);\n        } else {\n          showProgress(`Isolating vocals (${isolationMins.toFixed(0)} min remaining)...`);\n        }\n\n        const isolateResult = await isolateVocals();\n        if (isolateResult.success) {\n          audioPath = isolateResult.outputPath;\n          // Update credits display after isolation\n          await refreshCredits();\n        } else {\n          throw new Error(isolateResult.error || 'Vocal isolation failed');\n        }\n      }\n\n      // Step 4: Run detection (parallel if takes enabled)\n      const params = mapSensitivity(sensitivity);\n      let silences = [];\n      let takes = [];\n\n      if (detectTakes) {\n        // Parallel detection for performance\n        showProgress('Detecting silences and analyzing takes...');\n        const [silenceResult, transcriptResult] = await Promise.all([\n          detectSilences(audioPath, params),\n          transcribeAudio()\n        ]);\n\n        silences = silenceResult;\n\n        // Extract takes from transcript result\n        if (transcriptResult.success && transcriptResult.takes) {\n          if (transcriptResult.takes.takes) {\n            takes = transcriptResult.takes.takes;\n          } else if (Array.isArray(transcriptResult.takes)) {\n            takes = transcriptResult.takes;\n          }\n        }\n\n        // Store transcript for reference\n        window.currentTranscript = transcriptResult.transcript;\n      } else {\n        // Just detect silences\n        showProgress('Detecting silences...');\n        silences = await detectSilences(audioPath, params);\n      }\n\n      if (silences.length === 0) {\n        showEmptyState();\n        setStatus('No silences detected. Try increasing sensitivity.');\n        return;\n      }\n\n      // Step 5: Filter silences by takes (protect speech)\n      const filtered = filterSilencesByTakes(silences, takes);\n\n      // Step 6: Add timeline markers (clear old ones first)\n      showProgress('Adding timeline markers...');\n      await clearSpliceMarkers();\n      const markerResult = await addTimelineMarkers(filtered.safe, filtered.protected, takes);\n\n      // Step 7: Show combined preview\n      showCombinedPreview(filtered.safe, filtered.protected, takes);\n\n      const markerInfo = markerResult.added > 0 ? ` (${markerResult.added} markers added)` : '';\n      const statusMsg = takes.length > 0\n        ? `Found ${silences.length} silences, ${filtered.protected.length} protected by ${takes.length} takes${markerInfo}`\n        : `Found ${silences.length} silence(s) - review and apply${markerInfo}`;\n      setStatus(statusMsg);\n\n    } catch (err) {\n      showEmptyState();\n      setStatus('Error: ' + err.message);\n      console.error('[SPLICE] Unified workflow error:', err);\n    } finally {\n      isOperationInProgress = false;\n      ui.goBtn.disabled = false;\n    }\n  });\n}\n\n// ============================================================================\n// TAKE-AWARE SILENCE FILTERING\n// ============================================================================\n/**\n * Filter silences to protect those that overlap with takes (speech segments).\n * Returns { safe: [], protected: [] }\n */\nfunction filterSilencesByTakes(silences, takes) {\n  if (!takes || takes.length === 0) {\n    // No takes = all silences are safe to remove\n    return { safe: silences, protected: [] };\n  }\n\n  const safe = [];\n  const protected_ = [];\n\n  silences.forEach(silence => {\n    // Check if this silence overlaps with any take\n    const overlaps = takes.some(take => {\n      const takeStart = take.startTime || take.start;\n      const takeEnd = take.endTime || take.end;\n      // Overlap exists if silence starts before take ends AND silence ends after take starts\n      return silence.start < takeEnd && silence.end > takeStart;\n    });\n\n    if (overlaps) {\n      protected_.push(silence);\n    } else {\n      safe.push(silence);\n    }\n  });\n\n  console.log(`[SPLICE] Filtered silences: ${safe.length} safe, ${protected_.length} protected`);\n  return { safe, protected: protected_ };\n}\n\n// ============================================================================\n// TIMELINE MARKERS (Visual indicators on Premiere Pro timeline)\n// ============================================================================\n/**\n * Add colored markers to the timeline for silences and takes.\n * - Red markers: silences safe to remove\n * - Yellow markers: silences protected by takes\n * - Blue markers: takes (speech segments)\n *\n * @param {Array} safeToRemove - Silences safe to remove\n * @param {Array} protectedSilences - Silences overlapping speech\n * @param {Array} takes - Detected speech segments\n * @returns {Promise<{added: number, errors: number}>}\n */\nasync function addTimelineMarkers(safeToRemove, protectedSilences, takes) {\n  const results = { added: 0, errors: 0 };\n\n  try {\n    const context = await getActiveSequence();\n    if (!context) {\n      console.warn('[SPLICE] No sequence available for markers');\n      return results;\n    }\n\n    const { sequence, project } = context;\n\n    // Get markers object for the sequence\n    const markers = await ppro.Markers.getMarkers(sequence);\n    if (!markers) {\n      console.warn('[SPLICE] Markers API not available');\n      return results;\n    }\n\n    // Collect all marker actions\n    const markerActions = [];\n\n    // Red markers for safe-to-remove silences\n    for (const silence of safeToRemove) {\n      try {\n        const duration = silence.end - silence.start;\n\n        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n        const startTime = ppro.TickTime.createWithSeconds(silence.start);\n        const durationTime = ppro.TickTime.createWithSeconds(duration);\n\n        const action = markers.createAddMarkerAction(\n          `SPLICE: Remove (${duration.toFixed(2)}s)`,\n          ppro.Marker.MARKER_TYPE_COMMENT,\n          startTime,\n          durationTime,\n          `Silence to remove: ${formatTime(silence.start)} - ${formatTime(silence.end)}`\n        );\n        markerActions.push({ action, colorIndex: 1, type: 'silence' }); // Red = index 1\n      } catch (err) {\n        console.error('[SPLICE] Error creating silence marker:', err);\n        results.errors++;\n      }\n    }\n\n    // Yellow markers for protected silences\n    for (const silence of protectedSilences) {\n      try {\n        const duration = silence.end - silence.start;\n\n        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n        const startTime = ppro.TickTime.createWithSeconds(silence.start);\n        const durationTime = ppro.TickTime.createWithSeconds(duration);\n\n        const action = markers.createAddMarkerAction(\n          `SPLICE: Protected (${duration.toFixed(2)}s)`,\n          ppro.Marker.MARKER_TYPE_COMMENT,\n          startTime,\n          durationTime,\n          `Protected silence (overlaps speech): ${formatTime(silence.start)} - ${formatTime(silence.end)}`\n        );\n        markerActions.push({ action, colorIndex: 4, type: 'protected' }); // Yellow = index 4\n      } catch (err) {\n        console.error('[SPLICE] Error creating protected marker:', err);\n        results.errors++;\n      }\n    }\n\n    // Blue markers for takes (speech segments)\n    for (const take of takes) {\n      try {\n        const start = take.startTime || take.start;\n        const end = take.endTime || take.end;\n        const duration = end - start;\n        const textPreview = take.text?.substring(0, 30) || 'Speech';\n\n        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n        const startTime = ppro.TickTime.createWithSeconds(start);\n        const durationTime = ppro.TickTime.createWithSeconds(duration);\n\n        const action = markers.createAddMarkerAction(\n          `SPLICE: Take`,\n          ppro.Marker.MARKER_TYPE_COMMENT,\n          startTime,\n          durationTime,\n          `Speech segment: ${textPreview}${take.text?.length > 30 ? '...' : ''}`\n        );\n        markerActions.push({ action, colorIndex: 5, type: 'take' }); // Blue = index 5\n      } catch (err) {\n        console.error('[SPLICE] Error creating take marker:', err);\n        results.errors++;\n      }\n    }\n\n    // Execute all marker actions in a single transaction\n    if (markerActions.length > 0) {\n      await project.lockedAccess(async () => {\n        await project.executeTransaction((compoundAction) => {\n          for (const { action } of markerActions) {\n            compoundAction.addAction(action);\n          }\n        }, 'SPLICE: Add Timeline Markers');\n      });\n\n      results.added = markerActions.length;\n      console.log(`[SPLICE] Added ${results.added} timeline markers`);\n    }\n\n  } catch (err) {\n    console.error('[SPLICE] Timeline markers error:', err);\n    results.errors++;\n  }\n\n  return results;\n}\n\n/**\n * Read current SPLICE marker positions from timeline.\n * This allows users to manually adjust markers before applying.\n * Returns silences based on current marker positions (not original detection).\n */\nasync function readMarkersAssilences() {\n  const silences = [];\n\n  try {\n    const context = await getActiveSequence();\n    if (!context) return silences;\n\n    const { sequence } = context;\n    const markers = await ppro.Markers.getMarkers(sequence);\n    if (!markers) return silences;\n\n    const allMarkers = await markers.getMarkers();\n    if (!allMarkers || allMarkers.length === 0) return silences;\n\n    for (const marker of allMarkers) {\n      const name = await marker.getName();\n\n      // Only read \"Remove\" markers (red ones that will be cut)\n      if (name && name.startsWith('SPLICE: Remove')) {\n        const startTickTime = await marker.getStart();\n        const durationTickTime = await marker.getDuration();\n\n        // Use TickTime.seconds property to get seconds directly (no TICKS_PER_SECOND needed)\n        const start = startTickTime.seconds;\n        const duration = durationTickTime.seconds;\n        const end = start + duration;\n\n        silences.push({\n          start,\n          end,\n          duration,\n          fromMarker: true  // Flag to indicate this came from marker adjustment\n        });\n      }\n    }\n\n    // Sort by start time\n    silences.sort((a, b) => a.start - b.start);\n    console.log(`[SPLICE] Read ${silences.length} silences from adjusted markers`);\n\n  } catch (err) {\n    console.error('[SPLICE] Error reading markers:', err);\n  }\n\n  return silences;\n}\n\n/**\n * Remove all SPLICE markers from the timeline.\n * Call this before adding new markers to avoid duplicates.\n */\nasync function clearSpliceMarkers() {\n  try {\n    const context = await getActiveSequence();\n    if (!context) return 0;\n\n    const { sequence, project } = context;\n    const markers = await ppro.Markers.getMarkers(sequence);\n    if (!markers) return 0;\n\n    // Get all markers\n    const allMarkers = await markers.getMarkers();\n    if (!allMarkers || allMarkers.length === 0) return 0;\n\n    // Filter SPLICE markers\n    const spliceMarkers = [];\n    for (const marker of allMarkers) {\n      const name = await marker.getName();\n      if (name && name.startsWith('SPLICE:')) {\n        spliceMarkers.push(marker);\n      }\n    }\n\n    if (spliceMarkers.length === 0) return 0;\n\n    // Remove all SPLICE markers in a single transaction\n    await project.lockedAccess(async () => {\n      await project.executeTransaction((compoundAction) => {\n        for (const marker of spliceMarkers) {\n          const removeAction = markers.createRemoveMarkerAction(marker);\n          compoundAction.addAction(removeAction);\n        }\n      }, 'SPLICE: Clear Markers');\n    });\n\n    console.log(`[SPLICE] Cleared ${spliceMarkers.length} previous markers`);\n    return spliceMarkers.length;\n\n  } catch (err) {\n    console.error('[SPLICE] Error clearing markers:', err);\n    return 0;\n  }\n}\n\n// ============================================================================\n// COMBINED PREVIEW (Silences + Takes with colored markers)\n// ============================================================================\n/**\n * Show combined preview with:\n * - Red markers: silences safe to remove\n * - Yellow markers: silences protected by takes (won't be removed)\n * - Blue markers: takes (for reference)\n */\nfunction showCombinedPreview(safeToRemove, protectedSilences, takes) {\n  // Update global state\n  safeSilences = safeToRemove;\n  previewSilences = [...safeToRemove, ...protectedSilences];\n  previewTakes = takes;\n  selectedSilenceIndices = new Set(safeToRemove.map((_, i) => i));\n\n  // Hide other views\n  hideAllViews();\n\n  // Update summary counts\n  if (ui.silenceCountDisplay) ui.silenceCountDisplay.textContent = safeToRemove.length;\n  if (ui.takeCountDisplay) ui.takeCountDisplay.textContent = takes.length;\n  if (ui.selectedCount) ui.selectedCount.textContent = safeToRemove.length;\n\n  // Build combined items list (sorted by time)\n  const items = [\n    ...safeToRemove.map((s, i) => ({ type: 'silence', index: i, start: s.start, end: s.end, data: s })),\n    ...protectedSilences.map(s => ({ type: 'protected', start: s.start, end: s.end, data: s })),\n    ...takes.map(t => ({ type: 'take', start: t.startTime || t.start, end: t.endTime || t.end, data: t }))\n  ].sort((a, b) => a.start - b.start);\n\n  // Build preview list using DocumentFragment\n  const fragment = document.createDocumentFragment();\n\n  items.forEach((item) => {\n    const duration = item.end - item.start;\n    const div = document.createElement('div');\n\n    if (item.type === 'silence') {\n      // Safe to remove - red marker with checkbox\n      div.className = 'preview-item silence-marker';\n      div.dataset.index = item.index;\n      div.innerHTML = `\n        <input type=\"checkbox\" class=\"preview-item-check\" checked data-index=\"${item.index}\">\n        <div class=\"preview-item-info\">\n          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n          <div class=\"preview-item-duration\">${duration.toFixed(2)}s silence</div>\n        </div>\n        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek to this silence\">&gt;</button>\n      `;\n    } else if (item.type === 'protected') {\n      // Protected by take - yellow marker, no checkbox\n      div.className = 'preview-item protected-silence';\n      div.innerHTML = `\n        <div style=\"width: 20px; margin-right: 10px;\"></div>\n        <div class=\"preview-item-info\">\n          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n          <div class=\"preview-item-duration\">${duration.toFixed(2)}s (overlaps speech)</div>\n        </div>\n        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek\">&gt;</button>\n      `;\n    } else if (item.type === 'take') {\n      // Take (speech segment) - blue marker, no checkbox\n      div.className = 'preview-item take-marker';\n      const textPreview = item.data.text?.substring(0, 40) || 'Speech segment';\n      div.innerHTML = `\n        <div style=\"width: 20px; margin-right: 10px;\"></div>\n        <div class=\"preview-item-info\">\n          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n          <div class=\"preview-item-duration\">${textPreview}${item.data.text?.length > 40 ? '...' : ''}</div>\n        </div>\n        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek to take\">&gt;</button>\n      `;\n    }\n\n    fragment.appendChild(div);\n  });\n\n  if (ui.previewList) {\n    ui.previewList.textContent = '';\n    ui.previewList.appendChild(fragment);\n  }\n\n  // Reset UI state\n  if (ui.selectAllSilences) ui.selectAllSilences.checked = true;\n  updateApplyButton();\n\n  // Show combined preview section\n  if (ui.combinedPreview) ui.combinedPreview.style.display = 'block';\n}\n\nfunction initPreviewHandlers() {\n  // EVENT DELEGATION: Single listener on parent instead of many on children\n  if (ui.previewList) {\n    // Handle checkbox changes\n    ui.previewList.addEventListener('change', (e) => {\n      if (e.target.classList.contains('preview-item-check')) {\n        const index = parseInt(e.target.dataset.index);\n        const item = e.target.closest('.preview-item');\n\n        if (e.target.checked) {\n          selectedSilenceIndices.add(index);\n          item?.classList.remove('excluded');\n        } else {\n          selectedSilenceIndices.delete(index);\n          item?.classList.add('excluded');\n        }\n        updateSelectionCounts();\n      }\n    });\n\n    // Handle seek button clicks\n    ui.previewList.addEventListener('click', async (e) => {\n      if (e.target.classList.contains('preview-item-seek')) {\n        e.stopPropagation();\n        const btn = e.target;\n        const time = parseFloat(btn.dataset.time);\n\n        // Show loading state\n        btn.classList.add('loading');\n        btn.textContent = '...';\n\n        try {\n          await seekToTime(time);\n        } finally {\n          btn.classList.remove('loading');\n          btn.textContent = '>';\n        }\n      }\n    });\n  }\n\n  // Apply button\n  if (ui.applyPreviewBtn) {\n    ui.applyPreviewBtn.addEventListener('click', async () => {\n      if (isOperationInProgress) return;\n      isOperationInProgress = true;\n      ui.applyPreviewBtn.disabled = true;\n\n      const originalText = ui.applyPreviewBtn.textContent;\n      ui.applyPreviewBtn.textContent = 'Applying...';\n\n      try {\n        await applySelectedSilences();\n      } catch (err) {\n        setStatus('Error: ' + err.message);\n        console.error('[SPLICE] Apply error:', err);\n      } finally {\n        ui.applyPreviewBtn.textContent = originalText;\n        ui.applyPreviewBtn.disabled = false;\n        isOperationInProgress = false;\n      }\n    });\n  }\n\n  // Cancel button\n  if (ui.cancelPreviewBtn) {\n    ui.cancelPreviewBtn.addEventListener('click', async () => {\n      await cancelPreview();\n    });\n  }\n\n  // Select all checkbox\n  if (ui.selectAllSilences) {\n    ui.selectAllSilences.addEventListener('change', (e) => {\n      selectAllSilences(e.target.checked);\n    });\n  }\n\n  // Invert selection button\n  if (ui.invertSelectionBtn) {\n    ui.invertSelectionBtn.addEventListener('click', () => {\n      invertSilenceSelection();\n    });\n  }\n\n  // Duration filter buttons\n  document.querySelectorAll('.preview-filters .filter-btn[data-min]').forEach(btn => {\n    btn.addEventListener('click', () => {\n      const min = parseFloat(btn.dataset.min);\n      const max = parseFloat(btn.dataset.max);\n      selectByDuration(min, max);\n    });\n  });\n\n  // Re-run detection button\n  if (ui.reRunDetectionBtn) {\n    ui.reRunDetectionBtn.addEventListener('click', () => {\n      // Hide preview, show options, let user adjust sensitivity\n      if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n      if (ui.optionsPanel) ui.optionsPanel.classList.remove('collapsed');\n      if (ui.optionsToggle) ui.optionsToggle.classList.add('expanded');\n      showEmptyState();\n      setStatus('Adjust sensitivity and click GO again');\n    });\n  }\n\n  // Build Sequence button (v3.5 - direct DOM reconstruction)\n  if (ui.buildSequenceBtn) {\n    ui.buildSequenceBtn.addEventListener('click', async () => {\n      if (isOperationInProgress) return;\n\n      // Check if we have detection data\n      if (safeSilences.length === 0 && previewTakes.length === 0) {\n        setStatus('No detection data. Run analysis first.');\n        return;\n      }\n\n      isOperationInProgress = true;\n      ui.buildSequenceBtn.disabled = true;\n      const originalText = ui.buildSequenceBtn.textContent;\n      ui.buildSequenceBtn.textContent = 'Building...';\n\n      try {\n        showProgress('Building new sequence...');\n\n        const result = await buildSequenceWithCutList();\n\n        if (result.success) {\n          // Hide preview and show success\n          if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n          showEmptyState();\n\n          const metadata = result.metadata || {};\n          setStatus(`New sequence created: ${metadata.silencesRemoved || 0} silences removed, ${metadata.takesDetected || 0} takes detected`);\n\n          // Clear markers after successful build\n          await clearSpliceMarkers();\n\n          // Clear preview state\n          previewSilences = [];\n          safeSilences = [];\n          protectedSilences = [];\n          previewTakes = [];\n          selectedSilenceIndices.clear();\n        } else {\n          // User-friendly error messages\n          const errorMsg = result.error || 'Unknown error';\n          setStatus('Build failed: ' + formatBuildError(errorMsg));\n        }\n\n      } catch (err) {\n        console.error('[SPLICE] Build sequence error:', err);\n        // User-friendly error messages with recovery hints\n        setStatus('Build error: ' + formatBuildError(err.message));\n      } finally {\n        ui.buildSequenceBtn.textContent = originalText;\n        ui.buildSequenceBtn.disabled = false;\n        isOperationInProgress = false;\n      }\n    });\n  }\n}\n\nfunction selectAllSilences(checked) {\n  selectedSilenceIndices.clear();\n\n  if (checked) {\n    safeSilences.forEach((_, i) => selectedSilenceIndices.add(i));\n  }\n\n  // Batch update DOM (only silence-marker items have checkboxes)\n  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n    const index = parseInt(checkbox.dataset.index);\n    checkbox.checked = checked;\n    const item = checkbox.closest('.preview-item');\n    if (checked) {\n      item?.classList.remove('excluded');\n    } else {\n      item?.classList.add('excluded');\n    }\n  });\n\n  updateSelectionCounts();\n}\n\nfunction invertSilenceSelection() {\n  const allIndices = new Set(safeSilences.map((_, i) => i));\n  const newSelection = new Set([...allIndices].filter(i => !selectedSilenceIndices.has(i)));\n  selectedSilenceIndices = newSelection;\n\n  // Update DOM\n  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n    const index = parseInt(checkbox.dataset.index);\n    const isSelected = selectedSilenceIndices.has(index);\n    checkbox.checked = isSelected;\n    const item = checkbox.closest('.preview-item');\n    if (isSelected) {\n      item?.classList.remove('excluded');\n    } else {\n      item?.classList.add('excluded');\n    }\n  });\n\n  updateSelectionCounts();\n}\n\nfunction selectByDuration(minSec, maxSec) {\n  selectedSilenceIndices.clear();\n\n  safeSilences.forEach((silence, index) => {\n    const duration = silence.end - silence.start;\n    if (duration >= minSec && duration < maxSec) {\n      selectedSilenceIndices.add(index);\n    }\n  });\n\n  // Update DOM\n  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n    const index = parseInt(checkbox.dataset.index);\n    const isSelected = selectedSilenceIndices.has(index);\n    checkbox.checked = isSelected;\n    const item = checkbox.closest('.preview-item');\n    if (isSelected) {\n      item?.classList.remove('excluded');\n    } else {\n      item?.classList.add('excluded');\n    }\n  });\n\n  updateSelectionCounts();\n  setStatus(`Selected ${selectedSilenceIndices.size} silences (${minSec}s - ${maxSec}s)`);\n}\n\nasync function cancelPreview() {\n  // Keep silences available for Razor if detected\n  const hadSilences = safeSilences.length > 0;\n\n  previewSilences = [];\n  safeSilences = [];\n  protectedSilences = [];\n  previewTakes = [];\n  selectedSilenceIndices.clear();\n  showEmptyState();\n\n  // Clear timeline markers\n  const cleared = await clearSpliceMarkers();\n  const markerInfo = cleared > 0 ? ` (${cleared} markers removed)` : '';\n  setStatus('Preview cancelled' + markerInfo);\n\n  // Show Razor section if silences were detected\n  if (hadSilences && ui.advancedSection) {\n    ui.advancedSection.classList.remove('hidden');\n  }\n}\n\nasync function applySelectedSilences() {\n  if (selectedSilenceIndices.size === 0) {\n    setStatus('No silences selected');\n    return;\n  }\n\n  showProgress('Reading marker positions...');\n\n  // Read current marker positions from timeline\n  // This allows users to manually adjust markers before applying\n  const markerSilences = await readMarkersAssilences();\n\n  let silencesToApply;\n  let adjustedCount = 0;\n\n  if (markerSilences.length > 0) {\n    // Use marker positions (may have been adjusted by user)\n    silencesToApply = markerSilences;\n    adjustedCount = markerSilences.length;\n    console.log(`[SPLICE] Using ${adjustedCount} silences from adjusted markers`);\n  } else {\n    // Fallback to original detection data\n    silencesToApply = safeSilences.filter((_, i) => selectedSilenceIndices.has(i));\n    console.log(`[SPLICE] Using ${silencesToApply.length} silences from original detection`);\n  }\n\n  if (silencesToApply.length === 0) {\n    setStatus('No silences to apply');\n    return;\n  }\n\n  // Update global silences for the apply function\n  setCurrentSilences(silencesToApply);\n\n  showProgress('Applying changes...');\n\n  // Apply to timeline\n  const result = await removeSilencesFromTimeline();\n\n  // Clear markers after applying\n  await clearSpliceMarkers();\n\n  // Hide preview\n  if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n\n  // Show results\n  showSilenceResults({\n    count: silencesToApply.length,\n    timeSaved: silencesToApply.reduce((sum, s) => sum + (s.end - s.start), 0),\n    clipsModified: result\n  });\n\n  // Show advanced section\n  if (ui.advancedSection) ui.advancedSection.classList.remove('hidden');\n\n  const adjustedInfo = adjustedCount > 0 ? ' (from adjusted markers)' : '';\n  setStatus(`Removed ${silencesToApply.length} silence(s) - ${result} clip(s) modified${adjustedInfo}`);\n\n  // Refresh credits (await to prevent race conditions)\n  try {\n    await refreshCredits();\n  } catch (err) {\n    console.warn('[SPLICE] Credits refresh failed:', err);\n  }\n\n  // Clear preview state\n  previewSilences = [];\n  safeSilences = [];\n  protectedSilences = [];\n  previewTakes = [];\n  selectedSilenceIndices.clear();\n}\n\n// ============================================================================\n// UI HELPERS\n// ============================================================================\nfunction updateApplyButton() {\n  if (ui.applyPreviewBtn) {\n    const count = selectedSilenceIndices.size;\n    ui.applyPreviewBtn.disabled = count === 0;\n    ui.applyPreviewBtn.textContent = count === 0 ? 'Select silences' : `Apply ${count} Selected`;\n  }\n}\n\nfunction updateSelectAllCheckbox() {\n  if (ui.selectAllSilences) {\n    ui.selectAllSilences.checked = selectedSilenceIndices.size === safeSilences.length && safeSilences.length > 0;\n  }\n}\n\nfunction hideAllViews() {\n  if (ui.progressContainer) ui.progressContainer.style.display = 'none';\n  if (ui.resultsEmpty) ui.resultsEmpty.style.display = 'none';\n  if (ui.silenceResults) ui.silenceResults.style.display = 'none';\n  if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n}\n\nfunction showProgress(message) {\n  hideAllViews();\n  if (ui.progressContainer) ui.progressContainer.style.display = 'block';\n  if (ui.progressText) ui.progressText.textContent = message;\n}\n\nfunction showEmptyState() {\n  hideAllViews();\n  if (ui.resultsEmpty) ui.resultsEmpty.style.display = 'block';\n  if (ui.advancedSection) ui.advancedSection.classList.add('hidden');\n}\n\nfunction showSilenceResults({ count, timeSaved, clipsModified }) {\n  hideAllViews();\n\n  // Update the results display\n  if (ui.resultsCount) ui.resultsCount.textContent = count;\n  if (ui.timeSaved) ui.timeSaved.textContent = timeSaved.toFixed(1) + 's';\n  if (ui.clipsModified) ui.clipsModified.textContent = clipsModified;\n\n  if (ui.silenceResults) ui.silenceResults.style.display = 'block';\n}\n\nfunction formatTime(seconds) {\n  if (typeof seconds !== 'number' || isNaN(seconds)) return '0:00';\n  const mins = Math.floor(seconds / 60);\n  const secs = Math.floor(seconds % 60);\n  return `${mins}:${secs.toString().padStart(2, '0')}`;\n}\n\n/**\n * Format build errors into user-friendly messages\n * @param {string} error - Raw error message\n * @returns {string} User-friendly error message\n */\nfunction formatBuildError(error) {\n  if (!error) return 'Unknown error occurred';\n\n  // Map technical errors to user-friendly messages\n  const errorMap = {\n    'Builder not initialized': 'Builder module failed to load. Try restarting the plugin.',\n    'No active project': 'No project is open. Please open or create a project first.',\n    'No media items found': 'No media files in project. Import media before building.',\n    'No detection data': 'Run analysis first (click GO button).',\n    'Failed to generate cut list': 'Server error generating cuts. Check your connection.',\n    'Failed to build sequence': 'Premiere Pro sequence build failed. Try again.',\n    'fetch failed': 'Network error. Check if backend server is running.',\n    'NetworkError': 'Cannot connect to server. Check your internet connection.',\n    'Insufficient credits': 'Out of credits. Upgrade your plan to continue.',\n    'Authentication required': 'Login required. Click the credits badge to log in.'\n  };\n\n  // Check for matching error patterns\n  for (const [pattern, friendly] of Object.entries(errorMap)) {\n    if (error.includes(pattern)) {\n      return friendly;\n    }\n  }\n\n  // Return original if no match (already reasonably clear)\n  return error;\n}\n\nfunction setStatus(message) {\n  if (ui.status) ui.status.textContent = message;\n}\n\n// ============================================================================\n// UNDO HANDLERS\n// ============================================================================\nfunction initUndoHandlers() {\n  if (ui.undoBtn) {\n    ui.undoBtn.addEventListener('click', () => {\n      try {\n        ppro.app.executeCommand('Edit.Undo');\n        showEmptyState();\n        setStatus('Changes undone');\n      } catch (e) {\n        setStatus('Undo failed - use Cmd+Z');\n      }\n    });\n  }\n}\n\n// ============================================================================\n// MEDIA FOLDER HANDLERS (v3.5)\n// ============================================================================\nfunction initMediaFolderHandlers() {\n  // Update display on load\n  updateMediaFolderDisplay();\n\n  // Set media folder button\n  if (ui.setMediaFolderBtn) {\n    ui.setMediaFolderBtn.addEventListener('click', async () => {\n      ui.setMediaFolderBtn.disabled = true;\n      ui.setMediaFolderBtn.textContent = 'Selecting...';\n\n      try {\n        const result = await setupMediaFolder();\n        if (result.success) {\n          updateMediaFolderDisplay();\n          setStatus(`Media folder set: ${result.path}`);\n        } else {\n          setStatus(result.error || 'Failed to set media folder');\n        }\n      } catch (err) {\n        setStatus('Error: ' + err.message);\n      } finally {\n        ui.setMediaFolderBtn.disabled = false;\n        ui.setMediaFolderBtn.textContent = 'Set Folder';\n      }\n    });\n  }\n\n  // Clear media folder button\n  if (ui.clearMediaFolderBtn) {\n    ui.clearMediaFolderBtn.addEventListener('click', () => {\n      clearMediaFolder();\n      updateMediaFolderDisplay();\n      setStatus('Media folder cleared');\n    });\n  }\n}\n\nfunction updateMediaFolderDisplay() {\n  if (ui.mediaFolderDisplay) {\n    const path = getMediaFolderPath();\n    if (path) {\n      // Truncate long paths for display\n      const displayPath = path.length > 40\n        ? '...' + path.slice(-37)\n        : path;\n      ui.mediaFolderDisplay.textContent = displayPath;\n      ui.mediaFolderDisplay.title = path; // Full path on hover\n      ui.mediaFolderDisplay.style.color = '#4a9eff';\n    } else {\n      ui.mediaFolderDisplay.textContent = 'Not configured';\n      ui.mediaFolderDisplay.title = '';\n      ui.mediaFolderDisplay.style.color = '#888';\n    }\n  }\n}\n\n// ============================================================================\n// BUILD SEQUENCE (v3.5 - Direct DOM Reconstruction)\n// ============================================================================\n\n// 1. ROBUST CONSTANTS (Fixes 'undefined' error)\n// We define these integers explicitly because UXP API versions vary on where they store Enums.\nconst ProjectItemType = {\n    CLIP: 1,\n    BIN: 2,\n    ROOT: 3,\n    FILE: 4\n};\n\n/**\n * Find the first media item in the project bin (recursively)\n * @param {Object} item - ProjectItem (usually rootItem)\n * @returns {Promise<Object|null>} First media item found or null\n */\nasync function findFirstMediaItem(item) {\n  if (!item) return null;\n\n  // Fix: Use local Constant instead of app.Constants.ProjectItemType\n  if (item.type === ProjectItemType.CLIP) {\n    return item;\n  }\n\n  // Search Children (Bin or Root)\n  // Note: item.children is a collection, check numItems\n  if (item.children && item.children.numItems > 0) {\n    for (let i = 0; i < item.children.numItems; i++) {\n      const child = item.children[i];\n      const found = await findFirstMediaItem(child);\n      if (found) return found;\n    }\n  }\n\n  return null;\n}\n\n/**\n * Build a sequence from detection data locally\n * Used for testing via console: await buildSequenceV35(silences)\n */\nasync function buildSequenceV35(silences, takes = []) {\n    if (!window.spliceBuilder) {\n        throw new Error('Builder not initialized. Check if builder.js is loaded.');\n    }\n\n    const app = require('premierepro');\n    const project = await app.Project.getActiveProject();\n    if (!project) throw new Error(\"No active project found.\");\n\n    const rootItem = await project.getRootItem();\n    const sourceProjectItem = await findFirstMediaItem(rootItem);\n\n    if (!sourceProjectItem) {\n        throw new Error('No media items found in project bin.');\n    }\n\n    console.log(`[SPLICE] Found source media: ${sourceProjectItem.name}`);\n\n    return await window.spliceBuilder.buildSequenceFromDetection(silences, takes, sourceProjectItem);\n}\n\n/**\n * Request a cut list from the backend and build the sequence.\n * Full v3.5 workflow.\n */\nasync function buildSequenceWithCutList() {\n  if (!window.spliceBuilder) {\n    throw new Error('Builder not initialized');\n  }\n\n  // Ensure BACKEND_URL is available\n  const apiUrl = (typeof BACKEND_URL !== 'undefined') ? BACKEND_URL : 'http://localhost:3000';\n\n  if (typeof showProgress === 'function') showProgress('Getting source clip info...');\n\n  const app = require('premierepro');\n  const project = await app.Project.getActiveProject();\n  if (!project) throw new Error(\"No active project.\");\n\n  const rootItem = await project.getRootItem();\n  const sourceProjectItem = await findFirstMediaItem(rootItem);\n\n  if (!sourceProjectItem) {\n    throw new Error('No media items found in project. Please import a media file first.');\n  }\n\n  // Use properties strictly\n  const sourceName = sourceProjectItem.name;\n  const sourcePath = sourceProjectItem.treePath;\n\n  const durationObj = await sourceProjectItem.getDuration();\n  const duration = durationObj ? durationObj.seconds : 0;\n\n  // Retrieve Globals\n  const silences = window.splicePreviewState?.getSelectedSilences() || (typeof safeSilences !== 'undefined' ? safeSilences : []);\n  const takes = (typeof previewTakes !== 'undefined' ? previewTakes : []);\n\n  if (silences.length === 0 && (!takes || takes.length === 0)) {\n     throw new Error('No detection data found. Run analysis first.');\n  }\n\n  if (typeof showProgress === 'function') showProgress('Generating cut list...');\n\n  const response = await fetch(`${apiUrl}/cut-list`, {\n    method: 'POST',\n    headers: getAuthHeaders(),\n    body: JSON.stringify({\n      sourceName,\n      sourcePath,\n      duration,\n      silences,\n      takes\n    })\n  });\n\n  if (!response.ok) {\n    const err = await response.json();\n    throw new Error(err.error || 'Failed to generate cut list');\n  }\n\n  const data = await response.json();\n  const cutList = data.cutList;\n\n  if (typeof showProgress === 'function') showProgress('Building sequence...');\n\n  const result = await window.spliceBuilder.buildSequenceFromCutList(cutList, sourceProjectItem);\n\n  if (!result.success) {\n    throw new Error(result.error || 'Failed to build sequence');\n  }\n\n  return {\n    success: true,\n    clipsAdded: result.clipsAdded,\n    metadata: cutList.metadata\n  };\n}\n\n// Expose functions globally for UI and testing\nif (typeof window !== 'undefined') {\n    window.buildSequenceV35 = buildSequenceV35;\n    window.buildSequenceWithCutList = buildSequenceWithCutList;\n    window.findFirstMediaItem = findFirstMediaItem;\n}\n\n// ============================================================================\n// TIMELINE SEEK (with proper error handling)\n// ============================================================================\nasync function seekToTime(seconds) {\n  // Validate input\n  if (typeof seconds !== 'number' || isNaN(seconds) || !isFinite(seconds)) {\n    setStatus('Invalid seek time');\n    return;\n  }\n\n  if (seconds < 0) {\n    seconds = 0;\n  }\n\n  try {\n    const context = await getActiveSequence();\n    if (!context) {\n      setStatus('No sequence available');\n      return;\n    }\n\n    const { sequence } = context;\n\n    // Use TickTime API to convert seconds to the format needed\n    const position = ppro.TickTime.createWithSeconds(seconds);\n\n    await sequence.setPlayerPosition(position);\n    setStatus(`Seeked to ${formatTime(seconds)}`);\n  } catch (err) {\n    console.error('[SPLICE] Seek error:', err);\n    setStatus('Seek failed: ' + (err.message || 'Unknown error'));\n  }\n}\n\n// ============================================================================\n// AUTH HEADERS HELPER\n// ============================================================================\n/**\n * Build request headers with authentication\n * @returns {Object} Headers object with Content-Type and x-stripe-customer-id\n */\nfunction getAuthHeaders() {\n  const headers = { 'Content-Type': 'application/json' };\n  const settings = getSettings();\n  if (settings.customerId) {\n    headers['x-stripe-customer-id'] = settings.customerId;\n  }\n  return headers;\n}\n\n// ============================================================================\n// AUDIO EXPORT & BACKEND CALLS\n// ============================================================================\nasync function exportAudioInternal() {\n  try {\n    const context = await getActiveSequence();\n    if (!context) {\n      throw new Error('No project or sequence open');\n    }\n\n    const { sequence } = context;\n    const encoderManager = ppro.EncoderManager.getManager();\n    if (!encoderManager) {\n      throw new Error('EncoderManager not available');\n    }\n\n    const result = await encoderManager.exportSequence(\n      sequence,\n      ppro.EncoderManager.EXPORT_IMMEDIATELY,\n      WAV_PATH,\n      WAV_PRESET_PATH,\n      true\n    );\n\n    return result;\n  } catch (err) {\n    console.error('[SPLICE] Export error:', err);\n    return false;\n  }\n}\n\nasync function isolateVocals() {\n  const response = await fetch(`${BACKEND_URL}/isolate-vocals`, {\n    method: 'POST',\n    headers: getAuthHeaders(),\n    body: JSON.stringify({ audioPath: WAV_PATH })\n  });\n\n  if (!response.ok) {\n    const err = await response.json();\n    return { success: false, error: err.error || 'Isolation request failed' };\n  }\n\n  return await response.json();\n}\n\nasync function detectSilences(audioPath, params) {\n  const response = await fetch(`${BACKEND_URL}/silences-audio`, {\n    method: 'POST',\n    headers: getAuthHeaders(),\n    body: JSON.stringify({\n      wavPath: audioPath,\n      threshold: params.dbThreshold,\n      minDuration: params.minDuration,\n      padding: params.padding\n    })\n  });\n\n  if (!response.ok) {\n    const err = await response.json();\n    throw new Error(err.error || 'Silence detection failed');\n  }\n\n  const data = await response.json();\n  if (!data.success) {\n    throw new Error(data.error || 'Silence detection failed');\n  }\n\n  // Store silences globally for razor workflow\n  currentSilences = data.silences;\n\n  return data.silences;\n}\n\nasync function transcribeAudio() {\n  const response = await fetch(`${BACKEND_URL}/analyze`, {\n    method: 'POST',\n    headers: getAuthHeaders(),\n    body: JSON.stringify({ wavPath: WAV_PATH })\n  });\n\n  if (!response.ok) {\n    const err = await response.json();\n    return { success: false, error: err.error || 'Transcription failed' };\n  }\n\n  const data = await response.json();\n  return {\n    success: data.success,\n    transcript: data.transcript,\n    takes: data.takes,\n    error: data.error\n  };\n}\n\n// ============================================================================\n// BATCH PROCESSING\n// ============================================================================\nlet batchQueue = [];\nlet currentBatchJobId = null;\n\n/**\n * Initialize batch processing handlers\n */\nfunction initBatchHandlers() {\n  const addToBatchBtn = document.getElementById('addToBatchBtn');\n  const startBatchBtn = document.getElementById('startBatchBtn');\n  const clearBatchBtn = document.getElementById('clearBatchBtn');\n  const batchSection = document.getElementById('batchSection');\n\n  // Show batch section in settings or after first detection\n  if (batchSection) {\n    batchSection.classList.remove('hidden');\n  }\n\n  // Add files to batch queue\n  if (addToBatchBtn) {\n    addToBatchBtn.addEventListener('click', async () => {\n      try {\n        const uxpFs = require('uxp').storage.localFileSystem;\n        const files = await uxpFs.getFileForOpening({\n          allowMultiple: true,\n          types: ['wav', 'mp3', 'mp4', 'mov', 'aac', 'm4a']\n        });\n\n        if (files && files.length > 0) {\n          for (const file of files) {\n            const path = file.nativePath || file.name;\n            if (!batchQueue.includes(path)) {\n              batchQueue.push(path);\n            }\n          }\n          updateBatchQueueDisplay();\n          setStatus(`Added ${files.length} file(s) to batch queue`);\n        }\n      } catch (err) {\n        setStatus('Error adding files: ' + err.message);\n      }\n    });\n  }\n\n  // Start batch processing\n  if (startBatchBtn) {\n    startBatchBtn.addEventListener('click', async () => {\n      if (batchQueue.length === 0) {\n        setStatus('No files in batch queue');\n        return;\n      }\n\n      startBatchBtn.disabled = true;\n      addToBatchBtn.disabled = true;\n\n      try {\n        await runBatchProcessing();\n      } finally {\n        startBatchBtn.disabled = false;\n        addToBatchBtn.disabled = false;\n      }\n    });\n  }\n\n  // Clear batch queue\n  if (clearBatchBtn) {\n    clearBatchBtn.addEventListener('click', () => {\n      batchQueue = [];\n      currentBatchJobId = null;\n      updateBatchQueueDisplay();\n      setStatus('Batch queue cleared');\n    });\n  }\n}\n\n/**\n * Update batch queue display\n */\nfunction updateBatchQueueDisplay() {\n  const batchEmpty = document.getElementById('batchEmpty');\n  const batchList = document.getElementById('batchList');\n  const startBatchBtn = document.getElementById('startBatchBtn');\n\n  if (batchQueue.length === 0) {\n    if (batchEmpty) batchEmpty.style.display = 'block';\n    if (batchList) batchList.innerHTML = '';\n    if (startBatchBtn) startBatchBtn.disabled = true;\n    return;\n  }\n\n  if (batchEmpty) batchEmpty.style.display = 'none';\n  if (startBatchBtn) startBatchBtn.disabled = false;\n\n  if (batchList) {\n    batchList.innerHTML = batchQueue.map((path, i) => {\n      const fileName = path.split('/').pop();\n      return `<div style=\"display: flex; justify-content: space-between; align-items: center; padding: 4px 6px; background: #2a2a2a; border-radius: 3px; margin-bottom: 2px; font-size: 9px;\">\n        <span style=\"overflow: hidden; text-overflow: ellipsis; white-space: nowrap; max-width: 180px;\" title=\"${path}\">${fileName}</span>\n        <button onclick=\"removeBatchItem(${i})\" style=\"background: transparent; border: none; color: #888; cursor: pointer; font-size: 10px;\">x</button>\n      </div>`;\n    }).join('');\n  }\n}\n\n/**\n * Remove item from batch queue\n */\nfunction removeBatchItem(index) {\n  batchQueue.splice(index, 1);\n  updateBatchQueueDisplay();\n}\n\n/**\n * Run batch processing\n */\nasync function runBatchProcessing() {\n  const batchProgress = document.getElementById('batchProgress');\n  const batchProgressBar = document.getElementById('batchProgressBar');\n  const batchProgressText = document.getElementById('batchProgressText');\n\n  if (batchProgress) batchProgress.style.display = 'block';\n\n  try {\n    // Get settings\n    const sensitivity = parseInt(document.getElementById('sensitivitySlider')?.value || 50);\n    const params = mapSensitivity(sensitivity);\n\n    // Call batch endpoint\n    const response = await fetch(`${BACKEND_URL}/batch/silences`, {\n      method: 'POST',\n      headers: getAuthHeaders(),\n      body: JSON.stringify({\n        files: batchQueue,\n        options: {\n          sensitivity,\n          threshold: params.dbThreshold,\n          minSilenceLength: params.minDuration,\n          paddingStart: params.padding,\n          paddingEnd: params.padding / 2\n        }\n      })\n    });\n\n    if (!response.ok) {\n      const err = await response.json();\n      throw new Error(err.error || 'Batch request failed');\n    }\n\n    const data = await response.json();\n    currentBatchJobId = data.jobId;\n    setStatus(`Batch job started: ${data.jobId}`);\n\n    // Poll for status\n    await pollBatchStatus(currentBatchJobId, batchProgressBar, batchProgressText);\n\n  } catch (err) {\n    setStatus('Batch error: ' + err.message);\n  } finally {\n    if (batchProgress) batchProgress.style.display = 'none';\n  }\n}\n\n/**\n * Poll batch job status\n */\nasync function pollBatchStatus(jobId, progressBar, progressText) {\n  const pollInterval = 2000; // 2 seconds\n  const maxPolls = 300; // 10 minutes max\n  let polls = 0;\n\n  while (polls < maxPolls) {\n    try {\n      const response = await fetch(`${BACKEND_URL}/batch/status/${jobId}`, {\n        headers: getAuthHeaders()\n      });\n\n      if (!response.ok) {\n        throw new Error('Failed to get batch status');\n      }\n\n      const data = await response.json();\n      const job = data.job;\n\n      // Update progress\n      if (progressBar) progressBar.value = job.progress.percentage;\n      if (progressText) progressText.textContent = `${job.progress.completed + job.progress.failed} / ${job.progress.total} files`;\n\n      // Check if complete\n      if (job.status !== 'processing') {\n        if (job.status === 'completed') {\n          setStatus(`Batch complete: ${job.progress.completed} files processed`);\n        } else if (job.status === 'completed_with_errors') {\n          setStatus(`Batch complete with ${job.progress.failed} error(s)`);\n        } else {\n          setStatus('Batch failed');\n        }\n\n        // Clear queue on completion\n        batchQueue = [];\n        updateBatchQueueDisplay();\n        return;\n      }\n\n      await new Promise(resolve => setTimeout(resolve, pollInterval));\n      polls++;\n    } catch (err) {\n      setStatus('Polling error: ' + err.message);\n      return;\n    }\n  }\n\n  setStatus('Batch timed out');\n}\n\n// Expose for inline onclick\nif (typeof window !== 'undefined') {\n  window.removeBatchItem = removeBatchItem;\n}\n\n// ============================================================================\n// CUSTOM PRESETS UI (v3.5)\n// ============================================================================\n\n/**\n * Icon mapping for display\n */\nconst PRESET_ICON_MAP = {\n  'settings': '*',\n  'mic': 'M',\n  'people': 'P',\n  'bolt': '!',\n  'school': 'E',\n  'videocam': 'V',\n  'star': 'S',\n  'heart': 'H'\n};\n\n/**\n * Initialize custom presets UI handlers\n */\nfunction initCustomPresetsUI() {\n  // Refresh preset dropdown on load\n  refreshPresetDropdown();\n\n  // Save Preset button (+)\n  if (ui.savePresetBtn) {\n    ui.savePresetBtn.addEventListener('click', () => {\n      showPresetModal();\n    });\n  }\n\n  // Manage Presets button (...)\n  if (ui.managePresetsBtn) {\n    ui.managePresetsBtn.addEventListener('click', () => {\n      showManagePresetsModal();\n    });\n  }\n\n  // Close Preset Modal\n  if (ui.closePresetModalBtn) {\n    ui.closePresetModalBtn.addEventListener('click', () => {\n      hidePresetModal();\n    });\n  }\n\n  // Close on backdrop click\n  if (ui.presetModal) {\n    ui.presetModal.addEventListener('click', (e) => {\n      if (e.target === ui.presetModal) {\n        hidePresetModal();\n      }\n    });\n  }\n\n  // Icon picker\n  if (ui.presetIconPicker) {\n    ui.presetIconPicker.addEventListener('click', (e) => {\n      const btn = e.target.closest('.preset-icon-option');\n      if (btn) {\n        selectPresetIcon(btn.dataset.icon);\n      }\n    });\n  }\n\n  // Sensitivity slider value display (Phase 3)\n  if (ui.presetSensitivity && ui.presetSensitivityValue) {\n    ui.presetSensitivity.addEventListener('input', () => {\n      ui.presetSensitivityValue.textContent = ui.presetSensitivity.value;\n    });\n  }\n\n  // Save Preset Confirm\n  if (ui.savePresetConfirmBtn) {\n    ui.savePresetConfirmBtn.addEventListener('click', () => {\n      savePresetFromModal();\n    });\n  }\n\n  // Close Manage Presets Modal\n  if (ui.closeManagePresetsBtn) {\n    ui.closeManagePresetsBtn.addEventListener('click', () => {\n      hideManagePresetsModal();\n    });\n  }\n\n  // Close on backdrop click\n  if (ui.managePresetsModal) {\n    ui.managePresetsModal.addEventListener('click', (e) => {\n      if (e.target === ui.managePresetsModal) {\n        hideManagePresetsModal();\n      }\n    });\n  }\n\n  // Event delegation for preset list actions\n  if (ui.presetsList) {\n    ui.presetsList.addEventListener('click', (e) => {\n      const editBtn = e.target.closest('.preset-edit-btn');\n      const deleteBtn = e.target.closest('.preset-delete-btn');\n      const duplicateBtn = e.target.closest('.preset-duplicate-btn');\n\n      if (editBtn) {\n        const presetId = editBtn.dataset.presetId;\n        editPreset(presetId);\n      }\n\n      if (deleteBtn) {\n        const presetId = deleteBtn.dataset.presetId;\n        const presetName = deleteBtn.dataset.presetName;\n        deletePresetWithConfirm(presetId, presetName);\n      }\n\n      if (duplicateBtn) {\n        const presetId = duplicateBtn.dataset.presetId;\n        const presetName = duplicateBtn.dataset.presetName;\n        duplicatePresetFromUI(presetId, presetName);\n      }\n    });\n  }\n\n  // Export presets button (Phase 3)\n  if (ui.exportPresetsBtn) {\n    ui.exportPresetsBtn.addEventListener('click', () => {\n      exportPresetsToFile();\n    });\n  }\n\n  // Import presets button (Phase 3)\n  if (ui.importPresetsBtn && ui.importPresetsFile) {\n    ui.importPresetsBtn.addEventListener('click', () => {\n      ui.importPresetsFile.click();\n    });\n\n    ui.importPresetsFile.addEventListener('change', (e) => {\n      if (e.target.files && e.target.files[0]) {\n        importPresetsFromFile(e.target.files[0]);\n        e.target.value = ''; // Reset for next import\n      }\n    });\n  }\n}\n\n/**\n * Refresh the preset dropdown with all presets (built-in + custom)\n */\nfunction refreshPresetDropdown() {\n  if (!ui.presetSelector) return;\n\n  const presets = getAllPresets();\n  const currentValue = ui.presetSelector.value;\n\n  // Clear existing options\n  ui.presetSelector.innerHTML = '';\n\n  // Add all presets\n  presets.forEach(preset => {\n    const option = document.createElement('option');\n    option.value = preset.id;\n\n    // Format: \"Name - Description\" or just \"Name\"\n    const desc = preset.description ? ` - ${preset.description}` : '';\n    option.textContent = preset.name + desc;\n\n    // Add custom marker for custom presets\n    if (!preset.isBuiltIn) {\n      option.textContent = `${preset.name}${desc}`;\n      option.style.color = '#28a745';\n    }\n\n    ui.presetSelector.appendChild(option);\n  });\n\n  // Restore selection if it still exists\n  if ([...ui.presetSelector.options].some(opt => opt.value === currentValue)) {\n    ui.presetSelector.value = currentValue;\n  } else {\n    ui.presetSelector.value = 'custom';\n  }\n}\n\n/**\n * Show the preset modal for creating a new preset\n */\nfunction showPresetModal(editId = null) {\n  if (!ui.presetModal) return;\n\n  // Reset form\n  ui.presetNameInput.value = '';\n  ui.presetDescInput.value = '';\n  ui.presetEditId.value = '';\n  ui.presetSelectedIcon.value = 'settings';\n  selectPresetIcon('settings');\n  hidePresetError();\n\n  // Get current settings as defaults\n  const currentSettings = getSettings();\n  const defaultSensitivity = currentSettings.sensitivity || 50;\n  const defaultThreshold = currentSettings.threshold || -32;\n\n  if (editId) {\n    // Edit mode\n    const preset = getPresetById(editId);\n    if (preset && !preset.isBuiltIn) {\n      ui.presetModalTitle.textContent = 'Edit Preset';\n      ui.presetNameInput.value = preset.name;\n      ui.presetDescInput.value = preset.description || '';\n      ui.presetEditId.value = editId;\n      ui.presetSelectedIcon.value = preset.icon || 'settings';\n      selectPresetIcon(preset.icon || 'settings');\n      ui.savePresetConfirmBtn.textContent = 'Update Preset';\n\n      // Populate settings from preset (Phase 3)\n      const presetSensitivity = preset.settings?.sensitivity ?? defaultSensitivity;\n      const presetThreshold = preset.settings?.threshold ?? defaultThreshold;\n      if (ui.presetSensitivity) {\n        ui.presetSensitivity.value = presetSensitivity;\n        if (ui.presetSensitivityValue) ui.presetSensitivityValue.textContent = presetSensitivity;\n      }\n      if (ui.presetThreshold) {\n        ui.presetThreshold.value = presetThreshold;\n      }\n    }\n  } else {\n    // Create mode - use current settings from main UI\n    ui.presetModalTitle.textContent = 'Save Preset';\n    ui.savePresetConfirmBtn.textContent = 'Save Preset';\n\n    // Get sensitivity from main slider if available\n    const mainSensitivity = ui.sensitivitySlider?.value ?? defaultSensitivity;\n    if (ui.presetSensitivity) {\n      ui.presetSensitivity.value = mainSensitivity;\n      if (ui.presetSensitivityValue) ui.presetSensitivityValue.textContent = mainSensitivity;\n    }\n    if (ui.presetThreshold) {\n      ui.presetThreshold.value = defaultThreshold;\n    }\n  }\n\n  ui.presetModal.classList.remove('hidden');\n  ui.presetNameInput.focus();\n}\n\n/**\n * Hide the preset modal\n */\nfunction hidePresetModal() {\n  if (ui.presetModal) {\n    ui.presetModal.classList.add('hidden');\n  }\n}\n\n/**\n * Select an icon in the preset modal\n */\nfunction selectPresetIcon(iconName) {\n  if (!ui.presetIconPicker || !ui.presetSelectedIcon) return;\n\n  // Update hidden input\n  ui.presetSelectedIcon.value = iconName;\n\n  // Update visual selection\n  ui.presetIconPicker.querySelectorAll('.preset-icon-option').forEach(btn => {\n    if (btn.dataset.icon === iconName) {\n      btn.classList.add('selected');\n    } else {\n      btn.classList.remove('selected');\n    }\n  });\n}\n\n/**\n * Show error message in preset modal\n */\nfunction showPresetError(message) {\n  if (ui.presetModalError) {\n    ui.presetModalError.textContent = message;\n    ui.presetModalError.style.display = 'block';\n  }\n}\n\n/**\n * Hide error message in preset modal\n */\nfunction hidePresetError() {\n  if (ui.presetModalError) {\n    ui.presetModalError.style.display = 'none';\n  }\n}\n\n/**\n * Save preset from modal (create or update)\n */\nfunction savePresetFromModal() {\n  const name = ui.presetNameInput?.value?.trim();\n  const description = ui.presetDescInput?.value?.trim() || '';\n  const icon = ui.presetSelectedIcon?.value || 'settings';\n  const editId = ui.presetEditId?.value;\n\n  if (!name) {\n    showPresetError('Please enter a preset name');\n    return;\n  }\n\n  // Capture settings from modal inputs (Phase 3)\n  const sensitivityRaw = parseInt(ui.presetSensitivity?.value);\n  const thresholdRaw = parseInt(ui.presetThreshold?.value);\n  const sensitivity = isNaN(sensitivityRaw) ? 50 : sensitivityRaw;\n  const threshold = isNaN(thresholdRaw) ? -32 : thresholdRaw;\n\n  // Get base settings and override with modal values\n  const baseSettings = getCurrentPresetSettings();\n  const settings = {\n    ...baseSettings,\n    sensitivity,\n    threshold\n  };\n\n  let result;\n\n  if (editId) {\n    // Update existing preset (including settings)\n    result = updateCustomPreset(editId, {\n      name,\n      description,\n      icon,\n      settings\n    });\n\n    if (result.success) {\n      setStatus(`Preset \"${name}\" updated`);\n    }\n  } else {\n    // Create new preset with modal settings\n    result = createCustomPreset({\n      name,\n      description,\n      icon,\n      settings\n    });\n\n    if (result.success) {\n      setStatus(`Preset \"${name}\" saved`);\n      // Select the newly created preset\n      refreshPresetDropdown();\n      if (ui.presetSelector) {\n        ui.presetSelector.value = result.id;\n      }\n    }\n  }\n\n  if (result.success) {\n    hidePresetModal();\n    refreshPresetDropdown();\n  } else {\n    showPresetError(result.error || 'Failed to save preset');\n  }\n}\n\n/**\n * Show the manage presets modal\n */\nfunction showManagePresetsModal() {\n  if (!ui.managePresetsModal) return;\n\n  renderPresetsList();\n  ui.managePresetsModal.classList.remove('hidden');\n}\n\n/**\n * Hide the manage presets modal\n */\nfunction hideManagePresetsModal() {\n  if (ui.managePresetsModal) {\n    ui.managePresetsModal.classList.add('hidden');\n  }\n}\n\n/**\n * Render the presets list in the manage modal\n */\nfunction renderPresetsList() {\n  if (!ui.presetsList) return;\n\n  const presets = getAllPresets();\n  const customPresets = presets.filter(p => !p.isBuiltIn);\n\n  // Show/hide empty state\n  if (ui.noCustomPresets) {\n    ui.noCustomPresets.style.display = customPresets.length === 0 ? 'block' : 'none';\n  }\n\n  // Build HTML\n  const fragment = document.createDocumentFragment();\n\n  presets.forEach(preset => {\n    const div = document.createElement('div');\n    div.className = `preset-list-item ${preset.isBuiltIn ? 'built-in' : 'custom'}`;\n\n    const iconChar = PRESET_ICON_MAP[preset.icon] || '*';\n    const desc = preset.description || (preset.isBuiltIn ? 'Built-in preset' : 'Custom preset');\n\n    if (preset.isBuiltIn) {\n      // Built-in presets: only duplicate button (Phase 3)\n      div.innerHTML = `\n        <div class=\"preset-list-item-icon\">${iconChar}</div>\n        <div class=\"preset-list-item-info\">\n          <div class=\"preset-list-item-name\">${preset.name}</div>\n          <div class=\"preset-list-item-desc\">${desc}</div>\n        </div>\n        <div class=\"preset-list-item-actions\">\n          <button class=\"preset-duplicate-btn\" data-preset-id=\"${preset.id}\" data-preset-name=\"${preset.name}\" title=\"Duplicate preset\">D</button>\n        </div>\n      `;\n    } else {\n      // Custom presets: duplicate/edit/delete buttons (Phase 3)\n      div.innerHTML = `\n        <div class=\"preset-list-item-icon\">${iconChar}</div>\n        <div class=\"preset-list-item-info\">\n          <div class=\"preset-list-item-name\">${preset.name}</div>\n          <div class=\"preset-list-item-desc\">${desc}</div>\n        </div>\n        <div class=\"preset-list-item-actions\">\n          <button class=\"preset-duplicate-btn\" data-preset-id=\"${preset.id}\" data-preset-name=\"${preset.name}\" title=\"Duplicate preset\">D</button>\n          <button class=\"preset-edit-btn\" data-preset-id=\"${preset.id}\" title=\"Edit preset\">E</button>\n          <button class=\"preset-delete-btn delete\" data-preset-id=\"${preset.id}\" data-preset-name=\"${preset.name}\" title=\"Delete preset\">X</button>\n        </div>\n      `;\n    }\n\n    fragment.appendChild(div);\n  });\n\n  ui.presetsList.innerHTML = '';\n  ui.presetsList.appendChild(fragment);\n}\n\n/**\n * Edit a preset\n */\nfunction editPreset(presetId) {\n  hideManagePresetsModal();\n  showPresetModal(presetId);\n}\n\n/**\n * Delete a preset with confirmation\n */\nfunction deletePresetWithConfirm(presetId, presetName) {\n  // Simple confirmation (UXP doesn't have native confirm)\n  const confirmed = confirm(`Delete preset \"${presetName}\"?`);\n  if (!confirmed) return;\n\n  const result = deleteCustomPreset(presetId);\n\n  if (result.success) {\n    setStatus(`Preset \"${presetName}\" deleted`);\n    renderPresetsList();\n    refreshPresetDropdown();\n  } else {\n    setStatus('Failed to delete preset: ' + (result.error || 'Unknown error'));\n  }\n}\n\n/**\n * Duplicate a preset from the UI (Phase 3)\n */\nfunction duplicatePresetFromUI(presetId, presetName) {\n  const result = duplicatePreset(presetId);\n\n  if (result.success) {\n    setStatus(`Created copy of \"${presetName}\"`);\n    renderPresetsList();\n    refreshPresetDropdown();\n\n    // Select the new copy in dropdown\n    if (ui.presetSelector && result.id) {\n      ui.presetSelector.value = result.id;\n    }\n  } else {\n    setStatus('Failed to duplicate preset: ' + (result.error || 'Unknown error'));\n  }\n}\n\n/**\n * Export presets to a JSON file (Phase 3)\n */\nasync function exportPresetsToFile() {\n  const result = exportPresets();\n\n  if (!result.success) {\n    setStatus(result.error || 'No presets to export');\n    return;\n  }\n\n  try {\n    // Create a blob and download it\n    const blob = new Blob([result.data], { type: 'application/json' });\n    const filename = `splice-presets-${new Date().toISOString().slice(0, 10)}.json`;\n\n    // UXP approach: use file system API\n    const uxpFs = require('uxp').storage.localFileSystem;\n    const file = await uxpFs.getFileForSaving(filename, { types: ['json'] });\n\n    if (file) {\n      await file.write(result.data);\n      setStatus(`Exported ${result.count} preset(s) to ${file.name}`);\n    } else {\n      setStatus('Export cancelled');\n    }\n  } catch (err) {\n    console.error('[SPLICE] Export error:', err);\n    setStatus('Export failed: ' + err.message);\n  }\n}\n\n/**\n * Import presets from a JSON file (Phase 3)\n */\nasync function importPresetsFromFile(file) {\n  try {\n    // Read file contents\n    const reader = new FileReader();\n\n    reader.onload = (e) => {\n      const jsonString = e.target.result;\n      const result = importPresets(jsonString, true);\n\n      if (result.success) {\n        setStatus(`Imported ${result.imported} preset(s)${result.skipped > 0 ? `, ${result.skipped} skipped` : ''}`);\n        renderPresetsList();\n        refreshPresetDropdown();\n      } else {\n        setStatus('Import failed: ' + (result.error || 'Unknown error'));\n      }\n    };\n\n    reader.onerror = () => {\n      setStatus('Failed to read file');\n    };\n\n    reader.readAsText(file);\n  } catch (err) {\n    console.error('[SPLICE] Import error:', err);\n    setStatus('Import failed: ' + err.message);\n  }\n}\n\n/**\n * Apply a preset and update UI controls\n * Called when user selects a preset from dropdown\n */\nfunction applyPresetAndUpdateUI(presetId) {\n  const preset = getPresetById(presetId);\n  if (!preset) return;\n\n  // For custom presets, apply the settings\n  if (!preset.isBuiltIn) {\n    const settings = preset.settings;\n    if (settings) {\n      // Update sensitivity slider\n      if (ui.sensitivitySlider && settings.sensitivity !== undefined) {\n        ui.sensitivitySlider.value = settings.sensitivity;\n      }\n\n      // Update takes detection checkbox\n      if (ui.enableTakesDetection && settings.enableTakesDetection !== undefined) {\n        ui.enableTakesDetection.checked = settings.enableTakesDetection;\n      }\n\n      // Update auto-mark best checkbox\n      if (ui.autoMarkBest && settings.autoMarkBest !== undefined) {\n        ui.autoMarkBest.checked = settings.autoMarkBest;\n      }\n\n      // Save as active preset\n      saveSettings({ activePreset: presetId, ...settings });\n    }\n  } else {\n    // Built-in preset: use applyPreset from settings.js\n    const appliedSettings = applyPreset(presetId);\n\n    // Update sensitivity slider to match preset\n    if (ui.sensitivitySlider && appliedSettings.sensitivity !== undefined) {\n      ui.sensitivitySlider.value = appliedSettings.sensitivity;\n    }\n\n    // Update takes detection checkbox\n    if (ui.enableTakesDetection && appliedSettings.enableTakesDetection !== undefined) {\n      ui.enableTakesDetection.checked = appliedSettings.enableTakesDetection;\n    }\n\n    // Update auto-mark best checkbox\n    if (ui.autoMarkBest && appliedSettings.autoMarkBest !== undefined) {\n      ui.autoMarkBest.checked = appliedSettings.autoMarkBest;\n    }\n  }\n\n  setStatus(`Applied preset: ${preset.name}`);\n}\n","numLines":2332,"startLine":1,"totalLines":2332}}}
{"parentUuid":"311f2001-6a62-4efe-a061-81e2fb0e792f","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01T4TyP6R2YcRt5MUBPDAhLd","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me look at the remaining services and plugin files for a complete analysis:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":14962,"cache_creation_input_tokens":55373,"cache_read_input_tokens":43424,"cache_creation":{"ephemeral_5m_input_tokens":55373,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCP55hqoxaRLMERMxZa","type":"assistant","uuid":"58e20b8c-3a5b-45e6-9f40-640cff5de5b8","timestamp":"2025-12-25T22:30:37.843Z"}
{"parentUuid":"58e20b8c-3a5b-45e6-9f40-640cff5de5b8","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01T4TyP6R2YcRt5MUBPDAhLd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01FDK9oi5hDZ4pJg2oDJ5vAV","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":14962,"cache_creation_input_tokens":55373,"cache_read_input_tokens":43424,"cache_creation":{"ephemeral_5m_input_tokens":55373,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCP55hqoxaRLMERMxZa","type":"assistant","uuid":"90c364df-ae72-40a3-ac11-4e79d32a2814","timestamp":"2025-12-25T22:30:39.157Z"}
{"parentUuid":"90c364df-ae72-40a3-ac11-4e79d32a2814","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01T4TyP6R2YcRt5MUBPDAhLd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019UvyZJEFrWE8fCtpFVw8vQ","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-plugin/js/settings.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":14962,"cache_creation_input_tokens":55373,"cache_read_input_tokens":43424,"cache_creation":{"ephemeral_5m_input_tokens":55373,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCP55hqoxaRLMERMxZa","type":"assistant","uuid":"95064747-dc7b-4536-8314-f8606217ef55","timestamp":"2025-12-25T22:30:39.828Z"}
{"parentUuid":"95064747-dc7b-4536-8314-f8606217ef55","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01T4TyP6R2YcRt5MUBPDAhLd","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01XouPA1GjvLqGs4VTcvyFwj","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/middleware/rateLimiter.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":14962,"cache_creation_input_tokens":55373,"cache_read_input_tokens":43424,"cache_creation":{"ephemeral_5m_input_tokens":55373,"ephemeral_1h_input_tokens":0},"output_tokens":203,"service_tier":"standard"}},"requestId":"req_011CWUCP55hqoxaRLMERMxZa","type":"assistant","uuid":"2e6bd431-43d8-41ae-920e-a23cd7c04f55","timestamp":"2025-12-25T22:30:40.483Z"}
{"parentUuid":"2e6bd431-43d8-41ae-920e-a23cd7c04f55","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01FDK9oi5hDZ4pJg2oDJ5vAV","type":"tool_result","content":"     1→/**\n     2→ * Multitrack/Multicam Analysis Service\n     3→ *\n     4→ * Analyzes multiple audio tracks to determine optimal video angle selection.\n     5→ * Based on Fireside's proven multicam editing approach.\n     6→ *\n     7→ * Features:\n     8→ * - Per-speaker RMS audio level analysis\n     9→ * - Noise floor subtraction for cleaner detection\n    10→ * - Automatic video angle selection based on loudest speaker\n    11→ * - Wide shot detection (multiple speakers talking)\n    12→ * - Cutaway insertion for long clips\n    13→ * - Speaker screentime balancing\n    14→ * - Gaussian smoothing to reduce spurious cuts\n    15→ */\n    16→\n    17→const { exec } = require('child_process');\n    18→const { promisify } = require('util');\n    19→const fs = require('fs');\n    20→const path = require('path');\n    21→\n    22→const execAsync = promisify(exec);\n    23→\n    24→// =============================================================================\n    25→// Configuration\n    26→// =============================================================================\n    27→\n    28→const DEFAULT_OPTIONS = {\n    29→  // Analysis parameters\n    30→  analysisWindowMs: 100,       // RMS window size in milliseconds\n    31→  smoothingWindow: 5,          // Gaussian blur kernel size (frames)\n    32→\n    33→  // Switching parameters\n    34→  switchingFrequency: 50,      // How often to allow cuts (0-100)\n    35→  minShotDuration: 2.0,        // Minimum seconds before next cut\n    36→  speakerBoosts: {},           // Per-speaker dB adjustments\n    37→\n    38→  // Wide shot parameters\n    39→  wideShotEnabled: true,\n    40→  wideShotPercentage: 20,      // Target % of time in wide shots\n    41→  wideShotTolerance: 6,        // dB tolerance for multiple speaker detection\n    42→\n    43→  // Cutaway parameters\n    44→  cutawayEnabled: false,\n    45→  cutawayMinDuration: 1.5,\n    46→  cutawayMaxDuration: 4.0,\n    47→  cutawayTracks: [],           // Video track indices for cutaways\n    48→\n    49→  // Output\n    50→  frameRate: 30\n    51→};\n    52→\n    53→// =============================================================================\n    54→// Core Analysis\n    55→// =============================================================================\n    56→\n    57→/**\n    58→ * Analyze multiple audio tracks for multicam editing\n    59→ *\n    60→ * @param {Array<string>} audioPaths - Paths to audio files (one per speaker)\n    61→ * @param {Object} options - Analysis options\n    62→ * @returns {Promise<Object>} Analysis results with switching decisions\n    63→ */\n    64→async function analyzeMultitrack(audioPaths, options = {}) {\n    65→  const opts = { ...DEFAULT_OPTIONS, ...options };\n    66→\n    67→  console.log(`[SPLICE Multitrack] Analyzing ${audioPaths.length} audio track(s)`);\n    68→\n    69→  // Step 1: Extract RMS levels for each track\n    70→  const trackLevels = await Promise.all(\n    71→    audioPaths.map((audioPath, i) =>\n    72→      extractTrackLevels(audioPath, {\n    73→        windowMs: opts.analysisWindowMs,\n    74→        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`\n    75→      })\n    76→    )\n    77→  );\n    78→\n    79→  // Validate all tracks have same duration\n    80→  const durations = trackLevels.map(t => t.duration);\n    81→  const maxDuration = Math.max(...durations);\n    82→  const minDuration = Math.min(...durations);\n    83→  if (maxDuration - minDuration > 1) {\n    84→    console.warn('[SPLICE Multitrack] Track durations differ significantly');\n    85→  }\n    86→\n    87→  // Step 2: Calculate noise floor and clean up levels\n    88→  const cleanedLevels = cleanupAudioLevels(trackLevels, opts);\n    89→\n    90→  // Step 3: Apply speaker boosts\n    91→  const boostedLevels = applySpeakerBoosts(cleanedLevels, opts.speakerBoosts);\n    92→\n    93→  // Step 4: Apply Gaussian smoothing\n    94→  const smoothedLevels = opts.smoothingWindow > 0\n    95→    ? smoothAudioLevels(boostedLevels, opts.smoothingWindow)\n    96→    : boostedLevels;\n    97→\n    98→  // Step 5: Generate switching decisions\n    99→  const decisions = generateSwitchingDecisions(smoothedLevels, {\n   100→    videoTrackMapping: opts.videoTrackMapping,\n   101→    minShotDuration: opts.minShotDuration,\n   102→    switchingFrequency: opts.switchingFrequency,\n   103→    wideShotEnabled: opts.wideShotEnabled,\n   104→    wideShotTolerance: opts.wideShotTolerance,\n   105→    wideShotTracks: opts.wideShotTracks,\n   106→    frameRate: opts.frameRate\n   107→  });\n   108→\n   109→  // Step 6: Fine-tune wide shot percentage\n   110→  const tunedDecisions = opts.wideShotEnabled\n   111→    ? tuneWideShotPercentage(decisions, opts.wideShotPercentage)\n   112→    : decisions;\n   113→\n   114→  // Step 7: Insert cutaways if enabled\n   115→  const finalDecisions = opts.cutawayEnabled\n   116→    ? insertCutaways(tunedDecisions, {\n   117→        cutawayTracks: opts.cutawayTracks,\n   118→        minDuration: opts.cutawayMinDuration,\n   119→        maxDuration: opts.cutawayMaxDuration\n   120→      })\n   121→    : tunedDecisions;\n   122→\n   123→  // Step 8: Calculate statistics\n   124→  const stats = calculateMultitrackStats(finalDecisions, trackLevels[0].duration);\n   125→\n   126→  console.log(`[SPLICE Multitrack] Generated ${finalDecisions.length} switching decision(s)`);\n   127→\n   128→  return {\n   129→    decisions: finalDecisions,\n   130→    metadata: {\n   131→      trackCount: audioPaths.length,\n   132→      speakerNames: opts.speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n   133→      duration: maxDuration,\n   134→      frameRate: opts.frameRate,\n   135→      ...stats\n   136→    },\n   137→    levels: {\n   138→      raw: trackLevels.map(t => ({ speaker: t.speakerName, sampleCount: t.levels.length })),\n   139→      // Don't include full level data in response (too large)\n   140→    }\n   141→  };\n   142→}\n   143→\n   144→// =============================================================================\n   145→// Audio Level Extraction\n   146→// =============================================================================\n   147→\n   148→/**\n   149→ * Extract RMS levels from audio file\n   150→ *\n   151→ * @param {string} audioPath - Path to audio file\n   152→ * @param {Object} options - Extraction options\n   153→ * @returns {Promise<Object>} Track levels with timing\n   154→ */\n   155→async function extractTrackLevels(audioPath, options = {}) {\n   156→  const { windowMs = 100, speakerName = 'Unknown' } = options;\n   157→\n   158→  console.log(`[SPLICE Multitrack] Extracting levels: ${speakerName}`);\n   159→\n   160→  // Get audio info\n   161→  const infoCmd = `ffprobe -v error -select_streams a:0 -show_entries stream=sample_rate,duration -of json \"${audioPath}\"`;\n   162→  let sampleRate = 48000;\n   163→  let duration = 0;\n   164→\n   165→  try {\n   166→    const { stdout } = await execAsync(infoCmd);\n   167→    const info = JSON.parse(stdout);\n   168→    if (info.streams?.[0]) {\n   169→      sampleRate = parseInt(info.streams[0].sample_rate) || 48000;\n   170→      duration = parseFloat(info.streams[0].duration) || 0;\n   171→    }\n   172→  } catch (err) {\n   173→    console.warn(`[SPLICE Multitrack] Could not get audio info: ${err.message}`);\n   174→  }\n   175→\n   176→  // Get duration from format if not in stream\n   177→  if (!duration) {\n   178→    try {\n   179→      const durationCmd = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"${audioPath}\"`;\n   180→      const { stdout } = await execAsync(durationCmd);\n   181→      duration = parseFloat(stdout.trim()) || 0;\n   182→    } catch (err) {\n   183→      console.warn(`[SPLICE Multitrack] Could not get duration: ${err.message}`);\n   184→    }\n   185→  }\n   186→\n   187→  // Extract audio and calculate RMS per window\n   188→  const tempFile = path.join('/tmp', `splice_multitrack_${Date.now()}_${Math.random().toString(36).slice(2)}.raw`);\n   189→\n   190→  try {\n   191→    // Convert to mono, 16kHz for faster processing\n   192→    const extractCmd = `ffmpeg -y -i \"${audioPath}\" -ac 1 -ar 16000 -f s16le -acodec pcm_s16le \"${tempFile}\" 2>/dev/null`;\n   193→    await execAsync(extractCmd, { maxBuffer: 100 * 1024 * 1024 });\n   194→\n   195→    const rawData = fs.readFileSync(tempFile);\n   196→    const samples = new Float32Array(rawData.length / 2);\n   197→    for (let i = 0; i < samples.length; i++) {\n   198→      samples[i] = rawData.readInt16LE(i * 2) / 32768.0;\n   199→    }\n   200→\n   201→    // Calculate RMS per window\n   202→    const windowSamples = Math.floor((windowMs / 1000) * 16000);\n   203→    const hopSamples = windowSamples; // Non-overlapping windows\n   204→    const numWindows = Math.floor(samples.length / hopSamples);\n   205→\n   206→    const levels = new Float32Array(numWindows);\n   207→    const times = new Float32Array(numWindows);\n   208→\n   209→    for (let i = 0; i < numWindows; i++) {\n   210→      const start = i * hopSamples;\n   211→      const end = Math.min(start + windowSamples, samples.length);\n   212→\n   213→      let sumSquares = 0;\n   214→      for (let j = start; j < end; j++) {\n   215→        sumSquares += samples[j] * samples[j];\n   216→      }\n   217→\n   218→      const rms = Math.sqrt(sumSquares / (end - start));\n   219→      const dbfs = rms > 0 ? 20 * Math.log10(rms) : -100;\n   220→\n   221→      levels[i] = Math.max(-100, dbfs);\n   222→      times[i] = (start / 16000);\n   223→    }\n   224→\n   225→    fs.unlinkSync(tempFile);\n   226→\n   227→    return { levels, times, duration, speakerName, sampleRate: 16000 };\n   228→  } catch (err) {\n   229→    if (fs.existsSync(tempFile)) fs.unlinkSync(tempFile);\n   230→    throw new Error(`Failed to extract track levels: ${err.message}`);\n   231→  }\n   232→}\n   233→\n   234→// =============================================================================\n   235→// Level Processing\n   236→// =============================================================================\n   237→\n   238→/**\n   239→ * Clean up audio levels by subtracting noise floor\n   240→ *\n   241→ * @param {Array} trackLevels - Array of track level objects\n   242→ * @param {Object} options - Cleanup options\n   243→ * @returns {Array} Cleaned level arrays\n   244→ */\n   245→function cleanupAudioLevels(trackLevels, options = {}) {\n   246→  const numTracks = trackLevels.length;\n   247→  const numSamples = trackLevels[0].levels.length;\n   248→\n   249→  // Calculate average level (noise floor estimate)\n   250→  const averageLevels = new Float32Array(numSamples);\n   251→  for (let i = 0; i < numSamples; i++) {\n   252→    let sum = 0;\n   253→    for (let t = 0; t < numTracks; t++) {\n   254→      sum += trackLevels[t].levels[i];\n   255→    }\n   256→    averageLevels[i] = sum / numTracks;\n   257→  }\n   258→\n   259→  // Subtract average from each track (relative loudness)\n   260→  const cleaned = trackLevels.map(track => {\n   261→    const cleanedLevels = new Float32Array(numSamples);\n   262→    for (let i = 0; i < numSamples; i++) {\n   263→      // Normalize to -100 to 0 range\n   264→      cleanedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] - averageLevels[i]));\n   265→    }\n   266→    return {\n   267→      ...track,\n   268→      levels: cleanedLevels\n   269→    };\n   270→  });\n   271→\n   272→  return cleaned;\n   273→}\n   274→\n   275→/**\n   276→ * Apply per-speaker dB boosts\n   277→ *\n   278→ * @param {Array} trackLevels - Cleaned track levels\n   279→ * @param {Object} boosts - Speaker boost mapping { \"Speaker 1\": 5, ... }\n   280→ * @returns {Array} Boosted levels\n   281→ */\n   282→function applySpeakerBoosts(trackLevels, boosts = {}) {\n   283→  return trackLevels.map(track => {\n   284→    const boost = boosts[track.speakerName] || 0;\n   285→    if (boost === 0) return track;\n   286→\n   287→    const boostedLevels = new Float32Array(track.levels.length);\n   288→    for (let i = 0; i < track.levels.length; i++) {\n   289→      boostedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] + boost));\n   290→    }\n   291→\n   292→    return { ...track, levels: boostedLevels };\n   293→  });\n   294→}\n   295→\n   296→/**\n   297→ * Apply Gaussian smoothing to reduce noise\n   298→ *\n   299→ * @param {Array} trackLevels - Track levels\n   300→ * @param {number} windowSize - Gaussian kernel size\n   301→ * @returns {Array} Smoothed levels\n   302→ */\n   303→function smoothAudioLevels(trackLevels, windowSize = 5) {\n   304→  const kernel = generateGaussianKernel(windowSize);\n   305→\n   306→  return trackLevels.map(track => {\n   307→    const smoothed = applyConvolution(track.levels, kernel);\n   308→    return { ...track, levels: smoothed };\n   309→  });\n   310→}\n   311→\n   312→/**\n   313→ * Generate Gaussian kernel\n   314→ */\n   315→function generateGaussianKernel(size) {\n   316→  const sigma = size / 3;\n   317→  const kernel = new Float32Array(size);\n   318→  const half = Math.floor(size / 2);\n   319→  let sum = 0;\n   320→\n   321→  for (let i = 0; i < size; i++) {\n   322→    const x = i - half;\n   323→    kernel[i] = Math.exp(-(x * x) / (2 * sigma * sigma));\n   324→    sum += kernel[i];\n   325→  }\n   326→\n   327→  // Normalize\n   328→  for (let i = 0; i < size; i++) {\n   329→    kernel[i] /= sum;\n   330→  }\n   331→\n   332→  return kernel;\n   333→}\n   334→\n   335→/**\n   336→ * Apply 1D convolution\n   337→ */\n   338→function applyConvolution(data, kernel) {\n   339→  const result = new Float32Array(data.length);\n   340→  const half = Math.floor(kernel.length / 2);\n   341→\n   342→  for (let i = 0; i < data.length; i++) {\n   343→    let sum = 0;\n   344→    for (let j = 0; j < kernel.length; j++) {\n   345→      const idx = i + j - half;\n   346→      if (idx >= 0 && idx < data.length) {\n   347→        sum += data[idx] * kernel[j];\n   348→      }\n   349→    }\n   350→    result[i] = sum;\n   351→  }\n   352→\n   353→  return result;\n   354→}\n   355→\n   356→// =============================================================================\n   357→// Switching Decision Generation\n   358→// =============================================================================\n   359→\n   360→/**\n   361→ * Generate video switching decisions based on audio levels\n   362→ *\n   363→ * @param {Array} trackLevels - Processed track levels\n   364→ * @param {Object} options - Decision options\n   365→ * @returns {Array} Switching decisions\n   366→ */\n   367→function generateSwitchingDecisions(trackLevels, options = {}) {\n   368→  const {\n   369→    videoTrackMapping = {},\n   370→    minShotDuration = 2.0,\n   371→    switchingFrequency = 50,\n   372→    wideShotEnabled = true,\n   373→    wideShotTolerance = 6,\n   374→    wideShotTracks = [],\n   375→    frameRate = 30\n   376→  } = options;\n   377→\n   378→  const numSamples = trackLevels[0].levels.length;\n   379→  const times = trackLevels[0].times;\n   380→  const decisions = [];\n   381→\n   382→  // Calculate min samples between cuts based on switching frequency\n   383→  const minSamplesPerCut = Math.ceil(minShotDuration / (times[1] - times[0]));\n   384→  const switchingCooldown = Math.ceil(minSamplesPerCut * (1 - switchingFrequency / 100));\n   385→\n   386→  let currentSpeaker = -1;\n   387→  let currentDecisionStart = 0;\n   388→  let lastSwitchSample = 0;\n   389→\n   390→  for (let i = 0; i < numSamples; i++) {\n   391→    // Find loudest speaker\n   392→    let maxLevel = -100;\n   393→    let loudestSpeaker = 0;\n   394→    let speakersAboveThreshold = [];\n   395→\n   396→    for (let t = 0; t < trackLevels.length; t++) {\n   397→      const level = trackLevels[t].levels[i];\n   398→      if (level > maxLevel) {\n   399→        maxLevel = level;\n   400→        loudestSpeaker = t;\n   401→      }\n   402→\n   403→      // Track speakers within tolerance of max (for wide shot detection)\n   404→      if (wideShotEnabled && level >= maxLevel - wideShotTolerance) {\n   405→        speakersAboveThreshold.push(t);\n   406→      }\n   407→    }\n   408→\n   409→    // Determine if this should be a wide shot\n   410→    const isWideShot = wideShotEnabled && speakersAboveThreshold.length >= 2;\n   411→    const effectiveSpeaker = isWideShot ? -1 : loudestSpeaker; // -1 = wide shot\n   412→\n   413→    // Check if we should switch\n   414→    const canSwitch = (i - lastSwitchSample) >= switchingCooldown;\n   415→    const shouldSwitch = canSwitch && effectiveSpeaker !== currentSpeaker;\n   416→\n   417→    if (shouldSwitch || i === 0) {\n   418→      // Close previous decision\n   419→      if (i > 0 && decisions.length > 0) {\n   420→        decisions[decisions.length - 1].endTime = times[i];\n   421→        decisions[decisions.length - 1].endSample = i;\n   422→      }\n   423→\n   424→      // Start new decision\n   425→      const speakerName = isWideShot\n   426→        ? 'Wide Shot'\n   427→        : trackLevels[loudestSpeaker].speakerName;\n   428→\n   429→      const videoTrack = isWideShot\n   430→        ? (wideShotTracks[0] ?? 0)\n   431→        : (videoTrackMapping[loudestSpeaker] ?? loudestSpeaker);\n   432→\n   433→      decisions.push({\n   434→        speakerIndex: effectiveSpeaker,\n   435→        speakerName,\n   436→        speakers: speakersAboveThreshold.map(s => trackLevels[s].speakerName),\n   437→        videoTrack,\n   438→        startTime: times[i],\n   439→        startSample: i,\n   440→        endTime: times[numSamples - 1],\n   441→        endSample: numSamples - 1,\n   442→        isWideShot,\n   443→        reason: isWideShot ? 'Multiple speakers' : 'Loudest speaker'\n   444→      });\n   445→\n   446→      currentSpeaker = effectiveSpeaker;\n   447→      lastSwitchSample = i;\n   448→    }\n   449→  }\n   450→\n   451→  // Finalize last decision\n   452→  if (decisions.length > 0) {\n   453→    decisions[decisions.length - 1].endTime = times[numSamples - 1];\n   454→    decisions[decisions.length - 1].endSample = numSamples - 1;\n   455→  }\n   456→\n   457→  // Add duration to each decision\n   458→  return decisions.map(d => ({\n   459→    ...d,\n   460→    duration: parseFloat((d.endTime - d.startTime).toFixed(3))\n   461→  }));\n   462→}\n   463→\n   464→// =============================================================================\n   465→// Wide Shot Tuning\n   466→// =============================================================================\n   467→\n   468→/**\n   469→ * Fine-tune wide shot percentage using tolerance adjustment\n   470→ *\n   471→ * @param {Array} decisions - Initial decisions\n   472→ * @param {number} targetPercentage - Target wide shot percentage\n   473→ * @returns {Array} Tuned decisions\n   474→ */\n   475→function tuneWideShotPercentage(decisions, targetPercentage) {\n   476→  const totalDuration = decisions.reduce((sum, d) => sum + d.duration, 0);\n   477→  const currentWideTime = decisions\n   478→    .filter(d => d.isWideShot)\n   479→    .reduce((sum, d) => sum + d.duration, 0);\n   480→  const currentPercentage = (currentWideTime / totalDuration) * 100;\n   481→\n   482→  // If within 5% of target, don't adjust\n   483→  if (Math.abs(currentPercentage - targetPercentage) <= 5) {\n   484→    return decisions;\n   485→  }\n   486→\n   487→  console.log(`[SPLICE Multitrack] Wide shot: ${currentPercentage.toFixed(1)}% (target: ${targetPercentage}%)`);\n   488→\n   489→  // Simple approach: mark some single-speaker decisions as wide shots\n   490→  // or vice versa based on whether we need more or fewer wide shots\n   491→  // (Full implementation would re-run with adjusted tolerance)\n   492→\n   493→  return decisions; // Return as-is for now\n   494→}\n   495→\n   496→// =============================================================================\n   497→// Cutaway Insertion\n   498→// =============================================================================\n   499→\n   500→/**\n   501→ * Insert cutaways into long clips\n   502→ *\n   503→ * @param {Array} decisions - Switching decisions\n   504→ * @param {Object} options - Cutaway options\n   505→ * @returns {Array} Decisions with cutaways inserted\n   506→ */\n   507→function insertCutaways(decisions, options = {}) {\n   508→  const {\n   509→    cutawayTracks = [],\n   510→    minDuration = 1.5,\n   511→    maxDuration = 4.0,\n   512→    maxClipDuration = 30 // Insert cutaway if clip longer than this\n   513→  } = options;\n   514→\n   515→  if (cutawayTracks.length === 0) return decisions;\n   516→\n   517→  const result = [];\n   518→\n   519→  for (const decision of decisions) {\n   520→    if (decision.duration <= maxClipDuration) {\n   521→      result.push(decision);\n   522→      continue;\n   523→    }\n   524→\n   525→    // Split long clip with cutaways\n   526→    const numCutaways = Math.floor(decision.duration / maxClipDuration);\n   527→    const interval = decision.duration / (numCutaways + 1);\n   528→\n   529→    let currentStart = decision.startTime;\n   530→\n   531→    for (let i = 0; i < numCutaways; i++) {\n   532→      // Add main clip segment\n   533→      const cutawayStart = currentStart + interval - minDuration / 2;\n   534→      const cutawayDuration = minDuration + Math.random() * (maxDuration - minDuration);\n   535→      const cutawayEnd = cutawayStart + cutawayDuration;\n   536→\n   537→      result.push({\n   538→        ...decision,\n   539→        startTime: currentStart,\n   540→        endTime: cutawayStart,\n   541→        duration: cutawayStart - currentStart\n   542→      });\n   543→\n   544→      // Add cutaway\n   545→      const cutawayTrack = cutawayTracks[i % cutawayTracks.length];\n   546→      result.push({\n   547→        speakerIndex: -2, // Special index for cutaway\n   548→        speakerName: 'Cutaway',\n   549→        speakers: [],\n   550→        videoTrack: cutawayTrack,\n   551→        startTime: cutawayStart,\n   552→        endTime: cutawayEnd,\n   553→        duration: cutawayDuration,\n   554→        isWideShot: false,\n   555→        isCutaway: true,\n   556→        reason: 'Cutaway insert'\n   557→      });\n   558→\n   559→      currentStart = cutawayEnd;\n   560→    }\n   561→\n   562→    // Add final segment\n   563→    result.push({\n   564→      ...decision,\n   565→      startTime: currentStart,\n   566→      endTime: decision.endTime,\n   567→      duration: decision.endTime - currentStart\n   568→    });\n   569→  }\n   570→\n   571→  return result;\n   572→}\n   573→\n   574→// =============================================================================\n   575→// Statistics\n   576→// =============================================================================\n   577→\n   578→/**\n   579→ * Calculate multitrack analysis statistics\n   580→ *\n   581→ * @param {Array} decisions - Final decisions\n   582→ * @param {number} totalDuration - Total audio duration\n   583→ * @returns {Object} Statistics\n   584→ */\n   585→function calculateMultitrackStats(decisions, totalDuration) {\n   586→  const speakerTimes = {};\n   587→  let wideShotTime = 0;\n   588→  let cutawayTime = 0;\n   589→\n   590→  for (const d of decisions) {\n   591→    if (d.isCutaway) {\n   592→      cutawayTime += d.duration;\n   593→    } else if (d.isWideShot) {\n   594→      wideShotTime += d.duration;\n   595→    } else {\n   596→      speakerTimes[d.speakerName] = (speakerTimes[d.speakerName] || 0) + d.duration;\n   597→    }\n   598→  }\n   599→\n   600→  const speakerPercentages = {};\n   601→  for (const [speaker, time] of Object.entries(speakerTimes)) {\n   602→    speakerPercentages[speaker] = parseFloat(((time / totalDuration) * 100).toFixed(1));\n   603→  }\n   604→\n   605→  return {\n   606→    decisionCount: decisions.length,\n   607→    wideShotPercentage: parseFloat(((wideShotTime / totalDuration) * 100).toFixed(1)),\n   608→    cutawayPercentage: parseFloat(((cutawayTime / totalDuration) * 100).toFixed(1)),\n   609→    speakerPercentages,\n   610→    averageShotDuration: parseFloat((totalDuration / decisions.length).toFixed(2))\n   611→  };\n   612→}\n   613→\n   614→// =============================================================================\n   615→// Auto-Balance Optimizer\n   616→// =============================================================================\n   617→\n   618→/**\n   619→ * Auto-balance speaker screentime\n   620→ * Adjusts speaker boosts to achieve equal distribution\n   621→ *\n   622→ * @param {Array<string>} audioPaths - Audio file paths\n   623→ * @param {Object} options - Base analysis options\n   624→ * @returns {Promise<Object>} Optimized analysis with best parameters\n   625→ */\n   626→async function autoBalanceMultitrack(audioPaths, options = {}) {\n   627→  const opts = { ...DEFAULT_OPTIONS, ...options };\n   628→  const numSpeakers = audioPaths.length;\n   629→  const targetShare = 100 / numSpeakers;\n   630→\n   631→  console.log(`[SPLICE Multitrack] Auto-balancing for ${numSpeakers} speakers (target: ${targetShare.toFixed(1)}% each)`);\n   632→\n   633→  // Run initial analysis\n   634→  let bestResult = await analyzeMultitrack(audioPaths, opts);\n   635→  let bestError = calculateDistributionError(bestResult.metadata.speakerPercentages, targetShare);\n   636→\n   637→  // Iterative optimization\n   638→  const maxIterations = 10;\n   639→  const boostStep = 3; // dB step\n   640→\n   641→  for (let iter = 0; iter < maxIterations; iter++) {\n   642→    // Find most over/under-represented speakers\n   643→    const percentages = bestResult.metadata.speakerPercentages;\n   644→    let maxOver = { speaker: null, diff: 0 };\n   645→    let maxUnder = { speaker: null, diff: 0 };\n   646→\n   647→    for (const [speaker, pct] of Object.entries(percentages)) {\n   648→      const diff = pct - targetShare;\n   649→      if (diff > maxOver.diff) maxOver = { speaker, diff };\n   650→      if (diff < maxUnder.diff) maxUnder = { speaker, diff };\n   651→    }\n   652→\n   653→    if (Math.abs(maxOver.diff) < 1 && Math.abs(maxUnder.diff) < 1) {\n   654→      console.log(`[SPLICE Multitrack] Converged at iteration ${iter + 1}`);\n   655→      break;\n   656→    }\n   657→\n   658→    // Adjust boosts\n   659→    const newBoosts = { ...opts.speakerBoosts };\n   660→    if (maxOver.speaker) {\n   661→      newBoosts[maxOver.speaker] = (newBoosts[maxOver.speaker] || 0) - boostStep;\n   662→    }\n   663→    if (maxUnder.speaker) {\n   664→      newBoosts[maxUnder.speaker] = (newBoosts[maxUnder.speaker] || 0) + boostStep;\n   665→    }\n   666→\n   667→    // Run analysis with new boosts\n   668→    const testResult = await analyzeMultitrack(audioPaths, { ...opts, speakerBoosts: newBoosts });\n   669→    const testError = calculateDistributionError(testResult.metadata.speakerPercentages, targetShare);\n   670→\n   671→    if (testError < bestError) {\n   672→      bestResult = testResult;\n   673→      bestError = testError;\n   674→      opts.speakerBoosts = newBoosts;\n   675→      console.log(`[SPLICE Multitrack] Iteration ${iter + 1}: error ${testError.toFixed(2)}%`);\n   676→    }\n   677→  }\n   678→\n   679→  return {\n   680→    ...bestResult,\n   681→    optimized: true,\n   682→    speakerBoosts: opts.speakerBoosts,\n   683→    finalError: bestError\n   684→  };\n   685→}\n   686→\n   687→/**\n   688→ * Calculate distribution error (max absolute deviation from target)\n   689→ */\n   690→function calculateDistributionError(percentages, targetShare) {\n   691→  let maxError = 0;\n   692→  for (const pct of Object.values(percentages)) {\n   693→    maxError = Math.max(maxError, Math.abs(pct - targetShare));\n   694→  }\n   695→  return maxError;\n   696→}\n   697→\n   698→// =============================================================================\n   699→// Exports\n   700→// =============================================================================\n   701→\n   702→module.exports = {\n   703→  // Core analysis\n   704→  analyzeMultitrack,\n   705→  extractTrackLevels,\n   706→\n   707→  // Level processing\n   708→  cleanupAudioLevels,\n   709→  applySpeakerBoosts,\n   710→  smoothAudioLevels,\n   711→\n   712→  // Decision generation\n   713→  generateSwitchingDecisions,\n   714→  tuneWideShotPercentage,\n   715→  insertCutaways,\n   716→\n   717→  // Auto-balance\n   718→  autoBalanceMultitrack,\n   719→\n   720→  // Statistics\n   721→  calculateMultitrackStats,\n   722→\n   723→  // Config\n   724→  DEFAULT_OPTIONS\n   725→};\n   726→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"386dc485-9676-46b5-a88b-b719a6cdf8ff","timestamp":"2025-12-25T22:30:40.517Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js","content":"/**\n * Multitrack/Multicam Analysis Service\n *\n * Analyzes multiple audio tracks to determine optimal video angle selection.\n * Based on Fireside's proven multicam editing approach.\n *\n * Features:\n * - Per-speaker RMS audio level analysis\n * - Noise floor subtraction for cleaner detection\n * - Automatic video angle selection based on loudest speaker\n * - Wide shot detection (multiple speakers talking)\n * - Cutaway insertion for long clips\n * - Speaker screentime balancing\n * - Gaussian smoothing to reduce spurious cuts\n */\n\nconst { exec } = require('child_process');\nconst { promisify } = require('util');\nconst fs = require('fs');\nconst path = require('path');\n\nconst execAsync = promisify(exec);\n\n// =============================================================================\n// Configuration\n// =============================================================================\n\nconst DEFAULT_OPTIONS = {\n  // Analysis parameters\n  analysisWindowMs: 100,       // RMS window size in milliseconds\n  smoothingWindow: 5,          // Gaussian blur kernel size (frames)\n\n  // Switching parameters\n  switchingFrequency: 50,      // How often to allow cuts (0-100)\n  minShotDuration: 2.0,        // Minimum seconds before next cut\n  speakerBoosts: {},           // Per-speaker dB adjustments\n\n  // Wide shot parameters\n  wideShotEnabled: true,\n  wideShotPercentage: 20,      // Target % of time in wide shots\n  wideShotTolerance: 6,        // dB tolerance for multiple speaker detection\n\n  // Cutaway parameters\n  cutawayEnabled: false,\n  cutawayMinDuration: 1.5,\n  cutawayMaxDuration: 4.0,\n  cutawayTracks: [],           // Video track indices for cutaways\n\n  // Output\n  frameRate: 30\n};\n\n// =============================================================================\n// Core Analysis\n// =============================================================================\n\n/**\n * Analyze multiple audio tracks for multicam editing\n *\n * @param {Array<string>} audioPaths - Paths to audio files (one per speaker)\n * @param {Object} options - Analysis options\n * @returns {Promise<Object>} Analysis results with switching decisions\n */\nasync function analyzeMultitrack(audioPaths, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n\n  console.log(`[SPLICE Multitrack] Analyzing ${audioPaths.length} audio track(s)`);\n\n  // Step 1: Extract RMS levels for each track\n  const trackLevels = await Promise.all(\n    audioPaths.map((audioPath, i) =>\n      extractTrackLevels(audioPath, {\n        windowMs: opts.analysisWindowMs,\n        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`\n      })\n    )\n  );\n\n  // Validate all tracks have same duration\n  const durations = trackLevels.map(t => t.duration);\n  const maxDuration = Math.max(...durations);\n  const minDuration = Math.min(...durations);\n  if (maxDuration - minDuration > 1) {\n    console.warn('[SPLICE Multitrack] Track durations differ significantly');\n  }\n\n  // Step 2: Calculate noise floor and clean up levels\n  const cleanedLevels = cleanupAudioLevels(trackLevels, opts);\n\n  // Step 3: Apply speaker boosts\n  const boostedLevels = applySpeakerBoosts(cleanedLevels, opts.speakerBoosts);\n\n  // Step 4: Apply Gaussian smoothing\n  const smoothedLevels = opts.smoothingWindow > 0\n    ? smoothAudioLevels(boostedLevels, opts.smoothingWindow)\n    : boostedLevels;\n\n  // Step 5: Generate switching decisions\n  const decisions = generateSwitchingDecisions(smoothedLevels, {\n    videoTrackMapping: opts.videoTrackMapping,\n    minShotDuration: opts.minShotDuration,\n    switchingFrequency: opts.switchingFrequency,\n    wideShotEnabled: opts.wideShotEnabled,\n    wideShotTolerance: opts.wideShotTolerance,\n    wideShotTracks: opts.wideShotTracks,\n    frameRate: opts.frameRate\n  });\n\n  // Step 6: Fine-tune wide shot percentage\n  const tunedDecisions = opts.wideShotEnabled\n    ? tuneWideShotPercentage(decisions, opts.wideShotPercentage)\n    : decisions;\n\n  // Step 7: Insert cutaways if enabled\n  const finalDecisions = opts.cutawayEnabled\n    ? insertCutaways(tunedDecisions, {\n        cutawayTracks: opts.cutawayTracks,\n        minDuration: opts.cutawayMinDuration,\n        maxDuration: opts.cutawayMaxDuration\n      })\n    : tunedDecisions;\n\n  // Step 8: Calculate statistics\n  const stats = calculateMultitrackStats(finalDecisions, trackLevels[0].duration);\n\n  console.log(`[SPLICE Multitrack] Generated ${finalDecisions.length} switching decision(s)`);\n\n  return {\n    decisions: finalDecisions,\n    metadata: {\n      trackCount: audioPaths.length,\n      speakerNames: opts.speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n      duration: maxDuration,\n      frameRate: opts.frameRate,\n      ...stats\n    },\n    levels: {\n      raw: trackLevels.map(t => ({ speaker: t.speakerName, sampleCount: t.levels.length })),\n      // Don't include full level data in response (too large)\n    }\n  };\n}\n\n// =============================================================================\n// Audio Level Extraction\n// =============================================================================\n\n/**\n * Extract RMS levels from audio file\n *\n * @param {string} audioPath - Path to audio file\n * @param {Object} options - Extraction options\n * @returns {Promise<Object>} Track levels with timing\n */\nasync function extractTrackLevels(audioPath, options = {}) {\n  const { windowMs = 100, speakerName = 'Unknown' } = options;\n\n  console.log(`[SPLICE Multitrack] Extracting levels: ${speakerName}`);\n\n  // Get audio info\n  const infoCmd = `ffprobe -v error -select_streams a:0 -show_entries stream=sample_rate,duration -of json \"${audioPath}\"`;\n  let sampleRate = 48000;\n  let duration = 0;\n\n  try {\n    const { stdout } = await execAsync(infoCmd);\n    const info = JSON.parse(stdout);\n    if (info.streams?.[0]) {\n      sampleRate = parseInt(info.streams[0].sample_rate) || 48000;\n      duration = parseFloat(info.streams[0].duration) || 0;\n    }\n  } catch (err) {\n    console.warn(`[SPLICE Multitrack] Could not get audio info: ${err.message}`);\n  }\n\n  // Get duration from format if not in stream\n  if (!duration) {\n    try {\n      const durationCmd = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"${audioPath}\"`;\n      const { stdout } = await execAsync(durationCmd);\n      duration = parseFloat(stdout.trim()) || 0;\n    } catch (err) {\n      console.warn(`[SPLICE Multitrack] Could not get duration: ${err.message}`);\n    }\n  }\n\n  // Extract audio and calculate RMS per window\n  const tempFile = path.join('/tmp', `splice_multitrack_${Date.now()}_${Math.random().toString(36).slice(2)}.raw`);\n\n  try {\n    // Convert to mono, 16kHz for faster processing\n    const extractCmd = `ffmpeg -y -i \"${audioPath}\" -ac 1 -ar 16000 -f s16le -acodec pcm_s16le \"${tempFile}\" 2>/dev/null`;\n    await execAsync(extractCmd, { maxBuffer: 100 * 1024 * 1024 });\n\n    const rawData = fs.readFileSync(tempFile);\n    const samples = new Float32Array(rawData.length / 2);\n    for (let i = 0; i < samples.length; i++) {\n      samples[i] = rawData.readInt16LE(i * 2) / 32768.0;\n    }\n\n    // Calculate RMS per window\n    const windowSamples = Math.floor((windowMs / 1000) * 16000);\n    const hopSamples = windowSamples; // Non-overlapping windows\n    const numWindows = Math.floor(samples.length / hopSamples);\n\n    const levels = new Float32Array(numWindows);\n    const times = new Float32Array(numWindows);\n\n    for (let i = 0; i < numWindows; i++) {\n      const start = i * hopSamples;\n      const end = Math.min(start + windowSamples, samples.length);\n\n      let sumSquares = 0;\n      for (let j = start; j < end; j++) {\n        sumSquares += samples[j] * samples[j];\n      }\n\n      const rms = Math.sqrt(sumSquares / (end - start));\n      const dbfs = rms > 0 ? 20 * Math.log10(rms) : -100;\n\n      levels[i] = Math.max(-100, dbfs);\n      times[i] = (start / 16000);\n    }\n\n    fs.unlinkSync(tempFile);\n\n    return { levels, times, duration, speakerName, sampleRate: 16000 };\n  } catch (err) {\n    if (fs.existsSync(tempFile)) fs.unlinkSync(tempFile);\n    throw new Error(`Failed to extract track levels: ${err.message}`);\n  }\n}\n\n// =============================================================================\n// Level Processing\n// =============================================================================\n\n/**\n * Clean up audio levels by subtracting noise floor\n *\n * @param {Array} trackLevels - Array of track level objects\n * @param {Object} options - Cleanup options\n * @returns {Array} Cleaned level arrays\n */\nfunction cleanupAudioLevels(trackLevels, options = {}) {\n  const numTracks = trackLevels.length;\n  const numSamples = trackLevels[0].levels.length;\n\n  // Calculate average level (noise floor estimate)\n  const averageLevels = new Float32Array(numSamples);\n  for (let i = 0; i < numSamples; i++) {\n    let sum = 0;\n    for (let t = 0; t < numTracks; t++) {\n      sum += trackLevels[t].levels[i];\n    }\n    averageLevels[i] = sum / numTracks;\n  }\n\n  // Subtract average from each track (relative loudness)\n  const cleaned = trackLevels.map(track => {\n    const cleanedLevels = new Float32Array(numSamples);\n    for (let i = 0; i < numSamples; i++) {\n      // Normalize to -100 to 0 range\n      cleanedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] - averageLevels[i]));\n    }\n    return {\n      ...track,\n      levels: cleanedLevels\n    };\n  });\n\n  return cleaned;\n}\n\n/**\n * Apply per-speaker dB boosts\n *\n * @param {Array} trackLevels - Cleaned track levels\n * @param {Object} boosts - Speaker boost mapping { \"Speaker 1\": 5, ... }\n * @returns {Array} Boosted levels\n */\nfunction applySpeakerBoosts(trackLevels, boosts = {}) {\n  return trackLevels.map(track => {\n    const boost = boosts[track.speakerName] || 0;\n    if (boost === 0) return track;\n\n    const boostedLevels = new Float32Array(track.levels.length);\n    for (let i = 0; i < track.levels.length; i++) {\n      boostedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] + boost));\n    }\n\n    return { ...track, levels: boostedLevels };\n  });\n}\n\n/**\n * Apply Gaussian smoothing to reduce noise\n *\n * @param {Array} trackLevels - Track levels\n * @param {number} windowSize - Gaussian kernel size\n * @returns {Array} Smoothed levels\n */\nfunction smoothAudioLevels(trackLevels, windowSize = 5) {\n  const kernel = generateGaussianKernel(windowSize);\n\n  return trackLevels.map(track => {\n    const smoothed = applyConvolution(track.levels, kernel);\n    return { ...track, levels: smoothed };\n  });\n}\n\n/**\n * Generate Gaussian kernel\n */\nfunction generateGaussianKernel(size) {\n  const sigma = size / 3;\n  const kernel = new Float32Array(size);\n  const half = Math.floor(size / 2);\n  let sum = 0;\n\n  for (let i = 0; i < size; i++) {\n    const x = i - half;\n    kernel[i] = Math.exp(-(x * x) / (2 * sigma * sigma));\n    sum += kernel[i];\n  }\n\n  // Normalize\n  for (let i = 0; i < size; i++) {\n    kernel[i] /= sum;\n  }\n\n  return kernel;\n}\n\n/**\n * Apply 1D convolution\n */\nfunction applyConvolution(data, kernel) {\n  const result = new Float32Array(data.length);\n  const half = Math.floor(kernel.length / 2);\n\n  for (let i = 0; i < data.length; i++) {\n    let sum = 0;\n    for (let j = 0; j < kernel.length; j++) {\n      const idx = i + j - half;\n      if (idx >= 0 && idx < data.length) {\n        sum += data[idx] * kernel[j];\n      }\n    }\n    result[i] = sum;\n  }\n\n  return result;\n}\n\n// =============================================================================\n// Switching Decision Generation\n// =============================================================================\n\n/**\n * Generate video switching decisions based on audio levels\n *\n * @param {Array} trackLevels - Processed track levels\n * @param {Object} options - Decision options\n * @returns {Array} Switching decisions\n */\nfunction generateSwitchingDecisions(trackLevels, options = {}) {\n  const {\n    videoTrackMapping = {},\n    minShotDuration = 2.0,\n    switchingFrequency = 50,\n    wideShotEnabled = true,\n    wideShotTolerance = 6,\n    wideShotTracks = [],\n    frameRate = 30\n  } = options;\n\n  const numSamples = trackLevels[0].levels.length;\n  const times = trackLevels[0].times;\n  const decisions = [];\n\n  // Calculate min samples between cuts based on switching frequency\n  const minSamplesPerCut = Math.ceil(minShotDuration / (times[1] - times[0]));\n  const switchingCooldown = Math.ceil(minSamplesPerCut * (1 - switchingFrequency / 100));\n\n  let currentSpeaker = -1;\n  let currentDecisionStart = 0;\n  let lastSwitchSample = 0;\n\n  for (let i = 0; i < numSamples; i++) {\n    // Find loudest speaker\n    let maxLevel = -100;\n    let loudestSpeaker = 0;\n    let speakersAboveThreshold = [];\n\n    for (let t = 0; t < trackLevels.length; t++) {\n      const level = trackLevels[t].levels[i];\n      if (level > maxLevel) {\n        maxLevel = level;\n        loudestSpeaker = t;\n      }\n\n      // Track speakers within tolerance of max (for wide shot detection)\n      if (wideShotEnabled && level >= maxLevel - wideShotTolerance) {\n        speakersAboveThreshold.push(t);\n      }\n    }\n\n    // Determine if this should be a wide shot\n    const isWideShot = wideShotEnabled && speakersAboveThreshold.length >= 2;\n    const effectiveSpeaker = isWideShot ? -1 : loudestSpeaker; // -1 = wide shot\n\n    // Check if we should switch\n    const canSwitch = (i - lastSwitchSample) >= switchingCooldown;\n    const shouldSwitch = canSwitch && effectiveSpeaker !== currentSpeaker;\n\n    if (shouldSwitch || i === 0) {\n      // Close previous decision\n      if (i > 0 && decisions.length > 0) {\n        decisions[decisions.length - 1].endTime = times[i];\n        decisions[decisions.length - 1].endSample = i;\n      }\n\n      // Start new decision\n      const speakerName = isWideShot\n        ? 'Wide Shot'\n        : trackLevels[loudestSpeaker].speakerName;\n\n      const videoTrack = isWideShot\n        ? (wideShotTracks[0] ?? 0)\n        : (videoTrackMapping[loudestSpeaker] ?? loudestSpeaker);\n\n      decisions.push({\n        speakerIndex: effectiveSpeaker,\n        speakerName,\n        speakers: speakersAboveThreshold.map(s => trackLevels[s].speakerName),\n        videoTrack,\n        startTime: times[i],\n        startSample: i,\n        endTime: times[numSamples - 1],\n        endSample: numSamples - 1,\n        isWideShot,\n        reason: isWideShot ? 'Multiple speakers' : 'Loudest speaker'\n      });\n\n      currentSpeaker = effectiveSpeaker;\n      lastSwitchSample = i;\n    }\n  }\n\n  // Finalize last decision\n  if (decisions.length > 0) {\n    decisions[decisions.length - 1].endTime = times[numSamples - 1];\n    decisions[decisions.length - 1].endSample = numSamples - 1;\n  }\n\n  // Add duration to each decision\n  return decisions.map(d => ({\n    ...d,\n    duration: parseFloat((d.endTime - d.startTime).toFixed(3))\n  }));\n}\n\n// =============================================================================\n// Wide Shot Tuning\n// =============================================================================\n\n/**\n * Fine-tune wide shot percentage using tolerance adjustment\n *\n * @param {Array} decisions - Initial decisions\n * @param {number} targetPercentage - Target wide shot percentage\n * @returns {Array} Tuned decisions\n */\nfunction tuneWideShotPercentage(decisions, targetPercentage) {\n  const totalDuration = decisions.reduce((sum, d) => sum + d.duration, 0);\n  const currentWideTime = decisions\n    .filter(d => d.isWideShot)\n    .reduce((sum, d) => sum + d.duration, 0);\n  const currentPercentage = (currentWideTime / totalDuration) * 100;\n\n  // If within 5% of target, don't adjust\n  if (Math.abs(currentPercentage - targetPercentage) <= 5) {\n    return decisions;\n  }\n\n  console.log(`[SPLICE Multitrack] Wide shot: ${currentPercentage.toFixed(1)}% (target: ${targetPercentage}%)`);\n\n  // Simple approach: mark some single-speaker decisions as wide shots\n  // or vice versa based on whether we need more or fewer wide shots\n  // (Full implementation would re-run with adjusted tolerance)\n\n  return decisions; // Return as-is for now\n}\n\n// =============================================================================\n// Cutaway Insertion\n// =============================================================================\n\n/**\n * Insert cutaways into long clips\n *\n * @param {Array} decisions - Switching decisions\n * @param {Object} options - Cutaway options\n * @returns {Array} Decisions with cutaways inserted\n */\nfunction insertCutaways(decisions, options = {}) {\n  const {\n    cutawayTracks = [],\n    minDuration = 1.5,\n    maxDuration = 4.0,\n    maxClipDuration = 30 // Insert cutaway if clip longer than this\n  } = options;\n\n  if (cutawayTracks.length === 0) return decisions;\n\n  const result = [];\n\n  for (const decision of decisions) {\n    if (decision.duration <= maxClipDuration) {\n      result.push(decision);\n      continue;\n    }\n\n    // Split long clip with cutaways\n    const numCutaways = Math.floor(decision.duration / maxClipDuration);\n    const interval = decision.duration / (numCutaways + 1);\n\n    let currentStart = decision.startTime;\n\n    for (let i = 0; i < numCutaways; i++) {\n      // Add main clip segment\n      const cutawayStart = currentStart + interval - minDuration / 2;\n      const cutawayDuration = minDuration + Math.random() * (maxDuration - minDuration);\n      const cutawayEnd = cutawayStart + cutawayDuration;\n\n      result.push({\n        ...decision,\n        startTime: currentStart,\n        endTime: cutawayStart,\n        duration: cutawayStart - currentStart\n      });\n\n      // Add cutaway\n      const cutawayTrack = cutawayTracks[i % cutawayTracks.length];\n      result.push({\n        speakerIndex: -2, // Special index for cutaway\n        speakerName: 'Cutaway',\n        speakers: [],\n        videoTrack: cutawayTrack,\n        startTime: cutawayStart,\n        endTime: cutawayEnd,\n        duration: cutawayDuration,\n        isWideShot: false,\n        isCutaway: true,\n        reason: 'Cutaway insert'\n      });\n\n      currentStart = cutawayEnd;\n    }\n\n    // Add final segment\n    result.push({\n      ...decision,\n      startTime: currentStart,\n      endTime: decision.endTime,\n      duration: decision.endTime - currentStart\n    });\n  }\n\n  return result;\n}\n\n// =============================================================================\n// Statistics\n// =============================================================================\n\n/**\n * Calculate multitrack analysis statistics\n *\n * @param {Array} decisions - Final decisions\n * @param {number} totalDuration - Total audio duration\n * @returns {Object} Statistics\n */\nfunction calculateMultitrackStats(decisions, totalDuration) {\n  const speakerTimes = {};\n  let wideShotTime = 0;\n  let cutawayTime = 0;\n\n  for (const d of decisions) {\n    if (d.isCutaway) {\n      cutawayTime += d.duration;\n    } else if (d.isWideShot) {\n      wideShotTime += d.duration;\n    } else {\n      speakerTimes[d.speakerName] = (speakerTimes[d.speakerName] || 0) + d.duration;\n    }\n  }\n\n  const speakerPercentages = {};\n  for (const [speaker, time] of Object.entries(speakerTimes)) {\n    speakerPercentages[speaker] = parseFloat(((time / totalDuration) * 100).toFixed(1));\n  }\n\n  return {\n    decisionCount: decisions.length,\n    wideShotPercentage: parseFloat(((wideShotTime / totalDuration) * 100).toFixed(1)),\n    cutawayPercentage: parseFloat(((cutawayTime / totalDuration) * 100).toFixed(1)),\n    speakerPercentages,\n    averageShotDuration: parseFloat((totalDuration / decisions.length).toFixed(2))\n  };\n}\n\n// =============================================================================\n// Auto-Balance Optimizer\n// =============================================================================\n\n/**\n * Auto-balance speaker screentime\n * Adjusts speaker boosts to achieve equal distribution\n *\n * @param {Array<string>} audioPaths - Audio file paths\n * @param {Object} options - Base analysis options\n * @returns {Promise<Object>} Optimized analysis with best parameters\n */\nasync function autoBalanceMultitrack(audioPaths, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n  const numSpeakers = audioPaths.length;\n  const targetShare = 100 / numSpeakers;\n\n  console.log(`[SPLICE Multitrack] Auto-balancing for ${numSpeakers} speakers (target: ${targetShare.toFixed(1)}% each)`);\n\n  // Run initial analysis\n  let bestResult = await analyzeMultitrack(audioPaths, opts);\n  let bestError = calculateDistributionError(bestResult.metadata.speakerPercentages, targetShare);\n\n  // Iterative optimization\n  const maxIterations = 10;\n  const boostStep = 3; // dB step\n\n  for (let iter = 0; iter < maxIterations; iter++) {\n    // Find most over/under-represented speakers\n    const percentages = bestResult.metadata.speakerPercentages;\n    let maxOver = { speaker: null, diff: 0 };\n    let maxUnder = { speaker: null, diff: 0 };\n\n    for (const [speaker, pct] of Object.entries(percentages)) {\n      const diff = pct - targetShare;\n      if (diff > maxOver.diff) maxOver = { speaker, diff };\n      if (diff < maxUnder.diff) maxUnder = { speaker, diff };\n    }\n\n    if (Math.abs(maxOver.diff) < 1 && Math.abs(maxUnder.diff) < 1) {\n      console.log(`[SPLICE Multitrack] Converged at iteration ${iter + 1}`);\n      break;\n    }\n\n    // Adjust boosts\n    const newBoosts = { ...opts.speakerBoosts };\n    if (maxOver.speaker) {\n      newBoosts[maxOver.speaker] = (newBoosts[maxOver.speaker] || 0) - boostStep;\n    }\n    if (maxUnder.speaker) {\n      newBoosts[maxUnder.speaker] = (newBoosts[maxUnder.speaker] || 0) + boostStep;\n    }\n\n    // Run analysis with new boosts\n    const testResult = await analyzeMultitrack(audioPaths, { ...opts, speakerBoosts: newBoosts });\n    const testError = calculateDistributionError(testResult.metadata.speakerPercentages, targetShare);\n\n    if (testError < bestError) {\n      bestResult = testResult;\n      bestError = testError;\n      opts.speakerBoosts = newBoosts;\n      console.log(`[SPLICE Multitrack] Iteration ${iter + 1}: error ${testError.toFixed(2)}%`);\n    }\n  }\n\n  return {\n    ...bestResult,\n    optimized: true,\n    speakerBoosts: opts.speakerBoosts,\n    finalError: bestError\n  };\n}\n\n/**\n * Calculate distribution error (max absolute deviation from target)\n */\nfunction calculateDistributionError(percentages, targetShare) {\n  let maxError = 0;\n  for (const pct of Object.values(percentages)) {\n    maxError = Math.max(maxError, Math.abs(pct - targetShare));\n  }\n  return maxError;\n}\n\n// =============================================================================\n// Exports\n// =============================================================================\n\nmodule.exports = {\n  // Core analysis\n  analyzeMultitrack,\n  extractTrackLevels,\n\n  // Level processing\n  cleanupAudioLevels,\n  applySpeakerBoosts,\n  smoothAudioLevels,\n\n  // Decision generation\n  generateSwitchingDecisions,\n  tuneWideShotPercentage,\n  insertCutaways,\n\n  // Auto-balance\n  autoBalanceMultitrack,\n\n  // Statistics\n  calculateMultitrackStats,\n\n  // Config\n  DEFAULT_OPTIONS\n};\n","numLines":726,"startLine":1,"totalLines":726}}}
{"parentUuid":"386dc485-9676-46b5-a88b-b719a6cdf8ff","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01XouPA1GjvLqGs4VTcvyFwj","type":"tool_result","content":"     1→/**\n     2→ * Rate Limiter Middleware\n     3→ *\n     4→ * Checks if user has sufficient credits before processing.\n     5→ * Returns 402 Payment Required if insufficient credits.\n     6→ */\n     7→\n     8→const { hasCredits, deductUsage, getBalance } = require('../services/usageTracking');\n     9→\n    10→/**\n    11→ * Middleware to check credits before processing\n    12→ *\n    13→ * Expects req.stripeCustomerId to be set by auth middleware\n    14→ * Optionally uses req.estimatedDuration (in seconds) for pre-check\n    15→ */\n    16→function requireCredits(options = {}) {\n    17→  const { endpoint = 'unknown' } = options;\n    18→\n    19→  return async (req, res, next) => {\n    20→    const stripeCustomerId = req.stripeCustomerId || req.headers['x-stripe-customer-id'];\n    21→\n    22→    if (!stripeCustomerId) {\n    23→      return res.status(401).json({\n    24→        error: 'Authentication required',\n    25→        message: 'Missing Stripe customer ID'\n    26→      });\n    27→    }\n    28→\n    29→    // Store for later use\n    30→    req.stripeCustomerId = stripeCustomerId;\n    31→\n    32→    try {\n    33→      // Check if user has any credits\n    34→      const hasCreds = await hasCredits(stripeCustomerId, req.estimatedDuration || 0);\n    35→\n    36→      if (!hasCreds) {\n    37→        const balance = await getBalance(stripeCustomerId);\n    38→        return res.status(402).json({\n    39→          error: 'Insufficient credits',\n    40→          message: 'Please upgrade your plan or wait for your next billing cycle',\n    41→          balance: {\n    42→            hoursRemaining: balance.hoursRemaining,\n    43→            tier: balance.tier\n    44→          }\n    45→        });\n    46→      }\n    47→\n    48→      // Attach helper function to deduct after processing\n    49→      req.deductUsage = async (audioDurationSeconds) => {\n    50→        return await deductUsage(stripeCustomerId, audioDurationSeconds, endpoint);\n    51→      };\n    52→\n    53→      next();\n    54→    } catch (err) {\n    55→      console.error('[RateLimiter] Error checking credits:', err);\n    56→      return res.status(500).json({\n    57→        error: 'Credit check failed',\n    58→        message: err.message\n    59→      });\n    60→    }\n    61→  };\n    62→}\n    63→\n    64→/**\n    65→ * Middleware to track usage after successful response\n    66→ *\n    67→ * Call this after processing to deduct actual usage\n    68→ */\n    69→function trackUsage(endpoint = 'unknown') {\n    70→  return async (req, res, next) => {\n    71→    // Store original json method\n    72→    const originalJson = res.json.bind(res);\n    73→\n    74→    // Override json to track usage on success\n    75→    res.json = async (data) => {\n    76→      // Only deduct on successful responses\n    77→      if (res.statusCode >= 200 && res.statusCode < 300 && req.audioDuration) {\n    78→        try {\n    79→          const balance = await deductUsage(\n    80→            req.stripeCustomerId,\n    81→            req.audioDuration,\n    82→            endpoint\n    83→          );\n    84→          // Attach updated balance to response\n    85→          data.balance = balance;\n    86→        } catch (err) {\n    87→          console.error('[RateLimiter] Error deducting usage:', err);\n    88→        }\n    89→      }\n    90→      return originalJson(data);\n    91→    };\n    92→\n    93→    next();\n    94→  };\n    95→}\n    96→\n    97→module.exports = {\n    98→  requireCredits,\n    99→  trackUsage\n   100→};\n   101→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"16dea138-1fbe-4214-9ecd-8347bbaf7f4c","timestamp":"2025-12-25T22:30:40.518Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/middleware/rateLimiter.js","content":"/**\n * Rate Limiter Middleware\n *\n * Checks if user has sufficient credits before processing.\n * Returns 402 Payment Required if insufficient credits.\n */\n\nconst { hasCredits, deductUsage, getBalance } = require('../services/usageTracking');\n\n/**\n * Middleware to check credits before processing\n *\n * Expects req.stripeCustomerId to be set by auth middleware\n * Optionally uses req.estimatedDuration (in seconds) for pre-check\n */\nfunction requireCredits(options = {}) {\n  const { endpoint = 'unknown' } = options;\n\n  return async (req, res, next) => {\n    const stripeCustomerId = req.stripeCustomerId || req.headers['x-stripe-customer-id'];\n\n    if (!stripeCustomerId) {\n      return res.status(401).json({\n        error: 'Authentication required',\n        message: 'Missing Stripe customer ID'\n      });\n    }\n\n    // Store for later use\n    req.stripeCustomerId = stripeCustomerId;\n\n    try {\n      // Check if user has any credits\n      const hasCreds = await hasCredits(stripeCustomerId, req.estimatedDuration || 0);\n\n      if (!hasCreds) {\n        const balance = await getBalance(stripeCustomerId);\n        return res.status(402).json({\n          error: 'Insufficient credits',\n          message: 'Please upgrade your plan or wait for your next billing cycle',\n          balance: {\n            hoursRemaining: balance.hoursRemaining,\n            tier: balance.tier\n          }\n        });\n      }\n\n      // Attach helper function to deduct after processing\n      req.deductUsage = async (audioDurationSeconds) => {\n        return await deductUsage(stripeCustomerId, audioDurationSeconds, endpoint);\n      };\n\n      next();\n    } catch (err) {\n      console.error('[RateLimiter] Error checking credits:', err);\n      return res.status(500).json({\n        error: 'Credit check failed',\n        message: err.message\n      });\n    }\n  };\n}\n\n/**\n * Middleware to track usage after successful response\n *\n * Call this after processing to deduct actual usage\n */\nfunction trackUsage(endpoint = 'unknown') {\n  return async (req, res, next) => {\n    // Store original json method\n    const originalJson = res.json.bind(res);\n\n    // Override json to track usage on success\n    res.json = async (data) => {\n      // Only deduct on successful responses\n      if (res.statusCode >= 200 && res.statusCode < 300 && req.audioDuration) {\n        try {\n          const balance = await deductUsage(\n            req.stripeCustomerId,\n            req.audioDuration,\n            endpoint\n          );\n          // Attach updated balance to response\n          data.balance = balance;\n        } catch (err) {\n          console.error('[RateLimiter] Error deducting usage:', err);\n        }\n      }\n      return originalJson(data);\n    };\n\n    next();\n  };\n}\n\nmodule.exports = {\n  requireCredits,\n  trackUsage\n};\n","numLines":101,"startLine":1,"totalLines":101}}}
{"parentUuid":"16dea138-1fbe-4214-9ecd-8347bbaf7f4c","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019UvyZJEFrWE8fCtpFVw8vQ","type":"tool_result","content":"     1→/**\n     2→ * Settings Management\n     3→ *\n     4→ * Persists user preferences to localStorage\n     5→ * v3.5: Added Persistent Token support for media folder access\n     6→ */\n     7→\n     8→const uxpFs = require('uxp').storage.localFileSystem;\n     9→\n    10→// =============================================================================\n    11→// CUSTOM PRESETS STORAGE (v3.5+)\n    12→// =============================================================================\n    13→\n    14→const CUSTOM_PRESETS_KEY = 'spliceCustomPresets';\n    15→\n    16→/**\n    17→ * Default structure for custom presets storage\n    18→ */\n    19→const DEFAULT_CUSTOM_PRESETS = {\n    20→  version: 1,\n    21→  presets: {},\n    22→  order: []\n    23→};\n    24→\n    25→/**\n    26→ * Load custom presets from localStorage\n    27→ * @returns {Object} Custom presets data structure\n    28→ */\n    29→function loadCustomPresets() {\n    30→  try {\n    31→    const saved = localStorage.getItem(CUSTOM_PRESETS_KEY);\n    32→    if (saved) {\n    33→      const parsed = JSON.parse(saved);\n    34→      // Validate structure\n    35→      if (parsed && typeof parsed === 'object' && parsed.version && parsed.presets) {\n    36→        // Return deep copy to avoid mutation issues\n    37→        return {\n    38→          version: parsed.version || 1,\n    39→          presets: { ...parsed.presets },\n    40→          order: [...(parsed.order || [])]\n    41→        };\n    42→      }\n    43→    }\n    44→  } catch (e) {\n    45→    console.warn('[SPLICE] Could not load custom presets:', e);\n    46→  }\n    47→  // Return fresh objects to avoid mutation of defaults\n    48→  return {\n    49→    version: 1,\n    50→    presets: {},\n    51→    order: []\n    52→  };\n    53→}\n    54→\n    55→/**\n    56→ * Save custom presets to localStorage\n    57→ * @param {Object} data - Custom presets data structure\n    58→ * @returns {boolean} Success status\n    59→ */\n    60→function saveCustomPresets(data) {\n    61→  try {\n    62→    if (!data || typeof data !== 'object') {\n    63→      console.warn('[SPLICE] Invalid custom presets data');\n    64→      return false;\n    65→    }\n    66→    // Ensure required fields\n    67→    const toSave = {\n    68→      version: data.version || DEFAULT_CUSTOM_PRESETS.version,\n    69→      presets: data.presets || {},\n    70→      order: data.order || []\n    71→    };\n    72→    localStorage.setItem(CUSTOM_PRESETS_KEY, JSON.stringify(toSave));\n    73→    return true;\n    74→  } catch (e) {\n    75→    console.warn('[SPLICE] Could not save custom presets:', e);\n    76→    return false;\n    77→  }\n    78→}\n    79→\n    80→/**\n    81→ * Generate a slugified ID from a preset name\n    82→ * @param {string} name - Preset name\n    83→ * @returns {string} Slugified ID\n    84→ */\n    85→function slugifyPresetName(name) {\n    86→  return name\n    87→    .toLowerCase()\n    88→    .trim()\n    89→    .replace(/[^a-z0-9]+/g, '-')\n    90→    .replace(/^-+|-+$/g, '')\n    91→    .substring(0, 50) || 'preset';\n    92→}\n    93→\n    94→/**\n    95→ * Generate a unique ID for a custom preset\n    96→ * @param {string} baseName - Base name to slugify\n    97→ * @param {Object} existingPresets - Current presets object\n    98→ * @returns {string} Unique ID\n    99→ */\n   100→function generateUniquePresetId(baseName, existingPresets) {\n   101→  const baseId = slugifyPresetName(baseName);\n   102→  let id = baseId;\n   103→  let counter = 1;\n   104→\n   105→  while (existingPresets[id]) {\n   106→    id = `${baseId}-${counter}`;\n   107→    counter++;\n   108→  }\n   109→\n   110→  return id;\n   111→}\n   112→\n   113→/**\n   114→ * Create a new custom preset\n   115→ * @param {Object} preset - Preset data\n   116→ * @param {string} preset.name - Preset name (required)\n   117→ * @param {string} [preset.description] - Preset description\n   118→ * @param {string} [preset.icon] - Icon name (default: 'settings')\n   119→ * @param {Object} [preset.settings] - Preset settings (uses current if not provided)\n   120→ * @returns {{success: boolean, id?: string, error?: string}}\n   121→ */\n   122→function createCustomPreset(preset) {\n   123→  // Validate input\n   124→  if (!preset || typeof preset !== 'object') {\n   125→    return { success: false, error: 'Invalid preset data' };\n   126→  }\n   127→\n   128→  if (!preset.name || typeof preset.name !== 'string' || !preset.name.trim()) {\n   129→    return { success: false, error: 'Preset name is required' };\n   130→  }\n   131→\n   132→  const name = preset.name.trim();\n   133→\n   134→  // Load current data\n   135→  const data = loadCustomPresets();\n   136→\n   137→  // Generate unique ID\n   138→  const id = generateUniquePresetId(name, data.presets);\n   139→\n   140→  // Get settings from preset or current settings\n   141→  const settings = preset.settings && typeof preset.settings === 'object'\n   142→    ? { ...preset.settings }\n   143→    : getCurrentPresetSettings();\n   144→\n   145→  // Validate settings\n   146→  if (!settings || typeof settings !== 'object') {\n   147→    return { success: false, error: 'Invalid settings' };\n   148→  }\n   149→\n   150→  // Create the preset object\n   151→  const newPreset = {\n   152→    id,\n   153→    name,\n   154→    description: preset.description || '',\n   155→    icon: preset.icon || 'settings',\n   156→    createdAt: new Date().toISOString(),\n   157→    settings\n   158→  };\n   159→\n   160→  // Add to presets and order\n   161→  data.presets[id] = newPreset;\n   162→  data.order.push(id);\n   163→\n   164→  // Save\n   165→  const saved = saveCustomPresets(data);\n   166→  if (!saved) {\n   167→    return { success: false, error: 'Failed to save preset' };\n   168→  }\n   169→\n   170→  console.log(`[SPLICE] Created custom preset: ${name} (${id})`);\n   171→  return { success: true, id };\n   172→}\n   173→\n   174→/**\n   175→ * Get current settings suitable for a preset\n   176→ * @returns {Object} Current preset-compatible settings\n   177→ */\n   178→function getCurrentPresetSettings() {\n   179→  const current = typeof getSettings === 'function' ? getSettings() : {};\n   180→  return {\n   181→    sensitivity: current.sensitivity || 50,\n   182→    threshold: current.threshold || -32,\n   183→    minSilenceLength: current.minSilenceLength || 0.5,\n   184→    paddingStart: current.paddingStart || 0.1,\n   185→    paddingEnd: current.paddingEnd || 0.1,\n   186→    autoMarkBest: current.autoMarkBest !== false,\n   187→    enableTakesDetection: current.enableTakesDetection !== false\n   188→  };\n   189→}\n   190→\n   191→/**\n   192→ * Update an existing custom preset\n   193→ * @param {string} id - Preset ID to update\n   194→ * @param {Object} updates - Fields to update\n   195→ * @param {string} [updates.name] - New name\n   196→ * @param {string} [updates.description] - New description\n   197→ * @param {string} [updates.icon] - New icon\n   198→ * @param {Object} [updates.settings] - New settings (merged with existing)\n   199→ * @returns {{success: boolean, error?: string}}\n   200→ */\n   201→function updateCustomPreset(id, updates) {\n   202→  // Validate input\n   203→  if (!id || typeof id !== 'string') {\n   204→    return { success: false, error: 'Preset ID is required' };\n   205→  }\n   206→\n   207→  if (!updates || typeof updates !== 'object') {\n   208→    return { success: false, error: 'Updates must be an object' };\n   209→  }\n   210→\n   211→  // Load current data\n   212→  const data = loadCustomPresets();\n   213→\n   214→  // Check if preset exists\n   215→  if (!data.presets[id]) {\n   216→    return { success: false, error: 'Preset not found' };\n   217→  }\n   218→\n   219→  const existing = data.presets[id];\n   220→\n   221→  // Apply updates (preserve fields not being updated)\n   222→  const updated = {\n   223→    ...existing,\n   224→    updatedAt: new Date().toISOString()\n   225→  };\n   226→\n   227→  // Update name (with validation)\n   228→  if (updates.name !== undefined) {\n   229→    if (typeof updates.name !== 'string' || !updates.name.trim()) {\n   230→      return { success: false, error: 'Name must be a non-empty string' };\n   231→    }\n   232→    updated.name = updates.name.trim();\n   233→  }\n   234→\n   235→  // Update description\n   236→  if (updates.description !== undefined) {\n   237→    updated.description = String(updates.description);\n   238→  }\n   239→\n   240→  // Update icon\n   241→  if (updates.icon !== undefined) {\n   242→    updated.icon = String(updates.icon);\n   243→  }\n   244→\n   245→  // Update settings (merge with existing)\n   246→  if (updates.settings !== undefined) {\n   247→    if (typeof updates.settings !== 'object') {\n   248→      return { success: false, error: 'Settings must be an object' };\n   249→    }\n   250→    updated.settings = {\n   251→      ...existing.settings,\n   252→      ...updates.settings\n   253→    };\n   254→  }\n   255→\n   256→  // Save updated preset\n   257→  data.presets[id] = updated;\n   258→\n   259→  const saved = saveCustomPresets(data);\n   260→  if (!saved) {\n   261→    return { success: false, error: 'Failed to save preset' };\n   262→  }\n   263→\n   264→  console.log(`[SPLICE] Updated custom preset: ${updated.name} (${id})`);\n   265→  return { success: true };\n   266→}\n   267→\n   268→/**\n   269→ * Get a custom preset by ID\n   270→ * @param {string} id - Preset ID\n   271→ * @returns {Object|null} Preset object or null if not found\n   272→ */\n   273→function getCustomPreset(id) {\n   274→  if (!id || typeof id !== 'string') {\n   275→    return null;\n   276→  }\n   277→\n   278→  const data = loadCustomPresets();\n   279→  return data.presets[id] || null;\n   280→}\n   281→\n   282→/**\n   283→ * Duplicate a custom preset\n   284→ * @param {string} id - Preset ID to duplicate\n   285→ * @returns {{success: boolean, id?: string, error?: string}}\n   286→ */\n   287→function duplicatePreset(id) {\n   288→  // Validate input\n   289→  if (!id || typeof id !== 'string') {\n   290→    return { success: false, error: 'Preset ID is required' };\n   291→  }\n   292→\n   293→  // Get the original preset\n   294→  const original = getPresetById(id);\n   295→  if (!original) {\n   296→    return { success: false, error: 'Preset not found' };\n   297→  }\n   298→\n   299→  // Generate new name with copy suffix\n   300→  let copyName = `${original.name} (Copy)`;\n   301→\n   302→  // Check if copy name already exists and increment if needed\n   303→  const data = loadCustomPresets();\n   304→  let copyNumber = 1;\n   305→  let testId = slugifyPresetName(copyName);\n   306→  while (data.presets[testId]) {\n   307→    copyNumber++;\n   308→    copyName = `${original.name} (Copy ${copyNumber})`;\n   309→    testId = slugifyPresetName(copyName);\n   310→  }\n   311→\n   312→  // Create the duplicate\n   313→  return createCustomPreset({\n   314→    name: copyName,\n   315→    description: original.description || '',\n   316→    icon: original.icon || 'settings',\n   317→    settings: original.settings ? { ...original.settings } : getCurrentPresetSettings()\n   318→  });\n   319→}\n   320→\n   321→/**\n   322→ * Export all custom presets to JSON string\n   323→ * @returns {{success: boolean, data?: string, count?: number, error?: string}}\n   324→ */\n   325→function exportPresets() {\n   326→  try {\n   327→    const data = loadCustomPresets();\n   328→    const customPresets = data.order.map(id => data.presets[id]).filter(Boolean);\n   329→\n   330→    if (customPresets.length === 0) {\n   331→      return { success: false, error: 'No custom presets to export' };\n   332→    }\n   333→\n   334→    const exportData = {\n   335→      version: 1,\n   336→      exportedAt: new Date().toISOString(),\n   337→      presets: customPresets\n   338→    };\n   339→\n   340→    return {\n   341→      success: true,\n   342→      data: JSON.stringify(exportData, null, 2),\n   343→      count: customPresets.length\n   344→    };\n   345→  } catch (e) {\n   346→    return { success: false, error: 'Failed to export presets: ' + e.message };\n   347→  }\n   348→}\n   349→\n   350→/**\n   351→ * Import presets from JSON string\n   352→ * @param {string} jsonString - JSON string containing presets\n   353→ * @param {boolean} merge - If true, merge with existing; if false, skip duplicates\n   354→ * @returns {{success: boolean, imported?: number, skipped?: number, error?: string}}\n   355→ */\n   356→function importPresets(jsonString, merge = true) {\n   357→  try {\n   358→    // Parse JSON\n   359→    const imported = JSON.parse(jsonString);\n   360→\n   361→    // Validate structure\n   362→    if (!imported || !imported.presets || !Array.isArray(imported.presets)) {\n   363→      return { success: false, error: 'Invalid preset file format' };\n   364→    }\n   365→\n   366→    let importedCount = 0;\n   367→    let skippedCount = 0;\n   368→\n   369→    for (const preset of imported.presets) {\n   370→      // Validate each preset has required fields\n   371→      if (!preset.name || typeof preset.name !== 'string') {\n   372→        skippedCount++;\n   373→        continue;\n   374→      }\n   375→\n   376→      // Check if a preset with this name already exists\n   377→      const existingId = slugifyPresetName(preset.name);\n   378→      const existing = getCustomPreset(existingId);\n   379→\n   380→      if (existing && !merge) {\n   381→        // Skip existing if not merging\n   382→        skippedCount++;\n   383→        continue;\n   384→      }\n   385→\n   386→      // If existing and merge mode, create with unique name\n   387→      if (existing && merge) {\n   388→        let newName = `${preset.name} (Imported)`;\n   389→        let counter = 1;\n   390→        while (getCustomPreset(slugifyPresetName(newName))) {\n   391→          counter++;\n   392→          newName = `${preset.name} (Imported ${counter})`;\n   393→        }\n   394→        preset.name = newName;\n   395→      }\n   396→\n   397→      // Create the preset\n   398→      const result = createCustomPreset({\n   399→        name: preset.name,\n   400→        description: preset.description || '',\n   401→        icon: preset.icon || 'settings',\n   402→        settings: preset.settings || getCurrentPresetSettings()\n   403→      });\n   404→\n   405→      if (result.success) {\n   406→        importedCount++;\n   407→      } else {\n   408→        skippedCount++;\n   409→      }\n   410→    }\n   411→\n   412→    if (importedCount === 0 && skippedCount > 0) {\n   413→      return { success: false, error: 'All presets already exist or were invalid', skipped: skippedCount };\n   414→    }\n   415→\n   416→    return {\n   417→      success: true,\n   418→      imported: importedCount,\n   419→      skipped: skippedCount\n   420→    };\n   421→  } catch (e) {\n   422→    if (e instanceof SyntaxError) {\n   423→      return { success: false, error: 'Invalid JSON format' };\n   424→    }\n   425→    return { success: false, error: 'Failed to import presets: ' + e.message };\n   426→  }\n   427→}\n   428→\n   429→/**\n   430→ * Delete a custom preset\n   431→ * @param {string} id - Preset ID to delete\n   432→ * @returns {{success: boolean, error?: string}}\n   433→ */\n   434→function deleteCustomPreset(id) {\n   435→  // Validate input\n   436→  if (!id || typeof id !== 'string') {\n   437→    return { success: false, error: 'Preset ID is required' };\n   438→  }\n   439→\n   440→  // Load current data\n   441→  const data = loadCustomPresets();\n   442→\n   443→  // Check if preset exists\n   444→  if (!data.presets[id]) {\n   445→    return { success: false, error: 'Preset not found' };\n   446→  }\n   447→\n   448→  const presetName = data.presets[id].name;\n   449→\n   450→  // Remove from presets object\n   451→  delete data.presets[id];\n   452→\n   453→  // Remove from order array\n   454→  const orderIndex = data.order.indexOf(id);\n   455→  if (orderIndex !== -1) {\n   456→    data.order.splice(orderIndex, 1);\n   457→  }\n   458→\n   459→  // Save\n   460→  const saved = saveCustomPresets(data);\n   461→  if (!saved) {\n   462→    return { success: false, error: 'Failed to save after deletion' };\n   463→  }\n   464→\n   465→  console.log(`[SPLICE] Deleted custom preset: ${presetName} (${id})`);\n   466→  return { success: true };\n   467→}\n   468→\n   469→/**\n   470→ * List of built-in preset IDs (cannot be modified or deleted)\n   471→ */\n   472→const BUILT_IN_PRESET_IDS = ['custom', 'podcast', 'interview', 'reaction', 'tutorial', 'vlog'];\n   473→\n   474→/**\n   475→ * Check if a preset is a built-in preset\n   476→ * @param {string} id - Preset ID to check\n   477→ * @returns {boolean} True if built-in, false if custom\n   478→ */\n   479→function isBuiltInPreset(id) {\n   480→  return BUILT_IN_PRESET_IDS.includes(id);\n   481→}\n   482→\n   483→/**\n   484→ * Get all presets (built-in + custom merged)\n   485→ * Built-in presets appear first, then custom presets in their creation order.\n   486→ * Each preset has isBuiltIn flag added.\n   487→ *\n   488→ * @returns {Array<Object>} Array of preset objects with metadata\n   489→ */\n   490→function getAllPresets() {\n   491→  // Get built-in presets\n   492→  const builtInPresets = BUILT_IN_PRESET_IDS.map(id => {\n   493→    const preset = PRESETS[id];\n   494→    return {\n   495→      id,\n   496→      name: preset.name,\n   497→      description: preset.description,\n   498→      icon: preset.icon,\n   499→      settings: preset.settings,\n   500→      isBuiltIn: true\n   501→    };\n   502→  });\n   503→\n   504→  // Get custom presets\n   505→  const customData = loadCustomPresets();\n   506→  const customPresets = customData.order.map(id => {\n   507→    const preset = customData.presets[id];\n   508→    if (!preset) return null;\n   509→    return {\n   510→      ...preset,\n   511→      isBuiltIn: false\n   512→    };\n   513→  }).filter(Boolean);\n   514→\n   515→  // Merge: built-in first, then custom\n   516→  return [...builtInPresets, ...customPresets];\n   517→}\n   518→\n   519→/**\n   520→ * Get all preset IDs (built-in + custom)\n   521→ * @returns {Array<string>} Array of preset IDs\n   522→ */\n   523→function getAllPresetIds() {\n   524→  const customData = loadCustomPresets();\n   525→  return [...BUILT_IN_PRESET_IDS, ...customData.order];\n   526→}\n   527→\n   528→/**\n   529→ * Get a preset by ID (built-in or custom)\n   530→ * @param {string} id - Preset ID\n   531→ * @returns {Object|null} Preset object or null if not found\n   532→ */\n   533→function getPresetById(id) {\n   534→  if (!id || typeof id !== 'string') {\n   535→    return null;\n   536→  }\n   537→\n   538→  // Check built-in first\n   539→  if (isBuiltInPreset(id)) {\n   540→    const preset = PRESETS[id];\n   541→    return {\n   542→      id,\n   543→      name: preset.name,\n   544→      description: preset.description,\n   545→      icon: preset.icon,\n   546→      settings: preset.settings,\n   547→      isBuiltIn: true\n   548→    };\n   549→  }\n   550→\n   551→  // Check custom presets\n   552→  const custom = getCustomPreset(id);\n   553→  if (custom) {\n   554→    return {\n   555→      ...custom,\n   556→      isBuiltIn: false\n   557→    };\n   558→  }\n   559→\n   560→  return null;\n   561→}\n   562→\n   563→const DEFAULT_SETTINGS = {\n   564→  sensitivity: 50,\n   565→  audioSource: 'original',\n   566→  autoMarkBest: true,\n   567→  enableTakesDetection: true,\n   568→  rememberOptions: false,\n   569→  optionsExpanded: false,\n   570→  customerId: null,          // Stripe customer ID for billing\n   571→  mediaFolderToken: null,    // Persistent token for media folder access\n   572→  mediaFolderPath: null,     // Path to the media folder (for display)\n   573→  activePreset: 'custom'     // Currently active preset\n   574→};\n   575→\n   576→// =============================================================================\n   577→// PRESET PROFILES\n   578→// =============================================================================\n   579→\n   580→/**\n   581→ * Detection presets for different content types.\n   582→ * Each preset defines optimal settings for a specific use case.\n   583→ */\n   584→const PRESETS = {\n   585→  // Custom - user-defined settings (default)\n   586→  custom: {\n   587→    name: 'Custom',\n   588→    description: 'Your custom settings',\n   589→    icon: 'settings',\n   590→    settings: null // Uses current user settings\n   591→  },\n   592→\n   593→  // Podcast - longer pauses are natural, be conservative\n   594→  podcast: {\n   595→    name: 'Podcast',\n   596→    description: 'Longer natural pauses, conversational flow',\n   597→    icon: 'mic',\n   598→    settings: {\n   599→      sensitivity: 35,           // Lower sensitivity (fewer cuts)\n   600→      threshold: -35,            // Higher threshold (only deep silence)\n   601→      minSilenceLength: 0.8,     // Longer minimum silence\n   602→      paddingStart: 0.15,        // More padding for natural transitions\n   603→      paddingEnd: 0.15,\n   604→      mergeDistance: 0.3,        // Merge nearby silences\n   605→      autoMarkBest: false,       // Don't auto-mark (manual review)\n   606→      enableTakesDetection: true\n   607→    }\n   608→  },\n   609→\n   610→  // Interview - balanced, respects speaker pauses\n   611→  interview: {\n   612→    name: 'Interview',\n   613→    description: 'Balanced cuts, preserves Q&A rhythm',\n   614→    icon: 'people',\n   615→    settings: {\n   616→      sensitivity: 50,           // Medium sensitivity\n   617→      threshold: -32,            // Moderate threshold\n   618→      minSilenceLength: 0.5,     // Standard minimum silence\n   619→      paddingStart: 0.12,        // Moderate padding\n   620→      paddingEnd: 0.08,\n   621→      mergeDistance: 0.2,\n   622→      autoMarkBest: true,\n   623→      enableTakesDetection: true\n   624→    }\n   625→  },\n   626→\n   627→  // Reaction video - fast pacing, quick cuts\n   628→  reaction: {\n   629→    name: 'Reaction',\n   630→    description: 'Fast-paced, tight cuts for energy',\n   631→    icon: 'bolt',\n   632→    settings: {\n   633→      sensitivity: 70,           // Higher sensitivity (more cuts)\n   634→      threshold: -28,            // Lower threshold (catches more silence)\n   635→      minSilenceLength: 0.3,     // Short minimum silence\n   636→      paddingStart: 0.05,        // Minimal padding for snappy edits\n   637→      paddingEnd: 0.03,\n   638→      mergeDistance: 0.15,\n   639→      autoMarkBest: true,\n   640→      enableTakesDetection: false // Usually single-take content\n   641→    }\n   642→  },\n   643→\n   644→  // Tutorial/Educational - preserve thinking pauses\n   645→  tutorial: {\n   646→    name: 'Tutorial',\n   647→    description: 'Preserves teaching pace, minimal cuts',\n   648→    icon: 'school',\n   649→    settings: {\n   650→      sensitivity: 30,           // Low sensitivity\n   651→      threshold: -38,            // Only deep silence\n   652→      minSilenceLength: 1.0,     // Long minimum (preserve demo pauses)\n   653→      paddingStart: 0.2,         // Extra padding for clarity\n   654→      paddingEnd: 0.15,\n   655→      mergeDistance: 0.4,\n   656→      autoMarkBest: false,\n   657→      enableTakesDetection: true\n   658→    }\n   659→  },\n   660→\n   661→  // Vlog/YouTube - punchy edits, engagement-focused\n   662→  vlog: {\n   663→    name: 'Vlog',\n   664→    description: 'Punchy edits for YouTube engagement',\n   665→    icon: 'videocam',\n   666→    settings: {\n   667→      sensitivity: 65,           // Higher sensitivity\n   668→      threshold: -30,            // Moderate-low threshold\n   669→      minSilenceLength: 0.35,    // Short silences\n   670→      paddingStart: 0.08,        // Quick transitions\n   671→      paddingEnd: 0.05,\n   672→      mergeDistance: 0.2,\n   673→      autoMarkBest: true,\n   674→      enableTakesDetection: true\n   675→    }\n   676→  }\n   677→};\n   678→\n   679→/**\n   680→ * Get all available presets\n   681→ * @returns {Object} All preset definitions\n   682→ */\n   683→function getPresets() {\n   684→  return { ...PRESETS };\n   685→}\n   686→\n   687→/**\n   688→ * Get a specific preset by name\n   689→ * @param {string} presetName - Name of the preset\n   690→ * @returns {Object|null} Preset definition or null if not found\n   691→ */\n   692→function getPreset(presetName) {\n   693→  return PRESETS[presetName] || null;\n   694→}\n   695→\n   696→/**\n   697→ * Get preset names as array (for UI)\n   698→ * @returns {Array<string>} Array of preset names\n   699→ */\n   700→function getPresetNames() {\n   701→  return Object.keys(PRESETS);\n   702→}\n   703→\n   704→/**\n   705→ * Apply a preset to current settings\n   706→ * @param {string} presetName - Name of the preset to apply\n   707→ * @returns {Object} The applied settings\n   708→ */\n   709→function applyPreset(presetName) {\n   710→  const preset = PRESETS[presetName];\n   711→\n   712→  if (!preset) {\n   713→    console.warn(`[SPLICE] Unknown preset: ${presetName}`);\n   714→    return getSettings();\n   715→  }\n   716→\n   717→  // Custom preset uses current settings\n   718→  if (presetName === 'custom' || !preset.settings) {\n   719→    saveSettings({ activePreset: 'custom' });\n   720→    return getSettings();\n   721→  }\n   722→\n   723→  // Apply preset settings\n   724→  const newSettings = {\n   725→    ...preset.settings,\n   726→    activePreset: presetName\n   727→  };\n   728→\n   729→  saveSettings(newSettings);\n   730→  console.log(`[SPLICE] Applied preset: ${preset.name}`);\n   731→\n   732→  return getSettings();\n   733→}\n   734→\n   735→/**\n   736→ * Get the currently active preset\n   737→ * @returns {string} Active preset name\n   738→ */\n   739→function getActivePreset() {\n   740→  const settings = getSettings();\n   741→  return settings.activePreset || 'custom';\n   742→}\n   743→\n   744→/**\n   745→ * Check if current settings match a preset\n   746→ * @param {string} presetName - Preset to compare against\n   747→ * @returns {boolean} True if settings match the preset\n   748→ */\n   749→function settingsMatchPreset(presetName) {\n   750→  const preset = PRESETS[presetName];\n   751→  if (!preset || !preset.settings) return presetName === 'custom';\n   752→\n   753→  const current = getSettings();\n   754→\n   755→  // Compare relevant settings\n   756→  return Object.keys(preset.settings).every(key => {\n   757→    if (key === 'activePreset') return true;\n   758→    return current[key] === preset.settings[key];\n   759→  });\n   760→}\n   761→\n   762→let currentSettings = { ...DEFAULT_SETTINGS };\n   763→\n   764→/**\n   765→ * Load settings from localStorage\n   766→ */\n   767→function loadSettings() {\n   768→  try {\n   769→    const saved = localStorage.getItem('spliceSettings');\n   770→    if (saved) {\n   771→      currentSettings = { ...DEFAULT_SETTINGS, ...JSON.parse(saved) };\n   772→    }\n   773→  } catch (e) {\n   774→    console.warn('[SPLICE] Could not load settings:', e);\n   775→    currentSettings = { ...DEFAULT_SETTINGS };\n   776→  }\n   777→  return currentSettings;\n   778→}\n   779→\n   780→/**\n   781→ * Save settings to localStorage\n   782→ */\n   783→function saveSettings(settings) {\n   784→  try {\n   785→    currentSettings = { ...currentSettings, ...settings };\n   786→    localStorage.setItem('spliceSettings', JSON.stringify(currentSettings));\n   787→  } catch (e) {\n   788→    console.warn('[SPLICE] Could not save settings:', e);\n   789→  }\n   790→}\n   791→\n   792→/**\n   793→ * Get current settings\n   794→ */\n   795→function getSettings() {\n   796→  return { ...currentSettings };\n   797→}\n   798→\n   799→/**\n   800→ * Reset settings to defaults\n   801→ */\n   802→function resetSettings() {\n   803→  currentSettings = { ...DEFAULT_SETTINGS };\n   804→  try {\n   805→    localStorage.removeItem('spliceSettings');\n   806→  } catch (e) {\n   807→    console.warn('[SPLICE] Could not clear settings:', e);\n   808→  }\n   809→}\n   810→\n   811→/**\n   812→ * Initialize settings UI components\n   813→ */\n   814→function initSettingsUI() {\n   815→  const settings = loadSettings();\n   816→\n   817→  // Apply saved sensitivity to slider\n   818→  const sensitivitySlider = document.getElementById('sensitivitySlider');\n   819→  if (sensitivitySlider) {\n   820→    sensitivitySlider.value = settings.sensitivity;\n   821→  }\n   822→\n   823→  // Apply saved audio source\n   824→  const sourceOriginal = document.getElementById('sourceOriginal');\n   825→  const sourceIsolated = document.getElementById('sourceIsolated');\n   826→  if (sourceOriginal && sourceIsolated) {\n   827→    sourceOriginal.checked = settings.audioSource === 'original' || settings.audioSource === 'both';\n   828→    sourceIsolated.checked = settings.audioSource === 'isolated' || settings.audioSource === 'both';\n   829→  }\n   830→\n   831→  // Apply auto-mark best setting\n   832→  const autoMarkBest = document.getElementById('autoMarkBest');\n   833→  if (autoMarkBest) {\n   834→    autoMarkBest.checked = settings.autoMarkBest;\n   835→  }\n   836→\n   837→  // Apply takes detection setting\n   838→  const enableTakesDetection = document.getElementById('enableTakesDetection');\n   839→  if (enableTakesDetection) {\n   840→    enableTakesDetection.checked = settings.enableTakesDetection;\n   841→  }\n   842→\n   843→  // Apply remember options setting\n   844→  const rememberOptions = document.getElementById('rememberOptions');\n   845→  if (rememberOptions) {\n   846→    rememberOptions.checked = settings.rememberOptions;\n   847→  }\n   848→\n   849→  // Restore expanded state if remember is enabled\n   850→  if (settings.rememberOptions && settings.optionsExpanded) {\n   851→    toggleOptionsPanel(true);\n   852→  }\n   853→}\n   854→\n   855→/**\n   856→ * Toggle options panel visibility (unified panel)\n   857→ */\n   858→function toggleOptionsPanel(forceState) {\n   859→  const toggle = document.getElementById('optionsToggle');\n   860→  const panel = document.getElementById('optionsPanel');\n   861→\n   862→  if (!toggle || !panel) return;\n   863→\n   864→  const shouldExpand = forceState !== undefined ? forceState : panel.classList.contains('collapsed');\n   865→\n   866→  if (shouldExpand) {\n   867→    panel.classList.remove('collapsed');\n   868→    toggle.classList.add('expanded');\n   869→  } else {\n   870→    panel.classList.add('collapsed');\n   871→    toggle.classList.remove('expanded');\n   872→  }\n   873→\n   874→  // Save state if remember is enabled\n   875→  const settings = getSettings();\n   876→  if (settings.rememberOptions) {\n   877→    saveSettings({ optionsExpanded: shouldExpand });\n   878→  }\n   879→}\n   880→\n   881→/**\n   882→ * Initialize settings modal\n   883→ */\n   884→function initSettingsModal() {\n   885→  const settingsBtn = document.getElementById('settingsBtn');\n   886→  const modal = document.getElementById('settingsModal');\n   887→  const closeBtn = document.getElementById('closeSettingsBtn');\n   888→  const defaultSensitivity = document.getElementById('defaultSensitivity');\n   889→  const rememberOptions = document.getElementById('rememberOptions');\n   890→\n   891→  if (settingsBtn && modal) {\n   892→    settingsBtn.addEventListener('click', () => {\n   893→      modal.classList.remove('hidden');\n   894→      // Sync modal values with current settings\n   895→      const settings = getSettings();\n   896→      if (defaultSensitivity) defaultSensitivity.value = settings.sensitivity;\n   897→      if (rememberOptions) rememberOptions.checked = settings.rememberOptions;\n   898→    });\n   899→  }\n   900→\n   901→  if (closeBtn && modal) {\n   902→    closeBtn.addEventListener('click', () => {\n   903→      modal.classList.add('hidden');\n   904→    });\n   905→  }\n   906→\n   907→  // Close on backdrop click\n   908→  if (modal) {\n   909→    modal.addEventListener('click', (e) => {\n   910→      if (e.target === modal) {\n   911→        modal.classList.add('hidden');\n   912→      }\n   913→    });\n   914→  }\n   915→\n   916→  // Save default sensitivity when changed\n   917→  if (defaultSensitivity) {\n   918→    defaultSensitivity.addEventListener('change', () => {\n   919→      saveSettings({ sensitivity: parseInt(defaultSensitivity.value) });\n   920→      // Also update the main slider\n   921→      const mainSlider = document.getElementById('sensitivitySlider');\n   922→      if (mainSlider) mainSlider.value = defaultSensitivity.value;\n   923→    });\n   924→  }\n   925→\n   926→  // Save remember options when changed\n   927→  if (rememberOptions) {\n   928→    rememberOptions.addEventListener('change', () => {\n   929→      saveSettings({ rememberOptions: rememberOptions.checked });\n   930→    });\n   931→  }\n   932→}\n   933→\n   934→/**\n   935→ * Initialize options toggle (unified panel)\n   936→ */\n   937→function initOptionsToggles() {\n   938→  const toggle = document.getElementById('optionsToggle');\n   939→\n   940→  if (toggle) {\n   941→    toggle.addEventListener('click', () => toggleOptionsPanel());\n   942→  }\n   943→}\n   944→\n   945→/**\n   946→ * Initialize preset selector\n   947→ */\n   948→function initPresetSelector() {\n   949→  const presetSelector = document.getElementById('presetSelector');\n   950→  const sensitivitySlider = document.getElementById('sensitivitySlider');\n   951→\n   952→  if (!presetSelector) return;\n   953→\n   954→  // Set initial value from settings\n   955→  const settings = getSettings();\n   956→  presetSelector.value = settings.activePreset || 'custom';\n   957→\n   958→  // Handle preset change (supports both built-in and custom presets)\n   959→  presetSelector.addEventListener('change', () => {\n   960→    const presetId = presetSelector.value;\n   961→\n   962→    // Get preset info (works for both built-in and custom)\n   963→    const preset = getPresetById(presetId);\n   964→    if (!preset) {\n   965→      console.warn(`[SPLICE] Preset not found: ${presetId}`);\n   966→      return;\n   967→    }\n   968→\n   969→    let appliedSettings;\n   970→\n   971→    if (preset.isBuiltIn) {\n   972→      // Built-in preset: use the existing applyPreset function\n   973→      appliedSettings = applyPreset(presetId);\n   974→    } else {\n   975→      // Custom preset: apply stored settings\n   976→      appliedSettings = preset.settings || {};\n   977→      saveSettings({ activePreset: presetId, ...appliedSettings });\n   978→    }\n   979→\n   980→    // Update sensitivity slider to match preset\n   981→    if (sensitivitySlider && appliedSettings.sensitivity !== undefined) {\n   982→      sensitivitySlider.value = appliedSettings.sensitivity;\n   983→    }\n   984→\n   985→    // Update takes detection checkbox\n   986→    const enableTakesDetection = document.getElementById('enableTakesDetection');\n   987→    if (enableTakesDetection && appliedSettings.enableTakesDetection !== undefined) {\n   988→      enableTakesDetection.checked = appliedSettings.enableTakesDetection;\n   989→    }\n   990→\n   991→    // Update auto-mark best checkbox\n   992→    const autoMarkBest = document.getElementById('autoMarkBest');\n   993→    if (autoMarkBest && appliedSettings.autoMarkBest !== undefined) {\n   994→      autoMarkBest.checked = appliedSettings.autoMarkBest;\n   995→    }\n   996→\n   997→    if (typeof setStatus === 'function') {\n   998→      const desc = preset.description ? ` - ${preset.description}` : '';\n   999→      setStatus(`Preset: ${preset.name}${desc}`);\n  1000→    }\n  1001→  });\n  1002→\n  1003→  // Switch to custom when user manually changes sensitivity\n  1004→  if (sensitivitySlider) {\n  1005→    sensitivitySlider.addEventListener('change', () => {\n  1006→      if (presetSelector.value !== 'custom') {\n  1007→        presetSelector.value = 'custom';\n  1008→        saveSettings({ activePreset: 'custom' });\n  1009→      }\n  1010→    });\n  1011→  }\n  1012→}\n  1013→\n  1014→/**\n  1015→ * Initialize help button\n  1016→ */\n  1017→function initHelpButton() {\n  1018→  const helpBtn = document.getElementById('helpBtn');\n  1019→  if (helpBtn) {\n  1020→    helpBtn.addEventListener('click', () => {\n  1021→      setStatus('Silence: removes quiet gaps | Takes: detects repeated content');\n  1022→    });\n  1023→  }\n  1024→}\n  1025→\n  1026→// =============================================================================\n  1027→// PERSISTENT TOKEN MANAGEMENT (v3.5)\n  1028→// =============================================================================\n  1029→\n  1030→/**\n  1031→ * Media folder entry (cached after selection)\n  1032→ * @type {Object|null}\n  1033→ */\n  1034→let mediaFolderEntry = null;\n  1035→\n  1036→/**\n  1037→ * Prompt user to select a media folder and create a persistent token.\n  1038→ * This allows silent file access without repeated permission prompts.\n  1039→ *\n  1040→ * @returns {Promise<{success: boolean, path?: string, error?: string}>}\n  1041→ */\n  1042→async function setupMediaFolder() {\n  1043→  try {\n  1044→    // Open folder picker dialog\n  1045→    const folder = await uxpFs.getFolder();\n  1046→\n  1047→    if (!folder) {\n  1048→      return { success: false, error: 'No folder selected' };\n  1049→    }\n  1050→\n  1051→    // Create a persistent token for this folder\n  1052→    const token = await uxpFs.createPersistentToken(folder);\n  1053→\n  1054→    if (!token) {\n  1055→      return { success: false, error: 'Failed to create persistent token' };\n  1056→    }\n  1057→\n  1058→    // Get the folder path for display\n  1059→    const folderPath = folder.nativePath || folder.name;\n  1060→\n  1061→    // Save to settings\n  1062→    saveSettings({\n  1063→      mediaFolderToken: token,\n  1064→      mediaFolderPath: folderPath\n  1065→    });\n  1066→\n  1067→    // Cache the folder entry\n  1068→    mediaFolderEntry = folder;\n  1069→\n  1070→    console.log(`[SPLICE] Media folder set: ${folderPath}`);\n  1071→\n  1072→    return {\n  1073→      success: true,\n  1074→      path: folderPath,\n  1075→      token\n  1076→    };\n  1077→  } catch (err) {\n  1078→    console.error('[SPLICE] Error setting up media folder:', err);\n  1079→    return { success: false, error: err.message };\n  1080→  }\n  1081→}\n  1082→\n  1083→/**\n  1084→ * Get the media folder entry using the stored persistent token.\n  1085→ * Falls back to prompting if token is invalid.\n  1086→ *\n  1087→ * @returns {Promise<Object|null>} The folder entry or null\n  1088→ */\n  1089→async function getMediaFolder() {\n  1090→  // Return cached entry if available\n  1091→  if (mediaFolderEntry) {\n  1092→    return mediaFolderEntry;\n  1093→  }\n  1094→\n  1095→  const settings = getSettings();\n  1096→\n  1097→  // Try to restore from persistent token\n  1098→  if (settings.mediaFolderToken) {\n  1099→    try {\n  1100→      const folder = await uxpFs.getEntryForPersistentToken(settings.mediaFolderToken);\n  1101→      if (folder) {\n  1102→        mediaFolderEntry = folder;\n  1103→        console.log(`[SPLICE] Restored media folder from token: ${settings.mediaFolderPath}`);\n  1104→        return folder;\n  1105→      }\n  1106→    } catch (err) {\n  1107→      console.warn('[SPLICE] Could not restore media folder from token:', err.message);\n  1108→      // Clear invalid token\n  1109→      saveSettings({ mediaFolderToken: null, mediaFolderPath: null });\n  1110→    }\n  1111→  }\n  1112→\n  1113→  return null;\n  1114→}\n  1115→\n  1116→/**\n  1117→ * Check if a media folder is configured\n  1118→ * @returns {boolean}\n  1119→ */\n  1120→function hasMediaFolder() {\n  1121→  const settings = getSettings();\n  1122→  return !!settings.mediaFolderToken;\n  1123→}\n  1124→\n  1125→/**\n  1126→ * Get the configured media folder path (for display)\n  1127→ * @returns {string|null}\n  1128→ */\n  1129→function getMediaFolderPath() {\n  1130→  const settings = getSettings();\n  1131→  return settings.mediaFolderPath;\n  1132→}\n  1133→\n  1134→/**\n  1135→ * Clear the media folder configuration\n  1136→ */\n  1137→function clearMediaFolder() {\n  1138→  mediaFolderEntry = null;\n  1139→  saveSettings({\n  1140→    mediaFolderToken: null,\n  1141→    mediaFolderPath: null\n  1142→  });\n  1143→  console.log('[SPLICE] Media folder cleared');\n  1144→}\n  1145→\n  1146→/**\n  1147→ * Read a file from the media folder\n  1148→ * @param {string} filename - Name of the file to read\n  1149→ * @returns {Promise<ArrayBuffer|null>} File contents or null\n  1150→ */\n  1151→async function readMediaFile(filename) {\n  1152→  const folder = await getMediaFolder();\n  1153→  if (!folder) {\n  1154→    console.warn('[SPLICE] No media folder configured');\n  1155→    return null;\n  1156→  }\n  1157→\n  1158→  try {\n  1159→    const file = await folder.getEntry(filename);\n  1160→    if (!file) {\n  1161→      console.warn(`[SPLICE] File not found in media folder: ${filename}`);\n  1162→      return null;\n  1163→    }\n  1164→\n  1165→    const contents = await file.read({ format: uxpFs.formats.binary });\n  1166→    return contents;\n  1167→  } catch (err) {\n  1168→    console.error(`[SPLICE] Error reading file ${filename}:`, err);\n  1169→    return null;\n  1170→  }\n  1171→}\n  1172→\n  1173→// =============================================================================\n  1174→// LOGIN MODAL\n  1175→// =============================================================================\n  1176→\n  1177→/**\n  1178→ * Initialize login modal handlers\n  1179→ */\n  1180→function initLoginModal() {\n  1181→  const loginModal = document.getElementById('loginModal');\n  1182→  const closeLoginBtn = document.getElementById('closeLoginBtn');\n  1183→  const saveLoginBtn = document.getElementById('saveLoginBtn');\n  1184→  const customerIdInput = document.getElementById('customerIdInput');\n  1185→  const loginError = document.getElementById('loginError');\n  1186→  const creditBadge = document.getElementById('creditBadge');\n  1187→\n  1188→  // Show login modal when clicking credit badge (always - allows viewing/changing ID)\n  1189→  if (creditBadge) {\n  1190→    creditBadge.addEventListener('click', () => {\n  1191→      showLoginModal();\n  1192→    });\n  1193→  }\n  1194→\n  1195→  // Close login modal\n  1196→  if (closeLoginBtn && loginModal) {\n  1197→    closeLoginBtn.addEventListener('click', () => {\n  1198→      loginModal.classList.add('hidden');\n  1199→    });\n  1200→  }\n  1201→\n  1202→  // Close on backdrop click\n  1203→  if (loginModal) {\n  1204→    loginModal.addEventListener('click', (e) => {\n  1205→      if (e.target === loginModal) {\n  1206→        loginModal.classList.add('hidden');\n  1207→      }\n  1208→    });\n  1209→  }\n  1210→\n  1211→  // Save customer ID\n  1212→  if (saveLoginBtn) {\n  1213→    saveLoginBtn.addEventListener('click', async () => {\n  1214→      const customerId = customerIdInput?.value?.trim();\n  1215→\n  1216→      if (!customerId) {\n  1217→        showLoginError('Please enter your customer ID');\n  1218→        return;\n  1219→      }\n  1220→\n  1221→      if (!customerId.startsWith('cus_')) {\n  1222→        showLoginError('Customer ID should start with \"cus_\"');\n  1223→        return;\n  1224→      }\n  1225→\n  1226→      // Save to settings\n  1227→      saveSettings({ customerId });\n  1228→\n  1229→      // Hide modal\n  1230→      loginModal?.classList.add('hidden');\n  1231→\n  1232→      // Refresh credits display\n  1233→      if (typeof refreshCredits === 'function') {\n  1234→        await refreshCredits();\n  1235→      }\n  1236→\n  1237→      if (typeof setStatus === 'function') {\n  1238→        setStatus('Logged in successfully');\n  1239→      }\n  1240→    });\n  1241→  }\n  1242→\n  1243→  // Load existing customer ID into input\n  1244→  const settings = getSettings();\n  1245→  if (customerIdInput && settings.customerId) {\n  1246→    customerIdInput.value = settings.customerId;\n  1247→  }\n  1248→}\n  1249→\n  1250→/**\n  1251→ * Show login modal\n  1252→ */\n  1253→function showLoginModal() {\n  1254→  const loginModal = document.getElementById('loginModal');\n  1255→  const loginError = document.getElementById('loginError');\n  1256→\n  1257→  if (loginError) {\n  1258→    loginError.style.display = 'none';\n  1259→  }\n  1260→\n  1261→  if (loginModal) {\n  1262→    loginModal.classList.remove('hidden');\n  1263→  }\n  1264→}\n  1265→\n  1266→/**\n  1267→ * Show login error message\n  1268→ */\n  1269→function showLoginError(message) {\n  1270→  const loginError = document.getElementById('loginError');\n  1271→  if (loginError) {\n  1272→    loginError.textContent = message;\n  1273→    loginError.style.display = 'block';\n  1274→  }\n  1275→}\n  1276→\n  1277→/**\n  1278→ * Check if user is logged in\n  1279→ * @returns {boolean}\n  1280→ */\n  1281→function isLoggedIn() {\n  1282→  const settings = getSettings();\n  1283→  return !!settings.customerId;\n  1284→}\n  1285→\n  1286→/**\n  1287→ * Get customer ID\n  1288→ * @returns {string|null}\n  1289→ */\n  1290→function getCustomerId() {\n  1291→  const settings = getSettings();\n  1292→  return settings.customerId || null;\n  1293→}\n  1294→\n  1295→/**\n  1296→ * Logout - clear customer ID\n  1297→ */\n  1298→function logout() {\n  1299→  saveSettings({ customerId: null });\n  1300→  if (typeof setStatus === 'function') {\n  1301→    setStatus('Logged out');\n  1302→  }\n  1303→}\n  1304→\n  1305→// Load settings on script load\n  1306→loadSettings();\n  1307→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"0a1520f9-d934-4627-8598-8321806b5069","timestamp":"2025-12-25T22:30:40.689Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-plugin/js/settings.js","content":"/**\n * Settings Management\n *\n * Persists user preferences to localStorage\n * v3.5: Added Persistent Token support for media folder access\n */\n\nconst uxpFs = require('uxp').storage.localFileSystem;\n\n// =============================================================================\n// CUSTOM PRESETS STORAGE (v3.5+)\n// =============================================================================\n\nconst CUSTOM_PRESETS_KEY = 'spliceCustomPresets';\n\n/**\n * Default structure for custom presets storage\n */\nconst DEFAULT_CUSTOM_PRESETS = {\n  version: 1,\n  presets: {},\n  order: []\n};\n\n/**\n * Load custom presets from localStorage\n * @returns {Object} Custom presets data structure\n */\nfunction loadCustomPresets() {\n  try {\n    const saved = localStorage.getItem(CUSTOM_PRESETS_KEY);\n    if (saved) {\n      const parsed = JSON.parse(saved);\n      // Validate structure\n      if (parsed && typeof parsed === 'object' && parsed.version && parsed.presets) {\n        // Return deep copy to avoid mutation issues\n        return {\n          version: parsed.version || 1,\n          presets: { ...parsed.presets },\n          order: [...(parsed.order || [])]\n        };\n      }\n    }\n  } catch (e) {\n    console.warn('[SPLICE] Could not load custom presets:', e);\n  }\n  // Return fresh objects to avoid mutation of defaults\n  return {\n    version: 1,\n    presets: {},\n    order: []\n  };\n}\n\n/**\n * Save custom presets to localStorage\n * @param {Object} data - Custom presets data structure\n * @returns {boolean} Success status\n */\nfunction saveCustomPresets(data) {\n  try {\n    if (!data || typeof data !== 'object') {\n      console.warn('[SPLICE] Invalid custom presets data');\n      return false;\n    }\n    // Ensure required fields\n    const toSave = {\n      version: data.version || DEFAULT_CUSTOM_PRESETS.version,\n      presets: data.presets || {},\n      order: data.order || []\n    };\n    localStorage.setItem(CUSTOM_PRESETS_KEY, JSON.stringify(toSave));\n    return true;\n  } catch (e) {\n    console.warn('[SPLICE] Could not save custom presets:', e);\n    return false;\n  }\n}\n\n/**\n * Generate a slugified ID from a preset name\n * @param {string} name - Preset name\n * @returns {string} Slugified ID\n */\nfunction slugifyPresetName(name) {\n  return name\n    .toLowerCase()\n    .trim()\n    .replace(/[^a-z0-9]+/g, '-')\n    .replace(/^-+|-+$/g, '')\n    .substring(0, 50) || 'preset';\n}\n\n/**\n * Generate a unique ID for a custom preset\n * @param {string} baseName - Base name to slugify\n * @param {Object} existingPresets - Current presets object\n * @returns {string} Unique ID\n */\nfunction generateUniquePresetId(baseName, existingPresets) {\n  const baseId = slugifyPresetName(baseName);\n  let id = baseId;\n  let counter = 1;\n\n  while (existingPresets[id]) {\n    id = `${baseId}-${counter}`;\n    counter++;\n  }\n\n  return id;\n}\n\n/**\n * Create a new custom preset\n * @param {Object} preset - Preset data\n * @param {string} preset.name - Preset name (required)\n * @param {string} [preset.description] - Preset description\n * @param {string} [preset.icon] - Icon name (default: 'settings')\n * @param {Object} [preset.settings] - Preset settings (uses current if not provided)\n * @returns {{success: boolean, id?: string, error?: string}}\n */\nfunction createCustomPreset(preset) {\n  // Validate input\n  if (!preset || typeof preset !== 'object') {\n    return { success: false, error: 'Invalid preset data' };\n  }\n\n  if (!preset.name || typeof preset.name !== 'string' || !preset.name.trim()) {\n    return { success: false, error: 'Preset name is required' };\n  }\n\n  const name = preset.name.trim();\n\n  // Load current data\n  const data = loadCustomPresets();\n\n  // Generate unique ID\n  const id = generateUniquePresetId(name, data.presets);\n\n  // Get settings from preset or current settings\n  const settings = preset.settings && typeof preset.settings === 'object'\n    ? { ...preset.settings }\n    : getCurrentPresetSettings();\n\n  // Validate settings\n  if (!settings || typeof settings !== 'object') {\n    return { success: false, error: 'Invalid settings' };\n  }\n\n  // Create the preset object\n  const newPreset = {\n    id,\n    name,\n    description: preset.description || '',\n    icon: preset.icon || 'settings',\n    createdAt: new Date().toISOString(),\n    settings\n  };\n\n  // Add to presets and order\n  data.presets[id] = newPreset;\n  data.order.push(id);\n\n  // Save\n  const saved = saveCustomPresets(data);\n  if (!saved) {\n    return { success: false, error: 'Failed to save preset' };\n  }\n\n  console.log(`[SPLICE] Created custom preset: ${name} (${id})`);\n  return { success: true, id };\n}\n\n/**\n * Get current settings suitable for a preset\n * @returns {Object} Current preset-compatible settings\n */\nfunction getCurrentPresetSettings() {\n  const current = typeof getSettings === 'function' ? getSettings() : {};\n  return {\n    sensitivity: current.sensitivity || 50,\n    threshold: current.threshold || -32,\n    minSilenceLength: current.minSilenceLength || 0.5,\n    paddingStart: current.paddingStart || 0.1,\n    paddingEnd: current.paddingEnd || 0.1,\n    autoMarkBest: current.autoMarkBest !== false,\n    enableTakesDetection: current.enableTakesDetection !== false\n  };\n}\n\n/**\n * Update an existing custom preset\n * @param {string} id - Preset ID to update\n * @param {Object} updates - Fields to update\n * @param {string} [updates.name] - New name\n * @param {string} [updates.description] - New description\n * @param {string} [updates.icon] - New icon\n * @param {Object} [updates.settings] - New settings (merged with existing)\n * @returns {{success: boolean, error?: string}}\n */\nfunction updateCustomPreset(id, updates) {\n  // Validate input\n  if (!id || typeof id !== 'string') {\n    return { success: false, error: 'Preset ID is required' };\n  }\n\n  if (!updates || typeof updates !== 'object') {\n    return { success: false, error: 'Updates must be an object' };\n  }\n\n  // Load current data\n  const data = loadCustomPresets();\n\n  // Check if preset exists\n  if (!data.presets[id]) {\n    return { success: false, error: 'Preset not found' };\n  }\n\n  const existing = data.presets[id];\n\n  // Apply updates (preserve fields not being updated)\n  const updated = {\n    ...existing,\n    updatedAt: new Date().toISOString()\n  };\n\n  // Update name (with validation)\n  if (updates.name !== undefined) {\n    if (typeof updates.name !== 'string' || !updates.name.trim()) {\n      return { success: false, error: 'Name must be a non-empty string' };\n    }\n    updated.name = updates.name.trim();\n  }\n\n  // Update description\n  if (updates.description !== undefined) {\n    updated.description = String(updates.description);\n  }\n\n  // Update icon\n  if (updates.icon !== undefined) {\n    updated.icon = String(updates.icon);\n  }\n\n  // Update settings (merge with existing)\n  if (updates.settings !== undefined) {\n    if (typeof updates.settings !== 'object') {\n      return { success: false, error: 'Settings must be an object' };\n    }\n    updated.settings = {\n      ...existing.settings,\n      ...updates.settings\n    };\n  }\n\n  // Save updated preset\n  data.presets[id] = updated;\n\n  const saved = saveCustomPresets(data);\n  if (!saved) {\n    return { success: false, error: 'Failed to save preset' };\n  }\n\n  console.log(`[SPLICE] Updated custom preset: ${updated.name} (${id})`);\n  return { success: true };\n}\n\n/**\n * Get a custom preset by ID\n * @param {string} id - Preset ID\n * @returns {Object|null} Preset object or null if not found\n */\nfunction getCustomPreset(id) {\n  if (!id || typeof id !== 'string') {\n    return null;\n  }\n\n  const data = loadCustomPresets();\n  return data.presets[id] || null;\n}\n\n/**\n * Duplicate a custom preset\n * @param {string} id - Preset ID to duplicate\n * @returns {{success: boolean, id?: string, error?: string}}\n */\nfunction duplicatePreset(id) {\n  // Validate input\n  if (!id || typeof id !== 'string') {\n    return { success: false, error: 'Preset ID is required' };\n  }\n\n  // Get the original preset\n  const original = getPresetById(id);\n  if (!original) {\n    return { success: false, error: 'Preset not found' };\n  }\n\n  // Generate new name with copy suffix\n  let copyName = `${original.name} (Copy)`;\n\n  // Check if copy name already exists and increment if needed\n  const data = loadCustomPresets();\n  let copyNumber = 1;\n  let testId = slugifyPresetName(copyName);\n  while (data.presets[testId]) {\n    copyNumber++;\n    copyName = `${original.name} (Copy ${copyNumber})`;\n    testId = slugifyPresetName(copyName);\n  }\n\n  // Create the duplicate\n  return createCustomPreset({\n    name: copyName,\n    description: original.description || '',\n    icon: original.icon || 'settings',\n    settings: original.settings ? { ...original.settings } : getCurrentPresetSettings()\n  });\n}\n\n/**\n * Export all custom presets to JSON string\n * @returns {{success: boolean, data?: string, count?: number, error?: string}}\n */\nfunction exportPresets() {\n  try {\n    const data = loadCustomPresets();\n    const customPresets = data.order.map(id => data.presets[id]).filter(Boolean);\n\n    if (customPresets.length === 0) {\n      return { success: false, error: 'No custom presets to export' };\n    }\n\n    const exportData = {\n      version: 1,\n      exportedAt: new Date().toISOString(),\n      presets: customPresets\n    };\n\n    return {\n      success: true,\n      data: JSON.stringify(exportData, null, 2),\n      count: customPresets.length\n    };\n  } catch (e) {\n    return { success: false, error: 'Failed to export presets: ' + e.message };\n  }\n}\n\n/**\n * Import presets from JSON string\n * @param {string} jsonString - JSON string containing presets\n * @param {boolean} merge - If true, merge with existing; if false, skip duplicates\n * @returns {{success: boolean, imported?: number, skipped?: number, error?: string}}\n */\nfunction importPresets(jsonString, merge = true) {\n  try {\n    // Parse JSON\n    const imported = JSON.parse(jsonString);\n\n    // Validate structure\n    if (!imported || !imported.presets || !Array.isArray(imported.presets)) {\n      return { success: false, error: 'Invalid preset file format' };\n    }\n\n    let importedCount = 0;\n    let skippedCount = 0;\n\n    for (const preset of imported.presets) {\n      // Validate each preset has required fields\n      if (!preset.name || typeof preset.name !== 'string') {\n        skippedCount++;\n        continue;\n      }\n\n      // Check if a preset with this name already exists\n      const existingId = slugifyPresetName(preset.name);\n      const existing = getCustomPreset(existingId);\n\n      if (existing && !merge) {\n        // Skip existing if not merging\n        skippedCount++;\n        continue;\n      }\n\n      // If existing and merge mode, create with unique name\n      if (existing && merge) {\n        let newName = `${preset.name} (Imported)`;\n        let counter = 1;\n        while (getCustomPreset(slugifyPresetName(newName))) {\n          counter++;\n          newName = `${preset.name} (Imported ${counter})`;\n        }\n        preset.name = newName;\n      }\n\n      // Create the preset\n      const result = createCustomPreset({\n        name: preset.name,\n        description: preset.description || '',\n        icon: preset.icon || 'settings',\n        settings: preset.settings || getCurrentPresetSettings()\n      });\n\n      if (result.success) {\n        importedCount++;\n      } else {\n        skippedCount++;\n      }\n    }\n\n    if (importedCount === 0 && skippedCount > 0) {\n      return { success: false, error: 'All presets already exist or were invalid', skipped: skippedCount };\n    }\n\n    return {\n      success: true,\n      imported: importedCount,\n      skipped: skippedCount\n    };\n  } catch (e) {\n    if (e instanceof SyntaxError) {\n      return { success: false, error: 'Invalid JSON format' };\n    }\n    return { success: false, error: 'Failed to import presets: ' + e.message };\n  }\n}\n\n/**\n * Delete a custom preset\n * @param {string} id - Preset ID to delete\n * @returns {{success: boolean, error?: string}}\n */\nfunction deleteCustomPreset(id) {\n  // Validate input\n  if (!id || typeof id !== 'string') {\n    return { success: false, error: 'Preset ID is required' };\n  }\n\n  // Load current data\n  const data = loadCustomPresets();\n\n  // Check if preset exists\n  if (!data.presets[id]) {\n    return { success: false, error: 'Preset not found' };\n  }\n\n  const presetName = data.presets[id].name;\n\n  // Remove from presets object\n  delete data.presets[id];\n\n  // Remove from order array\n  const orderIndex = data.order.indexOf(id);\n  if (orderIndex !== -1) {\n    data.order.splice(orderIndex, 1);\n  }\n\n  // Save\n  const saved = saveCustomPresets(data);\n  if (!saved) {\n    return { success: false, error: 'Failed to save after deletion' };\n  }\n\n  console.log(`[SPLICE] Deleted custom preset: ${presetName} (${id})`);\n  return { success: true };\n}\n\n/**\n * List of built-in preset IDs (cannot be modified or deleted)\n */\nconst BUILT_IN_PRESET_IDS = ['custom', 'podcast', 'interview', 'reaction', 'tutorial', 'vlog'];\n\n/**\n * Check if a preset is a built-in preset\n * @param {string} id - Preset ID to check\n * @returns {boolean} True if built-in, false if custom\n */\nfunction isBuiltInPreset(id) {\n  return BUILT_IN_PRESET_IDS.includes(id);\n}\n\n/**\n * Get all presets (built-in + custom merged)\n * Built-in presets appear first, then custom presets in their creation order.\n * Each preset has isBuiltIn flag added.\n *\n * @returns {Array<Object>} Array of preset objects with metadata\n */\nfunction getAllPresets() {\n  // Get built-in presets\n  const builtInPresets = BUILT_IN_PRESET_IDS.map(id => {\n    const preset = PRESETS[id];\n    return {\n      id,\n      name: preset.name,\n      description: preset.description,\n      icon: preset.icon,\n      settings: preset.settings,\n      isBuiltIn: true\n    };\n  });\n\n  // Get custom presets\n  const customData = loadCustomPresets();\n  const customPresets = customData.order.map(id => {\n    const preset = customData.presets[id];\n    if (!preset) return null;\n    return {\n      ...preset,\n      isBuiltIn: false\n    };\n  }).filter(Boolean);\n\n  // Merge: built-in first, then custom\n  return [...builtInPresets, ...customPresets];\n}\n\n/**\n * Get all preset IDs (built-in + custom)\n * @returns {Array<string>} Array of preset IDs\n */\nfunction getAllPresetIds() {\n  const customData = loadCustomPresets();\n  return [...BUILT_IN_PRESET_IDS, ...customData.order];\n}\n\n/**\n * Get a preset by ID (built-in or custom)\n * @param {string} id - Preset ID\n * @returns {Object|null} Preset object or null if not found\n */\nfunction getPresetById(id) {\n  if (!id || typeof id !== 'string') {\n    return null;\n  }\n\n  // Check built-in first\n  if (isBuiltInPreset(id)) {\n    const preset = PRESETS[id];\n    return {\n      id,\n      name: preset.name,\n      description: preset.description,\n      icon: preset.icon,\n      settings: preset.settings,\n      isBuiltIn: true\n    };\n  }\n\n  // Check custom presets\n  const custom = getCustomPreset(id);\n  if (custom) {\n    return {\n      ...custom,\n      isBuiltIn: false\n    };\n  }\n\n  return null;\n}\n\nconst DEFAULT_SETTINGS = {\n  sensitivity: 50,\n  audioSource: 'original',\n  autoMarkBest: true,\n  enableTakesDetection: true,\n  rememberOptions: false,\n  optionsExpanded: false,\n  customerId: null,          // Stripe customer ID for billing\n  mediaFolderToken: null,    // Persistent token for media folder access\n  mediaFolderPath: null,     // Path to the media folder (for display)\n  activePreset: 'custom'     // Currently active preset\n};\n\n// =============================================================================\n// PRESET PROFILES\n// =============================================================================\n\n/**\n * Detection presets for different content types.\n * Each preset defines optimal settings for a specific use case.\n */\nconst PRESETS = {\n  // Custom - user-defined settings (default)\n  custom: {\n    name: 'Custom',\n    description: 'Your custom settings',\n    icon: 'settings',\n    settings: null // Uses current user settings\n  },\n\n  // Podcast - longer pauses are natural, be conservative\n  podcast: {\n    name: 'Podcast',\n    description: 'Longer natural pauses, conversational flow',\n    icon: 'mic',\n    settings: {\n      sensitivity: 35,           // Lower sensitivity (fewer cuts)\n      threshold: -35,            // Higher threshold (only deep silence)\n      minSilenceLength: 0.8,     // Longer minimum silence\n      paddingStart: 0.15,        // More padding for natural transitions\n      paddingEnd: 0.15,\n      mergeDistance: 0.3,        // Merge nearby silences\n      autoMarkBest: false,       // Don't auto-mark (manual review)\n      enableTakesDetection: true\n    }\n  },\n\n  // Interview - balanced, respects speaker pauses\n  interview: {\n    name: 'Interview',\n    description: 'Balanced cuts, preserves Q&A rhythm',\n    icon: 'people',\n    settings: {\n      sensitivity: 50,           // Medium sensitivity\n      threshold: -32,            // Moderate threshold\n      minSilenceLength: 0.5,     // Standard minimum silence\n      paddingStart: 0.12,        // Moderate padding\n      paddingEnd: 0.08,\n      mergeDistance: 0.2,\n      autoMarkBest: true,\n      enableTakesDetection: true\n    }\n  },\n\n  // Reaction video - fast pacing, quick cuts\n  reaction: {\n    name: 'Reaction',\n    description: 'Fast-paced, tight cuts for energy',\n    icon: 'bolt',\n    settings: {\n      sensitivity: 70,           // Higher sensitivity (more cuts)\n      threshold: -28,            // Lower threshold (catches more silence)\n      minSilenceLength: 0.3,     // Short minimum silence\n      paddingStart: 0.05,        // Minimal padding for snappy edits\n      paddingEnd: 0.03,\n      mergeDistance: 0.15,\n      autoMarkBest: true,\n      enableTakesDetection: false // Usually single-take content\n    }\n  },\n\n  // Tutorial/Educational - preserve thinking pauses\n  tutorial: {\n    name: 'Tutorial',\n    description: 'Preserves teaching pace, minimal cuts',\n    icon: 'school',\n    settings: {\n      sensitivity: 30,           // Low sensitivity\n      threshold: -38,            // Only deep silence\n      minSilenceLength: 1.0,     // Long minimum (preserve demo pauses)\n      paddingStart: 0.2,         // Extra padding for clarity\n      paddingEnd: 0.15,\n      mergeDistance: 0.4,\n      autoMarkBest: false,\n      enableTakesDetection: true\n    }\n  },\n\n  // Vlog/YouTube - punchy edits, engagement-focused\n  vlog: {\n    name: 'Vlog',\n    description: 'Punchy edits for YouTube engagement',\n    icon: 'videocam',\n    settings: {\n      sensitivity: 65,           // Higher sensitivity\n      threshold: -30,            // Moderate-low threshold\n      minSilenceLength: 0.35,    // Short silences\n      paddingStart: 0.08,        // Quick transitions\n      paddingEnd: 0.05,\n      mergeDistance: 0.2,\n      autoMarkBest: true,\n      enableTakesDetection: true\n    }\n  }\n};\n\n/**\n * Get all available presets\n * @returns {Object} All preset definitions\n */\nfunction getPresets() {\n  return { ...PRESETS };\n}\n\n/**\n * Get a specific preset by name\n * @param {string} presetName - Name of the preset\n * @returns {Object|null} Preset definition or null if not found\n */\nfunction getPreset(presetName) {\n  return PRESETS[presetName] || null;\n}\n\n/**\n * Get preset names as array (for UI)\n * @returns {Array<string>} Array of preset names\n */\nfunction getPresetNames() {\n  return Object.keys(PRESETS);\n}\n\n/**\n * Apply a preset to current settings\n * @param {string} presetName - Name of the preset to apply\n * @returns {Object} The applied settings\n */\nfunction applyPreset(presetName) {\n  const preset = PRESETS[presetName];\n\n  if (!preset) {\n    console.warn(`[SPLICE] Unknown preset: ${presetName}`);\n    return getSettings();\n  }\n\n  // Custom preset uses current settings\n  if (presetName === 'custom' || !preset.settings) {\n    saveSettings({ activePreset: 'custom' });\n    return getSettings();\n  }\n\n  // Apply preset settings\n  const newSettings = {\n    ...preset.settings,\n    activePreset: presetName\n  };\n\n  saveSettings(newSettings);\n  console.log(`[SPLICE] Applied preset: ${preset.name}`);\n\n  return getSettings();\n}\n\n/**\n * Get the currently active preset\n * @returns {string} Active preset name\n */\nfunction getActivePreset() {\n  const settings = getSettings();\n  return settings.activePreset || 'custom';\n}\n\n/**\n * Check if current settings match a preset\n * @param {string} presetName - Preset to compare against\n * @returns {boolean} True if settings match the preset\n */\nfunction settingsMatchPreset(presetName) {\n  const preset = PRESETS[presetName];\n  if (!preset || !preset.settings) return presetName === 'custom';\n\n  const current = getSettings();\n\n  // Compare relevant settings\n  return Object.keys(preset.settings).every(key => {\n    if (key === 'activePreset') return true;\n    return current[key] === preset.settings[key];\n  });\n}\n\nlet currentSettings = { ...DEFAULT_SETTINGS };\n\n/**\n * Load settings from localStorage\n */\nfunction loadSettings() {\n  try {\n    const saved = localStorage.getItem('spliceSettings');\n    if (saved) {\n      currentSettings = { ...DEFAULT_SETTINGS, ...JSON.parse(saved) };\n    }\n  } catch (e) {\n    console.warn('[SPLICE] Could not load settings:', e);\n    currentSettings = { ...DEFAULT_SETTINGS };\n  }\n  return currentSettings;\n}\n\n/**\n * Save settings to localStorage\n */\nfunction saveSettings(settings) {\n  try {\n    currentSettings = { ...currentSettings, ...settings };\n    localStorage.setItem('spliceSettings', JSON.stringify(currentSettings));\n  } catch (e) {\n    console.warn('[SPLICE] Could not save settings:', e);\n  }\n}\n\n/**\n * Get current settings\n */\nfunction getSettings() {\n  return { ...currentSettings };\n}\n\n/**\n * Reset settings to defaults\n */\nfunction resetSettings() {\n  currentSettings = { ...DEFAULT_SETTINGS };\n  try {\n    localStorage.removeItem('spliceSettings');\n  } catch (e) {\n    console.warn('[SPLICE] Could not clear settings:', e);\n  }\n}\n\n/**\n * Initialize settings UI components\n */\nfunction initSettingsUI() {\n  const settings = loadSettings();\n\n  // Apply saved sensitivity to slider\n  const sensitivitySlider = document.getElementById('sensitivitySlider');\n  if (sensitivitySlider) {\n    sensitivitySlider.value = settings.sensitivity;\n  }\n\n  // Apply saved audio source\n  const sourceOriginal = document.getElementById('sourceOriginal');\n  const sourceIsolated = document.getElementById('sourceIsolated');\n  if (sourceOriginal && sourceIsolated) {\n    sourceOriginal.checked = settings.audioSource === 'original' || settings.audioSource === 'both';\n    sourceIsolated.checked = settings.audioSource === 'isolated' || settings.audioSource === 'both';\n  }\n\n  // Apply auto-mark best setting\n  const autoMarkBest = document.getElementById('autoMarkBest');\n  if (autoMarkBest) {\n    autoMarkBest.checked = settings.autoMarkBest;\n  }\n\n  // Apply takes detection setting\n  const enableTakesDetection = document.getElementById('enableTakesDetection');\n  if (enableTakesDetection) {\n    enableTakesDetection.checked = settings.enableTakesDetection;\n  }\n\n  // Apply remember options setting\n  const rememberOptions = document.getElementById('rememberOptions');\n  if (rememberOptions) {\n    rememberOptions.checked = settings.rememberOptions;\n  }\n\n  // Restore expanded state if remember is enabled\n  if (settings.rememberOptions && settings.optionsExpanded) {\n    toggleOptionsPanel(true);\n  }\n}\n\n/**\n * Toggle options panel visibility (unified panel)\n */\nfunction toggleOptionsPanel(forceState) {\n  const toggle = document.getElementById('optionsToggle');\n  const panel = document.getElementById('optionsPanel');\n\n  if (!toggle || !panel) return;\n\n  const shouldExpand = forceState !== undefined ? forceState : panel.classList.contains('collapsed');\n\n  if (shouldExpand) {\n    panel.classList.remove('collapsed');\n    toggle.classList.add('expanded');\n  } else {\n    panel.classList.add('collapsed');\n    toggle.classList.remove('expanded');\n  }\n\n  // Save state if remember is enabled\n  const settings = getSettings();\n  if (settings.rememberOptions) {\n    saveSettings({ optionsExpanded: shouldExpand });\n  }\n}\n\n/**\n * Initialize settings modal\n */\nfunction initSettingsModal() {\n  const settingsBtn = document.getElementById('settingsBtn');\n  const modal = document.getElementById('settingsModal');\n  const closeBtn = document.getElementById('closeSettingsBtn');\n  const defaultSensitivity = document.getElementById('defaultSensitivity');\n  const rememberOptions = document.getElementById('rememberOptions');\n\n  if (settingsBtn && modal) {\n    settingsBtn.addEventListener('click', () => {\n      modal.classList.remove('hidden');\n      // Sync modal values with current settings\n      const settings = getSettings();\n      if (defaultSensitivity) defaultSensitivity.value = settings.sensitivity;\n      if (rememberOptions) rememberOptions.checked = settings.rememberOptions;\n    });\n  }\n\n  if (closeBtn && modal) {\n    closeBtn.addEventListener('click', () => {\n      modal.classList.add('hidden');\n    });\n  }\n\n  // Close on backdrop click\n  if (modal) {\n    modal.addEventListener('click', (e) => {\n      if (e.target === modal) {\n        modal.classList.add('hidden');\n      }\n    });\n  }\n\n  // Save default sensitivity when changed\n  if (defaultSensitivity) {\n    defaultSensitivity.addEventListener('change', () => {\n      saveSettings({ sensitivity: parseInt(defaultSensitivity.value) });\n      // Also update the main slider\n      const mainSlider = document.getElementById('sensitivitySlider');\n      if (mainSlider) mainSlider.value = defaultSensitivity.value;\n    });\n  }\n\n  // Save remember options when changed\n  if (rememberOptions) {\n    rememberOptions.addEventListener('change', () => {\n      saveSettings({ rememberOptions: rememberOptions.checked });\n    });\n  }\n}\n\n/**\n * Initialize options toggle (unified panel)\n */\nfunction initOptionsToggles() {\n  const toggle = document.getElementById('optionsToggle');\n\n  if (toggle) {\n    toggle.addEventListener('click', () => toggleOptionsPanel());\n  }\n}\n\n/**\n * Initialize preset selector\n */\nfunction initPresetSelector() {\n  const presetSelector = document.getElementById('presetSelector');\n  const sensitivitySlider = document.getElementById('sensitivitySlider');\n\n  if (!presetSelector) return;\n\n  // Set initial value from settings\n  const settings = getSettings();\n  presetSelector.value = settings.activePreset || 'custom';\n\n  // Handle preset change (supports both built-in and custom presets)\n  presetSelector.addEventListener('change', () => {\n    const presetId = presetSelector.value;\n\n    // Get preset info (works for both built-in and custom)\n    const preset = getPresetById(presetId);\n    if (!preset) {\n      console.warn(`[SPLICE] Preset not found: ${presetId}`);\n      return;\n    }\n\n    let appliedSettings;\n\n    if (preset.isBuiltIn) {\n      // Built-in preset: use the existing applyPreset function\n      appliedSettings = applyPreset(presetId);\n    } else {\n      // Custom preset: apply stored settings\n      appliedSettings = preset.settings || {};\n      saveSettings({ activePreset: presetId, ...appliedSettings });\n    }\n\n    // Update sensitivity slider to match preset\n    if (sensitivitySlider && appliedSettings.sensitivity !== undefined) {\n      sensitivitySlider.value = appliedSettings.sensitivity;\n    }\n\n    // Update takes detection checkbox\n    const enableTakesDetection = document.getElementById('enableTakesDetection');\n    if (enableTakesDetection && appliedSettings.enableTakesDetection !== undefined) {\n      enableTakesDetection.checked = appliedSettings.enableTakesDetection;\n    }\n\n    // Update auto-mark best checkbox\n    const autoMarkBest = document.getElementById('autoMarkBest');\n    if (autoMarkBest && appliedSettings.autoMarkBest !== undefined) {\n      autoMarkBest.checked = appliedSettings.autoMarkBest;\n    }\n\n    if (typeof setStatus === 'function') {\n      const desc = preset.description ? ` - ${preset.description}` : '';\n      setStatus(`Preset: ${preset.name}${desc}`);\n    }\n  });\n\n  // Switch to custom when user manually changes sensitivity\n  if (sensitivitySlider) {\n    sensitivitySlider.addEventListener('change', () => {\n      if (presetSelector.value !== 'custom') {\n        presetSelector.value = 'custom';\n        saveSettings({ activePreset: 'custom' });\n      }\n    });\n  }\n}\n\n/**\n * Initialize help button\n */\nfunction initHelpButton() {\n  const helpBtn = document.getElementById('helpBtn');\n  if (helpBtn) {\n    helpBtn.addEventListener('click', () => {\n      setStatus('Silence: removes quiet gaps | Takes: detects repeated content');\n    });\n  }\n}\n\n// =============================================================================\n// PERSISTENT TOKEN MANAGEMENT (v3.5)\n// =============================================================================\n\n/**\n * Media folder entry (cached after selection)\n * @type {Object|null}\n */\nlet mediaFolderEntry = null;\n\n/**\n * Prompt user to select a media folder and create a persistent token.\n * This allows silent file access without repeated permission prompts.\n *\n * @returns {Promise<{success: boolean, path?: string, error?: string}>}\n */\nasync function setupMediaFolder() {\n  try {\n    // Open folder picker dialog\n    const folder = await uxpFs.getFolder();\n\n    if (!folder) {\n      return { success: false, error: 'No folder selected' };\n    }\n\n    // Create a persistent token for this folder\n    const token = await uxpFs.createPersistentToken(folder);\n\n    if (!token) {\n      return { success: false, error: 'Failed to create persistent token' };\n    }\n\n    // Get the folder path for display\n    const folderPath = folder.nativePath || folder.name;\n\n    // Save to settings\n    saveSettings({\n      mediaFolderToken: token,\n      mediaFolderPath: folderPath\n    });\n\n    // Cache the folder entry\n    mediaFolderEntry = folder;\n\n    console.log(`[SPLICE] Media folder set: ${folderPath}`);\n\n    return {\n      success: true,\n      path: folderPath,\n      token\n    };\n  } catch (err) {\n    console.error('[SPLICE] Error setting up media folder:', err);\n    return { success: false, error: err.message };\n  }\n}\n\n/**\n * Get the media folder entry using the stored persistent token.\n * Falls back to prompting if token is invalid.\n *\n * @returns {Promise<Object|null>} The folder entry or null\n */\nasync function getMediaFolder() {\n  // Return cached entry if available\n  if (mediaFolderEntry) {\n    return mediaFolderEntry;\n  }\n\n  const settings = getSettings();\n\n  // Try to restore from persistent token\n  if (settings.mediaFolderToken) {\n    try {\n      const folder = await uxpFs.getEntryForPersistentToken(settings.mediaFolderToken);\n      if (folder) {\n        mediaFolderEntry = folder;\n        console.log(`[SPLICE] Restored media folder from token: ${settings.mediaFolderPath}`);\n        return folder;\n      }\n    } catch (err) {\n      console.warn('[SPLICE] Could not restore media folder from token:', err.message);\n      // Clear invalid token\n      saveSettings({ mediaFolderToken: null, mediaFolderPath: null });\n    }\n  }\n\n  return null;\n}\n\n/**\n * Check if a media folder is configured\n * @returns {boolean}\n */\nfunction hasMediaFolder() {\n  const settings = getSettings();\n  return !!settings.mediaFolderToken;\n}\n\n/**\n * Get the configured media folder path (for display)\n * @returns {string|null}\n */\nfunction getMediaFolderPath() {\n  const settings = getSettings();\n  return settings.mediaFolderPath;\n}\n\n/**\n * Clear the media folder configuration\n */\nfunction clearMediaFolder() {\n  mediaFolderEntry = null;\n  saveSettings({\n    mediaFolderToken: null,\n    mediaFolderPath: null\n  });\n  console.log('[SPLICE] Media folder cleared');\n}\n\n/**\n * Read a file from the media folder\n * @param {string} filename - Name of the file to read\n * @returns {Promise<ArrayBuffer|null>} File contents or null\n */\nasync function readMediaFile(filename) {\n  const folder = await getMediaFolder();\n  if (!folder) {\n    console.warn('[SPLICE] No media folder configured');\n    return null;\n  }\n\n  try {\n    const file = await folder.getEntry(filename);\n    if (!file) {\n      console.warn(`[SPLICE] File not found in media folder: ${filename}`);\n      return null;\n    }\n\n    const contents = await file.read({ format: uxpFs.formats.binary });\n    return contents;\n  } catch (err) {\n    console.error(`[SPLICE] Error reading file ${filename}:`, err);\n    return null;\n  }\n}\n\n// =============================================================================\n// LOGIN MODAL\n// =============================================================================\n\n/**\n * Initialize login modal handlers\n */\nfunction initLoginModal() {\n  const loginModal = document.getElementById('loginModal');\n  const closeLoginBtn = document.getElementById('closeLoginBtn');\n  const saveLoginBtn = document.getElementById('saveLoginBtn');\n  const customerIdInput = document.getElementById('customerIdInput');\n  const loginError = document.getElementById('loginError');\n  const creditBadge = document.getElementById('creditBadge');\n\n  // Show login modal when clicking credit badge (always - allows viewing/changing ID)\n  if (creditBadge) {\n    creditBadge.addEventListener('click', () => {\n      showLoginModal();\n    });\n  }\n\n  // Close login modal\n  if (closeLoginBtn && loginModal) {\n    closeLoginBtn.addEventListener('click', () => {\n      loginModal.classList.add('hidden');\n    });\n  }\n\n  // Close on backdrop click\n  if (loginModal) {\n    loginModal.addEventListener('click', (e) => {\n      if (e.target === loginModal) {\n        loginModal.classList.add('hidden');\n      }\n    });\n  }\n\n  // Save customer ID\n  if (saveLoginBtn) {\n    saveLoginBtn.addEventListener('click', async () => {\n      const customerId = customerIdInput?.value?.trim();\n\n      if (!customerId) {\n        showLoginError('Please enter your customer ID');\n        return;\n      }\n\n      if (!customerId.startsWith('cus_')) {\n        showLoginError('Customer ID should start with \"cus_\"');\n        return;\n      }\n\n      // Save to settings\n      saveSettings({ customerId });\n\n      // Hide modal\n      loginModal?.classList.add('hidden');\n\n      // Refresh credits display\n      if (typeof refreshCredits === 'function') {\n        await refreshCredits();\n      }\n\n      if (typeof setStatus === 'function') {\n        setStatus('Logged in successfully');\n      }\n    });\n  }\n\n  // Load existing customer ID into input\n  const settings = getSettings();\n  if (customerIdInput && settings.customerId) {\n    customerIdInput.value = settings.customerId;\n  }\n}\n\n/**\n * Show login modal\n */\nfunction showLoginModal() {\n  const loginModal = document.getElementById('loginModal');\n  const loginError = document.getElementById('loginError');\n\n  if (loginError) {\n    loginError.style.display = 'none';\n  }\n\n  if (loginModal) {\n    loginModal.classList.remove('hidden');\n  }\n}\n\n/**\n * Show login error message\n */\nfunction showLoginError(message) {\n  const loginError = document.getElementById('loginError');\n  if (loginError) {\n    loginError.textContent = message;\n    loginError.style.display = 'block';\n  }\n}\n\n/**\n * Check if user is logged in\n * @returns {boolean}\n */\nfunction isLoggedIn() {\n  const settings = getSettings();\n  return !!settings.customerId;\n}\n\n/**\n * Get customer ID\n * @returns {string|null}\n */\nfunction getCustomerId() {\n  const settings = getSettings();\n  return settings.customerId || null;\n}\n\n/**\n * Logout - clear customer ID\n */\nfunction logout() {\n  saveSettings({ customerId: null });\n  if (typeof setStatus === 'function') {\n    setStatus('Logged out');\n  }\n}\n\n// Load settings on script load\nloadSettings();\n","numLines":1307,"startLine":1,"totalLines":1307}}}
{"parentUuid":"0a1520f9-d934-4627-8598-8321806b5069","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"85f94ac8-c7d6-4c06-b3fe-8e85ea14763a","version":"2.0.76","gitBranch":"main","agentId":"a81d490","slug":"refactored-watching-quasar","message":{"model":"claude-opus-4-5-20251101","id":"msg_01NzsCMne2Rbe3uEaE1UgvwU","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have a comprehensive view of the codebase. Let me compile my performance analysis report.\n\n---\n\n# SPLICE Performance Bottleneck Analysis\n\n## Performance Summary\n\n| Metric | Current | Target | Gap |\n|--------|---------|--------|-----|\n| Database Queries/Request | 3-5 | 1-2 | 2-3x |\n| Memory (RMS Audio Processing) | Full file in RAM | Chunked | Variable |\n| Blocking I/O | sync fs operations | async | N/A |\n| Connection Pool | No limit | 10-20 | Potential exhaustion |\n| Cache Size | 50 entries | Configurable | Memory leak risk |\n\n---\n\n## Critical Bottlenecks\n\n### [PERF-001] Database Connection Pool Not Limited - HIGH\n\n**Impact:** Potential connection pool exhaustion under load\n**Frequency:** Every request that touches usageTracking.js\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js:10-14`\n\n```javascript\nconst pool = new Pool({\n  connectionString: process.env.DATABASE_URL,\n  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false\n});\n```\n\n**Problem:** No `max` pool size configured. Under high load, this can exhaust database connections.\n\n**Recommendation:** Add pool limits:\n```javascript\nconst pool = new Pool({\n  connectionString: process.env.DATABASE_URL,\n  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,\n  max: 20,          // Maximum connections\n  idleTimeoutMillis: 30000,\n  connectionTimeoutMillis: 2000\n});\n```\n\n---\n\n### [PERF-002] Multiple DB Queries Per Balance Check - HIGH\n\n**Impact:** 3-5 database round trips per billing check\n**Frequency:** Every API request with `requireCredits`\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js:150-177` + `:182-186`\n\n**Current Flow:**\n1. `hasCredits()` calls `getBalance()` (line 183)\n2. `getBalance()` calls `getOrCreateUser()` (line 151)\n3. `getOrCreateUser()` does SELECT then potentially INSERT (lines 124-145)\n4. If insufficient, `getBalance()` called again in rateLimiter (line 37)\n\n```javascript\n// rateLimiter.js:32-46\nconst hasCreds = await hasCredits(stripeCustomerId, req.estimatedDuration || 0);\nif (!hasCreds) {\n  const balance = await getBalance(stripeCustomerId);  // DUPLICATE QUERY\n  return res.status(402).json({...});\n}\n```\n\n**Recommendation:** Cache user balance in memory with short TTL (5-10 seconds) or return balance from `hasCredits()`:\n\n```javascript\nasync function hasCredits(stripeCustomerId, estimatedSeconds = 0) {\n  const balance = await getBalance(stripeCustomerId);\n  const estimatedHours = estimatedSeconds / 3600;\n  return {\n    hasCredits: balance.hoursRemaining >= estimatedHours,\n    balance  // Return balance to avoid second query\n  };\n}\n```\n\n---\n\n### [PERF-003] Synchronous File Read in multitrackAnalysis.js - HIGH\n\n**Impact:** Blocks event loop during file I/O\n**Frequency:** Every multitrack analysis request\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js:196-199`\n\n```javascript\nconst rawData = fs.readFileSync(tempFile);  // BLOCKING\nconst samples = new Float32Array(rawData.length / 2);\nfor (let i = 0; i < samples.length; i++) {\n  samples[i] = rawData.readInt16LE(i * 2) / 32768.0;\n}\n```\n\n**Problem:** `fs.readFileSync` blocks the Node.js event loop. For large audio files, this can cause request timeouts for other clients.\n\n**Recommendation:** Use `fs.promises.readFile`:\n```javascript\nconst rawData = await fs.promises.readFile(tempFile);\n```\n\nThe same pattern exists in `rmsSilenceDetection.js` but is already using async (`fsPromises.readFile`).\n\n---\n\n### [PERF-004] Temp File Cleanup Race Condition - MEDIUM\n\n**Impact:** Orphaned temp files accumulating on disk\n**Frequency:** When errors occur during processing\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js:229`\n\n```javascript\nif (fs.existsSync(tempFile)) fs.unlinkSync(tempFile);\n```\n\n**Problem:** Uses sync file operations and the cleanup is in the catch block but not guaranteed.\n\n**Recommendation:** Use async with try/finally pattern:\n```javascript\ntry {\n  // processing\n} finally {\n  try {\n    await fs.promises.unlink(tempFile);\n  } catch {}\n}\n```\n\n---\n\n### [PERF-005] RMS Calculation Loop Inefficiency - MEDIUM\n\n**Impact:** CPU-intensive for long audio files\n**Frequency:** Every RMS silence detection\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:401-421`\n\n```javascript\nfor (let i = 0; i < numWindows; i++) {\n  const start = i * hopSize;\n  const end = Math.min(start + windowSize, samples.length);\n  let sumSquares = 0;\n  for (let j = start; j < end; j++) {\n    sumSquares += samples[j] * samples[j];  // Hot path\n  }\n  const rms = Math.sqrt(sumSquares / (end - start));\n  rmsValues[i] = rms;\n}\n```\n\n**Problem:** Nested loop with O(n*m) complexity where n = windows, m = samples per window.\n\n**Recommendation:** Consider:\n1. Use SIMD via WebAssembly for audio processing\n2. Use a running sum algorithm that slides the window\n3. Downsample more aggressively (currently 16kHz, could go to 8kHz for detection)\n\n---\n\n### [PERF-006] Levenshtein Distance O(n*m) in Hot Path - MEDIUM\n\n**Impact:** Quadratic complexity for repetition detection\n**Frequency:** Every phrase comparison in repetition detection\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/repetitionDetection.js:429-456`\n\n```javascript\nfunction levenshteinDistance(s1, s2) {\n  const m = s1.length;\n  const n = s2.length;\n  const dp = Array(m + 1).fill(null).map(() => Array(n + 1).fill(0));  // O(m*n) allocation\n  // ...\n}\n```\n\n**Problem:** Creates new 2D array for every comparison. Called in `calculateSimilarity()` which is called for every word pair.\n\n**Recommendation:**\n1. Pre-allocate a reusable buffer\n2. Use only two rows instead of full matrix (rolling window)\n3. Add early termination when distance exceeds threshold\n\n---\n\n### [PERF-007] Plugin DOM Queries Not Cached - MEDIUM\n\n**Impact:** Repeated DOM lookups\n**Frequency:** Every UI update\n**Location:** `/Users/imorgado/SPLICE/splice-plugin/js/main.js:894-925`\n\n```javascript\nfunction selectAllSilences(checked) {\n  // ...\n  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n    // ...\n  });\n}\n\nfunction invertSilenceSelection() {\n  // ...\n  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n    // ...\n  });\n}\n```\n\n**Problem:** Same DOM query executed multiple times. `querySelectorAll` is relatively expensive.\n\n**Recommendation:** Cache the NodeList when the preview is built and reuse:\n```javascript\nlet cachedCheckboxes = null;\n\nfunction showCombinedPreview() {\n  // ... build DOM ...\n  cachedCheckboxes = ui.previewList.querySelectorAll('.preview-item-check');\n}\n```\n\n---\n\n### [PERF-008] In-Memory Batch Job Storage Unbounded - MEDIUM\n\n**Impact:** Memory leak over time\n**Frequency:** When batch jobs accumulate\n**Location:** `/Users/imorgado/SPLICE/splice-backend/server.js:1534-1538`\n\n```javascript\nconst batchJobs = new Map();\nconst MAX_BATCH_JOBS = 10000;  // Very high limit\nconst BATCH_JOB_MAX_AGE_MS = 24 * 60 * 60 * 1000; // 24 hours\n```\n\n**Problem:** While cleanup exists (line 1589), 10,000 jobs with full result data can consume significant memory (potentially 100MB+ if each job has many silences).\n\n**Recommendation:** \n1. Store only job metadata in memory, results in temp files or Redis\n2. Lower `MAX_BATCH_JOBS` to 100-500\n3. Reduce `BATCH_JOB_MAX_AGE_MS` to 1-2 hours\n\n---\n\n### [PERF-009] Missing Database Indexes - MEDIUM\n\n**Impact:** Full table scans on usage_log queries\n**Frequency:** Every usage history request\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js:413-421`\n\n```javascript\nconst result = await pool.query(\n  `SELECT * FROM usage_log\n   WHERE stripe_customer_id = $1\n   ORDER BY created_at DESC\n   LIMIT $2`,\n  [stripeCustomerId, limit]\n);\n```\n\n**Current indexes (line 90-95):**\n- `idx_users_stripe_customer_id`\n- `idx_usage_log_user_id`\n\n**Missing:**\n- `idx_usage_log_stripe_customer_id` - The query filters by `stripe_customer_id`, not `user_id`\n- `idx_usage_log_created_at` - For ORDER BY optimization\n\n**Recommendation:** Add:\n```sql\nCREATE INDEX IF NOT EXISTS idx_usage_log_customer_created \nON usage_log(stripe_customer_id, created_at DESC);\n```\n\n---\n\n### [PERF-010] Transcription Cache Memory Leak Risk - LOW\n\n**Impact:** Cache grows to 50 entries with potentially large objects\n**Frequency:** Every transcription\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/transcription.js:17-21`\n\n```javascript\nconst transcriptCache = new Map();\nconst MAX_CACHE_SIZE = 50;\n```\n\n**Problem:** Each cached entry contains full transcript with all words/segments. 50 large transcripts could be 10-50MB.\n\n**Recommendation:**\n1. Add memory-based eviction (not just count-based)\n2. Consider using an LRU cache library like `lru-cache` with max memory setting\n3. Store cache on disk for large transcripts\n\n---\n\n### [PERF-011] Sequential Batch Processing - LOW\n\n**Impact:** Suboptimal throughput for batch jobs\n**Frequency:** Batch processing requests\n**Location:** `/Users/imorgado/SPLICE/splice-backend/server.js:1685`\n\n```javascript\n// Process files sequentially to avoid overwhelming the system\nfor (let i = 0; i < job.files.length; i++) {\n```\n\n**Problem:** Files are processed one at a time. While intentional to avoid overload, this is slower than controlled parallelism.\n\n**Recommendation:** Use a semaphore pattern to process 2-3 files concurrently:\n```javascript\nconst pLimit = require('p-limit');\nconst limit = pLimit(3);  // 3 concurrent\n\nawait Promise.all(job.files.map(file => \n  limit(() => processFile(file))\n));\n```\n\n---\n\n## Quick Wins\n\n| Fix | Effort | Expected Improvement |\n|-----|--------|---------------------|\n| Add database connection pool limits | 5 min | Prevents connection exhaustion |\n| Cache balance in rateLimiter | 15 min | -2 DB queries per request |\n| Fix sync fs.readFileSync in multitrack | 5 min | Unblocks event loop |\n| Add missing usage_log index | 2 min | 10-100x faster history queries |\n| Lower MAX_BATCH_JOBS to 500 | 1 min | Prevents memory issues |\n\n---\n\n## Architecture Recommendations\n\n1. **Add Redis for Session/Balance Caching**\n   - Cache user balance for 10 seconds\n   - Store batch job results (not in-process memory)\n   - Reduces database load by 60-80%\n\n2. **Move Audio Processing to Worker Threads**\n   - RMS calculation is CPU-bound\n   - Use `worker_threads` module for parallel processing\n   - Keeps event loop responsive\n\n3. **Consider Streaming for Large Audio Files**\n   - Current approach loads entire file into memory\n   - Stream processing would reduce peak memory usage\n\n4. **Add Request-Level Metrics**\n   - Instrument endpoints with timing\n   - Track database query counts per request\n   - Enables data-driven optimization\n\n---\n\n*Analysis completed: 11 performance issues identified across database, memory, I/O, and algorithmic categories. Priority should be given to [PERF-001], [PERF-002], and [PERF-003] for immediate production impact.*"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1210,"cache_creation_input_tokens":39700,"cache_read_input_tokens":98797,"cache_creation":{"ephemeral_5m_input_tokens":39700,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUCPtrpr2uPdk1BrikqM","type":"assistant","uuid":"3095a569-fef1-4238-9b4b-d9806e7c1a01","timestamp":"2025-12-25T22:31:52.374Z"}
