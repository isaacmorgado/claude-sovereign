{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":"Analyze the SPLICE codebase for potential performance bottlenecks and issues. Focus on:\n\n1. Memory leaks or unbounded growth\n2. Missing error handling\n3. Blocking I/O operations\n4. Database connection handling\n5. API call inefficiencies\n6. Large file processing concerns\n\nKey files to check:\n- /Users/imorgado/SPLICE/splice-backend/server.js\n- /Users/imorgado/SPLICE/splice-backend/services/transcription.js\n- /Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js\n- /Users/imorgado/SPLICE/splice-backend/services/usageTracking.js\n- /Users/imorgado/SPLICE/splice-plugin/js/main.js\n- /Users/imorgado/SPLICE/splice-plugin/js/builder.js\n\nDO NOT make any changes - just report findings in a concise list format."},"uuid":"c4830135-dce0-4cef-ade3-96bf050f7067","timestamp":"2025-12-25T20:25:05.896Z"}
{"parentUuid":"c4830135-dce0-4cef-ade3-96bf050f7067","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Job9BQYUDtJXQRoKKDBAk8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017h5JGvzh6RAjhzmNHfJi62","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/server.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":16420,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":16420,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWU2pBLmiG99Zgo6bde5v","type":"assistant","uuid":"c04935e1-9bf1-47a0-a8a8-8679c2ebf252","timestamp":"2025-12-25T20:25:11.578Z"}
{"parentUuid":"c04935e1-9bf1-47a0-a8a8-8679c2ebf252","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Job9BQYUDtJXQRoKKDBAk8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01JYjNgSYZBE2PpAYHi2B1Rx","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/transcription.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":16420,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":16420,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWU2pBLmiG99Zgo6bde5v","type":"assistant","uuid":"27543354-d2bb-41c8-8d5f-62f4c10f5e07","timestamp":"2025-12-25T20:25:11.963Z"}
{"parentUuid":"27543354-d2bb-41c8-8d5f-62f4c10f5e07","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Job9BQYUDtJXQRoKKDBAk8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01LfWcEqX7SnUZnp29Vdr7sB","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":16420,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":16420,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWU2pBLmiG99Zgo6bde5v","type":"assistant","uuid":"4477b80d-dc4e-4459-915b-3f2306ca986a","timestamp":"2025-12-25T20:25:12.392Z"}
{"parentUuid":"4477b80d-dc4e-4459-915b-3f2306ca986a","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Job9BQYUDtJXQRoKKDBAk8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NF2hx567tPaAV55dD1gWbu","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":16420,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":16420,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWU2pBLmiG99Zgo6bde5v","type":"assistant","uuid":"f6dfd4da-6b9d-42ff-8309-dc6445cdde62","timestamp":"2025-12-25T20:25:12.723Z"}
{"parentUuid":"f6dfd4da-6b9d-42ff-8309-dc6445cdde62","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Job9BQYUDtJXQRoKKDBAk8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018wn9BfrZZuv4N9ZvpkNCjN","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-plugin/js/main.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":16420,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":16420,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWU2pBLmiG99Zgo6bde5v","type":"assistant","uuid":"fb3fe48b-aca2-4957-8fa4-3a4fff0b3378","timestamp":"2025-12-25T20:25:13.088Z"}
{"parentUuid":"fb3fe48b-aca2-4957-8fa4-3a4fff0b3378","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Job9BQYUDtJXQRoKKDBAk8","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MSxnGFFipa5YaS1dD9S3Ak","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-plugin/js/builder.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":16420,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":16420,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWU2pBLmiG99Zgo6bde5v","type":"assistant","uuid":"f76a2a9c-0d51-4c96-8a39-49989e5641b8","timestamp":"2025-12-25T20:25:13.447Z"}
{"parentUuid":"f76a2a9c-0d51-4c96-8a39-49989e5641b8","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01JYjNgSYZBE2PpAYHi2B1Rx","type":"tool_result","content":"     1→/**\n     2→ * Slice 4: GPT-4o-mini Transcription Service\n     3→ *\n     4→ * Handles audio transcription using OpenAI's gpt-4o-mini-transcribe model.\n     5→ * 50% cheaper than Whisper ($0.003/min vs $0.006/min).\n     6→ * Returns timestamped segments for take detection.\n     7→ * Includes caching to avoid repeated API calls.\n     8→ */\n     9→\n    10→const fs = require('fs');\n    11→const OpenAI = require('openai');\n    12→\n    13→// Initialize OpenAI client\n    14→const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n    15→\n    16→// In-memory cache: { wavPath: { mtime, result } }\n    17→const transcriptCache = new Map();\n    18→\n    19→// Cache configuration\n    20→const MAX_CACHE_SIZE = 50;\n    21→\n    22→/**\n    23→ * Retry function with exponential backoff\n    24→ * @param {Function} fn - Async function to retry\n    25→ * @param {number} retries - Number of retry attempts (default: 3)\n    26→ * @param {number} baseDelay - Base delay in ms (default: 1000)\n    27→ * @returns {Promise<any>} Result of the function\n    28→ */\n    29→async function withRetry(fn, retries = 3, baseDelay = 1000) {\n    30→  let lastError;\n    31→\n    32→  for (let attempt = 0; attempt <= retries; attempt++) {\n    33→    try {\n    34→      return await fn();\n    35→    } catch (err) {\n    36→      lastError = err;\n    37→\n    38→      // Don't retry on quota exceeded - fail immediately\n    39→      if (err.code === 'insufficient_quota' || err.message?.includes('quota')) {\n    40→        throw new Error('OpenAI API quota exceeded. Please check your billing at https://platform.openai.com/account/billing');\n    41→      }\n    42→\n    43→      // Don't retry on the last attempt\n    44→      if (attempt === retries) {\n    45→        break;\n    46→      }\n    47→\n    48→      // Calculate delay with exponential backoff + jitter\n    49→      const delay = baseDelay * Math.pow(2, attempt) + Math.random() * 500;\n    50→\n    51→      // Only retry on retryable errors\n    52→      const isRetryable =\n    53→        err.status === 429 ||                          // Rate limit\n    54→        err.status === 500 ||                          // Server error\n    55→        err.status === 502 ||                          // Bad gateway\n    56→        err.status === 503 ||                          // Service unavailable\n    57→        err.code === 'ECONNRESET' ||                   // Connection reset\n    58→        err.code === 'ETIMEDOUT' ||                    // Timeout\n    59→        err.message?.includes('Connection error') ||   // Connection error\n    60→        err.message?.includes('network');              // Network error\n    61→\n    62→      if (!isRetryable) {\n    63→        throw err;\n    64→      }\n    65→\n    66→      console.log(`[SPLICE] Retry ${attempt + 1}/${retries} after ${Math.round(delay)}ms: ${err.message}`);\n    67→      await new Promise(resolve => setTimeout(resolve, delay));\n    68→    }\n    69→  }\n    70→\n    71→  throw lastError;\n    72→}\n    73→\n    74→/**\n    75→ * LRU cache set with size limit\n    76→ * Evicts oldest entries when cache exceeds MAX_CACHE_SIZE\n    77→ * @param {string} key - Cache key\n    78→ * @param {any} value - Value to cache\n    79→ */\n    80→function cacheSet(key, value) {\n    81→  // Evict oldest entries if at capacity\n    82→  while (transcriptCache.size >= MAX_CACHE_SIZE) {\n    83→    const oldestKey = transcriptCache.keys().next().value;\n    84→    transcriptCache.delete(oldestKey);\n    85→    console.log(`[SPLICE] Cache eviction: removed ${oldestKey}`);\n    86→  }\n    87→\n    88→  transcriptCache.set(key, value);\n    89→}\n    90→\n    91→/**\n    92→ * Transcribe audio file using GPT-4o-mini-transcribe (with caching)\n    93→ * @param {string} wavPath - Path to the WAV file\n    94→ * @returns {Promise<{text: string, segments: Array, language: string, duration: number}>}\n    95→ */\n    96→async function transcribeAudio(wavPath) {\n    97→  // Check cache based on file modification time\n    98→  const stats = fs.statSync(wavPath);\n    99→  const mtime = stats.mtimeMs;\n   100→\n   101→  const cached = transcriptCache.get(wavPath);\n   102→  if (cached && cached.mtime === mtime) {\n   103→    console.log('[SPLICE] Using cached transcription (file unchanged)');\n   104→    return cached.result;\n   105→  }\n   106→\n   107→  console.log('[SPLICE] Starting GPT-4o-mini transcription...');\n   108→\n   109→  // GPT-4o-mini-transcribe uses the same transcriptions API as Whisper\n   110→  // but only supports 'json' response format (not verbose_json)\n   111→  const transcription = await withRetry(async () => {\n   112→    return await openai.audio.transcriptions.create({\n   113→      file: fs.createReadStream(wavPath),\n   114→      model: 'gpt-4o-mini-transcribe',\n   115→      response_format: 'json',\n   116→      language: 'en',\n   117→    });\n   118→  });\n   119→\n   120→  console.log(`[SPLICE] Transcription complete: ${transcription.text.slice(0, 100)}...`);\n   121→\n   122→  // GPT-4o-mini-transcribe returns simpler response than verbose_json\n   123→  // We need to estimate segments from the text if not provided\n   124→  const result = {\n   125→    text: transcription.text,\n   126→    segments: transcription.segments || estimateSegments(transcription.text),\n   127→    language: transcription.language || 'en',\n   128→    duration: transcription.duration || 0\n   129→  };\n   130→\n   131→  // Cache the result (with LRU eviction)\n   132→  cacheSet(wavPath, { mtime, result });\n   133→  console.log('[SPLICE] Transcription cached');\n   134→\n   135→  return result;\n   136→}\n   137→\n   138→/**\n   139→ * Estimate segments from text when not provided by API\n   140→ * Creates rough segments based on sentence boundaries\n   141→ * @param {string} text - Full transcript text\n   142→ * @returns {Array} Estimated segments\n   143→ */\n   144→function estimateSegments(text) {\n   145→  if (!text) return [];\n   146→\n   147→  // Split by sentence boundaries\n   148→  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];\n   149→\n   150→  // Create segments with estimated timing (will be refined by take detection)\n   151→  return sentences.map((sentence, index) => ({\n   152→    id: index,\n   153→    start: 0, // Will be refined by take detection\n   154→    end: 0,\n   155→    text: sentence.trim()\n   156→  }));\n   157→}\n   158→\n   159→/**\n   160→ * Transcribe audio with word-level timestamps using Whisper\n   161→ * Required for profanity, repetition, and stutter detection endpoints.\n   162→ *\n   163→ * @param {string} wavPath - Path to the WAV file\n   164→ * @returns {Promise<{text: string, words: Array<{word: string, start: number, end: number}>}>}\n   165→ */\n   166→async function transcribeWithWords(wavPath) {\n   167→  // Check cache based on file modification time\n   168→  const stats = fs.statSync(wavPath);\n   169→  const mtime = stats.mtimeMs;\n   170→  const cacheKey = `words:${wavPath}`;\n   171→\n   172→  const cached = transcriptCache.get(cacheKey);\n   173→  if (cached && cached.mtime === mtime) {\n   174→    console.log('[SPLICE] Using cached word-level transcription (file unchanged)');\n   175→    return cached.result;\n   176→  }\n   177→\n   178→  console.log('[SPLICE] Starting Whisper transcription with word timestamps...');\n   179→\n   180→  // Use whisper-1 with word-level timestamp granularity (with retry)\n   181→  const transcription = await withRetry(async () => {\n   182→    return await openai.audio.transcriptions.create({\n   183→      file: fs.createReadStream(wavPath),\n   184→      model: 'whisper-1',\n   185→      response_format: 'verbose_json',\n   186→      timestamp_granularities: ['word'],\n   187→      language: 'en',\n   188→    });\n   189→  });\n   190→\n   191→  console.log(`[SPLICE] Word transcription complete: ${transcription.words?.length || 0} words`);\n   192→\n   193→  // Map to consistent format\n   194→  const result = {\n   195→    text: transcription.text,\n   196→    words: (transcription.words || []).map(w => ({\n   197→      word: w.word,\n   198→      start: w.start,\n   199→      end: w.end\n   200→    })),\n   201→    language: transcription.language || 'en',\n   202→    duration: transcription.duration || 0\n   203→  };\n   204→\n   205→  // Cache the result (with LRU eviction)\n   206→  cacheSet(cacheKey, { mtime, result });\n   207→  console.log(`[SPLICE] Word transcription cached (${result.words.length} words)`);\n   208→\n   209→  return result;\n   210→}\n   211→\n   212→/**\n   213→ * Clear transcription cache\n   214→ * Useful for testing or memory management\n   215→ */\n   216→function clearCache() {\n   217→  const size = transcriptCache.size;\n   218→  transcriptCache.clear();\n   219→  console.log(`[SPLICE] Cleared ${size} cached transcription(s)`);\n   220→  return size;\n   221→}\n   222→\n   223→/**\n   224→ * Get cache statistics\n   225→ */\n   226→function getCacheStats() {\n   227→  return {\n   228→    entries: transcriptCache.size,\n   229→    keys: Array.from(transcriptCache.keys())\n   230→  };\n   231→}\n   232→\n   233→module.exports = { transcribeAudio, transcribeWithWords, clearCache, getCacheStats };\n   234→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"fd7479cb-700b-47a2-9e37-76fb6e53f2a4","timestamp":"2025-12-25T20:25:13.536Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/transcription.js","content":"/**\n * Slice 4: GPT-4o-mini Transcription Service\n *\n * Handles audio transcription using OpenAI's gpt-4o-mini-transcribe model.\n * 50% cheaper than Whisper ($0.003/min vs $0.006/min).\n * Returns timestamped segments for take detection.\n * Includes caching to avoid repeated API calls.\n */\n\nconst fs = require('fs');\nconst OpenAI = require('openai');\n\n// Initialize OpenAI client\nconst openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\n// In-memory cache: { wavPath: { mtime, result } }\nconst transcriptCache = new Map();\n\n// Cache configuration\nconst MAX_CACHE_SIZE = 50;\n\n/**\n * Retry function with exponential backoff\n * @param {Function} fn - Async function to retry\n * @param {number} retries - Number of retry attempts (default: 3)\n * @param {number} baseDelay - Base delay in ms (default: 1000)\n * @returns {Promise<any>} Result of the function\n */\nasync function withRetry(fn, retries = 3, baseDelay = 1000) {\n  let lastError;\n\n  for (let attempt = 0; attempt <= retries; attempt++) {\n    try {\n      return await fn();\n    } catch (err) {\n      lastError = err;\n\n      // Don't retry on quota exceeded - fail immediately\n      if (err.code === 'insufficient_quota' || err.message?.includes('quota')) {\n        throw new Error('OpenAI API quota exceeded. Please check your billing at https://platform.openai.com/account/billing');\n      }\n\n      // Don't retry on the last attempt\n      if (attempt === retries) {\n        break;\n      }\n\n      // Calculate delay with exponential backoff + jitter\n      const delay = baseDelay * Math.pow(2, attempt) + Math.random() * 500;\n\n      // Only retry on retryable errors\n      const isRetryable =\n        err.status === 429 ||                          // Rate limit\n        err.status === 500 ||                          // Server error\n        err.status === 502 ||                          // Bad gateway\n        err.status === 503 ||                          // Service unavailable\n        err.code === 'ECONNRESET' ||                   // Connection reset\n        err.code === 'ETIMEDOUT' ||                    // Timeout\n        err.message?.includes('Connection error') ||   // Connection error\n        err.message?.includes('network');              // Network error\n\n      if (!isRetryable) {\n        throw err;\n      }\n\n      console.log(`[SPLICE] Retry ${attempt + 1}/${retries} after ${Math.round(delay)}ms: ${err.message}`);\n      await new Promise(resolve => setTimeout(resolve, delay));\n    }\n  }\n\n  throw lastError;\n}\n\n/**\n * LRU cache set with size limit\n * Evicts oldest entries when cache exceeds MAX_CACHE_SIZE\n * @param {string} key - Cache key\n * @param {any} value - Value to cache\n */\nfunction cacheSet(key, value) {\n  // Evict oldest entries if at capacity\n  while (transcriptCache.size >= MAX_CACHE_SIZE) {\n    const oldestKey = transcriptCache.keys().next().value;\n    transcriptCache.delete(oldestKey);\n    console.log(`[SPLICE] Cache eviction: removed ${oldestKey}`);\n  }\n\n  transcriptCache.set(key, value);\n}\n\n/**\n * Transcribe audio file using GPT-4o-mini-transcribe (with caching)\n * @param {string} wavPath - Path to the WAV file\n * @returns {Promise<{text: string, segments: Array, language: string, duration: number}>}\n */\nasync function transcribeAudio(wavPath) {\n  // Check cache based on file modification time\n  const stats = fs.statSync(wavPath);\n  const mtime = stats.mtimeMs;\n\n  const cached = transcriptCache.get(wavPath);\n  if (cached && cached.mtime === mtime) {\n    console.log('[SPLICE] Using cached transcription (file unchanged)');\n    return cached.result;\n  }\n\n  console.log('[SPLICE] Starting GPT-4o-mini transcription...');\n\n  // GPT-4o-mini-transcribe uses the same transcriptions API as Whisper\n  // but only supports 'json' response format (not verbose_json)\n  const transcription = await withRetry(async () => {\n    return await openai.audio.transcriptions.create({\n      file: fs.createReadStream(wavPath),\n      model: 'gpt-4o-mini-transcribe',\n      response_format: 'json',\n      language: 'en',\n    });\n  });\n\n  console.log(`[SPLICE] Transcription complete: ${transcription.text.slice(0, 100)}...`);\n\n  // GPT-4o-mini-transcribe returns simpler response than verbose_json\n  // We need to estimate segments from the text if not provided\n  const result = {\n    text: transcription.text,\n    segments: transcription.segments || estimateSegments(transcription.text),\n    language: transcription.language || 'en',\n    duration: transcription.duration || 0\n  };\n\n  // Cache the result (with LRU eviction)\n  cacheSet(wavPath, { mtime, result });\n  console.log('[SPLICE] Transcription cached');\n\n  return result;\n}\n\n/**\n * Estimate segments from text when not provided by API\n * Creates rough segments based on sentence boundaries\n * @param {string} text - Full transcript text\n * @returns {Array} Estimated segments\n */\nfunction estimateSegments(text) {\n  if (!text) return [];\n\n  // Split by sentence boundaries\n  const sentences = text.match(/[^.!?]+[.!?]+/g) || [text];\n\n  // Create segments with estimated timing (will be refined by take detection)\n  return sentences.map((sentence, index) => ({\n    id: index,\n    start: 0, // Will be refined by take detection\n    end: 0,\n    text: sentence.trim()\n  }));\n}\n\n/**\n * Transcribe audio with word-level timestamps using Whisper\n * Required for profanity, repetition, and stutter detection endpoints.\n *\n * @param {string} wavPath - Path to the WAV file\n * @returns {Promise<{text: string, words: Array<{word: string, start: number, end: number}>}>}\n */\nasync function transcribeWithWords(wavPath) {\n  // Check cache based on file modification time\n  const stats = fs.statSync(wavPath);\n  const mtime = stats.mtimeMs;\n  const cacheKey = `words:${wavPath}`;\n\n  const cached = transcriptCache.get(cacheKey);\n  if (cached && cached.mtime === mtime) {\n    console.log('[SPLICE] Using cached word-level transcription (file unchanged)');\n    return cached.result;\n  }\n\n  console.log('[SPLICE] Starting Whisper transcription with word timestamps...');\n\n  // Use whisper-1 with word-level timestamp granularity (with retry)\n  const transcription = await withRetry(async () => {\n    return await openai.audio.transcriptions.create({\n      file: fs.createReadStream(wavPath),\n      model: 'whisper-1',\n      response_format: 'verbose_json',\n      timestamp_granularities: ['word'],\n      language: 'en',\n    });\n  });\n\n  console.log(`[SPLICE] Word transcription complete: ${transcription.words?.length || 0} words`);\n\n  // Map to consistent format\n  const result = {\n    text: transcription.text,\n    words: (transcription.words || []).map(w => ({\n      word: w.word,\n      start: w.start,\n      end: w.end\n    })),\n    language: transcription.language || 'en',\n    duration: transcription.duration || 0\n  };\n\n  // Cache the result (with LRU eviction)\n  cacheSet(cacheKey, { mtime, result });\n  console.log(`[SPLICE] Word transcription cached (${result.words.length} words)`);\n\n  return result;\n}\n\n/**\n * Clear transcription cache\n * Useful for testing or memory management\n */\nfunction clearCache() {\n  const size = transcriptCache.size;\n  transcriptCache.clear();\n  console.log(`[SPLICE] Cleared ${size} cached transcription(s)`);\n  return size;\n}\n\n/**\n * Get cache statistics\n */\nfunction getCacheStats() {\n  return {\n    entries: transcriptCache.size,\n    keys: Array.from(transcriptCache.keys())\n  };\n}\n\nmodule.exports = { transcribeAudio, transcribeWithWords, clearCache, getCacheStats };\n","numLines":234,"startLine":1,"totalLines":234}}}
{"parentUuid":"fd7479cb-700b-47a2-9e37-76fb6e53f2a4","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01LfWcEqX7SnUZnp29Vdr7sB","type":"tool_result","content":"     1→/**\n     2→ * RMS-Based Silence Detection Service\n     3→ *\n     4→ * Advanced silence detection using Root Mean Square (RMS) audio analysis.\n     5→ * Based on Fireside's proven approach with enhancements.\n     6→ *\n     7→ * Features:\n     8→ * - RMS calculation from raw audio data\n     9→ * - dBFS (decibels relative to full scale) conversion\n    10→ * - Auto-threshold detection from audio histogram\n    11→ * - Gaussian blur smoothing for noise reduction\n    12→ * - Local maxima detection for speech/silence peak identification\n    13→ * - Configurable padding, seek step, min duration\n    14→ * - Silence merging for adjacent regions\n    15→ */\n    16→\n    17→const { exec } = require('child_process');\n    18→const { promisify } = require('util');\n    19→const fs = require('fs');\n    20→const path = require('path');\n    21→\n    22→const execAsync = promisify(exec);\n    23→\n    24→// =============================================================================\n    25→// Configuration\n    26→// =============================================================================\n    27→\n    28→const DEFAULT_OPTIONS = {\n    29→  threshold: -30,           // dBFS threshold (-60 to -20)\n    30→  minSilenceLength: 0.5,    // Minimum silence duration in seconds\n    31→  seekStep: 0.05,           // Analysis window step in seconds (50ms)\n    32→  paddingStart: 0.1,        // Buffer before silence cut (seconds)\n    33→  paddingEnd: 0.05,         // Buffer after silence cut (seconds)\n    34→  autoThreshold: false,     // Auto-detect optimal threshold\n    35→  mergeDistance: 0.2,       // Merge silences closer than this (seconds)\n    36→  sampleRate: 48000         // Default sample rate\n    37→};\n    38→\n    39→// =============================================================================\n    40→// Core RMS Detection\n    41→// =============================================================================\n    42→\n    43→/**\n    44→ * Detect silences using RMS-based audio analysis\n    45→ *\n    46→ * @param {string} audioPath - Path to audio file (WAV, MP3, etc.)\n    47→ * @param {Object} options - Detection options\n    48→ * @returns {Promise<Object>} Detection results with silences and metadata\n    49→ */\n    50→async function detectSilencesRMS(audioPath, options = {}) {\n    51→  const opts = { ...DEFAULT_OPTIONS, ...options };\n    52→\n    53→  console.log(`[SPLICE RMS] Analyzing: ${audioPath}`);\n    54→  console.log(`[SPLICE RMS] Options: threshold=${opts.threshold}dB, minLength=${opts.minSilenceLength}s, seekStep=${opts.seekStep}s`);\n    55→\n    56→  // Step 1: Extract raw audio data using FFmpeg\n    57→  const audioData = await extractAudioData(audioPath);\n    58→\n    59→  // Step 2: Calculate RMS values for sliding windows\n    60→  const rmsValues = calculateRMSWindows(audioData.samples, {\n    61→    windowSize: Math.floor(opts.seekStep * audioData.sampleRate),\n    62→    hopSize: Math.floor(opts.seekStep * audioData.sampleRate / 2)\n    63→  });\n    64→\n    65→  // Step 3: Convert to dBFS\n    66→  const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n    67→\n    68→  // Step 4: Determine threshold (auto-detect or use provided)\n    69→  let threshold = opts.threshold;\n    70→  let thresholdInfo = { method: 'manual', value: threshold };\n    71→\n    72→  if (opts.autoThreshold) {\n    73→    const autoResult = autoDetectThreshold(dBFSValues);\n    74→    threshold = autoResult.threshold;\n    75→    thresholdInfo = autoResult;\n    76→    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB (${autoResult.method})`);\n    77→  }\n    78→\n    79→  // Step 5: Find silence regions\n    80→  const rawSilences = findSilenceRegions(dBFSValues, {\n    81→    threshold,\n    82→    seekStep: opts.seekStep,\n    83→    minSilenceLength: opts.minSilenceLength\n    84→  });\n    85→\n    86→  // Step 6: Merge adjacent silences\n    87→  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n    88→\n    89→  // Step 7: Apply padding\n    90→  const paddedSilences = applyPadding(mergedSilences, {\n    91→    paddingStart: opts.paddingStart,\n    92→    paddingEnd: opts.paddingEnd,\n    93→    audioDuration: audioData.duration\n    94→  });\n    95→\n    96→  // Calculate statistics\n    97→  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n    98→  const speechDuration = audioData.duration - totalSilenceDuration;\n    99→\n   100→  console.log(`[SPLICE RMS] Detected ${paddedSilences.length} silence(s), total ${totalSilenceDuration.toFixed(2)}s`);\n   101→\n   102→  return {\n   103→    silences: paddedSilences,\n   104→    metadata: {\n   105→      audioDuration: audioData.duration,\n   106→      sampleRate: audioData.sampleRate,\n   107→      channels: audioData.channels,\n   108→      silenceCount: paddedSilences.length,\n   109→      totalSilenceDuration,\n   110→      speechDuration,\n   111→      silencePercentage: ((totalSilenceDuration / audioData.duration) * 100).toFixed(1),\n   112→      threshold: thresholdInfo,\n   113→      rmsAnalysis: {\n   114→        windowCount: dBFSValues.length,\n   115→        minDB: Math.min(...dBFSValues).toFixed(1),\n   116→        maxDB: Math.max(...dBFSValues).toFixed(1),\n   117→        avgDB: (dBFSValues.reduce((a, b) => a + b, 0) / dBFSValues.length).toFixed(1)\n   118→      }\n   119→    }\n   120→  };\n   121→}\n   122→\n   123→// =============================================================================\n   124→// Audio Extraction\n   125→// =============================================================================\n   126→\n   127→/**\n   128→ * Extract raw audio samples from file using FFmpeg\n   129→ *\n   130→ * @param {string} audioPath - Path to audio file\n   131→ * @returns {Promise<Object>} Audio data with samples, sampleRate, channels, duration\n   132→ */\n   133→async function extractAudioData(audioPath) {\n   134→  // First, get audio info\n   135→  const infoCmd = `ffprobe -v error -select_streams a:0 -show_entries stream=sample_rate,channels,duration -of json \"${audioPath}\"`;\n   136→\n   137→  let sampleRate = 48000;\n   138→  let channels = 1;\n   139→  let duration = 0;\n   140→\n   141→  try {\n   142→    const { stdout } = await execAsync(infoCmd);\n   143→    const info = JSON.parse(stdout);\n   144→    if (info.streams && info.streams[0]) {\n   145→      sampleRate = parseInt(info.streams[0].sample_rate) || 48000;\n   146→      channels = parseInt(info.streams[0].channels) || 1;\n   147→      duration = parseFloat(info.streams[0].duration) || 0;\n   148→    }\n   149→  } catch (err) {\n   150→    console.warn('[SPLICE RMS] Could not get audio info, using defaults:', err.message);\n   151→  }\n   152→\n   153→  // If duration not available from stream, get from format\n   154→  if (!duration) {\n   155→    try {\n   156→      const durationCmd = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"${audioPath}\"`;\n   157→      const { stdout } = await execAsync(durationCmd);\n   158→      duration = parseFloat(stdout.trim()) || 0;\n   159→    } catch (err) {\n   160→      console.warn('[SPLICE RMS] Could not get duration:', err.message);\n   161→    }\n   162→  }\n   163→\n   164→  // Extract audio as raw PCM floating point\n   165→  const tempFile = path.join('/tmp', `splice_rms_${Date.now()}.raw`);\n   166→\n   167→  try {\n   168→    // Convert to mono, 16kHz for faster processing, output as signed 16-bit PCM\n   169→    const extractCmd = `ffmpeg -y -i \"${audioPath}\" -ac 1 -ar 16000 -f s16le -acodec pcm_s16le \"${tempFile}\" 2>/dev/null`;\n   170→    await execAsync(extractCmd, { maxBuffer: 100 * 1024 * 1024 });\n   171→\n   172→    // Read the raw PCM data\n   173→    const rawData = fs.readFileSync(tempFile);\n   174→\n   175→    // Convert to float samples (-1 to 1)\n   176→    const samples = new Float32Array(rawData.length / 2);\n   177→    for (let i = 0; i < samples.length; i++) {\n   178→      // Read signed 16-bit little-endian\n   179→      const int16 = rawData.readInt16LE(i * 2);\n   180→      samples[i] = int16 / 32768.0;\n   181→    }\n   182→\n   183→    // Clean up temp file\n   184→    fs.unlinkSync(tempFile);\n   185→\n   186→    return {\n   187→      samples,\n   188→      sampleRate: 16000, // We resampled to 16kHz\n   189→      channels: 1,\n   190→      duration\n   191→    };\n   192→  } catch (err) {\n   193→    // Clean up temp file if exists\n   194→    if (fs.existsSync(tempFile)) {\n   195→      fs.unlinkSync(tempFile);\n   196→    }\n   197→    throw new Error(`Failed to extract audio: ${err.message}`);\n   198→  }\n   199→}\n   200→\n   201→// =============================================================================\n   202→// RMS Calculation\n   203→// =============================================================================\n   204→\n   205→/**\n   206→ * Calculate RMS values for sliding windows across audio samples\n   207→ *\n   208→ * @param {Float32Array} samples - Audio samples (-1 to 1)\n   209→ * @param {Object} options - Window and hop size\n   210→ * @returns {Float32Array} RMS values for each window\n   211→ */\n   212→function calculateRMSWindows(samples, options = {}) {\n   213→  const { windowSize = 800, hopSize = 400 } = options; // Default ~50ms at 16kHz\n   214→\n   215→  const numWindows = Math.floor((samples.length - windowSize) / hopSize) + 1;\n   216→  const rmsValues = new Float32Array(numWindows);\n   217→\n   218→  for (let i = 0; i < numWindows; i++) {\n   219→    const start = i * hopSize;\n   220→    const end = Math.min(start + windowSize, samples.length);\n   221→\n   222→    // Calculate RMS for this window\n   223→    let sumSquares = 0;\n   224→    for (let j = start; j < end; j++) {\n   225→      sumSquares += samples[j] * samples[j];\n   226→    }\n   227→\n   228→    const rms = Math.sqrt(sumSquares / (end - start));\n   229→    rmsValues[i] = rms;\n   230→  }\n   231→\n   232→  return rmsValues;\n   233→}\n   234→\n   235→/**\n   236→ * Convert RMS value to dBFS (decibels relative to full scale)\n   237→ *\n   238→ * @param {number} rms - RMS value (0 to 1)\n   239→ * @returns {number} dBFS value (negative, -inf for silence)\n   240→ */\n   241→function rmsToDBFS(rms) {\n   242→  if (rms <= 0) return -100; // Effective silence floor\n   243→  const dbfs = 20 * Math.log10(rms);\n   244→  return Math.max(-100, dbfs); // Clamp to reasonable floor\n   245→}\n   246→\n   247→// =============================================================================\n   248→// Auto-Threshold Detection\n   249→// =============================================================================\n   250→\n   251→/**\n   252→ * Auto-detect optimal silence threshold from audio histogram\n   253→ * Uses Gaussian blur + local maxima detection\n   254→ *\n   255→ * @param {Float32Array} dBFSValues - Array of dBFS values\n   256→ * @returns {Object} Detected threshold and method info\n   257→ */\n   258→function autoDetectThreshold(dBFSValues) {\n   259→  // Build histogram of dBFS values\n   260→  const histBins = 100;\n   261→  const minDB = -80;\n   262→  const maxDB = 0;\n   263→  const binWidth = (maxDB - minDB) / histBins;\n   264→\n   265→  const histogram = new Array(histBins).fill(0);\n   266→\n   267→  for (const db of dBFSValues) {\n   268→    const binIndex = Math.min(histBins - 1, Math.max(0, Math.floor((db - minDB) / binWidth)));\n   269→    histogram[binIndex]++;\n   270→  }\n   271→\n   272→  // Apply Gaussian blur to smooth histogram\n   273→  const smoothed = gaussianBlur(histogram, 5);\n   274→\n   275→  // Find local maxima (peaks)\n   276→  const peaks = findLocalMaxima(smoothed, 3);\n   277→\n   278→  // Sort peaks by value (highest first)\n   279→  peaks.sort((a, b) => b.value - a.value);\n   280→\n   281→  if (peaks.length >= 2) {\n   282→    // Interpolate between the two highest peaks\n   283→    // Assume: higher peak = speech, lower peak = silence\n   284→    const peak1DB = minDB + peaks[0].index * binWidth;\n   285→    const peak2DB = minDB + peaks[1].index * binWidth;\n   286→\n   287→    const silencePeakDB = Math.min(peak1DB, peak2DB);\n   288→    const speechPeakDB = Math.max(peak1DB, peak2DB);\n   289→\n   290→    // Set threshold between peaks (closer to silence peak)\n   291→    const threshold = silencePeakDB + (speechPeakDB - silencePeakDB) * 0.3;\n   292→\n   293→    return {\n   294→      threshold: Math.max(-60, Math.min(-20, threshold)),\n   295→      method: 'dual-peak',\n   296→      silencePeak: silencePeakDB.toFixed(1),\n   297→      speechPeak: speechPeakDB.toFixed(1)\n   298→    };\n   299→  } else if (peaks.length === 1) {\n   300→    // Single peak - use percentile-based fallback\n   301→    const sorted = [...dBFSValues].sort((a, b) => a - b);\n   302→    const percentile25 = sorted[Math.floor(sorted.length * 0.25)];\n   303→\n   304→    return {\n   305→      threshold: Math.max(-60, Math.min(-20, percentile25)),\n   306→      method: 'percentile-25',\n   307→      value: percentile25.toFixed(1)\n   308→    };\n   309→  } else {\n   310→    // No peaks found - use conservative default\n   311→    return {\n   312→      threshold: -35,\n   313→      method: 'default-fallback'\n   314→    };\n   315→  }\n   316→}\n   317→\n   318→/**\n   319→ * Apply Gaussian blur to an array\n   320→ *\n   321→ * @param {Array<number>} arr - Input array\n   322→ * @param {number} sigma - Standard deviation\n   323→ * @returns {Array<number>} Smoothed array\n   324→ */\n   325→function gaussianBlur(arr, sigma = 5) {\n   326→  const kernelSize = Math.ceil(sigma * 3) * 2 + 1;\n   327→  const kernel = [];\n   328→  const halfSize = Math.floor(kernelSize / 2);\n   329→\n   330→  // Generate Gaussian kernel\n   331→  let sum = 0;\n   332→  for (let i = 0; i < kernelSize; i++) {\n   333→    const x = i - halfSize;\n   334→    const g = Math.exp(-(x * x) / (2 * sigma * sigma));\n   335→    kernel.push(g);\n   336→    sum += g;\n   337→  }\n   338→\n   339→  // Normalize kernel\n   340→  for (let i = 0; i < kernelSize; i++) {\n   341→    kernel[i] /= sum;\n   342→  }\n   343→\n   344→  // Apply convolution\n   345→  const result = new Array(arr.length).fill(0);\n   346→  for (let i = 0; i < arr.length; i++) {\n   347→    let value = 0;\n   348→    for (let j = 0; j < kernelSize; j++) {\n   349→      const idx = i + j - halfSize;\n   350→      if (idx >= 0 && idx < arr.length) {\n   351→        value += arr[idx] * kernel[j];\n   352→      }\n   353→    }\n   354→    result[i] = value;\n   355→  }\n   356→\n   357→  return result;\n   358→}\n   359→\n   360→/**\n   361→ * Find local maxima in an array\n   362→ *\n   363→ * @param {Array<number>} arr - Input array\n   364→ * @param {number} windowSize - Neighborhood size for local maximum detection\n   365→ * @returns {Array<{index: number, value: number}>} Array of peak positions and values\n   366→ */\n   367→function findLocalMaxima(arr, windowSize = 3) {\n   368→  const peaks = [];\n   369→  const halfWindow = Math.floor(windowSize / 2);\n   370→\n   371→  for (let i = halfWindow; i < arr.length - halfWindow; i++) {\n   372→    let isMax = true;\n   373→    for (let j = i - halfWindow; j <= i + halfWindow; j++) {\n   374→      if (j !== i && arr[j] >= arr[i]) {\n   375→        isMax = false;\n   376→        break;\n   377→      }\n   378→    }\n   379→    if (isMax && arr[i] > 0) {\n   380→      peaks.push({ index: i, value: arr[i] });\n   381→    }\n   382→  }\n   383→\n   384→  return peaks;\n   385→}\n   386→\n   387→// =============================================================================\n   388→// Silence Region Detection\n   389→// =============================================================================\n   390→\n   391→/**\n   392→ * Find silence regions based on dBFS threshold\n   393→ *\n   394→ * @param {Float32Array} dBFSValues - Array of dBFS values\n   395→ * @param {Object} options - Detection options\n   396→ * @returns {Array<{start: number, end: number, duration: number}>} Silence regions\n   397→ */\n   398→function findSilenceRegions(dBFSValues, options) {\n   399→  const { threshold, seekStep, minSilenceLength } = options;\n   400→\n   401→  const silences = [];\n   402→  let silenceStart = null;\n   403→\n   404→  for (let i = 0; i < dBFSValues.length; i++) {\n   405→    const time = i * seekStep;\n   406→    const isSilent = dBFSValues[i] < threshold;\n   407→\n   408→    if (isSilent && silenceStart === null) {\n   409→      // Start of silence\n   410→      silenceStart = time;\n   411→    } else if (!isSilent && silenceStart !== null) {\n   412→      // End of silence\n   413→      const duration = time - silenceStart;\n   414→      if (duration >= minSilenceLength) {\n   415→        silences.push({\n   416→          start: silenceStart,\n   417→          end: time,\n   418→          duration\n   419→        });\n   420→      }\n   421→      silenceStart = null;\n   422→    }\n   423→  }\n   424→\n   425→  // Handle silence at end of audio\n   426→  if (silenceStart !== null) {\n   427→    const endTime = dBFSValues.length * seekStep;\n   428→    const duration = endTime - silenceStart;\n   429→    if (duration >= minSilenceLength) {\n   430→      silences.push({\n   431→        start: silenceStart,\n   432→        end: endTime,\n   433→        duration\n   434→      });\n   435→    }\n   436→  }\n   437→\n   438→  return silences;\n   439→}\n   440→\n   441→// =============================================================================\n   442→// Post-Processing\n   443→// =============================================================================\n   444→\n   445→/**\n   446→ * Merge silences that are close together\n   447→ *\n   448→ * @param {Array} silences - Array of silence regions\n   449→ * @param {number} mergeDistance - Maximum gap to merge (seconds)\n   450→ * @returns {Array} Merged silence regions\n   451→ */\n   452→function mergeSilences(silences, mergeDistance) {\n   453→  if (silences.length <= 1) return silences;\n   454→\n   455→  const merged = [];\n   456→  let current = { ...silences[0] };\n   457→\n   458→  for (let i = 1; i < silences.length; i++) {\n   459→    const next = silences[i];\n   460→\n   461→    // If gap is small enough, merge\n   462→    if (next.start - current.end <= mergeDistance) {\n   463→      current.end = next.end;\n   464→      current.duration = current.end - current.start;\n   465→    } else {\n   466→      merged.push(current);\n   467→      current = { ...next };\n   468→    }\n   469→  }\n   470→\n   471→  merged.push(current);\n   472→  return merged;\n   473→}\n   474→\n   475→/**\n   476→ * Apply padding to silence boundaries\n   477→ *\n   478→ * @param {Array} silences - Array of silence regions\n   479→ * @param {Object} options - Padding options\n   480→ * @returns {Array} Padded silence regions\n   481→ */\n   482→function applyPadding(silences, options) {\n   483→  const { paddingStart, paddingEnd, audioDuration } = options;\n   484→\n   485→  return silences\n   486→    .map(silence => {\n   487→      const paddedStart = Math.max(0, silence.start + paddingStart);\n   488→      const paddedEnd = Math.min(audioDuration, silence.end - paddingEnd);\n   489→      const duration = paddedEnd - paddedStart;\n   490→\n   491→      // Skip if padding made the silence invalid\n   492→      if (duration <= 0) return null;\n   493→\n   494→      return {\n   495→        start: parseFloat(paddedStart.toFixed(3)),\n   496→        end: parseFloat(paddedEnd.toFixed(3)),\n   497→        duration: parseFloat(duration.toFixed(3))\n   498→      };\n   499→    })\n   500→    .filter(s => s !== null);\n   501→}\n   502→\n   503→// =============================================================================\n   504→// Sensitivity Mapping\n   505→// =============================================================================\n   506→\n   507→/**\n   508→ * Map UI sensitivity (0-100) to detection parameters\n   509→ * Based on Fireside's aggressiveness mapping\n   510→ *\n   511→ * @param {number} sensitivity - UI sensitivity value (0-100)\n   512→ * @returns {Object} Detection parameters\n   513→ */\n   514→function sensitivityToParams(sensitivity) {\n   515→  // Clamp to 0-100\n   516→  const s = Math.max(0, Math.min(100, sensitivity));\n   517→\n   518→  // Linear interpolation helper\n   519→  const lerp = (min, max, t) => min + (max - min) * (t / 100);\n   520→\n   521→  return {\n   522→    // Threshold: -50dB (conservative) to -20dB (aggressive)\n   523→    threshold: lerp(-50, -20, s),\n   524→\n   525→    // Min silence length: 2.0s (conservative) to 0.3s (aggressive)\n   526→    minSilenceLength: lerp(2.0, 0.3, s),\n   527→\n   528→    // Padding start: 0.2s (conservative) to 0.05s (aggressive)\n   529→    paddingStart: lerp(0.2, 0.05, s),\n   530→\n   531→    // Padding end: 0.15s (conservative) to 0.03s (aggressive)\n   532→    paddingEnd: lerp(0.15, 0.03, s),\n   533→\n   534→    // Seek step stays constant\n   535→    seekStep: 0.05\n   536→  };\n   537→}\n   538→\n   539→// =============================================================================\n   540→// Exports\n   541→// =============================================================================\n   542→\n   543→module.exports = {\n   544→  detectSilencesRMS,\n   545→  sensitivityToParams,\n   546→  autoDetectThreshold,\n   547→  // Export internal functions for testing\n   548→  calculateRMSWindows,\n   549→  rmsToDBFS,\n   550→  gaussianBlur,\n   551→  findLocalMaxima,\n   552→  mergeSilences,\n   553→  applyPadding,\n   554→  DEFAULT_OPTIONS\n   555→};\n   556→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"94a8025a-8cf6-4a4a-9f88-22a90a20a34e","timestamp":"2025-12-25T20:25:13.536Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js","content":"/**\n * RMS-Based Silence Detection Service\n *\n * Advanced silence detection using Root Mean Square (RMS) audio analysis.\n * Based on Fireside's proven approach with enhancements.\n *\n * Features:\n * - RMS calculation from raw audio data\n * - dBFS (decibels relative to full scale) conversion\n * - Auto-threshold detection from audio histogram\n * - Gaussian blur smoothing for noise reduction\n * - Local maxima detection for speech/silence peak identification\n * - Configurable padding, seek step, min duration\n * - Silence merging for adjacent regions\n */\n\nconst { exec } = require('child_process');\nconst { promisify } = require('util');\nconst fs = require('fs');\nconst path = require('path');\n\nconst execAsync = promisify(exec);\n\n// =============================================================================\n// Configuration\n// =============================================================================\n\nconst DEFAULT_OPTIONS = {\n  threshold: -30,           // dBFS threshold (-60 to -20)\n  minSilenceLength: 0.5,    // Minimum silence duration in seconds\n  seekStep: 0.05,           // Analysis window step in seconds (50ms)\n  paddingStart: 0.1,        // Buffer before silence cut (seconds)\n  paddingEnd: 0.05,         // Buffer after silence cut (seconds)\n  autoThreshold: false,     // Auto-detect optimal threshold\n  mergeDistance: 0.2,       // Merge silences closer than this (seconds)\n  sampleRate: 48000         // Default sample rate\n};\n\n// =============================================================================\n// Core RMS Detection\n// =============================================================================\n\n/**\n * Detect silences using RMS-based audio analysis\n *\n * @param {string} audioPath - Path to audio file (WAV, MP3, etc.)\n * @param {Object} options - Detection options\n * @returns {Promise<Object>} Detection results with silences and metadata\n */\nasync function detectSilencesRMS(audioPath, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n\n  console.log(`[SPLICE RMS] Analyzing: ${audioPath}`);\n  console.log(`[SPLICE RMS] Options: threshold=${opts.threshold}dB, minLength=${opts.minSilenceLength}s, seekStep=${opts.seekStep}s`);\n\n  // Step 1: Extract raw audio data using FFmpeg\n  const audioData = await extractAudioData(audioPath);\n\n  // Step 2: Calculate RMS values for sliding windows\n  const rmsValues = calculateRMSWindows(audioData.samples, {\n    windowSize: Math.floor(opts.seekStep * audioData.sampleRate),\n    hopSize: Math.floor(opts.seekStep * audioData.sampleRate / 2)\n  });\n\n  // Step 3: Convert to dBFS\n  const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n\n  // Step 4: Determine threshold (auto-detect or use provided)\n  let threshold = opts.threshold;\n  let thresholdInfo = { method: 'manual', value: threshold };\n\n  if (opts.autoThreshold) {\n    const autoResult = autoDetectThreshold(dBFSValues);\n    threshold = autoResult.threshold;\n    thresholdInfo = autoResult;\n    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB (${autoResult.method})`);\n  }\n\n  // Step 5: Find silence regions\n  const rawSilences = findSilenceRegions(dBFSValues, {\n    threshold,\n    seekStep: opts.seekStep,\n    minSilenceLength: opts.minSilenceLength\n  });\n\n  // Step 6: Merge adjacent silences\n  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n\n  // Step 7: Apply padding\n  const paddedSilences = applyPadding(mergedSilences, {\n    paddingStart: opts.paddingStart,\n    paddingEnd: opts.paddingEnd,\n    audioDuration: audioData.duration\n  });\n\n  // Calculate statistics\n  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n  const speechDuration = audioData.duration - totalSilenceDuration;\n\n  console.log(`[SPLICE RMS] Detected ${paddedSilences.length} silence(s), total ${totalSilenceDuration.toFixed(2)}s`);\n\n  return {\n    silences: paddedSilences,\n    metadata: {\n      audioDuration: audioData.duration,\n      sampleRate: audioData.sampleRate,\n      channels: audioData.channels,\n      silenceCount: paddedSilences.length,\n      totalSilenceDuration,\n      speechDuration,\n      silencePercentage: ((totalSilenceDuration / audioData.duration) * 100).toFixed(1),\n      threshold: thresholdInfo,\n      rmsAnalysis: {\n        windowCount: dBFSValues.length,\n        minDB: Math.min(...dBFSValues).toFixed(1),\n        maxDB: Math.max(...dBFSValues).toFixed(1),\n        avgDB: (dBFSValues.reduce((a, b) => a + b, 0) / dBFSValues.length).toFixed(1)\n      }\n    }\n  };\n}\n\n// =============================================================================\n// Audio Extraction\n// =============================================================================\n\n/**\n * Extract raw audio samples from file using FFmpeg\n *\n * @param {string} audioPath - Path to audio file\n * @returns {Promise<Object>} Audio data with samples, sampleRate, channels, duration\n */\nasync function extractAudioData(audioPath) {\n  // First, get audio info\n  const infoCmd = `ffprobe -v error -select_streams a:0 -show_entries stream=sample_rate,channels,duration -of json \"${audioPath}\"`;\n\n  let sampleRate = 48000;\n  let channels = 1;\n  let duration = 0;\n\n  try {\n    const { stdout } = await execAsync(infoCmd);\n    const info = JSON.parse(stdout);\n    if (info.streams && info.streams[0]) {\n      sampleRate = parseInt(info.streams[0].sample_rate) || 48000;\n      channels = parseInt(info.streams[0].channels) || 1;\n      duration = parseFloat(info.streams[0].duration) || 0;\n    }\n  } catch (err) {\n    console.warn('[SPLICE RMS] Could not get audio info, using defaults:', err.message);\n  }\n\n  // If duration not available from stream, get from format\n  if (!duration) {\n    try {\n      const durationCmd = `ffprobe -v error -show_entries format=duration -of default=noprint_wrappers=1:nokey=1 \"${audioPath}\"`;\n      const { stdout } = await execAsync(durationCmd);\n      duration = parseFloat(stdout.trim()) || 0;\n    } catch (err) {\n      console.warn('[SPLICE RMS] Could not get duration:', err.message);\n    }\n  }\n\n  // Extract audio as raw PCM floating point\n  const tempFile = path.join('/tmp', `splice_rms_${Date.now()}.raw`);\n\n  try {\n    // Convert to mono, 16kHz for faster processing, output as signed 16-bit PCM\n    const extractCmd = `ffmpeg -y -i \"${audioPath}\" -ac 1 -ar 16000 -f s16le -acodec pcm_s16le \"${tempFile}\" 2>/dev/null`;\n    await execAsync(extractCmd, { maxBuffer: 100 * 1024 * 1024 });\n\n    // Read the raw PCM data\n    const rawData = fs.readFileSync(tempFile);\n\n    // Convert to float samples (-1 to 1)\n    const samples = new Float32Array(rawData.length / 2);\n    for (let i = 0; i < samples.length; i++) {\n      // Read signed 16-bit little-endian\n      const int16 = rawData.readInt16LE(i * 2);\n      samples[i] = int16 / 32768.0;\n    }\n\n    // Clean up temp file\n    fs.unlinkSync(tempFile);\n\n    return {\n      samples,\n      sampleRate: 16000, // We resampled to 16kHz\n      channels: 1,\n      duration\n    };\n  } catch (err) {\n    // Clean up temp file if exists\n    if (fs.existsSync(tempFile)) {\n      fs.unlinkSync(tempFile);\n    }\n    throw new Error(`Failed to extract audio: ${err.message}`);\n  }\n}\n\n// =============================================================================\n// RMS Calculation\n// =============================================================================\n\n/**\n * Calculate RMS values for sliding windows across audio samples\n *\n * @param {Float32Array} samples - Audio samples (-1 to 1)\n * @param {Object} options - Window and hop size\n * @returns {Float32Array} RMS values for each window\n */\nfunction calculateRMSWindows(samples, options = {}) {\n  const { windowSize = 800, hopSize = 400 } = options; // Default ~50ms at 16kHz\n\n  const numWindows = Math.floor((samples.length - windowSize) / hopSize) + 1;\n  const rmsValues = new Float32Array(numWindows);\n\n  for (let i = 0; i < numWindows; i++) {\n    const start = i * hopSize;\n    const end = Math.min(start + windowSize, samples.length);\n\n    // Calculate RMS for this window\n    let sumSquares = 0;\n    for (let j = start; j < end; j++) {\n      sumSquares += samples[j] * samples[j];\n    }\n\n    const rms = Math.sqrt(sumSquares / (end - start));\n    rmsValues[i] = rms;\n  }\n\n  return rmsValues;\n}\n\n/**\n * Convert RMS value to dBFS (decibels relative to full scale)\n *\n * @param {number} rms - RMS value (0 to 1)\n * @returns {number} dBFS value (negative, -inf for silence)\n */\nfunction rmsToDBFS(rms) {\n  if (rms <= 0) return -100; // Effective silence floor\n  const dbfs = 20 * Math.log10(rms);\n  return Math.max(-100, dbfs); // Clamp to reasonable floor\n}\n\n// =============================================================================\n// Auto-Threshold Detection\n// =============================================================================\n\n/**\n * Auto-detect optimal silence threshold from audio histogram\n * Uses Gaussian blur + local maxima detection\n *\n * @param {Float32Array} dBFSValues - Array of dBFS values\n * @returns {Object} Detected threshold and method info\n */\nfunction autoDetectThreshold(dBFSValues) {\n  // Build histogram of dBFS values\n  const histBins = 100;\n  const minDB = -80;\n  const maxDB = 0;\n  const binWidth = (maxDB - minDB) / histBins;\n\n  const histogram = new Array(histBins).fill(0);\n\n  for (const db of dBFSValues) {\n    const binIndex = Math.min(histBins - 1, Math.max(0, Math.floor((db - minDB) / binWidth)));\n    histogram[binIndex]++;\n  }\n\n  // Apply Gaussian blur to smooth histogram\n  const smoothed = gaussianBlur(histogram, 5);\n\n  // Find local maxima (peaks)\n  const peaks = findLocalMaxima(smoothed, 3);\n\n  // Sort peaks by value (highest first)\n  peaks.sort((a, b) => b.value - a.value);\n\n  if (peaks.length >= 2) {\n    // Interpolate between the two highest peaks\n    // Assume: higher peak = speech, lower peak = silence\n    const peak1DB = minDB + peaks[0].index * binWidth;\n    const peak2DB = minDB + peaks[1].index * binWidth;\n\n    const silencePeakDB = Math.min(peak1DB, peak2DB);\n    const speechPeakDB = Math.max(peak1DB, peak2DB);\n\n    // Set threshold between peaks (closer to silence peak)\n    const threshold = silencePeakDB + (speechPeakDB - silencePeakDB) * 0.3;\n\n    return {\n      threshold: Math.max(-60, Math.min(-20, threshold)),\n      method: 'dual-peak',\n      silencePeak: silencePeakDB.toFixed(1),\n      speechPeak: speechPeakDB.toFixed(1)\n    };\n  } else if (peaks.length === 1) {\n    // Single peak - use percentile-based fallback\n    const sorted = [...dBFSValues].sort((a, b) => a - b);\n    const percentile25 = sorted[Math.floor(sorted.length * 0.25)];\n\n    return {\n      threshold: Math.max(-60, Math.min(-20, percentile25)),\n      method: 'percentile-25',\n      value: percentile25.toFixed(1)\n    };\n  } else {\n    // No peaks found - use conservative default\n    return {\n      threshold: -35,\n      method: 'default-fallback'\n    };\n  }\n}\n\n/**\n * Apply Gaussian blur to an array\n *\n * @param {Array<number>} arr - Input array\n * @param {number} sigma - Standard deviation\n * @returns {Array<number>} Smoothed array\n */\nfunction gaussianBlur(arr, sigma = 5) {\n  const kernelSize = Math.ceil(sigma * 3) * 2 + 1;\n  const kernel = [];\n  const halfSize = Math.floor(kernelSize / 2);\n\n  // Generate Gaussian kernel\n  let sum = 0;\n  for (let i = 0; i < kernelSize; i++) {\n    const x = i - halfSize;\n    const g = Math.exp(-(x * x) / (2 * sigma * sigma));\n    kernel.push(g);\n    sum += g;\n  }\n\n  // Normalize kernel\n  for (let i = 0; i < kernelSize; i++) {\n    kernel[i] /= sum;\n  }\n\n  // Apply convolution\n  const result = new Array(arr.length).fill(0);\n  for (let i = 0; i < arr.length; i++) {\n    let value = 0;\n    for (let j = 0; j < kernelSize; j++) {\n      const idx = i + j - halfSize;\n      if (idx >= 0 && idx < arr.length) {\n        value += arr[idx] * kernel[j];\n      }\n    }\n    result[i] = value;\n  }\n\n  return result;\n}\n\n/**\n * Find local maxima in an array\n *\n * @param {Array<number>} arr - Input array\n * @param {number} windowSize - Neighborhood size for local maximum detection\n * @returns {Array<{index: number, value: number}>} Array of peak positions and values\n */\nfunction findLocalMaxima(arr, windowSize = 3) {\n  const peaks = [];\n  const halfWindow = Math.floor(windowSize / 2);\n\n  for (let i = halfWindow; i < arr.length - halfWindow; i++) {\n    let isMax = true;\n    for (let j = i - halfWindow; j <= i + halfWindow; j++) {\n      if (j !== i && arr[j] >= arr[i]) {\n        isMax = false;\n        break;\n      }\n    }\n    if (isMax && arr[i] > 0) {\n      peaks.push({ index: i, value: arr[i] });\n    }\n  }\n\n  return peaks;\n}\n\n// =============================================================================\n// Silence Region Detection\n// =============================================================================\n\n/**\n * Find silence regions based on dBFS threshold\n *\n * @param {Float32Array} dBFSValues - Array of dBFS values\n * @param {Object} options - Detection options\n * @returns {Array<{start: number, end: number, duration: number}>} Silence regions\n */\nfunction findSilenceRegions(dBFSValues, options) {\n  const { threshold, seekStep, minSilenceLength } = options;\n\n  const silences = [];\n  let silenceStart = null;\n\n  for (let i = 0; i < dBFSValues.length; i++) {\n    const time = i * seekStep;\n    const isSilent = dBFSValues[i] < threshold;\n\n    if (isSilent && silenceStart === null) {\n      // Start of silence\n      silenceStart = time;\n    } else if (!isSilent && silenceStart !== null) {\n      // End of silence\n      const duration = time - silenceStart;\n      if (duration >= minSilenceLength) {\n        silences.push({\n          start: silenceStart,\n          end: time,\n          duration\n        });\n      }\n      silenceStart = null;\n    }\n  }\n\n  // Handle silence at end of audio\n  if (silenceStart !== null) {\n    const endTime = dBFSValues.length * seekStep;\n    const duration = endTime - silenceStart;\n    if (duration >= minSilenceLength) {\n      silences.push({\n        start: silenceStart,\n        end: endTime,\n        duration\n      });\n    }\n  }\n\n  return silences;\n}\n\n// =============================================================================\n// Post-Processing\n// =============================================================================\n\n/**\n * Merge silences that are close together\n *\n * @param {Array} silences - Array of silence regions\n * @param {number} mergeDistance - Maximum gap to merge (seconds)\n * @returns {Array} Merged silence regions\n */\nfunction mergeSilences(silences, mergeDistance) {\n  if (silences.length <= 1) return silences;\n\n  const merged = [];\n  let current = { ...silences[0] };\n\n  for (let i = 1; i < silences.length; i++) {\n    const next = silences[i];\n\n    // If gap is small enough, merge\n    if (next.start - current.end <= mergeDistance) {\n      current.end = next.end;\n      current.duration = current.end - current.start;\n    } else {\n      merged.push(current);\n      current = { ...next };\n    }\n  }\n\n  merged.push(current);\n  return merged;\n}\n\n/**\n * Apply padding to silence boundaries\n *\n * @param {Array} silences - Array of silence regions\n * @param {Object} options - Padding options\n * @returns {Array} Padded silence regions\n */\nfunction applyPadding(silences, options) {\n  const { paddingStart, paddingEnd, audioDuration } = options;\n\n  return silences\n    .map(silence => {\n      const paddedStart = Math.max(0, silence.start + paddingStart);\n      const paddedEnd = Math.min(audioDuration, silence.end - paddingEnd);\n      const duration = paddedEnd - paddedStart;\n\n      // Skip if padding made the silence invalid\n      if (duration <= 0) return null;\n\n      return {\n        start: parseFloat(paddedStart.toFixed(3)),\n        end: parseFloat(paddedEnd.toFixed(3)),\n        duration: parseFloat(duration.toFixed(3))\n      };\n    })\n    .filter(s => s !== null);\n}\n\n// =============================================================================\n// Sensitivity Mapping\n// =============================================================================\n\n/**\n * Map UI sensitivity (0-100) to detection parameters\n * Based on Fireside's aggressiveness mapping\n *\n * @param {number} sensitivity - UI sensitivity value (0-100)\n * @returns {Object} Detection parameters\n */\nfunction sensitivityToParams(sensitivity) {\n  // Clamp to 0-100\n  const s = Math.max(0, Math.min(100, sensitivity));\n\n  // Linear interpolation helper\n  const lerp = (min, max, t) => min + (max - min) * (t / 100);\n\n  return {\n    // Threshold: -50dB (conservative) to -20dB (aggressive)\n    threshold: lerp(-50, -20, s),\n\n    // Min silence length: 2.0s (conservative) to 0.3s (aggressive)\n    minSilenceLength: lerp(2.0, 0.3, s),\n\n    // Padding start: 0.2s (conservative) to 0.05s (aggressive)\n    paddingStart: lerp(0.2, 0.05, s),\n\n    // Padding end: 0.15s (conservative) to 0.03s (aggressive)\n    paddingEnd: lerp(0.15, 0.03, s),\n\n    // Seek step stays constant\n    seekStep: 0.05\n  };\n}\n\n// =============================================================================\n// Exports\n// =============================================================================\n\nmodule.exports = {\n  detectSilencesRMS,\n  sensitivityToParams,\n  autoDetectThreshold,\n  // Export internal functions for testing\n  calculateRMSWindows,\n  rmsToDBFS,\n  gaussianBlur,\n  findLocalMaxima,\n  mergeSilences,\n  applyPadding,\n  DEFAULT_OPTIONS\n};\n","numLines":556,"startLine":1,"totalLines":556}}}
{"parentUuid":"94a8025a-8cf6-4a4a-9f88-22a90a20a34e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01NF2hx567tPaAV55dD1gWbu","type":"tool_result","content":"     1→/**\n     2→ * Usage Tracking Service\n     3→ *\n     4→ * Tracks user credit balances and usage for billing.\n     5→ * Uses PostgreSQL for persistence.\n     6→ */\n     7→\n     8→const { Pool } = require('pg');\n     9→\n    10→// PostgreSQL connection pool\n    11→const pool = new Pool({\n    12→  connectionString: process.env.DATABASE_URL,\n    13→  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false\n    14→});\n    15→\n    16→// Hours per tier (regular processing)\n    17→const TIER_HOURS = {\n    18→  starter: 15,\n    19→  pro: 50,\n    20→  team: 150,\n    21→  cancelled: 0\n    22→};\n    23→\n    24→// Isolation minutes per tier (sized to maintain 80% margins)\n    25→const TIER_ISOLATION_MINUTES = {\n    26→  starter: 0,     // No isolation access\n    27→  pro: 45,        // 45 minutes included\n    28→  team: 180,      // 3 hours included\n    29→  cancelled: 0\n    30→};\n    31→\n    32→// Convert to hours for database storage\n    33→const TIER_ISOLATION_HOURS = {\n    34→  starter: 0,\n    35→  pro: TIER_ISOLATION_MINUTES.pro / 60,      // 0.75 hours\n    36→  team: TIER_ISOLATION_MINUTES.team / 60,    // 3 hours\n    37→  cancelled: 0\n    38→};\n    39→\n    40→// Overage rate for isolation beyond included minutes\n    41→const ISOLATION_OVERAGE_RATE = 0.10; // $0.10 per minute (85% margin)\n    42→\n    43→/**\n    44→ * Initialize database tables\n    45→ */\n    46→async function initDatabase() {\n    47→  const client = await pool.connect();\n    48→  try {\n    49→    await client.query(`\n    50→      CREATE TABLE IF NOT EXISTS users (\n    51→        id SERIAL PRIMARY KEY,\n    52→        stripe_customer_id VARCHAR(255) UNIQUE NOT NULL,\n    53→        email VARCHAR(255),\n    54→        tier VARCHAR(50) DEFAULT 'starter',\n    55→        hours_remaining DECIMAL(10,4) DEFAULT 15,\n    56→        hours_total DECIMAL(10,4) DEFAULT 15,\n    57→        isolation_hours_remaining DECIMAL(10,4) DEFAULT 0,\n    58→        isolation_hours_total DECIMAL(10,4) DEFAULT 0,\n    59→        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    60→        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    61→      )\n    62→    `);\n    63→\n    64→    // Add isolation columns if they don't exist (for existing databases)\n    65→    await client.query(`\n    66→      DO $$\n    67→      BEGIN\n    68→        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_remaining') THEN\n    69→          ALTER TABLE users ADD COLUMN isolation_hours_remaining DECIMAL(10,4) DEFAULT 0;\n    70→        END IF;\n    71→        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_total') THEN\n    72→          ALTER TABLE users ADD COLUMN isolation_hours_total DECIMAL(10,4) DEFAULT 0;\n    73→        END IF;\n    74→      END $$;\n    75→    `);\n    76→\n    77→    await client.query(`\n    78→      CREATE TABLE IF NOT EXISTS usage_log (\n    79→        id SERIAL PRIMARY KEY,\n    80→        user_id INTEGER REFERENCES users(id),\n    81→        stripe_customer_id VARCHAR(255),\n    82→        audio_duration_seconds DECIMAL(10,2),\n    83→        hours_used DECIMAL(10,6),\n    84→        endpoint VARCHAR(100),\n    85→        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    86→      )\n    87→    `);\n    88→\n    89→    await client.query(`\n    90→      CREATE INDEX IF NOT EXISTS idx_users_stripe_customer_id ON users(stripe_customer_id)\n    91→    `);\n    92→\n    93→    await client.query(`\n    94→      CREATE INDEX IF NOT EXISTS idx_usage_log_user_id ON usage_log(user_id)\n    95→    `);\n    96→\n    97→    // Webhook events table for idempotency\n    98→    await client.query(`\n    99→      CREATE TABLE IF NOT EXISTS webhook_events (\n   100→        id SERIAL PRIMARY KEY,\n   101→        event_id VARCHAR(255) UNIQUE NOT NULL,\n   102→        event_type VARCHAR(100),\n   103→        processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n   104→      )\n   105→    `);\n   106→\n   107→    await client.query(`\n   108→      CREATE INDEX IF NOT EXISTS idx_webhook_events_event_id ON webhook_events(event_id)\n   109→    `);\n   110→\n   111→    console.log('[UsageTracking] Database initialized');\n   112→  } finally {\n   113→    client.release();\n   114→  }\n   115→}\n   116→\n   117→/**\n   118→ * Get or create a user by Stripe customer ID\n   119→ */\n   120→async function getOrCreateUser(stripeCustomerId, email = null) {\n   121→  const client = await pool.connect();\n   122→  try {\n   123→    // Try to find existing user\n   124→    let result = await client.query(\n   125→      'SELECT * FROM users WHERE stripe_customer_id = $1',\n   126→      [stripeCustomerId]\n   127→    );\n   128→\n   129→    if (result.rows.length > 0) {\n   130→      return result.rows[0];\n   131→    }\n   132→\n   133→    // Create new user with starter tier (no isolation hours)\n   134→    result = await client.query(\n   135→      `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n   136→       VALUES ($1, $2, 'starter', $3, $3, $4, $4)\n   137→       RETURNING *`,\n   138→      [stripeCustomerId, email, TIER_HOURS.starter, TIER_ISOLATION_HOURS.starter]\n   139→    );\n   140→\n   141→    return result.rows[0];\n   142→  } finally {\n   143→    client.release();\n   144→  }\n   145→}\n   146→\n   147→/**\n   148→ * Get user's current credit balance\n   149→ */\n   150→async function getBalance(stripeCustomerId) {\n   151→  const user = await getOrCreateUser(stripeCustomerId);\n   152→  const hoursTotal = parseFloat(user.hours_total);\n   153→  const hoursRemaining = parseFloat(user.hours_remaining);\n   154→  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n   155→  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n   156→\n   157→  // Tier display names\n   158→  const tierNames = {\n   159→    starter: 'Starter',\n   160→    pro: 'Pro',\n   161→    team: 'Team',\n   162→    cancelled: 'Cancelled'\n   163→  };\n   164→\n   165→  return {\n   166→    hoursRemaining,\n   167→    hoursTotal,\n   168→    isolationHoursRemaining,\n   169→    isolationHoursTotal,\n   170→    tier: user.tier,\n   171→    tierName: tierNames[user.tier] || user.tier,\n   172→    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n   173→    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n   174→    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n   175→    isolationOverageRate: ISOLATION_OVERAGE_RATE\n   176→  };\n   177→}\n   178→\n   179→/**\n   180→ * Check if user has enough credits for estimated duration\n   181→ */\n   182→async function hasCredits(stripeCustomerId, estimatedSeconds = 0) {\n   183→  const balance = await getBalance(stripeCustomerId);\n   184→  const estimatedHours = estimatedSeconds / 3600;\n   185→  return balance.hoursRemaining >= estimatedHours;\n   186→}\n   187→\n   188→/**\n   189→ * Check if user can use vocal isolation\n   190→ * Returns: { allowed, hasIncludedHours, overageRequired, overageCost }\n   191→ */\n   192→async function checkIsolationAccess(stripeCustomerId, estimatedMinutes = 0) {\n   193→  const balance = await getBalance(stripeCustomerId);\n   194→\n   195→  // Starter tier has no isolation access\n   196→  if (!balance.hasIsolationAccess) {\n   197→    return {\n   198→      allowed: false,\n   199→      reason: 'upgrade_required',\n   200→      message: 'Vocal isolation requires Pro or Team tier',\n   201→      hasIncludedHours: false,\n   202→      overageRequired: false,\n   203→      overageCost: 0\n   204→    };\n   205→  }\n   206→\n   207→  const estimatedHours = estimatedMinutes / 60;\n   208→  const remainingIncluded = balance.isolationHoursRemaining;\n   209→\n   210→  // Check if within included hours\n   211→  if (remainingIncluded >= estimatedHours) {\n   212→    return {\n   213→      allowed: true,\n   214→      reason: 'included',\n   215→      message: `Using ${estimatedMinutes.toFixed(1)} min of included isolation time`,\n   216→      hasIncludedHours: true,\n   217→      overageRequired: false,\n   218→      overageCost: 0,\n   219→      isolationHoursRemaining: remainingIncluded - estimatedHours\n   220→    };\n   221→  }\n   222→\n   223→  // Calculate overage\n   224→  const overageMinutes = estimatedMinutes - (remainingIncluded * 60);\n   225→  const overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n   226→\n   227→  return {\n   228→    allowed: true,\n   229→    reason: 'overage',\n   230→    message: `${remainingIncluded > 0 ? `Using ${(remainingIncluded * 60).toFixed(1)} included min + ` : ''}${overageMinutes.toFixed(1)} min overage ($${overageCost.toFixed(2)})`,\n   231→    hasIncludedHours: remainingIncluded > 0,\n   232→    overageRequired: true,\n   233→    overageMinutes,\n   234→    overageCost,\n   235→    isolationHoursRemaining: 0\n   236→  };\n   237→}\n   238→\n   239→/**\n   240→ * Deduct isolation usage from user's balance\n   241→ * Returns overage cost if beyond included hours\n   242→ */\n   243→async function deductIsolationUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'isolate-vocals') {\n   244→  const durationMinutes = audioDurationSeconds / 60;\n   245→  const durationHours = audioDurationSeconds / 3600;\n   246→\n   247→  const client = await pool.connect();\n   248→  try {\n   249→    // Get user\n   250→    const userResult = await client.query(\n   251→      'SELECT * FROM users WHERE stripe_customer_id = $1',\n   252→      [stripeCustomerId]\n   253→    );\n   254→\n   255→    if (userResult.rows.length === 0) {\n   256→      throw new Error('User not found');\n   257→    }\n   258→\n   259→    const user = userResult.rows[0];\n   260→    const remainingIncluded = parseFloat(user.isolation_hours_remaining || 0);\n   261→\n   262→    let overageCost = 0;\n   263→    let hoursFromIncluded = 0;\n   264→    let overageMinutes = 0;\n   265→\n   266→    if (remainingIncluded >= durationHours) {\n   267→      // Fully covered by included hours\n   268→      hoursFromIncluded = durationHours;\n   269→    } else {\n   270→      // Partial or full overage\n   271→      hoursFromIncluded = remainingIncluded;\n   272→      overageMinutes = durationMinutes - (remainingIncluded * 60);\n   273→      overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n   274→    }\n   275→\n   276→    // Update isolation balance\n   277→    await client.query(\n   278→      `UPDATE users\n   279→       SET isolation_hours_remaining = GREATEST(0, isolation_hours_remaining - $1),\n   280→           updated_at = CURRENT_TIMESTAMP\n   281→       WHERE stripe_customer_id = $2`,\n   282→      [durationHours, stripeCustomerId]\n   283→    );\n   284→\n   285→    // Log usage (mark as isolation)\n   286→    await client.query(\n   287→      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   288→       VALUES ($1, $2, $3, $4, $5)`,\n   289→      [user.id, stripeCustomerId, audioDurationSeconds, durationHours, endpoint]\n   290→    );\n   291→\n   292→    // Return updated balance and overage info\n   293→    const newBalance = await getBalance(stripeCustomerId);\n   294→    return {\n   295→      ...newBalance,\n   296→      isolationUsed: {\n   297→        durationMinutes,\n   298→        hoursFromIncluded,\n   299→        overageMinutes,\n   300→        overageCost\n   301→      }\n   302→    };\n   303→  } finally {\n   304→    client.release();\n   305→  }\n   306→}\n   307→\n   308→/**\n   309→ * Deduct usage from user's balance\n   310→ */\n   311→async function deductUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'unknown') {\n   312→  const hoursUsed = audioDurationSeconds / 3600;\n   313→\n   314→  const client = await pool.connect();\n   315→  try {\n   316→    // Get user\n   317→    const userResult = await client.query(\n   318→      'SELECT * FROM users WHERE stripe_customer_id = $1',\n   319→      [stripeCustomerId]\n   320→    );\n   321→\n   322→    if (userResult.rows.length === 0) {\n   323→      throw new Error('User not found');\n   324→    }\n   325→\n   326→    const user = userResult.rows[0];\n   327→\n   328→    // Update balance\n   329→    await client.query(\n   330→      `UPDATE users\n   331→       SET hours_remaining = GREATEST(0, hours_remaining - $1),\n   332→           updated_at = CURRENT_TIMESTAMP\n   333→       WHERE stripe_customer_id = $2`,\n   334→      [hoursUsed, stripeCustomerId]\n   335→    );\n   336→\n   337→    // Log usage\n   338→    await client.query(\n   339→      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   340→       VALUES ($1, $2, $3, $4, $5)`,\n   341→      [user.id, stripeCustomerId, audioDurationSeconds, hoursUsed, endpoint]\n   342→    );\n   343→\n   344→    // Return updated balance\n   345→    return await getBalance(stripeCustomerId);\n   346→  } finally {\n   347→    client.release();\n   348→  }\n   349→}\n   350→\n   351→/**\n   352→ * Reset user's hours for new billing period\n   353→ */\n   354→async function resetHours(stripeCustomerId, tier) {\n   355→  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n   356→  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n   357→\n   358→  await pool.query(\n   359→    `UPDATE users\n   360→     SET hours_remaining = $1, hours_total = $1,\n   361→         isolation_hours_remaining = $2, isolation_hours_total = $2,\n   362→         tier = $3, updated_at = CURRENT_TIMESTAMP\n   363→     WHERE stripe_customer_id = $4`,\n   364→    [hours, isolationHours, tier, stripeCustomerId]\n   365→  );\n   366→\n   367→  return await getBalance(stripeCustomerId);\n   368→}\n   369→\n   370→/**\n   371→ * Update user's tier (from Stripe webhook)\n   372→ */\n   373→async function updateTier(stripeCustomerId, tier, email = null) {\n   374→  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n   375→  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n   376→\n   377→  const client = await pool.connect();\n   378→  try {\n   379→    // Check if user exists\n   380→    const result = await client.query(\n   381→      'SELECT * FROM users WHERE stripe_customer_id = $1',\n   382→      [stripeCustomerId]\n   383→    );\n   384→\n   385→    if (result.rows.length === 0) {\n   386→      // Create new user with this tier\n   387→      await client.query(\n   388→        `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n   389→         VALUES ($1, $2, $3, $4, $4, $5, $5)`,\n   390→        [stripeCustomerId, email, tier, hours, isolationHours]\n   391→      );\n   392→    } else {\n   393→      // Update existing user\n   394→      await client.query(\n   395→        `UPDATE users\n   396→         SET tier = $1, hours_remaining = $2, hours_total = $2,\n   397→             isolation_hours_remaining = $3, isolation_hours_total = $3,\n   398→             updated_at = CURRENT_TIMESTAMP\n   399→         WHERE stripe_customer_id = $4`,\n   400→        [tier, hours, isolationHours, stripeCustomerId]\n   401→      );\n   402→    }\n   403→\n   404→    return await getBalance(stripeCustomerId);\n   405→  } finally {\n   406→    client.release();\n   407→  }\n   408→}\n   409→\n   410→/**\n   411→ * Get usage history for a user\n   412→ */\n   413→async function getUsageHistory(stripeCustomerId, limit = 50) {\n   414→  const result = await pool.query(\n   415→    `SELECT * FROM usage_log\n   416→     WHERE stripe_customer_id = $1\n   417→     ORDER BY created_at DESC\n   418→     LIMIT $2`,\n   419→    [stripeCustomerId, limit]\n   420→  );\n   421→  return result.rows;\n   422→}\n   423→\n   424→/**\n   425→ * Check if webhook event has already been processed (idempotency)\n   426→ */\n   427→async function isEventProcessed(eventId) {\n   428→  const result = await pool.query(\n   429→    'SELECT id FROM webhook_events WHERE event_id = $1',\n   430→    [eventId]\n   431→  );\n   432→  return result.rows.length > 0;\n   433→}\n   434→\n   435→/**\n   436→ * Record a processed webhook event\n   437→ */\n   438→async function recordWebhookEvent(eventId, eventType) {\n   439→  try {\n   440→    await pool.query(\n   441→      'INSERT INTO webhook_events (event_id, event_type) VALUES ($1, $2) ON CONFLICT (event_id) DO NOTHING',\n   442→      [eventId, eventType]\n   443→    );\n   444→  } catch (err) {\n   445→    // Ignore duplicate key errors (race condition safety)\n   446→    if (err.code !== '23505') throw err;\n   447→  }\n   448→}\n   449→\n   450→module.exports = {\n   451→  initDatabase,\n   452→  getOrCreateUser,\n   453→  getBalance,\n   454→  hasCredits,\n   455→  checkIsolationAccess,\n   456→  deductUsage,\n   457→  deductIsolationUsage,\n   458→  resetHours,\n   459→  updateTier,\n   460→  getUsageHistory,\n   461→  isEventProcessed,\n   462→  recordWebhookEvent,\n   463→  TIER_HOURS,\n   464→  TIER_ISOLATION_MINUTES,\n   465→  TIER_ISOLATION_HOURS,\n   466→  ISOLATION_OVERAGE_RATE\n   467→};\n   468→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"ec636f58-b099-41fe-901e-bbd7efe88fea","timestamp":"2025-12-25T20:25:13.536Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js","content":"/**\n * Usage Tracking Service\n *\n * Tracks user credit balances and usage for billing.\n * Uses PostgreSQL for persistence.\n */\n\nconst { Pool } = require('pg');\n\n// PostgreSQL connection pool\nconst pool = new Pool({\n  connectionString: process.env.DATABASE_URL,\n  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false\n});\n\n// Hours per tier (regular processing)\nconst TIER_HOURS = {\n  starter: 15,\n  pro: 50,\n  team: 150,\n  cancelled: 0\n};\n\n// Isolation minutes per tier (sized to maintain 80% margins)\nconst TIER_ISOLATION_MINUTES = {\n  starter: 0,     // No isolation access\n  pro: 45,        // 45 minutes included\n  team: 180,      // 3 hours included\n  cancelled: 0\n};\n\n// Convert to hours for database storage\nconst TIER_ISOLATION_HOURS = {\n  starter: 0,\n  pro: TIER_ISOLATION_MINUTES.pro / 60,      // 0.75 hours\n  team: TIER_ISOLATION_MINUTES.team / 60,    // 3 hours\n  cancelled: 0\n};\n\n// Overage rate for isolation beyond included minutes\nconst ISOLATION_OVERAGE_RATE = 0.10; // $0.10 per minute (85% margin)\n\n/**\n * Initialize database tables\n */\nasync function initDatabase() {\n  const client = await pool.connect();\n  try {\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS users (\n        id SERIAL PRIMARY KEY,\n        stripe_customer_id VARCHAR(255) UNIQUE NOT NULL,\n        email VARCHAR(255),\n        tier VARCHAR(50) DEFAULT 'starter',\n        hours_remaining DECIMAL(10,4) DEFAULT 15,\n        hours_total DECIMAL(10,4) DEFAULT 15,\n        isolation_hours_remaining DECIMAL(10,4) DEFAULT 0,\n        isolation_hours_total DECIMAL(10,4) DEFAULT 0,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n\n    // Add isolation columns if they don't exist (for existing databases)\n    await client.query(`\n      DO $$\n      BEGIN\n        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_remaining') THEN\n          ALTER TABLE users ADD COLUMN isolation_hours_remaining DECIMAL(10,4) DEFAULT 0;\n        END IF;\n        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_total') THEN\n          ALTER TABLE users ADD COLUMN isolation_hours_total DECIMAL(10,4) DEFAULT 0;\n        END IF;\n      END $$;\n    `);\n\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS usage_log (\n        id SERIAL PRIMARY KEY,\n        user_id INTEGER REFERENCES users(id),\n        stripe_customer_id VARCHAR(255),\n        audio_duration_seconds DECIMAL(10,2),\n        hours_used DECIMAL(10,6),\n        endpoint VARCHAR(100),\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_users_stripe_customer_id ON users(stripe_customer_id)\n    `);\n\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_usage_log_user_id ON usage_log(user_id)\n    `);\n\n    // Webhook events table for idempotency\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS webhook_events (\n        id SERIAL PRIMARY KEY,\n        event_id VARCHAR(255) UNIQUE NOT NULL,\n        event_type VARCHAR(100),\n        processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_webhook_events_event_id ON webhook_events(event_id)\n    `);\n\n    console.log('[UsageTracking] Database initialized');\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Get or create a user by Stripe customer ID\n */\nasync function getOrCreateUser(stripeCustomerId, email = null) {\n  const client = await pool.connect();\n  try {\n    // Try to find existing user\n    let result = await client.query(\n      'SELECT * FROM users WHERE stripe_customer_id = $1',\n      [stripeCustomerId]\n    );\n\n    if (result.rows.length > 0) {\n      return result.rows[0];\n    }\n\n    // Create new user with starter tier (no isolation hours)\n    result = await client.query(\n      `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n       VALUES ($1, $2, 'starter', $3, $3, $4, $4)\n       RETURNING *`,\n      [stripeCustomerId, email, TIER_HOURS.starter, TIER_ISOLATION_HOURS.starter]\n    );\n\n    return result.rows[0];\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Get user's current credit balance\n */\nasync function getBalance(stripeCustomerId) {\n  const user = await getOrCreateUser(stripeCustomerId);\n  const hoursTotal = parseFloat(user.hours_total);\n  const hoursRemaining = parseFloat(user.hours_remaining);\n  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n\n  // Tier display names\n  const tierNames = {\n    starter: 'Starter',\n    pro: 'Pro',\n    team: 'Team',\n    cancelled: 'Cancelled'\n  };\n\n  return {\n    hoursRemaining,\n    hoursTotal,\n    isolationHoursRemaining,\n    isolationHoursTotal,\n    tier: user.tier,\n    tierName: tierNames[user.tier] || user.tier,\n    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n    isolationOverageRate: ISOLATION_OVERAGE_RATE\n  };\n}\n\n/**\n * Check if user has enough credits for estimated duration\n */\nasync function hasCredits(stripeCustomerId, estimatedSeconds = 0) {\n  const balance = await getBalance(stripeCustomerId);\n  const estimatedHours = estimatedSeconds / 3600;\n  return balance.hoursRemaining >= estimatedHours;\n}\n\n/**\n * Check if user can use vocal isolation\n * Returns: { allowed, hasIncludedHours, overageRequired, overageCost }\n */\nasync function checkIsolationAccess(stripeCustomerId, estimatedMinutes = 0) {\n  const balance = await getBalance(stripeCustomerId);\n\n  // Starter tier has no isolation access\n  if (!balance.hasIsolationAccess) {\n    return {\n      allowed: false,\n      reason: 'upgrade_required',\n      message: 'Vocal isolation requires Pro or Team tier',\n      hasIncludedHours: false,\n      overageRequired: false,\n      overageCost: 0\n    };\n  }\n\n  const estimatedHours = estimatedMinutes / 60;\n  const remainingIncluded = balance.isolationHoursRemaining;\n\n  // Check if within included hours\n  if (remainingIncluded >= estimatedHours) {\n    return {\n      allowed: true,\n      reason: 'included',\n      message: `Using ${estimatedMinutes.toFixed(1)} min of included isolation time`,\n      hasIncludedHours: true,\n      overageRequired: false,\n      overageCost: 0,\n      isolationHoursRemaining: remainingIncluded - estimatedHours\n    };\n  }\n\n  // Calculate overage\n  const overageMinutes = estimatedMinutes - (remainingIncluded * 60);\n  const overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n\n  return {\n    allowed: true,\n    reason: 'overage',\n    message: `${remainingIncluded > 0 ? `Using ${(remainingIncluded * 60).toFixed(1)} included min + ` : ''}${overageMinutes.toFixed(1)} min overage ($${overageCost.toFixed(2)})`,\n    hasIncludedHours: remainingIncluded > 0,\n    overageRequired: true,\n    overageMinutes,\n    overageCost,\n    isolationHoursRemaining: 0\n  };\n}\n\n/**\n * Deduct isolation usage from user's balance\n * Returns overage cost if beyond included hours\n */\nasync function deductIsolationUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'isolate-vocals') {\n  const durationMinutes = audioDurationSeconds / 60;\n  const durationHours = audioDurationSeconds / 3600;\n\n  const client = await pool.connect();\n  try {\n    // Get user\n    const userResult = await client.query(\n      'SELECT * FROM users WHERE stripe_customer_id = $1',\n      [stripeCustomerId]\n    );\n\n    if (userResult.rows.length === 0) {\n      throw new Error('User not found');\n    }\n\n    const user = userResult.rows[0];\n    const remainingIncluded = parseFloat(user.isolation_hours_remaining || 0);\n\n    let overageCost = 0;\n    let hoursFromIncluded = 0;\n    let overageMinutes = 0;\n\n    if (remainingIncluded >= durationHours) {\n      // Fully covered by included hours\n      hoursFromIncluded = durationHours;\n    } else {\n      // Partial or full overage\n      hoursFromIncluded = remainingIncluded;\n      overageMinutes = durationMinutes - (remainingIncluded * 60);\n      overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n    }\n\n    // Update isolation balance\n    await client.query(\n      `UPDATE users\n       SET isolation_hours_remaining = GREATEST(0, isolation_hours_remaining - $1),\n           updated_at = CURRENT_TIMESTAMP\n       WHERE stripe_customer_id = $2`,\n      [durationHours, stripeCustomerId]\n    );\n\n    // Log usage (mark as isolation)\n    await client.query(\n      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n       VALUES ($1, $2, $3, $4, $5)`,\n      [user.id, stripeCustomerId, audioDurationSeconds, durationHours, endpoint]\n    );\n\n    // Return updated balance and overage info\n    const newBalance = await getBalance(stripeCustomerId);\n    return {\n      ...newBalance,\n      isolationUsed: {\n        durationMinutes,\n        hoursFromIncluded,\n        overageMinutes,\n        overageCost\n      }\n    };\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Deduct usage from user's balance\n */\nasync function deductUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'unknown') {\n  const hoursUsed = audioDurationSeconds / 3600;\n\n  const client = await pool.connect();\n  try {\n    // Get user\n    const userResult = await client.query(\n      'SELECT * FROM users WHERE stripe_customer_id = $1',\n      [stripeCustomerId]\n    );\n\n    if (userResult.rows.length === 0) {\n      throw new Error('User not found');\n    }\n\n    const user = userResult.rows[0];\n\n    // Update balance\n    await client.query(\n      `UPDATE users\n       SET hours_remaining = GREATEST(0, hours_remaining - $1),\n           updated_at = CURRENT_TIMESTAMP\n       WHERE stripe_customer_id = $2`,\n      [hoursUsed, stripeCustomerId]\n    );\n\n    // Log usage\n    await client.query(\n      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n       VALUES ($1, $2, $3, $4, $5)`,\n      [user.id, stripeCustomerId, audioDurationSeconds, hoursUsed, endpoint]\n    );\n\n    // Return updated balance\n    return await getBalance(stripeCustomerId);\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Reset user's hours for new billing period\n */\nasync function resetHours(stripeCustomerId, tier) {\n  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n\n  await pool.query(\n    `UPDATE users\n     SET hours_remaining = $1, hours_total = $1,\n         isolation_hours_remaining = $2, isolation_hours_total = $2,\n         tier = $3, updated_at = CURRENT_TIMESTAMP\n     WHERE stripe_customer_id = $4`,\n    [hours, isolationHours, tier, stripeCustomerId]\n  );\n\n  return await getBalance(stripeCustomerId);\n}\n\n/**\n * Update user's tier (from Stripe webhook)\n */\nasync function updateTier(stripeCustomerId, tier, email = null) {\n  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n\n  const client = await pool.connect();\n  try {\n    // Check if user exists\n    const result = await client.query(\n      'SELECT * FROM users WHERE stripe_customer_id = $1',\n      [stripeCustomerId]\n    );\n\n    if (result.rows.length === 0) {\n      // Create new user with this tier\n      await client.query(\n        `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n         VALUES ($1, $2, $3, $4, $4, $5, $5)`,\n        [stripeCustomerId, email, tier, hours, isolationHours]\n      );\n    } else {\n      // Update existing user\n      await client.query(\n        `UPDATE users\n         SET tier = $1, hours_remaining = $2, hours_total = $2,\n             isolation_hours_remaining = $3, isolation_hours_total = $3,\n             updated_at = CURRENT_TIMESTAMP\n         WHERE stripe_customer_id = $4`,\n        [tier, hours, isolationHours, stripeCustomerId]\n      );\n    }\n\n    return await getBalance(stripeCustomerId);\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Get usage history for a user\n */\nasync function getUsageHistory(stripeCustomerId, limit = 50) {\n  const result = await pool.query(\n    `SELECT * FROM usage_log\n     WHERE stripe_customer_id = $1\n     ORDER BY created_at DESC\n     LIMIT $2`,\n    [stripeCustomerId, limit]\n  );\n  return result.rows;\n}\n\n/**\n * Check if webhook event has already been processed (idempotency)\n */\nasync function isEventProcessed(eventId) {\n  const result = await pool.query(\n    'SELECT id FROM webhook_events WHERE event_id = $1',\n    [eventId]\n  );\n  return result.rows.length > 0;\n}\n\n/**\n * Record a processed webhook event\n */\nasync function recordWebhookEvent(eventId, eventType) {\n  try {\n    await pool.query(\n      'INSERT INTO webhook_events (event_id, event_type) VALUES ($1, $2) ON CONFLICT (event_id) DO NOTHING',\n      [eventId, eventType]\n    );\n  } catch (err) {\n    // Ignore duplicate key errors (race condition safety)\n    if (err.code !== '23505') throw err;\n  }\n}\n\nmodule.exports = {\n  initDatabase,\n  getOrCreateUser,\n  getBalance,\n  hasCredits,\n  checkIsolationAccess,\n  deductUsage,\n  deductIsolationUsage,\n  resetHours,\n  updateTier,\n  getUsageHistory,\n  isEventProcessed,\n  recordWebhookEvent,\n  TIER_HOURS,\n  TIER_ISOLATION_MINUTES,\n  TIER_ISOLATION_HOURS,\n  ISOLATION_OVERAGE_RATE\n};\n","numLines":468,"startLine":1,"totalLines":468}}}
{"parentUuid":"ec636f58-b099-41fe-901e-bbd7efe88fea","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MSxnGFFipa5YaS1dD9S3Ak","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Direct DOM Sequence Builder (v3.5)\n     3→ *\n     4→ * Builds sequences directly using UXP DOM APIs.\n     5→ * No XML export/import required - zero manual steps.\n     6→ *\n     7→ * Key APIs:\n     8→ * - SequenceEditor.createInsertProjectItemAction() - Insert clips at timecode\n     9→ * - ProjectItem.createSetColorLabelAction(index) - Color clips (set BEFORE insert)\n    10→ * - TrackItem.createSetInPointAction/OutPointAction() - Set in/out points\n    11→ * - project.executeTransaction(actions) - Batch execute atomically\n    12→ */\n    13→\n    14→const pproBuilder = require('premierepro');\n    15→\n    16→// No TICKS_PER_SECOND constant - we use native TickTime API instead\n    17→// This avoids variable collision with config.js\n    18→\n    19→// Color label indices in Premiere Pro\n    20→const COLOR_LABELS = {\n    21→  NONE: 0,\n    22→  VIOLET: 1,    // Purple\n    23→  IRIS: 2,      // Light purple\n    24→  CARIBBEAN: 3, // Teal\n    25→  LAVENDER: 4,  // Light lavender\n    26→  CERULEAN: 5,  // Light blue\n    27→  FOREST: 6,    // Dark green\n    28→  ROSE: 7,      // Pink\n    29→  MANGO: 8,     // Orange\n    30→  PURPLE: 9,    // Dark purple\n    31→  BLUE: 10,     // Blue\n    32→  TEAL: 11,     // Teal/Cyan\n    33→  MAGENTA: 12,  // Magenta\n    34→  TAN: 13,      // Tan/Brown\n    35→  GREEN: 14,    // Green\n    36→  BROWN: 15,    // Dark brown\n    37→  YELLOW: 16    // Yellow\n    38→};\n    39→\n    40→// SPLICE color scheme for cut list clips\n    41→const SPLICE_COLORS = {\n    42→  SPEECH: COLOR_LABELS.GREEN,      // Green for speech/takes\n    43→  SILENCE: COLOR_LABELS.VIOLET,    // Purple for silence segments (if kept)\n    44→  BEST_TAKE: COLOR_LABELS.CERULEAN // Light blue for best take\n    45→};\n    46→\n    47→/**\n    48→ * Find a project item by name in the project bin\n    49→ * @param {Object} project - The active project\n    50→ * @param {string} name - Name of the clip to find\n    51→ * @returns {Promise<Object|null>} The project item or null\n    52→ */\n    53→async function findProjectItemByName(project, name) {\n    54→  const rootItem = await project.getRootItem();\n    55→  return await searchItemRecursively(rootItem, name);\n    56→}\n    57→\n    58→/**\n    59→ * Recursively search for an item in the project bin\n    60→ * @param {Object} item - Current item to search\n    61→ * @param {string} name - Name to search for\n    62→ * @returns {Promise<Object|null>} The found item or null\n    63→ */\n    64→async function searchItemRecursively(item, name) {\n    65→  if (!item || !item.children) return null;\n    66→\n    67→  if (item.children.numItems === 0) return null;\n    68→\n    69→  for (let i = 0; i < item.children.numItems; i++) {\n    70→    const child = item.children[i];\n    71→\n    72→    // Check name using property access\n    73→    if (child.name === name) {\n    74→      return child;\n    75→    }\n    76→\n    77→    // Check if it's a bin (Type 2) and search recursively\n    78→    if (child.type === pproBuilder.Constants.ProjectItemType.BIN) {\n    79→      const found = await searchItemRecursively(child, name);\n    80→      if (found) return found;\n    81→    }\n    82→  }\n    83→\n    84→  return null;\n    85→}\n    86→\n    87→/**\n    88→ * Find project item by file path\n    89→ * @param {Object} project - The active project\n    90→ * @param {string} filePath - Full path to the media file\n    91→ * @returns {Promise<Object|null>} The project item or null\n    92→ */\n    93→async function findProjectItemByPath(project, filePath) {\n    94→  const rootItem = await project.getRootItem();\n    95→  return await searchItemByPathRecursively(rootItem, filePath);\n    96→}\n    97→\n    98→/**\n    99→ * Recursively search for an item by its file path\n   100→ * @param {Object} item - Current item to search\n   101→ * @param {string} filePath - Path to search for\n   102→ * @returns {Promise<Object|null>} The found item or null\n   103→ */\n   104→async function searchItemByPathRecursively(item, filePath) {\n   105→  if (!item || !item.children) return null;\n   106→\n   107→  if (item.children.numItems === 0) return null;\n   108→\n   109→  for (let i = 0; i < item.children.numItems; i++) {\n   110→    const child = item.children[i];\n   111→\n   112→    // Check if this item has a matching path (using property access)\n   113→    if (child.treePath === filePath) {\n   114→      return child;\n   115→    }\n   116→\n   117→    // Check if it's a bin (Type 2) and search recursively\n   118→    if (child.type === pproBuilder.Constants.ProjectItemType.BIN) {\n   119→      const found = await searchItemByPathRecursively(child, filePath);\n   120→      if (found) return found;\n   121→    }\n   122→  }\n   123→\n   124→  return null;\n   125→}\n   126→\n   127→/**\n   128→ * Create a new sequence from a cut list\n   129→ * @param {Object} cutList - The cut list from backend\n   130→ * @param {Object} options - Build options\n   131→ * @returns {Promise<{success: boolean, sequence?: Object, error?: string, stats?: Object}>}\n   132→ */\n   133→async function buildSequenceFromCutList(cutList, options = {}) {\n   134→  const stats = {\n   135→    clipsInserted: 0,\n   136→    totalDuration: 0,\n   137→    errors: []\n   138→  };\n   139→\n   140→  try {\n   141→    const context = await getActiveSequence();\n   142→    if (!context) {\n   143→      return { success: false, error: 'No project or sequence open' };\n   144→    }\n   145→\n   146→    const { project, sequence } = context;\n   147→\n   148→    // Create new sequence with \"_SPLICE\" suffix by cloning the existing sequence\n   149→    const originalName = sequence.name;\n   150→    const newSequenceName = `${originalName}_SPLICE`;\n   151→\n   152→    // Clone the existing sequence to preserve settings\n   153→    let newSequence = null;\n   154→    try {\n   155→      await project.lockedAccess(async () => {\n   156→        await project.executeTransaction((compoundAction) => {\n   157→          const cloneAction = sequence.createCloneAction();\n   158→          compoundAction.addAction(cloneAction);\n   159→        }, 'SPLICE: Clone Sequence');\n   160→      });\n   161→    } catch (cloneErr) {\n   162→      console.error('[SPLICE Builder] Failed to clone sequence:', cloneErr);\n   163→      return { success: false, error: `Failed to clone sequence: ${cloneErr.message}`, stats };\n   164→    }\n   165→\n   166→    // Find the newly created sequence (it will have the same name as original)\n   167→    // We need to rename it\n   168→    const sequences = await project.getSequences();\n   169→    for (const seq of sequences) {\n   170→      const name = seq.name;\n   171→      if (name === originalName && seq !== sequence) {\n   172→        newSequence = seq;\n   173→        // Rename the cloned sequence\n   174→        try {\n   175→          await project.lockedAccess(async () => {\n   176→            await project.executeTransaction((compoundAction) => {\n   177→              const renameAction = newSequence.createSetNameAction(newSequenceName);\n   178→              compoundAction.addAction(renameAction);\n   179→            }, 'SPLICE: Rename Cloned Sequence');\n   180→          });\n   181→        } catch (renameErr) {\n   182→          console.error('[SPLICE Builder] Failed to rename sequence:', renameErr);\n   183→          // Continue anyway - sequence was cloned, just has wrong name\n   184→        }\n   185→        break;\n   186→      }\n   187→    }\n   188→\n   189→    if (!newSequence) {\n   190→      // Fallback: create a new sequence from scratch\n   191→      newSequence = await project.createSequence(newSequenceName);\n   192→      if (!newSequence) {\n   193→        return { success: false, error: 'Failed to create new sequence' };\n   194→      }\n   195→    }\n   196→\n   197→    // Get the sequence editor for inserting clips\n   198→    const editor = await pproBuilder.SequenceEditor.getEditor(newSequence);\n   199→    if (!editor) {\n   200→      return { success: false, error: 'Failed to get sequence editor' };\n   201→    }\n   202→\n   203→    // Process cut list segments\n   204→    const segments = cutList.segments || [];\n   205→    if (segments.length === 0) {\n   206→      return { success: false, error: 'Cut list has no segments' };\n   207→    }\n   208→\n   209→    // Prepare segment data for insertion\n   210→    const segmentsToInsert = [];\n   211→    let currentPositionSeconds = 0;\n   212→\n   213→    for (const segment of segments) {\n   214→      try {\n   215→        // Find the source project item\n   216→        const sourceItem = await findProjectItemByPath(project, segment.sourcePath)\n   217→          || await findProjectItemByName(project, segment.sourceName);\n   218→\n   219→        if (!sourceItem) {\n   220→          stats.errors.push(`Source not found: ${segment.sourceName || segment.sourcePath}`);\n   221→          continue;\n   222→        }\n   223→\n   224→        // Create TickTime objects directly from seconds (no TICKS_PER_SECOND needed)\n   225→        const inPoint = pproBuilder.TickTime.createWithSeconds(segment.inPoint);\n   226→        const outPoint = pproBuilder.TickTime.createWithSeconds(segment.outPoint);\n   227→        const position = pproBuilder.TickTime.createWithSeconds(currentPositionSeconds);\n   228→\n   229→        // Calculate duration in seconds for next position\n   230→        const durationSeconds = segment.outPoint - segment.inPoint;\n   231→\n   232→        // Determine color based on segment type\n   233→        const colorIndex = getColorForSegmentType(segment.type);\n   234→\n   235→        segmentsToInsert.push({\n   236→          sourceItem,\n   237→          positionTime: position,\n   238→          inPoint,\n   239→          outPoint,\n   240→          colorIndex\n   241→        });\n   242→\n   243→        // Update position for next clip\n   244→        currentPositionSeconds += durationSeconds;\n   245→        stats.totalDuration += durationSeconds;\n   246→\n   247→      } catch (err) {\n   248→        stats.errors.push(`Error processing segment: ${err.message}`);\n   249→        console.error('[SPLICE Builder] Segment error:', err);\n   250→      }\n   251→    }\n   252→\n   253→    if (segmentsToInsert.length === 0) {\n   254→      return { success: false, error: 'No valid segments to insert' };\n   255→    }\n   256→\n   257→    // Clear the new sequence first (it's a clone, so it has content)\n   258→    // We'll insert clips from scratch\n   259→    await clearSequence(newSequence);\n   260→\n   261→    // Execute all insert actions in a single transaction\n   262→    try {\n   263→      await project.lockedAccess(async () => {\n   264→        await project.executeTransaction((compoundAction) => {\n   265→          for (const segData of segmentsToInsert) {\n   266→            // Set color on source item if needed\n   267→            if (segData.colorIndex !== null) {\n   268→              const colorAction = segData.sourceItem.createSetColorLabelAction(segData.colorIndex);\n   269→              compoundAction.addAction(colorAction);\n   270→            }\n   271→\n   272→            // Use the pre-created TickTime object (no conversion needed)\n   273→            const insertTime = segData.positionTime;\n   274→\n   275→            // Create insert action (must be done inside transaction)\n   276→            const insertAction = editor.createInsertProjectItemAction(\n   277→              segData.sourceItem,\n   278→              insertTime,\n   279→              0,  // Video track index\n   280→              0,  // Audio track index\n   281→              false // Don't limit shift - insert on all tracks\n   282→            );\n   283→            compoundAction.addAction(insertAction);\n   284→          }\n   285→        }, 'SPLICE: Build Sequence from Cut List');\n   286→      });\n   287→    } catch (insertErr) {\n   288→      console.error('[SPLICE Builder] Failed to insert clips:', insertErr);\n   289→      return { success: false, error: `Failed to insert clips: ${insertErr.message}`, stats };\n   290→    }\n   291→\n   292→    stats.clipsInserted = segmentsToInsert.length;\n   293→\n   294→    // Set in/out points for each track item (must be done after insert)\n   295→    await setTrackItemInOutPoints(newSequence, segmentsToInsert);\n   296→\n   297→    console.log(`[SPLICE Builder] Built sequence with ${stats.clipsInserted} clips`);\n   298→\n   299→    return {\n   300→      success: true,\n   301→      sequence: newSequence,\n   302→      sequenceName: newSequenceName,\n   303→      stats\n   304→    };\n   305→\n   306→  } catch (err) {\n   307→    console.error('[SPLICE Builder] Build error:', err);\n   308→    return { success: false, error: err.message, stats };\n   309→  }\n   310→}\n   311→\n   312→/**\n   313→ * Get color index for a segment type\n   314→ * @param {string} type - Segment type (speech, silence, best_take)\n   315→ * @returns {number|null} Color label index or null for no color\n   316→ */\n   317→function getColorForSegmentType(type) {\n   318→  switch (type) {\n   319→    case 'speech':\n   320→    case 'take':\n   321→      return SPLICE_COLORS.SPEECH;\n   322→    case 'best_take':\n   323→      return SPLICE_COLORS.BEST_TAKE;\n   324→    case 'silence':\n   325→      return SPLICE_COLORS.SILENCE;\n   326→    default:\n   327→      return null;\n   328→  }\n   329→}\n   330→\n   331→/**\n   332→ * Clear all clips from a sequence\n   333→ * @param {Object} sequence - The sequence to clear\n   334→ */\n   335→async function clearSequence(sequence) {\n   336→  try {\n   337→    const project = await pproBuilder.Project.getActiveProject();\n   338→\n   339→    // Get all video tracks\n   340→    const videoTrackCount = await sequence.getVideoTrackCount();\n   341→    const audioTrackCount = await sequence.getAudioTrackCount();\n   342→\n   343→    const itemsToDelete = [];\n   344→\n   345→    // Collect all video track items\n   346→    for (let i = 0; i < videoTrackCount; i++) {\n   347→      const track = await sequence.getVideoTrack(i);\n   348→      if (track) {\n   349→        const items = await track.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n   350→        if (items) {\n   351→          itemsToDelete.push(...items);\n   352→        }\n   353→      }\n   354→    }\n   355→\n   356→    // Collect all audio track items\n   357→    for (let i = 0; i < audioTrackCount; i++) {\n   358→      const track = await sequence.getAudioTrack(i);\n   359→      if (track) {\n   360→        const items = await track.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n   361→        if (items) {\n   362→          itemsToDelete.push(...items);\n   363→        }\n   364→      }\n   365→    }\n   366→\n   367→    // Delete all items in one transaction\n   368→    if (itemsToDelete.length > 0) {\n   369→      try {\n   370→        await project.lockedAccess(async () => {\n   371→          await project.executeTransaction((compoundAction) => {\n   372→            for (const item of itemsToDelete) {\n   373→              const deleteAction = item.createRemoveAction();\n   374→              compoundAction.addAction(deleteAction);\n   375→            }\n   376→          }, 'SPLICE: Clear Sequence');\n   377→        });\n   378→      } catch (clearErr) {\n   379→        console.error('[SPLICE Builder] Failed to clear sequence:', clearErr);\n   380→        // Non-fatal - continue with insert\n   381→      }\n   382→    }\n   383→  } catch (err) {\n   384→    console.error('[SPLICE Builder] Error clearing sequence:', err);\n   385→  }\n   386→}\n   387→\n   388→/**\n   389→ * Set in/out points for track items after insertion\n   390→ * @param {Object} sequence - The sequence containing the clips\n   391→ * @param {Array} segmentsToInsert - The segment data with in/out points\n   392→ */\n   393→async function setTrackItemInOutPoints(sequence, segmentsToInsert) {\n   394→  try {\n   395→    // Get video track\n   396→    const videoTrack = await sequence.getVideoTrack(0);\n   397→    if (!videoTrack) return;\n   398→\n   399→    const trackItems = await videoTrack.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n   400→    if (!trackItems || trackItems.length === 0) return;\n   401→\n   402→    const project = await pproBuilder.Project.getActiveProject();\n   403→\n   404→    // Match track items with segments (in order)\n   405→    const itemCount = Math.min(trackItems.length, segmentsToInsert.length);\n   406→\n   407→    if (itemCount > 0) {\n   408→      try {\n   409→        await project.lockedAccess(async () => {\n   410→          await project.executeTransaction((compoundAction) => {\n   411→            for (let i = 0; i < itemCount; i++) {\n   412→              const trackItem = trackItems[i];\n   413→              const segData = segmentsToInsert[i];\n   414→\n   415→              // Use the pre-created TickTime objects (no conversion needed)\n   416→              const inPointTime = segData.inPoint;\n   417→              const outPointTime = segData.outPoint;\n   418→\n   419→              // Create actions to set in/out points\n   420→              const inAction = trackItem.createSetInPointAction(inPointTime);\n   421→              const outAction = trackItem.createSetOutPointAction(outPointTime);\n   422→\n   423→              compoundAction.addAction(inAction);\n   424→              compoundAction.addAction(outAction);\n   425→            }\n   426→          }, 'SPLICE: Set In/Out Points');\n   427→        });\n   428→      } catch (inOutErr) {\n   429→        console.error('[SPLICE Builder] Failed to set in/out points:', inOutErr);\n   430→        // Non-fatal - clips are inserted but may have wrong in/out points\n   431→      }\n   432→    }\n   433→\n   434→  } catch (err) {\n   435→    console.error('[SPLICE Builder] Error setting in/out points:', err);\n   436→  }\n   437→}\n   438→\n   439→\n   440→/**\n   441→ * Build sequence from detected silences and takes\n   442→ * Convenience wrapper that creates cut list from detection results\n   443→ * @param {Array} silences - Detected silence segments\n   444→ * @param {Array} takes - Detected speech takes\n   445→ * @param {Object} sourceClip - Source project item\n   446→ * @returns {Promise<Object>} Build result\n   447→ */\n   448→async function buildSequenceFromDetection(silences, takes, sourceClip) {\n   449→  // Create cut list from detection results\n   450→  // Keep speech segments, remove silences\n   451→  const segments = [];\n   452→\n   453→  // Get source clip info (using synchronous property access)\n   454→  const sourceName = sourceClip.name;\n   455→  const sourcePath = sourceClip.treePath;\n   456→\n   457→  // Get clip duration\n   458→  const durationObj = await sourceClip.getDuration();\n   459→  const durationSecs = durationObj ? durationObj.seconds : 0;\n   460→\n   461→  // Sort silences by start time\n   462→  const sortedSilences = [...silences].sort((a, b) => a.start - b.start);\n   463→\n   464→  // Build segments from gaps between silences (speech segments)\n   465→  let lastEnd = 0;\n   466→\n   467→  for (const silence of sortedSilences) {\n   468→    // Add speech segment before this silence\n   469→    if (silence.start > lastEnd) {\n   470→      segments.push({\n   471→        type: 'speech',\n   472→        sourceName,\n   473→        sourcePath,\n   474→        inPoint: lastEnd,\n   475→        outPoint: silence.start\n   476→      });\n   477→    }\n   478→    lastEnd = silence.end;\n   479→  }\n   480→\n   481→  // Add final segment after last silence\n   482→  if (lastEnd < durationSecs) {\n   483→    segments.push({\n   484→      type: 'speech',\n   485→      sourceName,\n   486→      sourcePath,\n   487→      inPoint: lastEnd,\n   488→      outPoint: durationSecs\n   489→    });\n   490→  }\n   491→\n   492→  // Build the cut list\n   493→  const cutList = {\n   494→    version: '3.5',\n   495→    source: sourceName,\n   496→    segments,\n   497→    metadata: {\n   498→      silencesRemoved: silences.length,\n   499→      takesDetected: takes?.length || 0,\n   500→      generatedAt: new Date().toISOString()\n   501→    }\n   502→  };\n   503→\n   504→  return await buildSequenceFromCutList(cutList);\n   505→}\n   506→\n   507→/**\n   508→ * Apply cut list to existing sequence (in-place edit)\n   509→ * Alternative to creating new sequence\n   510→ * @param {Object} cutList - The cut list from backend\n   511→ * @returns {Promise<Object>} Result object\n   512→ */\n   513→async function applyCutListToSequence(cutList) {\n   514→  // TODO: Implement in-place editing as alternative to new sequence\n   515→  // This would use razor cuts on existing clips rather than rebuilding\n   516→  return { success: false, error: 'In-place editing not yet implemented' };\n   517→}\n   518→\n   519→// Export functions for use in main.js\n   520→window.spliceBuilder = {\n   521→  buildSequenceFromCutList,\n   522→  buildSequenceFromDetection,\n   523→  applyCutListToSequence,\n   524→  findProjectItemByName,\n   525→  findProjectItemByPath,\n   526→  COLOR_LABELS,\n   527→  SPLICE_COLORS\n   528→};\n   529→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"d2190a15-619a-499b-9f39-53b2bf13e304","timestamp":"2025-12-25T20:25:13.537Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-plugin/js/builder.js","content":"/**\n * SPLICE Direct DOM Sequence Builder (v3.5)\n *\n * Builds sequences directly using UXP DOM APIs.\n * No XML export/import required - zero manual steps.\n *\n * Key APIs:\n * - SequenceEditor.createInsertProjectItemAction() - Insert clips at timecode\n * - ProjectItem.createSetColorLabelAction(index) - Color clips (set BEFORE insert)\n * - TrackItem.createSetInPointAction/OutPointAction() - Set in/out points\n * - project.executeTransaction(actions) - Batch execute atomically\n */\n\nconst pproBuilder = require('premierepro');\n\n// No TICKS_PER_SECOND constant - we use native TickTime API instead\n// This avoids variable collision with config.js\n\n// Color label indices in Premiere Pro\nconst COLOR_LABELS = {\n  NONE: 0,\n  VIOLET: 1,    // Purple\n  IRIS: 2,      // Light purple\n  CARIBBEAN: 3, // Teal\n  LAVENDER: 4,  // Light lavender\n  CERULEAN: 5,  // Light blue\n  FOREST: 6,    // Dark green\n  ROSE: 7,      // Pink\n  MANGO: 8,     // Orange\n  PURPLE: 9,    // Dark purple\n  BLUE: 10,     // Blue\n  TEAL: 11,     // Teal/Cyan\n  MAGENTA: 12,  // Magenta\n  TAN: 13,      // Tan/Brown\n  GREEN: 14,    // Green\n  BROWN: 15,    // Dark brown\n  YELLOW: 16    // Yellow\n};\n\n// SPLICE color scheme for cut list clips\nconst SPLICE_COLORS = {\n  SPEECH: COLOR_LABELS.GREEN,      // Green for speech/takes\n  SILENCE: COLOR_LABELS.VIOLET,    // Purple for silence segments (if kept)\n  BEST_TAKE: COLOR_LABELS.CERULEAN // Light blue for best take\n};\n\n/**\n * Find a project item by name in the project bin\n * @param {Object} project - The active project\n * @param {string} name - Name of the clip to find\n * @returns {Promise<Object|null>} The project item or null\n */\nasync function findProjectItemByName(project, name) {\n  const rootItem = await project.getRootItem();\n  return await searchItemRecursively(rootItem, name);\n}\n\n/**\n * Recursively search for an item in the project bin\n * @param {Object} item - Current item to search\n * @param {string} name - Name to search for\n * @returns {Promise<Object|null>} The found item or null\n */\nasync function searchItemRecursively(item, name) {\n  if (!item || !item.children) return null;\n\n  if (item.children.numItems === 0) return null;\n\n  for (let i = 0; i < item.children.numItems; i++) {\n    const child = item.children[i];\n\n    // Check name using property access\n    if (child.name === name) {\n      return child;\n    }\n\n    // Check if it's a bin (Type 2) and search recursively\n    if (child.type === pproBuilder.Constants.ProjectItemType.BIN) {\n      const found = await searchItemRecursively(child, name);\n      if (found) return found;\n    }\n  }\n\n  return null;\n}\n\n/**\n * Find project item by file path\n * @param {Object} project - The active project\n * @param {string} filePath - Full path to the media file\n * @returns {Promise<Object|null>} The project item or null\n */\nasync function findProjectItemByPath(project, filePath) {\n  const rootItem = await project.getRootItem();\n  return await searchItemByPathRecursively(rootItem, filePath);\n}\n\n/**\n * Recursively search for an item by its file path\n * @param {Object} item - Current item to search\n * @param {string} filePath - Path to search for\n * @returns {Promise<Object|null>} The found item or null\n */\nasync function searchItemByPathRecursively(item, filePath) {\n  if (!item || !item.children) return null;\n\n  if (item.children.numItems === 0) return null;\n\n  for (let i = 0; i < item.children.numItems; i++) {\n    const child = item.children[i];\n\n    // Check if this item has a matching path (using property access)\n    if (child.treePath === filePath) {\n      return child;\n    }\n\n    // Check if it's a bin (Type 2) and search recursively\n    if (child.type === pproBuilder.Constants.ProjectItemType.BIN) {\n      const found = await searchItemByPathRecursively(child, filePath);\n      if (found) return found;\n    }\n  }\n\n  return null;\n}\n\n/**\n * Create a new sequence from a cut list\n * @param {Object} cutList - The cut list from backend\n * @param {Object} options - Build options\n * @returns {Promise<{success: boolean, sequence?: Object, error?: string, stats?: Object}>}\n */\nasync function buildSequenceFromCutList(cutList, options = {}) {\n  const stats = {\n    clipsInserted: 0,\n    totalDuration: 0,\n    errors: []\n  };\n\n  try {\n    const context = await getActiveSequence();\n    if (!context) {\n      return { success: false, error: 'No project or sequence open' };\n    }\n\n    const { project, sequence } = context;\n\n    // Create new sequence with \"_SPLICE\" suffix by cloning the existing sequence\n    const originalName = sequence.name;\n    const newSequenceName = `${originalName}_SPLICE`;\n\n    // Clone the existing sequence to preserve settings\n    let newSequence = null;\n    try {\n      await project.lockedAccess(async () => {\n        await project.executeTransaction((compoundAction) => {\n          const cloneAction = sequence.createCloneAction();\n          compoundAction.addAction(cloneAction);\n        }, 'SPLICE: Clone Sequence');\n      });\n    } catch (cloneErr) {\n      console.error('[SPLICE Builder] Failed to clone sequence:', cloneErr);\n      return { success: false, error: `Failed to clone sequence: ${cloneErr.message}`, stats };\n    }\n\n    // Find the newly created sequence (it will have the same name as original)\n    // We need to rename it\n    const sequences = await project.getSequences();\n    for (const seq of sequences) {\n      const name = seq.name;\n      if (name === originalName && seq !== sequence) {\n        newSequence = seq;\n        // Rename the cloned sequence\n        try {\n          await project.lockedAccess(async () => {\n            await project.executeTransaction((compoundAction) => {\n              const renameAction = newSequence.createSetNameAction(newSequenceName);\n              compoundAction.addAction(renameAction);\n            }, 'SPLICE: Rename Cloned Sequence');\n          });\n        } catch (renameErr) {\n          console.error('[SPLICE Builder] Failed to rename sequence:', renameErr);\n          // Continue anyway - sequence was cloned, just has wrong name\n        }\n        break;\n      }\n    }\n\n    if (!newSequence) {\n      // Fallback: create a new sequence from scratch\n      newSequence = await project.createSequence(newSequenceName);\n      if (!newSequence) {\n        return { success: false, error: 'Failed to create new sequence' };\n      }\n    }\n\n    // Get the sequence editor for inserting clips\n    const editor = await pproBuilder.SequenceEditor.getEditor(newSequence);\n    if (!editor) {\n      return { success: false, error: 'Failed to get sequence editor' };\n    }\n\n    // Process cut list segments\n    const segments = cutList.segments || [];\n    if (segments.length === 0) {\n      return { success: false, error: 'Cut list has no segments' };\n    }\n\n    // Prepare segment data for insertion\n    const segmentsToInsert = [];\n    let currentPositionSeconds = 0;\n\n    for (const segment of segments) {\n      try {\n        // Find the source project item\n        const sourceItem = await findProjectItemByPath(project, segment.sourcePath)\n          || await findProjectItemByName(project, segment.sourceName);\n\n        if (!sourceItem) {\n          stats.errors.push(`Source not found: ${segment.sourceName || segment.sourcePath}`);\n          continue;\n        }\n\n        // Create TickTime objects directly from seconds (no TICKS_PER_SECOND needed)\n        const inPoint = pproBuilder.TickTime.createWithSeconds(segment.inPoint);\n        const outPoint = pproBuilder.TickTime.createWithSeconds(segment.outPoint);\n        const position = pproBuilder.TickTime.createWithSeconds(currentPositionSeconds);\n\n        // Calculate duration in seconds for next position\n        const durationSeconds = segment.outPoint - segment.inPoint;\n\n        // Determine color based on segment type\n        const colorIndex = getColorForSegmentType(segment.type);\n\n        segmentsToInsert.push({\n          sourceItem,\n          positionTime: position,\n          inPoint,\n          outPoint,\n          colorIndex\n        });\n\n        // Update position for next clip\n        currentPositionSeconds += durationSeconds;\n        stats.totalDuration += durationSeconds;\n\n      } catch (err) {\n        stats.errors.push(`Error processing segment: ${err.message}`);\n        console.error('[SPLICE Builder] Segment error:', err);\n      }\n    }\n\n    if (segmentsToInsert.length === 0) {\n      return { success: false, error: 'No valid segments to insert' };\n    }\n\n    // Clear the new sequence first (it's a clone, so it has content)\n    // We'll insert clips from scratch\n    await clearSequence(newSequence);\n\n    // Execute all insert actions in a single transaction\n    try {\n      await project.lockedAccess(async () => {\n        await project.executeTransaction((compoundAction) => {\n          for (const segData of segmentsToInsert) {\n            // Set color on source item if needed\n            if (segData.colorIndex !== null) {\n              const colorAction = segData.sourceItem.createSetColorLabelAction(segData.colorIndex);\n              compoundAction.addAction(colorAction);\n            }\n\n            // Use the pre-created TickTime object (no conversion needed)\n            const insertTime = segData.positionTime;\n\n            // Create insert action (must be done inside transaction)\n            const insertAction = editor.createInsertProjectItemAction(\n              segData.sourceItem,\n              insertTime,\n              0,  // Video track index\n              0,  // Audio track index\n              false // Don't limit shift - insert on all tracks\n            );\n            compoundAction.addAction(insertAction);\n          }\n        }, 'SPLICE: Build Sequence from Cut List');\n      });\n    } catch (insertErr) {\n      console.error('[SPLICE Builder] Failed to insert clips:', insertErr);\n      return { success: false, error: `Failed to insert clips: ${insertErr.message}`, stats };\n    }\n\n    stats.clipsInserted = segmentsToInsert.length;\n\n    // Set in/out points for each track item (must be done after insert)\n    await setTrackItemInOutPoints(newSequence, segmentsToInsert);\n\n    console.log(`[SPLICE Builder] Built sequence with ${stats.clipsInserted} clips`);\n\n    return {\n      success: true,\n      sequence: newSequence,\n      sequenceName: newSequenceName,\n      stats\n    };\n\n  } catch (err) {\n    console.error('[SPLICE Builder] Build error:', err);\n    return { success: false, error: err.message, stats };\n  }\n}\n\n/**\n * Get color index for a segment type\n * @param {string} type - Segment type (speech, silence, best_take)\n * @returns {number|null} Color label index or null for no color\n */\nfunction getColorForSegmentType(type) {\n  switch (type) {\n    case 'speech':\n    case 'take':\n      return SPLICE_COLORS.SPEECH;\n    case 'best_take':\n      return SPLICE_COLORS.BEST_TAKE;\n    case 'silence':\n      return SPLICE_COLORS.SILENCE;\n    default:\n      return null;\n  }\n}\n\n/**\n * Clear all clips from a sequence\n * @param {Object} sequence - The sequence to clear\n */\nasync function clearSequence(sequence) {\n  try {\n    const project = await pproBuilder.Project.getActiveProject();\n\n    // Get all video tracks\n    const videoTrackCount = await sequence.getVideoTrackCount();\n    const audioTrackCount = await sequence.getAudioTrackCount();\n\n    const itemsToDelete = [];\n\n    // Collect all video track items\n    for (let i = 0; i < videoTrackCount; i++) {\n      const track = await sequence.getVideoTrack(i);\n      if (track) {\n        const items = await track.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n        if (items) {\n          itemsToDelete.push(...items);\n        }\n      }\n    }\n\n    // Collect all audio track items\n    for (let i = 0; i < audioTrackCount; i++) {\n      const track = await sequence.getAudioTrack(i);\n      if (track) {\n        const items = await track.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n        if (items) {\n          itemsToDelete.push(...items);\n        }\n      }\n    }\n\n    // Delete all items in one transaction\n    if (itemsToDelete.length > 0) {\n      try {\n        await project.lockedAccess(async () => {\n          await project.executeTransaction((compoundAction) => {\n            for (const item of itemsToDelete) {\n              const deleteAction = item.createRemoveAction();\n              compoundAction.addAction(deleteAction);\n            }\n          }, 'SPLICE: Clear Sequence');\n        });\n      } catch (clearErr) {\n        console.error('[SPLICE Builder] Failed to clear sequence:', clearErr);\n        // Non-fatal - continue with insert\n      }\n    }\n  } catch (err) {\n    console.error('[SPLICE Builder] Error clearing sequence:', err);\n  }\n}\n\n/**\n * Set in/out points for track items after insertion\n * @param {Object} sequence - The sequence containing the clips\n * @param {Array} segmentsToInsert - The segment data with in/out points\n */\nasync function setTrackItemInOutPoints(sequence, segmentsToInsert) {\n  try {\n    // Get video track\n    const videoTrack = await sequence.getVideoTrack(0);\n    if (!videoTrack) return;\n\n    const trackItems = await videoTrack.getTrackItems(pproBuilder.Constants.TrackItemType.CLIP, false);\n    if (!trackItems || trackItems.length === 0) return;\n\n    const project = await pproBuilder.Project.getActiveProject();\n\n    // Match track items with segments (in order)\n    const itemCount = Math.min(trackItems.length, segmentsToInsert.length);\n\n    if (itemCount > 0) {\n      try {\n        await project.lockedAccess(async () => {\n          await project.executeTransaction((compoundAction) => {\n            for (let i = 0; i < itemCount; i++) {\n              const trackItem = trackItems[i];\n              const segData = segmentsToInsert[i];\n\n              // Use the pre-created TickTime objects (no conversion needed)\n              const inPointTime = segData.inPoint;\n              const outPointTime = segData.outPoint;\n\n              // Create actions to set in/out points\n              const inAction = trackItem.createSetInPointAction(inPointTime);\n              const outAction = trackItem.createSetOutPointAction(outPointTime);\n\n              compoundAction.addAction(inAction);\n              compoundAction.addAction(outAction);\n            }\n          }, 'SPLICE: Set In/Out Points');\n        });\n      } catch (inOutErr) {\n        console.error('[SPLICE Builder] Failed to set in/out points:', inOutErr);\n        // Non-fatal - clips are inserted but may have wrong in/out points\n      }\n    }\n\n  } catch (err) {\n    console.error('[SPLICE Builder] Error setting in/out points:', err);\n  }\n}\n\n\n/**\n * Build sequence from detected silences and takes\n * Convenience wrapper that creates cut list from detection results\n * @param {Array} silences - Detected silence segments\n * @param {Array} takes - Detected speech takes\n * @param {Object} sourceClip - Source project item\n * @returns {Promise<Object>} Build result\n */\nasync function buildSequenceFromDetection(silences, takes, sourceClip) {\n  // Create cut list from detection results\n  // Keep speech segments, remove silences\n  const segments = [];\n\n  // Get source clip info (using synchronous property access)\n  const sourceName = sourceClip.name;\n  const sourcePath = sourceClip.treePath;\n\n  // Get clip duration\n  const durationObj = await sourceClip.getDuration();\n  const durationSecs = durationObj ? durationObj.seconds : 0;\n\n  // Sort silences by start time\n  const sortedSilences = [...silences].sort((a, b) => a.start - b.start);\n\n  // Build segments from gaps between silences (speech segments)\n  let lastEnd = 0;\n\n  for (const silence of sortedSilences) {\n    // Add speech segment before this silence\n    if (silence.start > lastEnd) {\n      segments.push({\n        type: 'speech',\n        sourceName,\n        sourcePath,\n        inPoint: lastEnd,\n        outPoint: silence.start\n      });\n    }\n    lastEnd = silence.end;\n  }\n\n  // Add final segment after last silence\n  if (lastEnd < durationSecs) {\n    segments.push({\n      type: 'speech',\n      sourceName,\n      sourcePath,\n      inPoint: lastEnd,\n      outPoint: durationSecs\n    });\n  }\n\n  // Build the cut list\n  const cutList = {\n    version: '3.5',\n    source: sourceName,\n    segments,\n    metadata: {\n      silencesRemoved: silences.length,\n      takesDetected: takes?.length || 0,\n      generatedAt: new Date().toISOString()\n    }\n  };\n\n  return await buildSequenceFromCutList(cutList);\n}\n\n/**\n * Apply cut list to existing sequence (in-place edit)\n * Alternative to creating new sequence\n * @param {Object} cutList - The cut list from backend\n * @returns {Promise<Object>} Result object\n */\nasync function applyCutListToSequence(cutList) {\n  // TODO: Implement in-place editing as alternative to new sequence\n  // This would use razor cuts on existing clips rather than rebuilding\n  return { success: false, error: 'In-place editing not yet implemented' };\n}\n\n// Export functions for use in main.js\nwindow.spliceBuilder = {\n  buildSequenceFromCutList,\n  buildSequenceFromDetection,\n  applyCutListToSequence,\n  findProjectItemByName,\n  findProjectItemByPath,\n  COLOR_LABELS,\n  SPLICE_COLORS\n};\n","numLines":529,"startLine":1,"totalLines":529}}}
{"parentUuid":"d2190a15-619a-499b-9f39-53b2bf13e304","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017h5JGvzh6RAjhzmNHfJi62","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Backend Server\n     3→ *\n     4→ * Main entry point for the SPLICE backend API.\n     5→ * Orchestrates the audio analysis pipeline.\n     6→ *\n     7→ * Slices:\n     8→ * - Slice 4: Transcription (services/transcription.js)\n     9→ * - Slice 5: Take Detection (services/takeDetection.js)\n    10→ */\n    11→\n    12→require('dotenv').config();\n    13→\n    14→const express = require('express');\n    15→const cors = require('cors');\n    16→const fs = require('fs');\n    17→const https = require('https');\n    18→const http = require('http');\n    19→const path = require('path');\n    20→\n    21→// Check if running in production (Railway injects RAILWAY_ENVIRONMENT)\n    22→const isProduction = process.env.NODE_ENV === 'production' || process.env.RAILWAY_ENVIRONMENT;\n    23→\n    24→// Import slice services\n    25→const { transcribeAudio, transcribeWithWords } = require('./services/transcription');\n    26→const { detectTakes } = require('./services/takeDetection');\n    27→const { detectSilences } = require('./services/silenceDetection');\n    28→const { detectAudioSilences, isFFprobeInstalled, getAudioDuration } = require('./services/ffprobeSilence');\n    29→const { detectSilencesRMS, sensitivityToParams } = require('./services/rmsSilenceDetection');\n    30→const {\n    31→  detectProfanity,\n    32→  getProfanityList,\n    33→  getSupportedLanguages,\n    34→  getAvailableBleepSounds,\n    35→  parseWordList\n    36→} = require('./services/profanityDetection');\n    37→const {\n    38→  detectRepetitionsBasic,\n    39→  detectRepetitionsAdvanced,\n    40→  detectStutters,\n    41→  detectAllRepetitions\n    42→} = require('./services/repetitionDetection');\n    43→const {\n    44→  analyzeMultitrack,\n    45→  autoBalanceMultitrack\n    46→} = require('./services/multitrackAnalysis');\n    47→const {\n    48→  toSRT,\n    49→  toVTT,\n    50→  toPlainText,\n    51→  toJSON,\n    52→  exportToFile,\n    53→  getSupportedFormats\n    54→} = require('./services/captionExporter');\n    55→const { processXMLFile } = require('./services/xmlProcessor');\n    56→const { isolateVocals, isReplicateConfigured } = require('./services/vocalIsolation');\n    57→const { generateCutList, generateTakesCutList, validateCutList } = require('./services/cutListGenerator');\n    58→\n    59→// Usage tracking and billing\n    60→const usageTracking = require('./services/usageTracking');\n    61→// Rate limiter for usage-based endpoints\n    62→const { requireCredits } = require('./middleware/rateLimiter');\n    63→\n    64→// Stripe for webhooks\n    65→const Stripe = require('stripe');\n    66→const stripe = new Stripe(process.env.STRIPE_SECRET_KEY);\n    67→\n    68→// =============================================================================\n    69→// Server Configuration\n    70→// =============================================================================\n    71→\n    72→const app = express();\n    73→const PORT = process.env.PORT || 3847;\n    74→\n    75→// HTTPS certificates (generated by mkcert) - only for local development\n    76→let httpsOptions = null;\n    77→if (!isProduction) {\n    78→  const keyPath = path.join(__dirname, 'localhost+1-key.pem');\n    79→  const certPath = path.join(__dirname, 'localhost+1.pem');\n    80→  if (fs.existsSync(keyPath) && fs.existsSync(certPath)) {\n    81→    httpsOptions = {\n    82→      key: fs.readFileSync(keyPath),\n    83→      cert: fs.readFileSync(certPath)\n    84→    };\n    85→  }\n    86→}\n    87→\n    88→app.use(cors());\n    89→\n    90→// Helper to determine tier from price ID with logging\n    91→function getTierFromPriceId(priceId) {\n    92→  if (priceId === process.env.STRIPE_PRICE_STARTER) return 'starter';\n    93→  if (priceId === process.env.STRIPE_PRICE_PRO) return 'pro';\n    94→  if (priceId === process.env.STRIPE_PRICE_TEAM) return 'team';\n    95→\n    96→  // Log unknown price ID for debugging\n    97→  console.warn(`[SPLICE] Unknown price ID: ${priceId} - defaulting to starter tier`);\n    98→  return 'starter';\n    99→}\n   100→\n   101→// Stripe webhook needs raw body - must be before express.json()\n   102→app.post('/webhooks/stripe', express.raw({ type: 'application/json' }), async (req, res) => {\n   103→  const sig = req.headers['stripe-signature'];\n   104→  const webhookSecret = process.env.STRIPE_WEBHOOK_SECRET;\n   105→\n   106→  let event;\n   107→\n   108→  try {\n   109→    if (webhookSecret) {\n   110→      event = stripe.webhooks.constructEvent(req.body, sig, webhookSecret);\n   111→    } else if (isProduction) {\n   112→      // SECURITY: Reject unsigned webhooks in production\n   113→      console.error('[SPLICE] CRITICAL: STRIPE_WEBHOOK_SECRET not set in production');\n   114→      return res.status(500).json({ error: 'Webhook configuration error: secret not configured' });\n   115→    } else {\n   116→      // For local development testing only\n   117→      event = JSON.parse(req.body);\n   118→      console.warn('[SPLICE] Warning: Processing webhook without signature verification (dev only)');\n   119→    }\n   120→  } catch (err) {\n   121→    console.error('[SPLICE] Webhook signature verification failed:', err.message);\n   122→    return res.status(400).json({ error: 'Webhook signature verification failed' });\n   123→  }\n   124→\n   125→  console.log(`[SPLICE] Webhook received: ${event.type} (${event.id})`);\n   126→\n   127→  // Idempotency check - skip if already processed\n   128→  if (await usageTracking.isEventProcessed(event.id)) {\n   129→    console.log(`[SPLICE] Event ${event.id} already processed, skipping`);\n   130→    return res.json({ received: true, skipped: true });\n   131→  }\n   132→\n   133→  try {\n   134→    switch (event.type) {\n   135→      case 'customer.subscription.created':\n   136→      case 'customer.subscription.updated': {\n   137→        const subscription = event.data.object;\n   138→        const customerId = subscription.customer;\n   139→\n   140→        // Validate customerId\n   141→        if (!customerId) {\n   142→          console.error('[SPLICE] Missing customer ID in subscription event');\n   143→          return res.status(400).json({ error: 'Missing customer ID' });\n   144→        }\n   145→\n   146→        // Get tier from price ID\n   147→        const priceId = subscription.items?.data?.[0]?.price?.id;\n   148→        const tier = getTierFromPriceId(priceId);\n   149→\n   150→        // Update user tier and reset hours\n   151→        await usageTracking.updateTier(customerId, tier);\n   152→        console.log(`[SPLICE] Updated customer ${customerId} to tier: ${tier}`);\n   153→        break;\n   154→      }\n   155→\n   156→      case 'customer.subscription.deleted': {\n   157→        const subscription = event.data.object;\n   158→        const customerId = subscription.customer;\n   159→\n   160→        // Validate customerId\n   161→        if (!customerId) {\n   162→          console.error('[SPLICE] Missing customer ID in subscription.deleted event');\n   163→          return res.status(400).json({ error: 'Missing customer ID' });\n   164→        }\n   165→\n   166→        // Downgrade to cancelled (0 hours)\n   167→        await usageTracking.updateTier(customerId, 'cancelled');\n   168→        console.log(`[SPLICE] Subscription cancelled for customer ${customerId}`);\n   169→        break;\n   170→      }\n   171→\n   172→      case 'invoice.payment_succeeded': {\n   173→        const invoice = event.data.object;\n   174→        const customerId = invoice.customer;\n   175→        const subscriptionId = invoice.subscription;\n   176→\n   177→        // Validate customerId\n   178→        if (!customerId) {\n   179→          console.error('[SPLICE] Missing customer ID in invoice event');\n   180→          return res.status(400).json({ error: 'Missing customer ID' });\n   181→        }\n   182→\n   183→        // Reset hours on successful payment (new billing period)\n   184→        if (subscriptionId) {\n   185→          const subscription = await stripe.subscriptions.retrieve(subscriptionId);\n   186→          const priceId = subscription.items?.data?.[0]?.price?.id;\n   187→          const tier = getTierFromPriceId(priceId);\n   188→\n   189→          await usageTracking.resetHours(customerId, tier);\n   190→          console.log(`[SPLICE] Reset hours for customer ${customerId} (tier: ${tier})`);\n   191→        }\n   192→        break;\n   193→      }\n   194→\n   195→      default:\n   196→        console.log(`[SPLICE] Unhandled event type: ${event.type}`);\n   197→    }\n   198→\n   199→    // Record event as processed (idempotency)\n   200→    await usageTracking.recordWebhookEvent(event.id, event.type);\n   201→\n   202→    res.json({ received: true });\n   203→  } catch (err) {\n   204→    console.error('[SPLICE] Webhook handler error:', err);\n   205→    res.status(500).json({ error: err.message });\n   206→  }\n   207→});\n   208→\n   209→app.use(express.json());\n   210→\n   211→// =============================================================================\n   212→// Routes\n   213→// =============================================================================\n   214→\n   215→/**\n   216→ * GET / - API information\n   217→ */\n   218→app.get('/', (req, res) => {\n   219→  res.json({\n   220→    service: 'splice-backend',\n   221→    version: '0.3.0',\n   222→    endpoints: {\n   223→      'GET /': 'This info',\n   224→      'GET /health': 'Health check',\n   225→      'GET /ffprobe-check': 'Check if FFprobe is installed',\n   226→      'GET /replicate-check': 'Check if Replicate API is configured',\n   227→      'POST /analyze': 'Analyze WAV file { wavPath }',\n   228→      'POST /silences': 'Detect silences via Whisper gaps { wavPath, threshold: 0.5 }',\n   229→      'POST /silences-audio': 'Detect silences via FFprobe { wavPath, threshold: -30, minDuration: 0.5, padding: 0.1 }',\n   230→      'POST /silences-rms': 'Detect silences via RMS analysis { wavPath, threshold: -30, minSilenceLength: 0.5, paddingStart: 0.1, paddingEnd: 0.05, autoThreshold: false, sensitivity: 50 }',\n   231→      'POST /profanity': 'Detect profanity in transcript { wavPath, language: \"en\", customBlocklist: [], customAllowlist: [] }',\n   232→      'GET /profanity/languages': 'Get supported languages for profanity detection',\n   233→      'GET /profanity/bleeps': 'Get available bleep sounds',\n   234→      'POST /repetitions': 'Detect phrase repetitions and stutters { wavPath, phraseSize: 5, tolerance: 0.7, useOpenAI: false }',\n   235→      'POST /fillers': 'Detect filler words (um, uh, like, etc.) { wavPath, customFillers: [] }',\n   236→      'POST /stutters': 'Detect single-word stutters only { wavPath, minRepeats: 2 }',\n   237→      'POST /export/captions': 'Export transcript to caption format { wavPath, format: srt|vtt|txt|json, outputPath? }',\n   238→      'GET /export/formats': 'Get supported caption export formats',\n   239→      'POST /multitrack': 'Analyze multiple audio tracks for multicam { audioPaths: [], speakerNames: [], videoTrackMapping: {} }',\n   240→      'POST /multitrack/auto-balance': 'Auto-balance speaker screentime { audioPaths: [], speakerNames: [] }',\n   241→      'POST /process-xml': 'Process FCP XML { xmlPath, silences, removeGaps: true }',\n   242→      'POST /cut-list': 'Generate JSON cut list for DOM building (v3.5) { sourceName, sourcePath, duration, silences, takes?, settings? }',\n   243→      'POST /cut-list/takes': 'Generate cut list keeping only takes { sourceName, sourcePath, duration, takes, settings? }',\n   244→      'POST /isolate-vocals': 'Isolate vocals from audio { audioPath }',\n   245→      'POST /batch/silences': 'Batch process multiple files for silence detection { files: [], options: {} }',\n   246→      'GET /batch/status/:jobId': 'Get batch job status',\n   247→      'GET /batch/results/:jobId': 'Get full batch job results',\n   248→      'GET /batch/jobs': 'List all batch jobs',\n   249→      'DELETE /batch/:jobId': 'Delete a batch job',\n   250→      'GET /credits': 'Get user credit balance (requires x-stripe-customer-id header)',\n   251→      'GET /usage-history': 'Get usage history (requires x-stripe-customer-id header)',\n   252→      'POST /webhooks/stripe': 'Stripe webhook endpoint'\n   253→    }\n   254→  });\n   255→});\n   256→\n   257→/**\n   258→ * GET /health - Health check\n   259→ */\n   260→app.get('/health', (req, res) => {\n   261→  res.json({ status: 'ok', service: 'splice-backend' });\n   262→});\n   263→\n   264→/**\n   265→ * POST /analyze - Main analysis endpoint\n   266→ *\n   267→ * Pipeline:\n   268→ * 1. Validate input (wavPath)\n   269→ * 2. Slice 4: Transcribe audio with Groq Whisper\n   270→ * 3. Slice 5: Detect takes with GPT-4o-mini\n   271→ * 4. Return combined results\n   272→ */\n   273→app.post('/analyze', requireCredits({ endpoint: 'analyze' }), async (req, res) => {\n   274→  const { wavPath } = req.body;\n   275→\n   276→  // Validate input\n   277→  if (!wavPath) {\n   278→    return res.status(400).json({ error: 'wavPath is required' });\n   279→  }\n   280→\n   281→  if (!fs.existsSync(wavPath)) {\n   282→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   283→  }\n   284→\n   285→  console.log(`[SPLICE] Analyzing: ${wavPath}`);\n   286→\n   287→  try {\n   288→    // Slice 4 - GPT-4o-mini transcription\n   289→    const transcript = await transcribeAudio(wavPath);\n   290→\n   291→    // Slice 5 - GPT-4o-mini take detection\n   292→    const takes = await detectTakes(transcript);\n   293→\n   294→    res.json({\n   295→      success: true,\n   296→      wavPath,\n   297→      transcript,\n   298→      takes\n   299→    });\n   300→  } catch (err) {\n   301→    console.error('[SPLICE] Error:', err);\n   302→    res.status(500).json({ error: err.message });\n   303→  }\n   304→});\n   305→\n   306→/**\n   307→ * POST /silences - Detect silent gaps in audio\n   308→ *\n   309→ * Pipeline:\n   310→ * 1. Transcribe audio with Whisper (reuses transcription)\n   311→ * 2. Analyze gaps between segments\n   312→ * 3. Return silence regions\n   313→ */\n   314→app.post('/silences', requireCredits({ endpoint: 'silences' }), async (req, res) => {\n   315→  const { wavPath, threshold = 0.5 } = req.body;\n   316→\n   317→  if (!wavPath) {\n   318→    return res.status(400).json({ error: 'wavPath is required' });\n   319→  }\n   320→\n   321→  if (!fs.existsSync(wavPath)) {\n   322→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   323→  }\n   324→\n   325→  console.log(`[SPLICE] Detecting silences: ${wavPath} (threshold: ${threshold}s)`);\n   326→\n   327→  try {\n   328→    const transcript = await transcribeAudio(wavPath);\n   329→    const silences = detectSilences(transcript.segments, threshold);\n   330→\n   331→    res.json({\n   332→      success: true,\n   333→      wavPath,\n   334→      threshold,\n   335→      silences,\n   336→      count: silences.length,\n   337→      totalSilenceDuration: silences.reduce((sum, s) => sum + s.duration, 0).toFixed(2)\n   338→    });\n   339→  } catch (err) {\n   340→    console.error('[SPLICE] Silence detection error:', err);\n   341→    res.status(500).json({ error: err.message });\n   342→  }\n   343→});\n   344→\n   345→/**\n   346→ * POST /silences-audio - Detect silences using FFprobe audio analysis\n   347→ *\n   348→ * Uses actual audio levels (dB threshold) instead of transcript gaps.\n   349→ * More accurate for detecting silence vs background noise.\n   350→ */\n   351→app.post('/silences-audio', async (req, res) => {\n   352→  const {\n   353→    wavPath,\n   354→    threshold = -30,\n   355→    minDuration = 0.5,\n   356→    padding = 0.1\n   357→  } = req.body;\n   358→\n   359→  if (!wavPath) {\n   360→    return res.status(400).json({ error: 'wavPath is required' });\n   361→  }\n   362→\n   363→  if (!fs.existsSync(wavPath)) {\n   364→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   365→  }\n   366→\n   367→  // Check FFprobe availability\n   368→  const ffprobeAvailable = await isFFprobeInstalled();\n   369→  if (!ffprobeAvailable) {\n   370→    return res.status(500).json({\n   371→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n   372→    });\n   373→  }\n   374→\n   375→  console.log(`[SPLICE] FFprobe silence detection: ${wavPath} (threshold: ${threshold}dB, min: ${minDuration}s)`);\n   376→\n   377→  try {\n   378→    const silences = await detectAudioSilences(wavPath, {\n   379→      threshold,\n   380→      minDuration,\n   381→      padding\n   382→    });\n   383→\n   384→    const totalDuration = silences.reduce((sum, s) => sum + s.duration, 0);\n   385→\n   386→    res.json({\n   387→      success: true,\n   388→      wavPath,\n   389→      threshold,\n   390→      minDuration,\n   391→      padding,\n   392→      silences,\n   393→      count: silences.length,\n   394→      totalSilenceDuration: totalDuration.toFixed(2)\n   395→    });\n   396→  } catch (err) {\n   397→    console.error('[SPLICE] FFprobe silence detection error:', err);\n   398→    res.status(500).json({ error: err.message });\n   399→  }\n   400→});\n   401→\n   402→/**\n   403→ * POST /silences-rms - Detect silences using RMS audio analysis\n   404→ *\n   405→ * Advanced silence detection with:\n   406→ * - RMS (Root Mean Square) audio level analysis\n   407→ * - Auto-threshold detection from audio histogram\n   408→ * - Configurable padding (before/after cuts)\n   409→ * - Sensitivity slider mapping (0-100)\n   410→ *\n   411→ * Options:\n   412→ * - threshold: dBFS threshold (-60 to -20, default: -30)\n   413→ * - minSilenceLength: Minimum silence duration in seconds (default: 0.5)\n   414→ * - seekStep: Analysis window step in seconds (default: 0.05)\n   415→ * - paddingStart: Buffer before silence in seconds (default: 0.1)\n   416→ * - paddingEnd: Buffer after silence in seconds (default: 0.05)\n   417→ * - autoThreshold: Auto-detect optimal threshold (default: false)\n   418→ * - sensitivity: UI sensitivity 0-100 (overrides other params if provided)\n   419→ */\n   420→app.post('/silences-rms', requireCredits({ endpoint: 'silences-rms' }), async (req, res) => {\n   421→  const { wavPath, sensitivity, ...manualOptions } = req.body;\n   422→\n   423→  if (!wavPath) {\n   424→    return res.status(400).json({ error: 'wavPath is required' });\n   425→  }\n   426→\n   427→  if (!fs.existsSync(wavPath)) {\n   428→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   429→  }\n   430→\n   431→  // Check FFprobe availability (needed for audio extraction)\n   432→  const ffprobeAvailable = await isFFprobeInstalled();\n   433→  if (!ffprobeAvailable) {\n   434→    return res.status(500).json({\n   435→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n   436→    });\n   437→  }\n   438→\n   439→  // Build options - use sensitivity if provided, otherwise use manual options\n   440→  let options = {};\n   441→  if (typeof sensitivity === 'number') {\n   442→    options = sensitivityToParams(sensitivity);\n   443→    console.log(`[SPLICE] RMS detection with sensitivity ${sensitivity}`);\n   444→  } else {\n   445→    options = {\n   446→      threshold: manualOptions.threshold ?? -30,\n   447→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n   448→      seekStep: manualOptions.seekStep ?? 0.05,\n   449→      paddingStart: manualOptions.paddingStart ?? 0.1,\n   450→      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n   451→      autoThreshold: manualOptions.autoThreshold ?? false,\n   452→      mergeDistance: manualOptions.mergeDistance ?? 0.2\n   453→    };\n   454→  }\n   455→\n   456→  console.log(`[SPLICE] RMS silence detection: ${wavPath}`);\n   457→\n   458→  try {\n   459→    const result = await detectSilencesRMS(wavPath, options);\n   460→\n   461→    res.json({\n   462→      success: true,\n   463→      wavPath,\n   464→      ...result,\n   465→      count: result.silences.length,\n   466→      totalSilenceDuration: result.metadata.totalSilenceDuration.toFixed(2)\n   467→    });\n   468→  } catch (err) {\n   469→    console.error('[SPLICE] RMS silence detection error:', err);\n   470→    res.status(500).json({ error: err.message });\n   471→  }\n   472→});\n   473→\n   474→// =============================================================================\n   475→// Profanity Detection Routes\n   476→// =============================================================================\n   477→\n   478→/**\n   479→ * POST /profanity - Detect profanity in audio/transcript\n   480→ *\n   481→ * Transcribes audio (if needed) and detects profanity words.\n   482→ * Returns word-level and segment-level results for muting/bleeping.\n   483→ *\n   484→ * Options:\n   485→ * - wavPath: Path to audio file (required)\n   486→ * - transcript: Pre-existing transcript (optional, skips transcription)\n   487→ * - language: Language code (en, es, fr, de) - default: en\n   488→ * - customBlocklist: Array or comma-separated string of additional words to censor\n   489→ * - customAllowlist: Array or comma-separated string of words to allow\n   490→ * - frameRate: Frame rate for boundary alignment (default: 30)\n   491→ */\n   492→app.post('/profanity', requireCredits({ endpoint: 'profanity' }), async (req, res) => {\n   493→  const {\n   494→    wavPath,\n   495→    transcript: providedTranscript,\n   496→    language = 'en',\n   497→    customBlocklist = [],\n   498→    customAllowlist = [],\n   499→    frameRate = 30\n   500→  } = req.body;\n   501→\n   502→  if (!wavPath && !providedTranscript) {\n   503→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   504→  }\n   505→\n   506→  if (wavPath && !fs.existsSync(wavPath)) {\n   507→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   508→  }\n   509→\n   510→  console.log(`[SPLICE] Profanity detection: ${wavPath || 'provided transcript'} (language: ${language})`);\n   511→\n   512→  try {\n   513→    // Get or create transcript with word-level timestamps\n   514→    let transcript = providedTranscript;\n   515→    if (!transcript && wavPath) {\n   516→      // Use transcribeWithWords for word-level timestamps required by profanity detection\n   517→      transcript = await transcribeWithWords(wavPath);\n   518→    }\n   519→\n   520→    // Validate transcript has words\n   521→    if (!transcript || !transcript.words || transcript.words.length === 0) {\n   522→      return res.status(400).json({\n   523→        error: 'Transcript must contain word-level timing data',\n   524→        hint: 'Ensure transcription returns words array with start/end times'\n   525→      });\n   526→    }\n   527→\n   528→    // Parse custom lists\n   529→    const blocklist = parseWordList(customBlocklist);\n   530→    const allowlist = parseWordList(customAllowlist);\n   531→\n   532→    // Detect profanity\n   533→    const result = detectProfanity(transcript, {\n   534→      language,\n   535→      customBlocklist: blocklist,\n   536→      customAllowlist: allowlist,\n   537→      frameRate\n   538→    });\n   539→\n   540→    res.json({\n   541→      success: true,\n   542→      wavPath,\n   543→      ...result\n   544→    });\n   545→  } catch (err) {\n   546→    console.error('[SPLICE] Profanity detection error:', err);\n   547→    res.status(500).json({ error: err.message });\n   548→  }\n   549→});\n   550→\n   551→/**\n   552→ * GET /profanity/languages - Get supported languages\n   553→ */\n   554→app.get('/profanity/languages', (req, res) => {\n   555→  res.json({\n   556→    success: true,\n   557→    languages: getSupportedLanguages()\n   558→  });\n   559→});\n   560→\n   561→/**\n   562→ * GET /profanity/bleeps - Get available bleep sounds\n   563→ */\n   564→app.get('/profanity/bleeps', (req, res) => {\n   565→  res.json({\n   566→    success: true,\n   567→    sounds: getAvailableBleepSounds()\n   568→  });\n   569→});\n   570→\n   571→/**\n   572→ * GET /profanity/list/:language - Get default profanity list for a language\n   573→ */\n   574→app.get('/profanity/list/:language', (req, res) => {\n   575→  const { language } = req.params;\n   576→  const list = getProfanityList(language);\n   577→\n   578→  res.json({\n   579→    success: true,\n   580→    language,\n   581→    wordCount: list.length,\n   582→    // Return first 50 words as sample (full list is large)\n   583→    sample: list.slice(0, 50),\n   584→    note: 'Full list available but truncated for response size'\n   585→  });\n   586→});\n   587→\n   588→// =============================================================================\n   589→// Repetition/Stutter Detection Routes\n   590→// =============================================================================\n   591→\n   592→/**\n   593→ * POST /repetitions - Detect phrase repetitions and stutters\n   594→ *\n   595→ * Analyzes transcript for repeated phrases and stutters.\n   596→ * Returns segments that can be removed to clean up the edit.\n   597→ *\n   598→ * Options:\n   599→ * - wavPath: Path to audio file (required unless transcript provided)\n   600→ * - transcript: Pre-existing transcript (optional)\n   601→ * - phraseSize: Words per comparison window (default: 5)\n   602→ * - tolerance: Similarity threshold 0-1 (default: 0.7)\n   603→ * - searchRadius: Words to search ahead (default: 100)\n   604→ * - useOpenAI: Use OpenAI for boundary refinement (default: false)\n   605→ * - includeStutters: Also detect single-word stutters (default: true)\n   606→ */\n   607→app.post('/repetitions', async (req, res) => {\n   608→  const {\n   609→    wavPath,\n   610→    transcript: providedTranscript,\n   611→    phraseSize = 5,\n   612→    tolerance = 0.7,\n   613→    searchRadius = 100,\n   614→    useOpenAI = false,\n   615→    includeStutters = true\n   616→  } = req.body;\n   617→\n   618→  if (!wavPath && !providedTranscript) {\n   619→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   620→  }\n   621→\n   622→  if (wavPath && !fs.existsSync(wavPath)) {\n   623→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   624→  }\n   625→\n   626→  console.log(`[SPLICE] Repetition detection: ${wavPath || 'provided transcript'}`);\n   627→\n   628→  try {\n   629→    // Get or create transcript with word-level timestamps\n   630→    let transcript = providedTranscript;\n   631→    if (!transcript && wavPath) {\n   632→      // Use transcribeWithWords for word-level timestamps required by repetition detection\n   633→      transcript = await transcribeWithWords(wavPath);\n   634→    }\n   635→\n   636→    // Validate transcript has words\n   637→    if (!transcript || !transcript.words || transcript.words.length === 0) {\n   638→      return res.status(400).json({\n   639→        error: 'Transcript must contain word-level timing data'\n   640→      });\n   641→    }\n   642→\n   643→    // Detect all repetitions (phrases + stutters)\n   644→    const result = await detectAllRepetitions(transcript, {\n   645→      phraseSize,\n   646→      tolerance,\n   647→      searchRadius,\n   648→      useOpenAI\n   649→    });\n   650→\n   651→    // Optionally filter out stutters\n   652→    if (!includeStutters) {\n   653→      result.stutters = [];\n   654→      result.removalSegments = result.removalSegments.filter(s => s.type !== 'stutter');\n   655→    }\n   656→\n   657→    res.json({\n   658→      success: true,\n   659→      wavPath,\n   660→      ...result\n   661→    });\n   662→  } catch (err) {\n   663→    console.error('[SPLICE] Repetition detection error:', err);\n   664→    res.status(500).json({ error: err.message });\n   665→  }\n   666→});\n   667→\n   668→/**\n   669→ * POST /fillers - Detect filler words (um, uh, like, etc.)\n   670→ *\n   671→ * Transcribes audio and identifies filler words with timestamps.\n   672→ * Returns segments that can be cut or reviewed for removal.\n   673→ *\n   674→ * Options:\n   675→ * - wavPath: Path to audio file (required unless transcript provided)\n   676→ * - transcript: Pre-existing transcript with word-level timing (optional)\n   677→ * - customFillers: Additional filler words to detect (optional)\n   678→ */\n   679→app.post('/fillers', async (req, res) => {\n   680→  const {\n   681→    wavPath,\n   682→    transcript: providedTranscript,\n   683→    customFillers = []\n   684→  } = req.body;\n   685→\n   686→  if (!wavPath && !providedTranscript) {\n   687→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   688→  }\n   689→\n   690→  if (wavPath && !fs.existsSync(wavPath)) {\n   691→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   692→  }\n   693→\n   694→  console.log(`[SPLICE] Filler word detection: ${wavPath || 'provided transcript'}`);\n   695→\n   696→  try {\n   697→    // Get or create transcript with word-level timestamps\n   698→    let transcript = providedTranscript;\n   699→    if (!transcript && wavPath) {\n   700→      transcript = await transcribeWithWords(wavPath);\n   701→    }\n   702→\n   703→    // Validate transcript has words\n   704→    if (!transcript || !transcript.words || transcript.words.length === 0) {\n   705→      return res.status(400).json({\n   706→        error: 'Transcript must contain word-level timing data'\n   707→      });\n   708→    }\n   709→\n   710→    // Default filler words (common in English speech)\n   711→    const defaultFillers = [\n   712→      'um', 'uh', 'ah', 'er', 'eh',           // Hesitation sounds\n   713→      'like', 'so', 'well', 'right',           // Discourse markers\n   714→      'you know', 'i mean', 'basically',       // Filler phrases\n   715→      'actually', 'literally', 'honestly',     // Overused qualifiers\n   716→      'kind of', 'sort of', 'you see'          // Hedging phrases\n   717→    ];\n   718→\n   719→    // Combine default + custom fillers (lowercase for matching)\n   720→    const fillerSet = new Set([\n   721→      ...defaultFillers,\n   722→      ...customFillers.map(f => f.toLowerCase().trim())\n   723→    ]);\n   724→\n   725→    // Detect filler words\n   726→    const fillers = [];\n   727→    const words = transcript.words;\n   728→\n   729→    for (let i = 0; i < words.length; i++) {\n   730→      const word = words[i];\n   731→      const normalizedWord = word.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim();\n   732→\n   733→      // Check single-word fillers\n   734→      if (fillerSet.has(normalizedWord)) {\n   735→        fillers.push({\n   736→          word: word.word,\n   737→          normalizedWord,\n   738→          start: word.start,\n   739→          end: word.end,\n   740→          duration: word.end - word.start,\n   741→          index: i,\n   742→          type: 'filler'\n   743→        });\n   744→        continue;\n   745→      }\n   746→\n   747→      // Check two-word phrases (e.g., \"you know\", \"kind of\")\n   748→      if (i < words.length - 1) {\n   749→        const nextWord = words[i + 1];\n   750→        const twoWordPhrase = `${normalizedWord} ${nextWord.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim()}`;\n   751→        if (fillerSet.has(twoWordPhrase)) {\n   752→          fillers.push({\n   753→            word: `${word.word} ${nextWord.word}`,\n   754→            normalizedWord: twoWordPhrase,\n   755→            start: word.start,\n   756→            end: nextWord.end,\n   757→            duration: nextWord.end - word.start,\n   758→            index: i,\n   759→            type: 'filler_phrase'\n   760→          });\n   761→          // Skip next word since it's part of this phrase\n   762→          i++;\n   763→        }\n   764→      }\n   765→    }\n   766→\n   767→    // Calculate total filler time\n   768→    const totalFillerDuration = fillers.reduce((sum, f) => sum + f.duration, 0);\n   769→    const audioDuration = transcript.duration || (words.length > 0 ? words[words.length - 1].end : 0);\n   770→    const fillerPercentage = audioDuration > 0 ? (totalFillerDuration / audioDuration) * 100 : 0;\n   771→\n   772→    res.json({\n   773→      success: true,\n   774→      wavPath,\n   775→      fillers,\n   776→      metadata: {\n   777→        totalWords: words.length,\n   778→        fillerCount: fillers.length,\n   779→        totalFillerDuration: parseFloat(totalFillerDuration.toFixed(3)),\n   780→        audioDuration: parseFloat(audioDuration.toFixed(3)),\n   781→        fillerPercentage: parseFloat(fillerPercentage.toFixed(2)),\n   782→        fillersPerMinute: audioDuration > 0 ? parseFloat((fillers.length / (audioDuration / 60)).toFixed(2)) : 0\n   783→      },\n   784→      removalSegments: fillers.map(f => ({\n   785→        start: f.start,\n   786→        end: f.end,\n   787→        duration: f.duration,\n   788→        reason: `Filler: \"${f.word}\"`,\n   789→        type: f.type\n   790→      }))\n   791→    });\n   792→  } catch (err) {\n   793→    console.error('[SPLICE] Filler detection error:', err);\n   794→    res.status(500).json({ error: err.message });\n   795→  }\n   796→});\n   797→\n   798→/**\n   799→ * POST /stutters - Detect single-word stutters only\n   800→ *\n   801→ * Focused detection for word-level stutters (e.g., \"I I I think\").\n   802→ * Faster than full repetition detection.\n   803→ */\n   804→app.post('/stutters', async (req, res) => {\n   805→  const {\n   806→    wavPath,\n   807→    transcript: providedTranscript,\n   808→    options = {},\n   809→    // Support both top-level and nested options for flexibility\n   810→    minRepeats = options.minRepeats ?? 2,\n   811→    maxGapMs = options.maxGapMs ?? 500,\n   812→    ignoreFillers = options.ignoreFillers ?? true,\n   813→    minWordLength = options.minWordLength ?? 1\n   814→  } = req.body;\n   815→\n   816→  if (!wavPath && !providedTranscript) {\n   817→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   818→  }\n   819→\n   820→  if (wavPath && !fs.existsSync(wavPath)) {\n   821→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   822→  }\n   823→\n   824→  console.log(`[SPLICE] Stutter detection: ${wavPath || 'provided transcript'}`);\n   825→\n   826→  try {\n   827→    // Get or create transcript with word-level timestamps\n   828→    let transcript = providedTranscript;\n   829→    if (!transcript && wavPath) {\n   830→      // Use transcribeWithWords for word-level timestamps required by stutter detection\n   831→      transcript = await transcribeWithWords(wavPath);\n   832→    }\n   833→\n   834→    // Validate transcript exists and has words array\n   835→    if (!transcript || !transcript.words) {\n   836→      return res.status(400).json({\n   837→        error: 'Transcript must contain word-level timing data'\n   838→      });\n   839→    }\n   840→\n   841→    // Empty transcript returns empty result (not an error)\n   842→    if (transcript.words.length === 0) {\n   843→      return res.json({\n   844→        success: true,\n   845→        stutters: [],\n   846→        metadata: { type: 'stutters', totalWords: 0, stutterCount: 0, totalRepeatedWords: 0 }\n   847→      });\n   848→    }\n   849→\n   850→    // Detect stutters only\n   851→    const result = detectStutters(transcript, {\n   852→      minRepeats,\n   853→      maxGapMs,\n   854→      ignoreFillers,\n   855→      minWordLength\n   856→    });\n   857→\n   858→    res.json({\n   859→      success: true,\n   860→      wavPath,\n   861→      ...result\n   862→    });\n   863→  } catch (err) {\n   864→    console.error('[SPLICE] Stutter detection error:', err);\n   865→    res.status(500).json({ error: err.message });\n   866→  }\n   867→});\n   868→\n   869→// =============================================================================\n   870→// Caption Export Routes\n   871→// =============================================================================\n   872→\n   873→/**\n   874→ * POST /export/captions - Export transcript to caption format (SRT, VTT, etc.)\n   875→ *\n   876→ * Converts a transcript to the specified caption format.\n   877→ * Can optionally save to file.\n   878→ *\n   879→ * Options:\n   880→ * - wavPath: Path to audio file (to transcribe first)\n   881→ * - transcript: Pre-existing transcript with word-level timing\n   882→ * - format: Export format (srt, vtt, txt, json) - default: srt\n   883→ * - outputPath: Optional file path to save to\n   884→ * - maxWordsPerCaption: Max words per caption (default: 8)\n   885→ * - maxDuration: Max duration per caption in seconds (default: 5)\n   886→ */\n   887→app.post('/export/captions', async (req, res) => {\n   888→  const {\n   889→    wavPath,\n   890→    transcript: providedTranscript,\n   891→    format = 'srt',\n   892→    outputPath = null,\n   893→    maxWordsPerCaption = 8,\n   894→    maxDuration = 5\n   895→  } = req.body;\n   896→\n   897→  if (!wavPath && !providedTranscript) {\n   898→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   899→  }\n   900→\n   901→  if (wavPath && !fs.existsSync(wavPath)) {\n   902→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   903→  }\n   904→\n   905→  console.log(`[SPLICE] Caption export: ${wavPath || 'provided transcript'} -> ${format}`);\n   906→\n   907→  try {\n   908→    // Get or create transcript with word-level timestamps\n   909→    let transcript = providedTranscript;\n   910→    if (!transcript && wavPath) {\n   911→      transcript = await transcribeWithWords(wavPath);\n   912→    }\n   913→\n   914→    const exportOptions = { maxWordsPerCaption, maxDuration };\n   915→\n   916→    // Generate caption content based on format\n   917→    let content;\n   918→    let mimeType;\n   919→\n   920→    switch (format.toLowerCase()) {\n   921→      case 'srt':\n   922→        content = toSRT(transcript, exportOptions);\n   923→        mimeType = 'application/x-subrip';\n   924→        break;\n   925→      case 'vtt':\n   926→      case 'webvtt':\n   927→        content = toVTT(transcript, exportOptions);\n   928→        mimeType = 'text/vtt';\n   929→        break;\n   930→      case 'txt':\n   931→      case 'text':\n   932→        content = toPlainText(transcript, { ...exportOptions, includeTimestamps: true });\n   933→        mimeType = 'text/plain';\n   934→        break;\n   935→      case 'json':\n   936→        content = toJSON(transcript);\n   937→        mimeType = 'application/json';\n   938→        break;\n   939→      default:\n   940→        return res.status(400).json({\n   941→          error: `Unsupported format: ${format}`,\n   942→          supportedFormats: getSupportedFormats()\n   943→        });\n   944→    }\n   945→\n   946→    // Save to file if outputPath provided\n   947→    let savedPath = null;\n   948→    if (outputPath) {\n   949→      const result = await exportToFile(transcript, outputPath, format, exportOptions);\n   950→      savedPath = result.path;\n   951→      console.log(`[SPLICE] Saved captions to: ${savedPath}`);\n   952→    }\n   953→\n   954→    res.json({\n   955→      success: true,\n   956→      format,\n   957→      content,\n   958→      mimeType,\n   959→      savedPath,\n   960→      wordCount: transcript.words?.length || 0,\n   961→      duration: transcript.duration || 0\n   962→    });\n   963→  } catch (err) {\n   964→    console.error('[SPLICE] Caption export error:', err);\n   965→    res.status(500).json({ error: err.message });\n   966→  }\n   967→});\n   968→\n   969→/**\n   970→ * GET /export/formats - Get supported export formats\n   971→ */\n   972→app.get('/export/formats', (req, res) => {\n   973→  res.json({\n   974→    success: true,\n   975→    formats: getSupportedFormats()\n   976→  });\n   977→});\n   978→\n   979→// =============================================================================\n   980→// Multitrack/Multicam Analysis Routes\n   981→// =============================================================================\n   982→\n   983→/**\n   984→ * POST /multitrack - Analyze multiple audio tracks for multicam editing\n   985→ *\n   986→ * Analyzes audio levels across multiple tracks to determine optimal\n   987→ * video angle selection based on who is speaking.\n   988→ *\n   989→ * Options:\n   990→ * - audioPaths: Array of paths to audio files (one per speaker) - required\n   991→ * - speakerNames: Array of speaker names (optional)\n   992→ * - videoTrackMapping: Object mapping speaker index to video track { 0: 0, 1: 1 }\n   993→ * - minShotDuration: Minimum seconds before next cut (default: 2.0)\n   994→ * - switchingFrequency: How often to allow cuts 0-100 (default: 50)\n   995→ * - wideShotEnabled: Enable wide shot detection (default: true)\n   996→ * - wideShotPercentage: Target % of wide shots (default: 20)\n   997→ * - wideShotTracks: Video track indices for wide shots\n   998→ * - cutawayEnabled: Enable cutaway insertion (default: false)\n   999→ * - cutawayTracks: Video track indices for cutaways\n  1000→ * - speakerBoosts: Per-speaker dB adjustments { \"Speaker 1\": 5 }\n  1001→ */\n  1002→app.post('/multitrack', requireCredits({ endpoint: 'multitrack' }), async (req, res) => {\n  1003→  const {\n  1004→    audioPaths,\n  1005→    speakerNames,\n  1006→    videoTrackMapping = {},\n  1007→    minShotDuration = 2.0,\n  1008→    switchingFrequency = 50,\n  1009→    wideShotEnabled = true,\n  1010→    wideShotPercentage = 20,\n  1011→    wideShotTracks = [],\n  1012→    cutawayEnabled = false,\n  1013→    cutawayTracks = [],\n  1014→    speakerBoosts = {},\n  1015→    frameRate = 30\n  1016→  } = req.body;\n  1017→\n  1018→  // Validate audioPaths\n  1019→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length === 0) {\n  1020→    return res.status(400).json({ error: 'audioPaths array is required (at least 1 path)' });\n  1021→  }\n  1022→\n  1023→  // Validate all files exist\n  1024→  for (const audioPath of audioPaths) {\n  1025→    if (!fs.existsSync(audioPath)) {\n  1026→      return res.status(404).json({ error: `File not found: ${audioPath}` });\n  1027→    }\n  1028→  }\n  1029→\n  1030→  // Check FFprobe availability\n  1031→  const ffprobeAvailable = await isFFprobeInstalled();\n  1032→  if (!ffprobeAvailable) {\n  1033→    return res.status(500).json({\n  1034→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n  1035→    });\n  1036→  }\n  1037→\n  1038→  console.log(`[SPLICE] Multitrack analysis: ${audioPaths.length} track(s)`);\n  1039→\n  1040→  try {\n  1041→    const result = await analyzeMultitrack(audioPaths, {\n  1042→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n  1043→      videoTrackMapping,\n  1044→      minShotDuration,\n  1045→      switchingFrequency,\n  1046→      wideShotEnabled,\n  1047→      wideShotPercentage,\n  1048→      wideShotTracks,\n  1049→      cutawayEnabled,\n  1050→      cutawayTracks,\n  1051→      speakerBoosts,\n  1052→      frameRate\n  1053→    });\n  1054→\n  1055→    res.json({\n  1056→      success: true,\n  1057→      ...result\n  1058→    });\n  1059→  } catch (err) {\n  1060→    console.error('[SPLICE] Multitrack analysis error:', err);\n  1061→    res.status(500).json({ error: err.message });\n  1062→  }\n  1063→});\n  1064→\n  1065→/**\n  1066→ * POST /multitrack/auto-balance - Auto-balance speaker screentime\n  1067→ *\n  1068→ * Automatically adjusts speaker boosts to achieve equal screentime distribution.\n  1069→ * Runs multiple iterations to find optimal parameters.\n  1070→ */\n  1071→app.post('/multitrack/auto-balance', async (req, res) => {\n  1072→  const {\n  1073→    audioPaths,\n  1074→    speakerNames,\n  1075→    videoTrackMapping = {},\n  1076→    minShotDuration = 2.0,\n  1077→    switchingFrequency = 50,\n  1078→    wideShotEnabled = false, // Disable wide shots for balance calc\n  1079→    frameRate = 30\n  1080→  } = req.body;\n  1081→\n  1082→  // Validate audioPaths\n  1083→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length < 2) {\n  1084→    return res.status(400).json({ error: 'audioPaths array requires at least 2 tracks for balancing' });\n  1085→  }\n  1086→\n  1087→  // Validate all files exist\n  1088→  for (const audioPath of audioPaths) {\n  1089→    if (!fs.existsSync(audioPath)) {\n  1090→      return res.status(404).json({ error: `File not found: ${audioPath}` });\n  1091→    }\n  1092→  }\n  1093→\n  1094→  // Check FFprobe availability\n  1095→  const ffprobeAvailable = await isFFprobeInstalled();\n  1096→  if (!ffprobeAvailable) {\n  1097→    return res.status(500).json({\n  1098→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n  1099→    });\n  1100→  }\n  1101→\n  1102→  console.log(`[SPLICE] Auto-balancing multitrack: ${audioPaths.length} track(s)`);\n  1103→\n  1104→  try {\n  1105→    const result = await autoBalanceMultitrack(audioPaths, {\n  1106→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n  1107→      videoTrackMapping,\n  1108→      minShotDuration,\n  1109→      switchingFrequency,\n  1110→      wideShotEnabled,\n  1111→      frameRate\n  1112→    });\n  1113→\n  1114→    res.json({\n  1115→      success: true,\n  1116→      ...result\n  1117→    });\n  1118→  } catch (err) {\n  1119→    console.error('[SPLICE] Auto-balance error:', err);\n  1120→    res.status(500).json({ error: err.message });\n  1121→  }\n  1122→});\n  1123→\n  1124→/**\n  1125→ * POST /process-xml - Process FCP XML to split clips at silences\n  1126→ *\n  1127→ * Takes an FCP XML file and silence timestamps, splits clips\n  1128→ * at silence boundaries, and optionally removes gaps.\n  1129→ */\n  1130→app.post('/process-xml', async (req, res) => {\n  1131→  const {\n  1132→    xmlPath,\n  1133→    silences,\n  1134→    removeGaps = true,\n  1135→    outputPath = null\n  1136→  } = req.body;\n  1137→\n  1138→  if (!xmlPath) {\n  1139→    return res.status(400).json({ error: 'xmlPath is required' });\n  1140→  }\n  1141→\n  1142→  if (!silences || !Array.isArray(silences)) {\n  1143→    return res.status(400).json({ error: 'silences array is required' });\n  1144→  }\n  1145→\n  1146→  if (!fs.existsSync(xmlPath)) {\n  1147→    return res.status(404).json({ error: `XML file not found: ${xmlPath}` });\n  1148→  }\n  1149→\n  1150→  console.log(`[SPLICE] Processing XML: ${xmlPath} with ${silences.length} silence(s)`);\n  1151→\n  1152→  try {\n  1153→    const result = await processXMLFile(xmlPath, silences, {\n  1154→      outputPath,\n  1155→      removeGaps\n  1156→    });\n  1157→\n  1158→    res.json({\n  1159→      success: true,\n  1160→      inputPath: xmlPath,\n  1161→      outputPath: result.outputPath,\n  1162→      stats: result.stats\n  1163→    });\n  1164→  } catch (err) {\n  1165→    console.error('[SPLICE] XML processing error:', err);\n  1166→    res.status(500).json({ error: err.message });\n  1167→  }\n  1168→});\n  1169→\n  1170→/**\n  1171→ * POST /cut-list - Generate a JSON cut list for direct DOM building (v3.5)\n  1172→ *\n  1173→ * Takes silences and optionally takes, returns a cut list that the\n  1174→ * plugin can use to build sequences directly via UXP APIs.\n  1175→ *\n  1176→ * Body:\n  1177→ * - sourceName: Name of the source clip\n  1178→ * - sourcePath: Full path to the source file\n  1179→ * - duration: Total duration in seconds\n  1180→ * - silences: Array of silence segments [{start, end, duration}]\n  1181→ * - takes: (optional) Array of detected takes\n  1182→ * - settings: (optional) Generation settings\n  1183→ */\n  1184→app.post('/cut-list', async (req, res) => {\n  1185→  const {\n  1186→    sourceName,\n  1187→    sourcePath,\n  1188→    duration,\n  1189→    silences,\n  1190→    takes = [],\n  1191→    settings = {}\n  1192→  } = req.body;\n  1193→\n  1194→  // Validate required fields\n  1195→  if (!sourceName && !sourcePath) {\n  1196→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });\n  1197→  }\n  1198→\n  1199→  if (typeof duration !== 'number' || duration <= 0) {\n  1200→    return res.status(400).json({ error: 'duration must be a positive number' });\n  1201→  }\n  1202→\n  1203→  if (!silences || !Array.isArray(silences)) {\n  1204→    return res.status(400).json({ error: 'silences array is required' });\n  1205→  }\n  1206→\n  1207→  console.log(`[SPLICE] Generating cut list for ${sourceName || sourcePath} (${silences.length} silences)`);\n  1208→\n  1209→  try {\n  1210→    const cutList = generateCutList({\n  1211→      sourceName: sourceName || path.basename(sourcePath),\n  1212→      sourcePath,\n  1213→      duration,\n  1214→      silences,\n  1215→      takes,\n  1216→      settings\n  1217→    });\n  1218→\n  1219→    // Validate the generated cut list\n  1220→    const validation = validateCutList(cutList);\n  1221→    if (!validation.valid) {\n  1222→      return res.status(500).json({\n  1223→        error: 'Generated cut list is invalid',\n  1224→        validationErrors: validation.errors\n  1225→      });\n  1226→    }\n  1227→\n  1228→    res.json({\n  1229→      success: true,\n  1230→      cutList\n  1231→    });\n  1232→  } catch (err) {\n  1233→    console.error('[SPLICE] Cut list generation error:', err);\n  1234→    res.status(500).json({ error: err.message });\n  1235→  }\n  1236→});\n  1237→\n  1238→/**\n  1239→ * POST /cut-list/takes - Generate a cut list that keeps only takes\n  1240→ *\n  1241→ * Alternative endpoint for \"keep best takes only\" workflow.\n  1242→ */\n  1243→app.post('/cut-list/takes', async (req, res) => {\n  1244→  const {\n  1245→    sourceName,\n  1246→    sourcePath,\n  1247→    duration,\n  1248→    takes,\n  1249→    settings = {}\n  1250→  } = req.body;\n  1251→\n  1252→  // Validate required fields\n  1253→  if (!sourceName && !sourcePath) {\n  1254→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });\n  1255→  }\n  1256→\n  1257→  if (typeof duration !== 'number' || duration <= 0) {\n  1258→    return res.status(400).json({ error: 'duration must be a positive number' });\n  1259→  }\n  1260→\n  1261→  if (!takes || !Array.isArray(takes) || takes.length === 0) {\n  1262→    return res.status(400).json({ error: 'takes array is required and must not be empty' });\n  1263→  }\n  1264→\n  1265→  console.log(`[SPLICE] Generating takes cut list for ${sourceName || sourcePath} (${takes.length} takes)`);\n  1266→\n  1267→  try {\n  1268→    const cutList = generateTakesCutList({\n  1269→      sourceName: sourceName || path.basename(sourcePath),\n  1270→      sourcePath,\n  1271→      duration,\n  1272→      takes,\n  1273→      settings\n  1274→    });\n  1275→\n  1276→    res.json({\n  1277→      success: true,\n  1278→      cutList\n  1279→    });\n  1280→  } catch (err) {\n  1281→    console.error('[SPLICE] Takes cut list generation error:', err);\n  1282→    res.status(500).json({ error: err.message });\n  1283→  }\n  1284→});\n  1285→\n  1286→/**\n  1287→ * GET /ffprobe-check - Check if FFprobe is installed\n  1288→ */\n  1289→app.get('/ffprobe-check', async (req, res) => {\n  1290→  const installed = await isFFprobeInstalled();\n  1291→  res.json({\n  1292→    installed,\n  1293→    message: installed\n  1294→      ? 'FFprobe is available'\n  1295→      : 'FFprobe not found. Install with: brew install ffmpeg'\n  1296→  });\n  1297→});\n  1298→\n  1299→/**\n  1300→ * GET /replicate-check - Check if Replicate API is configured\n  1301→ */\n  1302→app.get('/replicate-check', async (req, res) => {\n  1303→  const configured = isReplicateConfigured();\n  1304→  res.json({\n  1305→    configured,\n  1306→    message: configured\n  1307→      ? 'Replicate API is configured'\n  1308→      : 'REPLICATE_API_TOKEN not set. Add to .env file.'\n  1309→  });\n  1310→});\n  1311→\n  1312→/**\n  1313→ * POST /isolate-vocals - Isolate vocals from audio using Demucs\n  1314→ *\n  1315→ * Uses Replicate's Demucs model to separate vocals from background audio.\n  1316→ * Cost: ~$0.015/min of audio\n  1317→ *\n  1318→ * Tier access:\n  1319→ * - Starter: No access (upgrade required)\n  1320→ * - Pro: 2 hours included, then $0.08/min overage\n  1321→ * - Team: 5 hours included, then $0.08/min overage\n  1322→ */\n  1323→app.post('/isolate-vocals', async (req, res) => {\n  1324→  const { audioPath, stem = 'vocals', outputDir = null } = req.body;\n  1325→  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n  1326→\n  1327→  if (!audioPath) {\n  1328→    return res.status(400).json({ error: 'audioPath is required' });\n  1329→  }\n  1330→\n  1331→  if (!fs.existsSync(audioPath)) {\n  1332→    return res.status(404).json({ error: `File not found: ${audioPath}` });\n  1333→  }\n  1334→\n  1335→  // Check Replicate configuration\n  1336→  if (!isReplicateConfigured()) {\n  1337→    return res.status(500).json({\n  1338→      error: 'Replicate API not configured. Set REPLICATE_API_TOKEN in .env'\n  1339→    });\n  1340→  }\n  1341→\n  1342→  // Get audio duration for billing\n  1343→  let audioDurationSeconds = 0;\n  1344→  try {\n  1345→    audioDurationSeconds = await getAudioDuration(audioPath);\n  1346→  } catch (err) {\n  1347→    console.warn('[SPLICE] Could not get audio duration:', err.message);\n  1348→  }\n  1349→\n  1350→  const audioDurationMinutes = audioDurationSeconds / 60;\n  1351→\n  1352→  // Check isolation access if customer ID provided\n  1353→  if (stripeCustomerId) {\n  1354→    const accessCheck = await usageTracking.checkIsolationAccess(stripeCustomerId, audioDurationMinutes);\n  1355→\n  1356→    if (!accessCheck.allowed) {\n  1357→      return res.status(403).json({\n  1358→        error: accessCheck.message,\n  1359→        reason: accessCheck.reason,\n  1360→        upgradeRequired: accessCheck.reason === 'upgrade_required'\n  1361→      });\n  1362→    }\n  1363→\n  1364→    console.log(`[SPLICE] Isolation access: ${accessCheck.message}`);\n  1365→  }\n  1366→\n  1367→  console.log(`[SPLICE] Isolating vocals: ${audioPath} (${audioDurationMinutes.toFixed(1)} min)`);\n  1368→\n  1369→  try {\n  1370→    const result = await isolateVocals(audioPath, {\n  1371→      stem,\n  1372→      outputDir: outputDir || undefined\n  1373→    });\n  1374→\n  1375→    // Deduct isolation usage if customer ID provided\n  1376→    let usageInfo = null;\n  1377→    if (stripeCustomerId) {\n  1378→      usageInfo = await usageTracking.deductIsolationUsage(\n  1379→        stripeCustomerId,\n  1380→        audioDurationSeconds,\n  1381→        'isolate-vocals'\n  1382→      );\n  1383→      console.log(`[SPLICE] Isolation usage deducted: ${audioDurationMinutes.toFixed(1)} min`);\n  1384→      if (usageInfo.isolationUsed?.overageCost > 0) {\n  1385→        console.log(`[SPLICE] Overage cost: $${usageInfo.isolationUsed.overageCost.toFixed(2)}`);\n  1386→      }\n  1387→    }\n  1388→\n  1389→    res.json({\n  1390→      success: true,\n  1391→      inputPath: audioPath,\n  1392→      outputPath: result.outputPath,\n  1393→      stem: result.stem,\n  1394→      processingTime: result.processingTime,\n  1395→      availableStems: result.allStems,\n  1396→      audioDurationMinutes,\n  1397→      usage: usageInfo ? {\n  1398→        isolationHoursRemaining: usageInfo.isolationHoursRemaining,\n  1399→        overageCost: usageInfo.isolationUsed?.overageCost || 0\n  1400→      } : null\n  1401→    });\n  1402→  } catch (err) {\n  1403→    console.error('[SPLICE] Vocal isolation error:', err);\n  1404→    res.status(500).json({ error: err.message });\n  1405→  }\n  1406→});\n  1407→\n  1408→// =============================================================================\n  1409→// Batch Processing Routes\n  1410→// =============================================================================\n  1411→\n  1412→// In-memory job queue for batch processing\n  1413→const batchJobs = new Map();\n  1414→\n  1415→// Batch job limits to prevent memory leak\n  1416→const MAX_BATCH_JOBS = 10000;\n  1417→const BATCH_JOB_MAX_AGE_MS = 24 * 60 * 60 * 1000; // 24 hours\n  1418→\n  1419→/**\n  1420→ * Generate a unique job ID\n  1421→ */\n  1422→function generateJobId() {\n  1423→  return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  1424→}\n  1425→\n  1426→/**\n  1427→ * Clean up old batch jobs to prevent memory leak\n  1428→ * Removes jobs older than 24 hours\n  1429→ */\n  1430→function cleanupOldBatchJobs() {\n  1431→  const now = Date.now();\n  1432→  let removedCount = 0;\n  1433→\n  1434→  for (const [jobId, job] of batchJobs.entries()) {\n  1435→    const createdAt = new Date(job.createdAt).getTime();\n  1436→    if (now - createdAt > BATCH_JOB_MAX_AGE_MS) {\n  1437→      batchJobs.delete(jobId);\n  1438→      removedCount++;\n  1439→    }\n  1440→  }\n  1441→\n  1442→  if (removedCount > 0) {\n  1443→    console.log(`[SPLICE] Cleaned up ${removedCount} old batch job(s)`);\n  1444→  }\n  1445→}\n  1446→\n  1447→/**\n  1448→ * Enforce max job limit by removing oldest completed jobs\n  1449→ */\n  1450→function enforceJobLimit() {\n  1451→  if (batchJobs.size < MAX_BATCH_JOBS) return;\n  1452→\n  1453→  // Get completed jobs sorted by creation date (oldest first)\n  1454→  const completedJobs = Array.from(batchJobs.entries())\n  1455→    .filter(([_, job]) => job.status !== 'processing')\n  1456→    .sort((a, b) => new Date(a[1].createdAt) - new Date(b[1].createdAt));\n  1457→\n  1458→  // Remove oldest completed jobs until under limit\n  1459→  const toRemove = batchJobs.size - MAX_BATCH_JOBS + 1;\n  1460→  for (let i = 0; i < Math.min(toRemove, completedJobs.length); i++) {\n  1461→    batchJobs.delete(completedJobs[i][0]);\n  1462→  }\n  1463→\n  1464→  console.log(`[SPLICE] Enforced job limit, removed ${Math.min(toRemove, completedJobs.length)} job(s)`);\n  1465→}\n  1466→\n  1467→// Run cleanup every hour\n  1468→setInterval(cleanupOldBatchJobs, 60 * 60 * 1000);\n  1469→\n  1470→/**\n  1471→ * POST /batch/silences - Process multiple files for silence detection\n  1472→ *\n  1473→ * Creates a batch job that processes multiple audio files.\n  1474→ * Returns a job ID for tracking progress.\n  1475→ *\n  1476→ * Body:\n  1477→ * - files: Array of file paths to process\n  1478→ * - options: Detection options (sensitivity, threshold, etc.)\n  1479→ */\n  1480→app.post('/batch/silences', async (req, res) => {\n  1481→  const { files, options = {} } = req.body;\n  1482→\n  1483→  if (!files || !Array.isArray(files) || files.length === 0) {\n  1484→    return res.status(400).json({ error: 'files array is required' });\n  1485→  }\n  1486→\n  1487→  // Validate all files exist\n  1488→  const missingFiles = files.filter(f => !fs.existsSync(f));\n  1489→  if (missingFiles.length > 0) {\n  1490→    return res.status(404).json({\n  1491→      error: 'Some files not found',\n  1492→      missingFiles\n  1493→    });\n  1494→  }\n  1495→\n  1496→  // Enforce job limit before creating new job\n  1497→  enforceJobLimit();\n  1498→\n  1499→  const jobId = generateJobId();\n  1500→\n  1501→  // Initialize job\n  1502→  const job = {\n  1503→    id: jobId,\n  1504→    type: 'silences',\n  1505→    status: 'processing',\n  1506→    createdAt: new Date().toISOString(),\n  1507→    files: files.map(f => ({\n  1508→      path: f,\n  1509→      status: 'pending',\n  1510→      result: null,\n  1511→      error: null\n  1512→    })),\n  1513→    options,\n  1514→    progress: {\n  1515→      total: files.length,\n  1516→      completed: 0,\n  1517→      failed: 0,\n  1518→      percentage: 0\n  1519→    },\n  1520→    results: [],\n  1521→    errors: []\n  1522→  };\n  1523→\n  1524→  batchJobs.set(jobId, job);\n  1525→  console.log(`[SPLICE] Batch job ${jobId} created with ${files.length} files`);\n  1526→\n  1527→  // Start processing in background\n  1528→  processBatchJob(jobId);\n  1529→\n  1530→  res.json({\n  1531→    success: true,\n  1532→    jobId,\n  1533→    message: `Batch job created with ${files.length} files`,\n  1534→    statusUrl: `/batch/status/${jobId}`\n  1535→  });\n  1536→});\n  1537→\n  1538→/**\n  1539→ * Process a batch job (runs in background)\n  1540→ */\n  1541→async function processBatchJob(jobId) {\n  1542→  const job = batchJobs.get(jobId);\n  1543→  if (!job) return;\n  1544→\n  1545→  const { sensitivity, ...manualOptions } = job.options;\n  1546→\n  1547→  // Build detection options\n  1548→  let detectionOptions = {};\n  1549→  if (typeof sensitivity === 'number') {\n  1550→    detectionOptions = sensitivityToParams(sensitivity);\n  1551→  } else {\n  1552→    detectionOptions = {\n  1553→      threshold: manualOptions.threshold ?? -30,\n  1554→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n  1555→      paddingStart: manualOptions.paddingStart ?? 0.1,\n  1556→      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n  1557→      autoThreshold: manualOptions.autoThreshold ?? false\n  1558→    };\n  1559→  }\n  1560→\n  1561→  // Process files sequentially to avoid overwhelming the system\n  1562→  for (let i = 0; i < job.files.length; i++) {\n  1563→    const fileEntry = job.files[i];\n  1564→    fileEntry.status = 'processing';\n  1565→\n  1566→    try {\n  1567→      const result = await detectSilencesRMS(fileEntry.path, detectionOptions);\n  1568→\n  1569→      fileEntry.status = 'completed';\n  1570→      fileEntry.result = {\n  1571→        silences: result.silences,\n  1572→        count: result.silences.length,\n  1573→        totalSilenceDuration: result.metadata.totalSilenceDuration,\n  1574→        audioDuration: result.metadata.audioDuration\n  1575→      };\n  1576→\n  1577→      job.results.push({\n  1578→        file: fileEntry.path,\n  1579→        ...fileEntry.result\n  1580→      });\n  1581→\n  1582→      job.progress.completed++;\n  1583→      console.log(`[SPLICE] Batch ${jobId}: ${i + 1}/${job.files.length} completed`);\n  1584→    } catch (err) {\n  1585→      fileEntry.status = 'failed';\n  1586→      fileEntry.error = err.message;\n  1587→\n  1588→      job.errors.push({\n  1589→        file: fileEntry.path,\n  1590→        error: err.message\n  1591→      });\n  1592→\n  1593→      job.progress.failed++;\n  1594→      console.error(`[SPLICE] Batch ${jobId}: ${fileEntry.path} failed:`, err.message);\n  1595→    }\n  1596→\n  1597→    // Update progress\n  1598→    job.progress.percentage = Math.round(\n  1599→      ((job.progress.completed + job.progress.failed) / job.progress.total) * 100\n  1600→    );\n  1601→  }\n  1602→\n  1603→  // Mark job as complete\n  1604→  job.status = job.progress.failed === job.progress.total ? 'failed' :\n  1605→               job.progress.failed > 0 ? 'completed_with_errors' : 'completed';\n  1606→  job.completedAt = new Date().toISOString();\n  1607→\n  1608→  console.log(`[SPLICE] Batch job ${jobId} ${job.status}`);\n  1609→}\n  1610→\n  1611→/**\n  1612→ * GET /batch/status/:jobId - Get batch job status and results\n  1613→ */\n  1614→app.get('/batch/status/:jobId', (req, res) => {\n  1615→  const { jobId } = req.params;\n  1616→  const job = batchJobs.get(jobId);\n  1617→\n  1618→  if (!job) {\n  1619→    return res.status(404).json({ error: 'Job not found' });\n  1620→  }\n  1621→\n  1622→  res.json({\n  1623→    success: true,\n  1624→    job: {\n  1625→      id: job.id,\n  1626→      type: job.type,\n  1627→      status: job.status,\n  1628→      createdAt: job.createdAt,\n  1629→      completedAt: job.completedAt,\n  1630→      progress: job.progress,\n  1631→      files: job.files.map(f => ({\n  1632→        path: f.path,\n  1633→        status: f.status,\n  1634→        silenceCount: f.result?.count,\n  1635→        error: f.error\n  1636→      }))\n  1637→    }\n  1638→  });\n  1639→});\n  1640→\n  1641→/**\n  1642→ * GET /batch/results/:jobId - Get full results for a completed batch job\n  1643→ */\n  1644→app.get('/batch/results/:jobId', (req, res) => {\n  1645→  const { jobId } = req.params;\n  1646→  const job = batchJobs.get(jobId);\n  1647→\n  1648→  if (!job) {\n  1649→    return res.status(404).json({ error: 'Job not found' });\n  1650→  }\n  1651→\n  1652→  if (job.status === 'processing') {\n  1653→    return res.status(202).json({\n  1654→      success: false,\n  1655→      message: 'Job still processing',\n  1656→      progress: job.progress\n  1657→    });\n  1658→  }\n  1659→\n  1660→  res.json({\n  1661→    success: true,\n  1662→    jobId: job.id,\n  1663→    status: job.status,\n  1664→    progress: job.progress,\n  1665→    results: job.results,\n  1666→    errors: job.errors,\n  1667→    summary: {\n  1668→      totalFiles: job.progress.total,\n  1669→      successful: job.progress.completed,\n  1670→      failed: job.progress.failed,\n  1671→      totalSilences: job.results.reduce((sum, r) => sum + (r.count || 0), 0),\n  1672→      totalSilenceDuration: job.results.reduce((sum, r) => sum + (r.totalSilenceDuration || 0), 0)\n  1673→    }\n  1674→  });\n  1675→});\n  1676→\n  1677→/**\n  1678→ * DELETE /batch/:jobId - Cancel or delete a batch job\n  1679→ */\n  1680→app.delete('/batch/:jobId', (req, res) => {\n  1681→  const { jobId } = req.params;\n  1682→  const job = batchJobs.get(jobId);\n  1683→\n  1684→  if (!job) {\n  1685→    return res.status(404).json({ error: 'Job not found' });\n  1686→  }\n  1687→\n  1688→  // Note: This doesn't actually cancel in-progress processing\n  1689→  // but prevents the job from being queried\n  1690→  batchJobs.delete(jobId);\n  1691→\n  1692→  res.json({\n  1693→    success: true,\n  1694→    message: `Job ${jobId} deleted`\n  1695→  });\n  1696→});\n  1697→\n  1698→/**\n  1699→ * GET /batch/jobs - List all batch jobs\n  1700→ */\n  1701→app.get('/batch/jobs', (req, res) => {\n  1702→  const jobs = Array.from(batchJobs.values()).map(job => ({\n  1703→    id: job.id,\n  1704→    type: job.type,\n  1705→    status: job.status,\n  1706→    createdAt: job.createdAt,\n  1707→    completedAt: job.completedAt,\n  1708→    progress: job.progress\n  1709→  }));\n  1710→\n  1711→  // Sort by creation date (newest first)\n  1712→  jobs.sort((a, b) => new Date(b.createdAt) - new Date(a.createdAt));\n  1713→\n  1714→  res.json({\n  1715→    success: true,\n  1716→    count: jobs.length,\n  1717→    jobs\n  1718→  });\n  1719→});\n  1720→\n  1721→// =============================================================================\n  1722→// Billing & Credits Routes\n  1723→// =============================================================================\n  1724→\n  1725→/**\n  1726→ * GET /credits - Get user's credit balance\n  1727→ *\n  1728→ * Requires x-stripe-customer-id header\n  1729→ */\n  1730→app.get('/credits', async (req, res) => {\n  1731→  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n  1732→\n  1733→  if (!stripeCustomerId) {\n  1734→    return res.status(401).json({\n  1735→      error: 'Authentication required',\n  1736→      message: 'Missing x-stripe-customer-id header'\n  1737→    });\n  1738→  }\n  1739→\n  1740→  try {\n  1741→    const balance = await usageTracking.getBalance(stripeCustomerId);\n  1742→    res.json({\n  1743→      success: true,\n  1744→      ...balance\n  1745→    });\n  1746→  } catch (err) {\n  1747→    console.error('[SPLICE] Credits error:', err);\n  1748→    res.status(500).json({ error: err.message });\n  1749→  }\n  1750→});\n  1751→\n  1752→/**\n  1753→ * GET /usage-history - Get user's usage history\n  1754→ */\n  1755→app.get('/usage-history', async (req, res) => {\n  1756→  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n  1757→\n  1758→  if (!stripeCustomerId) {\n  1759→    return res.status(401).json({\n  1760→      error: 'Authentication required',\n  1761→      message: 'Missing x-stripe-customer-id header'\n  1762→    });\n  1763→  }\n  1764→\n  1765→  try {\n  1766→    const history = await usageTracking.getUsageHistory(stripeCustomerId);\n  1767→    res.json({\n  1768→      success: true,\n  1769→      history\n  1770→    });\n  1771→  } catch (err) {\n  1772→    console.error('[SPLICE] Usage history error:', err);\n  1773→    res.status(500).json({ error: err.message });\n  1774→  }\n  1775→});\n  1776→\n  1777→// =============================================================================\n  1778→// Start Server\n  1779→// =============================================================================\n  1780→\n  1781→// Initialize database and start server\n  1782→async function startServer() {\n  1783→  try {\n  1784→    await usageTracking.initDatabase();\n  1785→    console.log('[SPLICE] Database initialized');\n  1786→\n  1787→    if (isProduction || !httpsOptions) {\n  1788→      // Production: Railway provides TLS termination, use HTTP\n  1789→      http.createServer(app).listen(PORT, () => {\n  1790→        console.log(`[SPLICE] Backend running at http://0.0.0.0:${PORT} (production)`);\n  1791→      });\n  1792→    } else {\n  1793→      // Development: Use HTTPS with local certificates\n  1794→      https.createServer(httpsOptions, app).listen(PORT, () => {\n  1795→        console.log(`[SPLICE] Backend running at https://127.0.0.1:${PORT} (development)`);\n  1796→        console.log(`[SPLICE] POST /analyze with { \"wavPath\": \"/path/to/audio.wav\" }`);\n  1797→      });\n  1798→    }\n  1799→  } catch (err) {\n  1800→    console.error('[SPLICE] Failed to start server:', err);\n  1801→    process.exit(1);\n  1802→  }\n  1803→}\n  1804→\n  1805→startServer();\n  1806→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"b26e540f-4464-4756-a44a-c92816a775dc","timestamp":"2025-12-25T20:25:13.735Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/server.js","content":"/**\n * SPLICE Backend Server\n *\n * Main entry point for the SPLICE backend API.\n * Orchestrates the audio analysis pipeline.\n *\n * Slices:\n * - Slice 4: Transcription (services/transcription.js)\n * - Slice 5: Take Detection (services/takeDetection.js)\n */\n\nrequire('dotenv').config();\n\nconst express = require('express');\nconst cors = require('cors');\nconst fs = require('fs');\nconst https = require('https');\nconst http = require('http');\nconst path = require('path');\n\n// Check if running in production (Railway injects RAILWAY_ENVIRONMENT)\nconst isProduction = process.env.NODE_ENV === 'production' || process.env.RAILWAY_ENVIRONMENT;\n\n// Import slice services\nconst { transcribeAudio, transcribeWithWords } = require('./services/transcription');\nconst { detectTakes } = require('./services/takeDetection');\nconst { detectSilences } = require('./services/silenceDetection');\nconst { detectAudioSilences, isFFprobeInstalled, getAudioDuration } = require('./services/ffprobeSilence');\nconst { detectSilencesRMS, sensitivityToParams } = require('./services/rmsSilenceDetection');\nconst {\n  detectProfanity,\n  getProfanityList,\n  getSupportedLanguages,\n  getAvailableBleepSounds,\n  parseWordList\n} = require('./services/profanityDetection');\nconst {\n  detectRepetitionsBasic,\n  detectRepetitionsAdvanced,\n  detectStutters,\n  detectAllRepetitions\n} = require('./services/repetitionDetection');\nconst {\n  analyzeMultitrack,\n  autoBalanceMultitrack\n} = require('./services/multitrackAnalysis');\nconst {\n  toSRT,\n  toVTT,\n  toPlainText,\n  toJSON,\n  exportToFile,\n  getSupportedFormats\n} = require('./services/captionExporter');\nconst { processXMLFile } = require('./services/xmlProcessor');\nconst { isolateVocals, isReplicateConfigured } = require('./services/vocalIsolation');\nconst { generateCutList, generateTakesCutList, validateCutList } = require('./services/cutListGenerator');\n\n// Usage tracking and billing\nconst usageTracking = require('./services/usageTracking');\n// Rate limiter for usage-based endpoints\nconst { requireCredits } = require('./middleware/rateLimiter');\n\n// Stripe for webhooks\nconst Stripe = require('stripe');\nconst stripe = new Stripe(process.env.STRIPE_SECRET_KEY);\n\n// =============================================================================\n// Server Configuration\n// =============================================================================\n\nconst app = express();\nconst PORT = process.env.PORT || 3847;\n\n// HTTPS certificates (generated by mkcert) - only for local development\nlet httpsOptions = null;\nif (!isProduction) {\n  const keyPath = path.join(__dirname, 'localhost+1-key.pem');\n  const certPath = path.join(__dirname, 'localhost+1.pem');\n  if (fs.existsSync(keyPath) && fs.existsSync(certPath)) {\n    httpsOptions = {\n      key: fs.readFileSync(keyPath),\n      cert: fs.readFileSync(certPath)\n    };\n  }\n}\n\napp.use(cors());\n\n// Helper to determine tier from price ID with logging\nfunction getTierFromPriceId(priceId) {\n  if (priceId === process.env.STRIPE_PRICE_STARTER) return 'starter';\n  if (priceId === process.env.STRIPE_PRICE_PRO) return 'pro';\n  if (priceId === process.env.STRIPE_PRICE_TEAM) return 'team';\n\n  // Log unknown price ID for debugging\n  console.warn(`[SPLICE] Unknown price ID: ${priceId} - defaulting to starter tier`);\n  return 'starter';\n}\n\n// Stripe webhook needs raw body - must be before express.json()\napp.post('/webhooks/stripe', express.raw({ type: 'application/json' }), async (req, res) => {\n  const sig = req.headers['stripe-signature'];\n  const webhookSecret = process.env.STRIPE_WEBHOOK_SECRET;\n\n  let event;\n\n  try {\n    if (webhookSecret) {\n      event = stripe.webhooks.constructEvent(req.body, sig, webhookSecret);\n    } else if (isProduction) {\n      // SECURITY: Reject unsigned webhooks in production\n      console.error('[SPLICE] CRITICAL: STRIPE_WEBHOOK_SECRET not set in production');\n      return res.status(500).json({ error: 'Webhook configuration error: secret not configured' });\n    } else {\n      // For local development testing only\n      event = JSON.parse(req.body);\n      console.warn('[SPLICE] Warning: Processing webhook without signature verification (dev only)');\n    }\n  } catch (err) {\n    console.error('[SPLICE] Webhook signature verification failed:', err.message);\n    return res.status(400).json({ error: 'Webhook signature verification failed' });\n  }\n\n  console.log(`[SPLICE] Webhook received: ${event.type} (${event.id})`);\n\n  // Idempotency check - skip if already processed\n  if (await usageTracking.isEventProcessed(event.id)) {\n    console.log(`[SPLICE] Event ${event.id} already processed, skipping`);\n    return res.json({ received: true, skipped: true });\n  }\n\n  try {\n    switch (event.type) {\n      case 'customer.subscription.created':\n      case 'customer.subscription.updated': {\n        const subscription = event.data.object;\n        const customerId = subscription.customer;\n\n        // Validate customerId\n        if (!customerId) {\n          console.error('[SPLICE] Missing customer ID in subscription event');\n          return res.status(400).json({ error: 'Missing customer ID' });\n        }\n\n        // Get tier from price ID\n        const priceId = subscription.items?.data?.[0]?.price?.id;\n        const tier = getTierFromPriceId(priceId);\n\n        // Update user tier and reset hours\n        await usageTracking.updateTier(customerId, tier);\n        console.log(`[SPLICE] Updated customer ${customerId} to tier: ${tier}`);\n        break;\n      }\n\n      case 'customer.subscription.deleted': {\n        const subscription = event.data.object;\n        const customerId = subscription.customer;\n\n        // Validate customerId\n        if (!customerId) {\n          console.error('[SPLICE] Missing customer ID in subscription.deleted event');\n          return res.status(400).json({ error: 'Missing customer ID' });\n        }\n\n        // Downgrade to cancelled (0 hours)\n        await usageTracking.updateTier(customerId, 'cancelled');\n        console.log(`[SPLICE] Subscription cancelled for customer ${customerId}`);\n        break;\n      }\n\n      case 'invoice.payment_succeeded': {\n        const invoice = event.data.object;\n        const customerId = invoice.customer;\n        const subscriptionId = invoice.subscription;\n\n        // Validate customerId\n        if (!customerId) {\n          console.error('[SPLICE] Missing customer ID in invoice event');\n          return res.status(400).json({ error: 'Missing customer ID' });\n        }\n\n        // Reset hours on successful payment (new billing period)\n        if (subscriptionId) {\n          const subscription = await stripe.subscriptions.retrieve(subscriptionId);\n          const priceId = subscription.items?.data?.[0]?.price?.id;\n          const tier = getTierFromPriceId(priceId);\n\n          await usageTracking.resetHours(customerId, tier);\n          console.log(`[SPLICE] Reset hours for customer ${customerId} (tier: ${tier})`);\n        }\n        break;\n      }\n\n      default:\n        console.log(`[SPLICE] Unhandled event type: ${event.type}`);\n    }\n\n    // Record event as processed (idempotency)\n    await usageTracking.recordWebhookEvent(event.id, event.type);\n\n    res.json({ received: true });\n  } catch (err) {\n    console.error('[SPLICE] Webhook handler error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\napp.use(express.json());\n\n// =============================================================================\n// Routes\n// =============================================================================\n\n/**\n * GET / - API information\n */\napp.get('/', (req, res) => {\n  res.json({\n    service: 'splice-backend',\n    version: '0.3.0',\n    endpoints: {\n      'GET /': 'This info',\n      'GET /health': 'Health check',\n      'GET /ffprobe-check': 'Check if FFprobe is installed',\n      'GET /replicate-check': 'Check if Replicate API is configured',\n      'POST /analyze': 'Analyze WAV file { wavPath }',\n      'POST /silences': 'Detect silences via Whisper gaps { wavPath, threshold: 0.5 }',\n      'POST /silences-audio': 'Detect silences via FFprobe { wavPath, threshold: -30, minDuration: 0.5, padding: 0.1 }',\n      'POST /silences-rms': 'Detect silences via RMS analysis { wavPath, threshold: -30, minSilenceLength: 0.5, paddingStart: 0.1, paddingEnd: 0.05, autoThreshold: false, sensitivity: 50 }',\n      'POST /profanity': 'Detect profanity in transcript { wavPath, language: \"en\", customBlocklist: [], customAllowlist: [] }',\n      'GET /profanity/languages': 'Get supported languages for profanity detection',\n      'GET /profanity/bleeps': 'Get available bleep sounds',\n      'POST /repetitions': 'Detect phrase repetitions and stutters { wavPath, phraseSize: 5, tolerance: 0.7, useOpenAI: false }',\n      'POST /fillers': 'Detect filler words (um, uh, like, etc.) { wavPath, customFillers: [] }',\n      'POST /stutters': 'Detect single-word stutters only { wavPath, minRepeats: 2 }',\n      'POST /export/captions': 'Export transcript to caption format { wavPath, format: srt|vtt|txt|json, outputPath? }',\n      'GET /export/formats': 'Get supported caption export formats',\n      'POST /multitrack': 'Analyze multiple audio tracks for multicam { audioPaths: [], speakerNames: [], videoTrackMapping: {} }',\n      'POST /multitrack/auto-balance': 'Auto-balance speaker screentime { audioPaths: [], speakerNames: [] }',\n      'POST /process-xml': 'Process FCP XML { xmlPath, silences, removeGaps: true }',\n      'POST /cut-list': 'Generate JSON cut list for DOM building (v3.5) { sourceName, sourcePath, duration, silences, takes?, settings? }',\n      'POST /cut-list/takes': 'Generate cut list keeping only takes { sourceName, sourcePath, duration, takes, settings? }',\n      'POST /isolate-vocals': 'Isolate vocals from audio { audioPath }',\n      'POST /batch/silences': 'Batch process multiple files for silence detection { files: [], options: {} }',\n      'GET /batch/status/:jobId': 'Get batch job status',\n      'GET /batch/results/:jobId': 'Get full batch job results',\n      'GET /batch/jobs': 'List all batch jobs',\n      'DELETE /batch/:jobId': 'Delete a batch job',\n      'GET /credits': 'Get user credit balance (requires x-stripe-customer-id header)',\n      'GET /usage-history': 'Get usage history (requires x-stripe-customer-id header)',\n      'POST /webhooks/stripe': 'Stripe webhook endpoint'\n    }\n  });\n});\n\n/**\n * GET /health - Health check\n */\napp.get('/health', (req, res) => {\n  res.json({ status: 'ok', service: 'splice-backend' });\n});\n\n/**\n * POST /analyze - Main analysis endpoint\n *\n * Pipeline:\n * 1. Validate input (wavPath)\n * 2. Slice 4: Transcribe audio with Groq Whisper\n * 3. Slice 5: Detect takes with GPT-4o-mini\n * 4. Return combined results\n */\napp.post('/analyze', requireCredits({ endpoint: 'analyze' }), async (req, res) => {\n  const { wavPath } = req.body;\n\n  // Validate input\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Analyzing: ${wavPath}`);\n\n  try {\n    // Slice 4 - GPT-4o-mini transcription\n    const transcript = await transcribeAudio(wavPath);\n\n    // Slice 5 - GPT-4o-mini take detection\n    const takes = await detectTakes(transcript);\n\n    res.json({\n      success: true,\n      wavPath,\n      transcript,\n      takes\n    });\n  } catch (err) {\n    console.error('[SPLICE] Error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /silences - Detect silent gaps in audio\n *\n * Pipeline:\n * 1. Transcribe audio with Whisper (reuses transcription)\n * 2. Analyze gaps between segments\n * 3. Return silence regions\n */\napp.post('/silences', requireCredits({ endpoint: 'silences' }), async (req, res) => {\n  const { wavPath, threshold = 0.5 } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Detecting silences: ${wavPath} (threshold: ${threshold}s)`);\n\n  try {\n    const transcript = await transcribeAudio(wavPath);\n    const silences = detectSilences(transcript.segments, threshold);\n\n    res.json({\n      success: true,\n      wavPath,\n      threshold,\n      silences,\n      count: silences.length,\n      totalSilenceDuration: silences.reduce((sum, s) => sum + s.duration, 0).toFixed(2)\n    });\n  } catch (err) {\n    console.error('[SPLICE] Silence detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /silences-audio - Detect silences using FFprobe audio analysis\n *\n * Uses actual audio levels (dB threshold) instead of transcript gaps.\n * More accurate for detecting silence vs background noise.\n */\napp.post('/silences-audio', async (req, res) => {\n  const {\n    wavPath,\n    threshold = -30,\n    minDuration = 0.5,\n    padding = 0.1\n  } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  // Check FFprobe availability\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  console.log(`[SPLICE] FFprobe silence detection: ${wavPath} (threshold: ${threshold}dB, min: ${minDuration}s)`);\n\n  try {\n    const silences = await detectAudioSilences(wavPath, {\n      threshold,\n      minDuration,\n      padding\n    });\n\n    const totalDuration = silences.reduce((sum, s) => sum + s.duration, 0);\n\n    res.json({\n      success: true,\n      wavPath,\n      threshold,\n      minDuration,\n      padding,\n      silences,\n      count: silences.length,\n      totalSilenceDuration: totalDuration.toFixed(2)\n    });\n  } catch (err) {\n    console.error('[SPLICE] FFprobe silence detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /silences-rms - Detect silences using RMS audio analysis\n *\n * Advanced silence detection with:\n * - RMS (Root Mean Square) audio level analysis\n * - Auto-threshold detection from audio histogram\n * - Configurable padding (before/after cuts)\n * - Sensitivity slider mapping (0-100)\n *\n * Options:\n * - threshold: dBFS threshold (-60 to -20, default: -30)\n * - minSilenceLength: Minimum silence duration in seconds (default: 0.5)\n * - seekStep: Analysis window step in seconds (default: 0.05)\n * - paddingStart: Buffer before silence in seconds (default: 0.1)\n * - paddingEnd: Buffer after silence in seconds (default: 0.05)\n * - autoThreshold: Auto-detect optimal threshold (default: false)\n * - sensitivity: UI sensitivity 0-100 (overrides other params if provided)\n */\napp.post('/silences-rms', requireCredits({ endpoint: 'silences-rms' }), async (req, res) => {\n  const { wavPath, sensitivity, ...manualOptions } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  // Check FFprobe availability (needed for audio extraction)\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  // Build options - use sensitivity if provided, otherwise use manual options\n  let options = {};\n  if (typeof sensitivity === 'number') {\n    options = sensitivityToParams(sensitivity);\n    console.log(`[SPLICE] RMS detection with sensitivity ${sensitivity}`);\n  } else {\n    options = {\n      threshold: manualOptions.threshold ?? -30,\n      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n      seekStep: manualOptions.seekStep ?? 0.05,\n      paddingStart: manualOptions.paddingStart ?? 0.1,\n      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n      autoThreshold: manualOptions.autoThreshold ?? false,\n      mergeDistance: manualOptions.mergeDistance ?? 0.2\n    };\n  }\n\n  console.log(`[SPLICE] RMS silence detection: ${wavPath}`);\n\n  try {\n    const result = await detectSilencesRMS(wavPath, options);\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result,\n      count: result.silences.length,\n      totalSilenceDuration: result.metadata.totalSilenceDuration.toFixed(2)\n    });\n  } catch (err) {\n    console.error('[SPLICE] RMS silence detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Profanity Detection Routes\n// =============================================================================\n\n/**\n * POST /profanity - Detect profanity in audio/transcript\n *\n * Transcribes audio (if needed) and detects profanity words.\n * Returns word-level and segment-level results for muting/bleeping.\n *\n * Options:\n * - wavPath: Path to audio file (required)\n * - transcript: Pre-existing transcript (optional, skips transcription)\n * - language: Language code (en, es, fr, de) - default: en\n * - customBlocklist: Array or comma-separated string of additional words to censor\n * - customAllowlist: Array or comma-separated string of words to allow\n * - frameRate: Frame rate for boundary alignment (default: 30)\n */\napp.post('/profanity', requireCredits({ endpoint: 'profanity' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    language = 'en',\n    customBlocklist = [],\n    customAllowlist = [],\n    frameRate = 30\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Profanity detection: ${wavPath || 'provided transcript'} (language: ${language})`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      // Use transcribeWithWords for word-level timestamps required by profanity detection\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript has words\n    if (!transcript || !transcript.words || transcript.words.length === 0) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data',\n        hint: 'Ensure transcription returns words array with start/end times'\n      });\n    }\n\n    // Parse custom lists\n    const blocklist = parseWordList(customBlocklist);\n    const allowlist = parseWordList(customAllowlist);\n\n    // Detect profanity\n    const result = detectProfanity(transcript, {\n      language,\n      customBlocklist: blocklist,\n      customAllowlist: allowlist,\n      frameRate\n    });\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result\n    });\n  } catch (err) {\n    console.error('[SPLICE] Profanity detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /profanity/languages - Get supported languages\n */\napp.get('/profanity/languages', (req, res) => {\n  res.json({\n    success: true,\n    languages: getSupportedLanguages()\n  });\n});\n\n/**\n * GET /profanity/bleeps - Get available bleep sounds\n */\napp.get('/profanity/bleeps', (req, res) => {\n  res.json({\n    success: true,\n    sounds: getAvailableBleepSounds()\n  });\n});\n\n/**\n * GET /profanity/list/:language - Get default profanity list for a language\n */\napp.get('/profanity/list/:language', (req, res) => {\n  const { language } = req.params;\n  const list = getProfanityList(language);\n\n  res.json({\n    success: true,\n    language,\n    wordCount: list.length,\n    // Return first 50 words as sample (full list is large)\n    sample: list.slice(0, 50),\n    note: 'Full list available but truncated for response size'\n  });\n});\n\n// =============================================================================\n// Repetition/Stutter Detection Routes\n// =============================================================================\n\n/**\n * POST /repetitions - Detect phrase repetitions and stutters\n *\n * Analyzes transcript for repeated phrases and stutters.\n * Returns segments that can be removed to clean up the edit.\n *\n * Options:\n * - wavPath: Path to audio file (required unless transcript provided)\n * - transcript: Pre-existing transcript (optional)\n * - phraseSize: Words per comparison window (default: 5)\n * - tolerance: Similarity threshold 0-1 (default: 0.7)\n * - searchRadius: Words to search ahead (default: 100)\n * - useOpenAI: Use OpenAI for boundary refinement (default: false)\n * - includeStutters: Also detect single-word stutters (default: true)\n */\napp.post('/repetitions', async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    phraseSize = 5,\n    tolerance = 0.7,\n    searchRadius = 100,\n    useOpenAI = false,\n    includeStutters = true\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Repetition detection: ${wavPath || 'provided transcript'}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      // Use transcribeWithWords for word-level timestamps required by repetition detection\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript has words\n    if (!transcript || !transcript.words || transcript.words.length === 0) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data'\n      });\n    }\n\n    // Detect all repetitions (phrases + stutters)\n    const result = await detectAllRepetitions(transcript, {\n      phraseSize,\n      tolerance,\n      searchRadius,\n      useOpenAI\n    });\n\n    // Optionally filter out stutters\n    if (!includeStutters) {\n      result.stutters = [];\n      result.removalSegments = result.removalSegments.filter(s => s.type !== 'stutter');\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result\n    });\n  } catch (err) {\n    console.error('[SPLICE] Repetition detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /fillers - Detect filler words (um, uh, like, etc.)\n *\n * Transcribes audio and identifies filler words with timestamps.\n * Returns segments that can be cut or reviewed for removal.\n *\n * Options:\n * - wavPath: Path to audio file (required unless transcript provided)\n * - transcript: Pre-existing transcript with word-level timing (optional)\n * - customFillers: Additional filler words to detect (optional)\n */\napp.post('/fillers', async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    customFillers = []\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Filler word detection: ${wavPath || 'provided transcript'}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript has words\n    if (!transcript || !transcript.words || transcript.words.length === 0) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data'\n      });\n    }\n\n    // Default filler words (common in English speech)\n    const defaultFillers = [\n      'um', 'uh', 'ah', 'er', 'eh',           // Hesitation sounds\n      'like', 'so', 'well', 'right',           // Discourse markers\n      'you know', 'i mean', 'basically',       // Filler phrases\n      'actually', 'literally', 'honestly',     // Overused qualifiers\n      'kind of', 'sort of', 'you see'          // Hedging phrases\n    ];\n\n    // Combine default + custom fillers (lowercase for matching)\n    const fillerSet = new Set([\n      ...defaultFillers,\n      ...customFillers.map(f => f.toLowerCase().trim())\n    ]);\n\n    // Detect filler words\n    const fillers = [];\n    const words = transcript.words;\n\n    for (let i = 0; i < words.length; i++) {\n      const word = words[i];\n      const normalizedWord = word.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim();\n\n      // Check single-word fillers\n      if (fillerSet.has(normalizedWord)) {\n        fillers.push({\n          word: word.word,\n          normalizedWord,\n          start: word.start,\n          end: word.end,\n          duration: word.end - word.start,\n          index: i,\n          type: 'filler'\n        });\n        continue;\n      }\n\n      // Check two-word phrases (e.g., \"you know\", \"kind of\")\n      if (i < words.length - 1) {\n        const nextWord = words[i + 1];\n        const twoWordPhrase = `${normalizedWord} ${nextWord.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim()}`;\n        if (fillerSet.has(twoWordPhrase)) {\n          fillers.push({\n            word: `${word.word} ${nextWord.word}`,\n            normalizedWord: twoWordPhrase,\n            start: word.start,\n            end: nextWord.end,\n            duration: nextWord.end - word.start,\n            index: i,\n            type: 'filler_phrase'\n          });\n          // Skip next word since it's part of this phrase\n          i++;\n        }\n      }\n    }\n\n    // Calculate total filler time\n    const totalFillerDuration = fillers.reduce((sum, f) => sum + f.duration, 0);\n    const audioDuration = transcript.duration || (words.length > 0 ? words[words.length - 1].end : 0);\n    const fillerPercentage = audioDuration > 0 ? (totalFillerDuration / audioDuration) * 100 : 0;\n\n    res.json({\n      success: true,\n      wavPath,\n      fillers,\n      metadata: {\n        totalWords: words.length,\n        fillerCount: fillers.length,\n        totalFillerDuration: parseFloat(totalFillerDuration.toFixed(3)),\n        audioDuration: parseFloat(audioDuration.toFixed(3)),\n        fillerPercentage: parseFloat(fillerPercentage.toFixed(2)),\n        fillersPerMinute: audioDuration > 0 ? parseFloat((fillers.length / (audioDuration / 60)).toFixed(2)) : 0\n      },\n      removalSegments: fillers.map(f => ({\n        start: f.start,\n        end: f.end,\n        duration: f.duration,\n        reason: `Filler: \"${f.word}\"`,\n        type: f.type\n      }))\n    });\n  } catch (err) {\n    console.error('[SPLICE] Filler detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /stutters - Detect single-word stutters only\n *\n * Focused detection for word-level stutters (e.g., \"I I I think\").\n * Faster than full repetition detection.\n */\napp.post('/stutters', async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    options = {},\n    // Support both top-level and nested options for flexibility\n    minRepeats = options.minRepeats ?? 2,\n    maxGapMs = options.maxGapMs ?? 500,\n    ignoreFillers = options.ignoreFillers ?? true,\n    minWordLength = options.minWordLength ?? 1\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Stutter detection: ${wavPath || 'provided transcript'}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      // Use transcribeWithWords for word-level timestamps required by stutter detection\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript exists and has words array\n    if (!transcript || !transcript.words) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data'\n      });\n    }\n\n    // Empty transcript returns empty result (not an error)\n    if (transcript.words.length === 0) {\n      return res.json({\n        success: true,\n        stutters: [],\n        metadata: { type: 'stutters', totalWords: 0, stutterCount: 0, totalRepeatedWords: 0 }\n      });\n    }\n\n    // Detect stutters only\n    const result = detectStutters(transcript, {\n      minRepeats,\n      maxGapMs,\n      ignoreFillers,\n      minWordLength\n    });\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result\n    });\n  } catch (err) {\n    console.error('[SPLICE] Stutter detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Caption Export Routes\n// =============================================================================\n\n/**\n * POST /export/captions - Export transcript to caption format (SRT, VTT, etc.)\n *\n * Converts a transcript to the specified caption format.\n * Can optionally save to file.\n *\n * Options:\n * - wavPath: Path to audio file (to transcribe first)\n * - transcript: Pre-existing transcript with word-level timing\n * - format: Export format (srt, vtt, txt, json) - default: srt\n * - outputPath: Optional file path to save to\n * - maxWordsPerCaption: Max words per caption (default: 8)\n * - maxDuration: Max duration per caption in seconds (default: 5)\n */\napp.post('/export/captions', async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    format = 'srt',\n    outputPath = null,\n    maxWordsPerCaption = 8,\n    maxDuration = 5\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !fs.existsSync(wavPath)) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Caption export: ${wavPath || 'provided transcript'} -> ${format}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    const exportOptions = { maxWordsPerCaption, maxDuration };\n\n    // Generate caption content based on format\n    let content;\n    let mimeType;\n\n    switch (format.toLowerCase()) {\n      case 'srt':\n        content = toSRT(transcript, exportOptions);\n        mimeType = 'application/x-subrip';\n        break;\n      case 'vtt':\n      case 'webvtt':\n        content = toVTT(transcript, exportOptions);\n        mimeType = 'text/vtt';\n        break;\n      case 'txt':\n      case 'text':\n        content = toPlainText(transcript, { ...exportOptions, includeTimestamps: true });\n        mimeType = 'text/plain';\n        break;\n      case 'json':\n        content = toJSON(transcript);\n        mimeType = 'application/json';\n        break;\n      default:\n        return res.status(400).json({\n          error: `Unsupported format: ${format}`,\n          supportedFormats: getSupportedFormats()\n        });\n    }\n\n    // Save to file if outputPath provided\n    let savedPath = null;\n    if (outputPath) {\n      const result = await exportToFile(transcript, outputPath, format, exportOptions);\n      savedPath = result.path;\n      console.log(`[SPLICE] Saved captions to: ${savedPath}`);\n    }\n\n    res.json({\n      success: true,\n      format,\n      content,\n      mimeType,\n      savedPath,\n      wordCount: transcript.words?.length || 0,\n      duration: transcript.duration || 0\n    });\n  } catch (err) {\n    console.error('[SPLICE] Caption export error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /export/formats - Get supported export formats\n */\napp.get('/export/formats', (req, res) => {\n  res.json({\n    success: true,\n    formats: getSupportedFormats()\n  });\n});\n\n// =============================================================================\n// Multitrack/Multicam Analysis Routes\n// =============================================================================\n\n/**\n * POST /multitrack - Analyze multiple audio tracks for multicam editing\n *\n * Analyzes audio levels across multiple tracks to determine optimal\n * video angle selection based on who is speaking.\n *\n * Options:\n * - audioPaths: Array of paths to audio files (one per speaker) - required\n * - speakerNames: Array of speaker names (optional)\n * - videoTrackMapping: Object mapping speaker index to video track { 0: 0, 1: 1 }\n * - minShotDuration: Minimum seconds before next cut (default: 2.0)\n * - switchingFrequency: How often to allow cuts 0-100 (default: 50)\n * - wideShotEnabled: Enable wide shot detection (default: true)\n * - wideShotPercentage: Target % of wide shots (default: 20)\n * - wideShotTracks: Video track indices for wide shots\n * - cutawayEnabled: Enable cutaway insertion (default: false)\n * - cutawayTracks: Video track indices for cutaways\n * - speakerBoosts: Per-speaker dB adjustments { \"Speaker 1\": 5 }\n */\napp.post('/multitrack', requireCredits({ endpoint: 'multitrack' }), async (req, res) => {\n  const {\n    audioPaths,\n    speakerNames,\n    videoTrackMapping = {},\n    minShotDuration = 2.0,\n    switchingFrequency = 50,\n    wideShotEnabled = true,\n    wideShotPercentage = 20,\n    wideShotTracks = [],\n    cutawayEnabled = false,\n    cutawayTracks = [],\n    speakerBoosts = {},\n    frameRate = 30\n  } = req.body;\n\n  // Validate audioPaths\n  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length === 0) {\n    return res.status(400).json({ error: 'audioPaths array is required (at least 1 path)' });\n  }\n\n  // Validate all files exist\n  for (const audioPath of audioPaths) {\n    if (!fs.existsSync(audioPath)) {\n      return res.status(404).json({ error: `File not found: ${audioPath}` });\n    }\n  }\n\n  // Check FFprobe availability\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  console.log(`[SPLICE] Multitrack analysis: ${audioPaths.length} track(s)`);\n\n  try {\n    const result = await analyzeMultitrack(audioPaths, {\n      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n      videoTrackMapping,\n      minShotDuration,\n      switchingFrequency,\n      wideShotEnabled,\n      wideShotPercentage,\n      wideShotTracks,\n      cutawayEnabled,\n      cutawayTracks,\n      speakerBoosts,\n      frameRate\n    });\n\n    res.json({\n      success: true,\n      ...result\n    });\n  } catch (err) {\n    console.error('[SPLICE] Multitrack analysis error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /multitrack/auto-balance - Auto-balance speaker screentime\n *\n * Automatically adjusts speaker boosts to achieve equal screentime distribution.\n * Runs multiple iterations to find optimal parameters.\n */\napp.post('/multitrack/auto-balance', async (req, res) => {\n  const {\n    audioPaths,\n    speakerNames,\n    videoTrackMapping = {},\n    minShotDuration = 2.0,\n    switchingFrequency = 50,\n    wideShotEnabled = false, // Disable wide shots for balance calc\n    frameRate = 30\n  } = req.body;\n\n  // Validate audioPaths\n  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length < 2) {\n    return res.status(400).json({ error: 'audioPaths array requires at least 2 tracks for balancing' });\n  }\n\n  // Validate all files exist\n  for (const audioPath of audioPaths) {\n    if (!fs.existsSync(audioPath)) {\n      return res.status(404).json({ error: `File not found: ${audioPath}` });\n    }\n  }\n\n  // Check FFprobe availability\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  console.log(`[SPLICE] Auto-balancing multitrack: ${audioPaths.length} track(s)`);\n\n  try {\n    const result = await autoBalanceMultitrack(audioPaths, {\n      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n      videoTrackMapping,\n      minShotDuration,\n      switchingFrequency,\n      wideShotEnabled,\n      frameRate\n    });\n\n    res.json({\n      success: true,\n      ...result\n    });\n  } catch (err) {\n    console.error('[SPLICE] Auto-balance error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /process-xml - Process FCP XML to split clips at silences\n *\n * Takes an FCP XML file and silence timestamps, splits clips\n * at silence boundaries, and optionally removes gaps.\n */\napp.post('/process-xml', async (req, res) => {\n  const {\n    xmlPath,\n    silences,\n    removeGaps = true,\n    outputPath = null\n  } = req.body;\n\n  if (!xmlPath) {\n    return res.status(400).json({ error: 'xmlPath is required' });\n  }\n\n  if (!silences || !Array.isArray(silences)) {\n    return res.status(400).json({ error: 'silences array is required' });\n  }\n\n  if (!fs.existsSync(xmlPath)) {\n    return res.status(404).json({ error: `XML file not found: ${xmlPath}` });\n  }\n\n  console.log(`[SPLICE] Processing XML: ${xmlPath} with ${silences.length} silence(s)`);\n\n  try {\n    const result = await processXMLFile(xmlPath, silences, {\n      outputPath,\n      removeGaps\n    });\n\n    res.json({\n      success: true,\n      inputPath: xmlPath,\n      outputPath: result.outputPath,\n      stats: result.stats\n    });\n  } catch (err) {\n    console.error('[SPLICE] XML processing error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /cut-list - Generate a JSON cut list for direct DOM building (v3.5)\n *\n * Takes silences and optionally takes, returns a cut list that the\n * plugin can use to build sequences directly via UXP APIs.\n *\n * Body:\n * - sourceName: Name of the source clip\n * - sourcePath: Full path to the source file\n * - duration: Total duration in seconds\n * - silences: Array of silence segments [{start, end, duration}]\n * - takes: (optional) Array of detected takes\n * - settings: (optional) Generation settings\n */\napp.post('/cut-list', async (req, res) => {\n  const {\n    sourceName,\n    sourcePath,\n    duration,\n    silences,\n    takes = [],\n    settings = {}\n  } = req.body;\n\n  // Validate required fields\n  if (!sourceName && !sourcePath) {\n    return res.status(400).json({ error: 'sourceName or sourcePath is required' });\n  }\n\n  if (typeof duration !== 'number' || duration <= 0) {\n    return res.status(400).json({ error: 'duration must be a positive number' });\n  }\n\n  if (!silences || !Array.isArray(silences)) {\n    return res.status(400).json({ error: 'silences array is required' });\n  }\n\n  console.log(`[SPLICE] Generating cut list for ${sourceName || sourcePath} (${silences.length} silences)`);\n\n  try {\n    const cutList = generateCutList({\n      sourceName: sourceName || path.basename(sourcePath),\n      sourcePath,\n      duration,\n      silences,\n      takes,\n      settings\n    });\n\n    // Validate the generated cut list\n    const validation = validateCutList(cutList);\n    if (!validation.valid) {\n      return res.status(500).json({\n        error: 'Generated cut list is invalid',\n        validationErrors: validation.errors\n      });\n    }\n\n    res.json({\n      success: true,\n      cutList\n    });\n  } catch (err) {\n    console.error('[SPLICE] Cut list generation error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /cut-list/takes - Generate a cut list that keeps only takes\n *\n * Alternative endpoint for \"keep best takes only\" workflow.\n */\napp.post('/cut-list/takes', async (req, res) => {\n  const {\n    sourceName,\n    sourcePath,\n    duration,\n    takes,\n    settings = {}\n  } = req.body;\n\n  // Validate required fields\n  if (!sourceName && !sourcePath) {\n    return res.status(400).json({ error: 'sourceName or sourcePath is required' });\n  }\n\n  if (typeof duration !== 'number' || duration <= 0) {\n    return res.status(400).json({ error: 'duration must be a positive number' });\n  }\n\n  if (!takes || !Array.isArray(takes) || takes.length === 0) {\n    return res.status(400).json({ error: 'takes array is required and must not be empty' });\n  }\n\n  console.log(`[SPLICE] Generating takes cut list for ${sourceName || sourcePath} (${takes.length} takes)`);\n\n  try {\n    const cutList = generateTakesCutList({\n      sourceName: sourceName || path.basename(sourcePath),\n      sourcePath,\n      duration,\n      takes,\n      settings\n    });\n\n    res.json({\n      success: true,\n      cutList\n    });\n  } catch (err) {\n    console.error('[SPLICE] Takes cut list generation error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /ffprobe-check - Check if FFprobe is installed\n */\napp.get('/ffprobe-check', async (req, res) => {\n  const installed = await isFFprobeInstalled();\n  res.json({\n    installed,\n    message: installed\n      ? 'FFprobe is available'\n      : 'FFprobe not found. Install with: brew install ffmpeg'\n  });\n});\n\n/**\n * GET /replicate-check - Check if Replicate API is configured\n */\napp.get('/replicate-check', async (req, res) => {\n  const configured = isReplicateConfigured();\n  res.json({\n    configured,\n    message: configured\n      ? 'Replicate API is configured'\n      : 'REPLICATE_API_TOKEN not set. Add to .env file.'\n  });\n});\n\n/**\n * POST /isolate-vocals - Isolate vocals from audio using Demucs\n *\n * Uses Replicate's Demucs model to separate vocals from background audio.\n * Cost: ~$0.015/min of audio\n *\n * Tier access:\n * - Starter: No access (upgrade required)\n * - Pro: 2 hours included, then $0.08/min overage\n * - Team: 5 hours included, then $0.08/min overage\n */\napp.post('/isolate-vocals', async (req, res) => {\n  const { audioPath, stem = 'vocals', outputDir = null } = req.body;\n  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n\n  if (!audioPath) {\n    return res.status(400).json({ error: 'audioPath is required' });\n  }\n\n  if (!fs.existsSync(audioPath)) {\n    return res.status(404).json({ error: `File not found: ${audioPath}` });\n  }\n\n  // Check Replicate configuration\n  if (!isReplicateConfigured()) {\n    return res.status(500).json({\n      error: 'Replicate API not configured. Set REPLICATE_API_TOKEN in .env'\n    });\n  }\n\n  // Get audio duration for billing\n  let audioDurationSeconds = 0;\n  try {\n    audioDurationSeconds = await getAudioDuration(audioPath);\n  } catch (err) {\n    console.warn('[SPLICE] Could not get audio duration:', err.message);\n  }\n\n  const audioDurationMinutes = audioDurationSeconds / 60;\n\n  // Check isolation access if customer ID provided\n  if (stripeCustomerId) {\n    const accessCheck = await usageTracking.checkIsolationAccess(stripeCustomerId, audioDurationMinutes);\n\n    if (!accessCheck.allowed) {\n      return res.status(403).json({\n        error: accessCheck.message,\n        reason: accessCheck.reason,\n        upgradeRequired: accessCheck.reason === 'upgrade_required'\n      });\n    }\n\n    console.log(`[SPLICE] Isolation access: ${accessCheck.message}`);\n  }\n\n  console.log(`[SPLICE] Isolating vocals: ${audioPath} (${audioDurationMinutes.toFixed(1)} min)`);\n\n  try {\n    const result = await isolateVocals(audioPath, {\n      stem,\n      outputDir: outputDir || undefined\n    });\n\n    // Deduct isolation usage if customer ID provided\n    let usageInfo = null;\n    if (stripeCustomerId) {\n      usageInfo = await usageTracking.deductIsolationUsage(\n        stripeCustomerId,\n        audioDurationSeconds,\n        'isolate-vocals'\n      );\n      console.log(`[SPLICE] Isolation usage deducted: ${audioDurationMinutes.toFixed(1)} min`);\n      if (usageInfo.isolationUsed?.overageCost > 0) {\n        console.log(`[SPLICE] Overage cost: $${usageInfo.isolationUsed.overageCost.toFixed(2)}`);\n      }\n    }\n\n    res.json({\n      success: true,\n      inputPath: audioPath,\n      outputPath: result.outputPath,\n      stem: result.stem,\n      processingTime: result.processingTime,\n      availableStems: result.allStems,\n      audioDurationMinutes,\n      usage: usageInfo ? {\n        isolationHoursRemaining: usageInfo.isolationHoursRemaining,\n        overageCost: usageInfo.isolationUsed?.overageCost || 0\n      } : null\n    });\n  } catch (err) {\n    console.error('[SPLICE] Vocal isolation error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Batch Processing Routes\n// =============================================================================\n\n// In-memory job queue for batch processing\nconst batchJobs = new Map();\n\n// Batch job limits to prevent memory leak\nconst MAX_BATCH_JOBS = 10000;\nconst BATCH_JOB_MAX_AGE_MS = 24 * 60 * 60 * 1000; // 24 hours\n\n/**\n * Generate a unique job ID\n */\nfunction generateJobId() {\n  return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n}\n\n/**\n * Clean up old batch jobs to prevent memory leak\n * Removes jobs older than 24 hours\n */\nfunction cleanupOldBatchJobs() {\n  const now = Date.now();\n  let removedCount = 0;\n\n  for (const [jobId, job] of batchJobs.entries()) {\n    const createdAt = new Date(job.createdAt).getTime();\n    if (now - createdAt > BATCH_JOB_MAX_AGE_MS) {\n      batchJobs.delete(jobId);\n      removedCount++;\n    }\n  }\n\n  if (removedCount > 0) {\n    console.log(`[SPLICE] Cleaned up ${removedCount} old batch job(s)`);\n  }\n}\n\n/**\n * Enforce max job limit by removing oldest completed jobs\n */\nfunction enforceJobLimit() {\n  if (batchJobs.size < MAX_BATCH_JOBS) return;\n\n  // Get completed jobs sorted by creation date (oldest first)\n  const completedJobs = Array.from(batchJobs.entries())\n    .filter(([_, job]) => job.status !== 'processing')\n    .sort((a, b) => new Date(a[1].createdAt) - new Date(b[1].createdAt));\n\n  // Remove oldest completed jobs until under limit\n  const toRemove = batchJobs.size - MAX_BATCH_JOBS + 1;\n  for (let i = 0; i < Math.min(toRemove, completedJobs.length); i++) {\n    batchJobs.delete(completedJobs[i][0]);\n  }\n\n  console.log(`[SPLICE] Enforced job limit, removed ${Math.min(toRemove, completedJobs.length)} job(s)`);\n}\n\n// Run cleanup every hour\nsetInterval(cleanupOldBatchJobs, 60 * 60 * 1000);\n\n/**\n * POST /batch/silences - Process multiple files for silence detection\n *\n * Creates a batch job that processes multiple audio files.\n * Returns a job ID for tracking progress.\n *\n * Body:\n * - files: Array of file paths to process\n * - options: Detection options (sensitivity, threshold, etc.)\n */\napp.post('/batch/silences', async (req, res) => {\n  const { files, options = {} } = req.body;\n\n  if (!files || !Array.isArray(files) || files.length === 0) {\n    return res.status(400).json({ error: 'files array is required' });\n  }\n\n  // Validate all files exist\n  const missingFiles = files.filter(f => !fs.existsSync(f));\n  if (missingFiles.length > 0) {\n    return res.status(404).json({\n      error: 'Some files not found',\n      missingFiles\n    });\n  }\n\n  // Enforce job limit before creating new job\n  enforceJobLimit();\n\n  const jobId = generateJobId();\n\n  // Initialize job\n  const job = {\n    id: jobId,\n    type: 'silences',\n    status: 'processing',\n    createdAt: new Date().toISOString(),\n    files: files.map(f => ({\n      path: f,\n      status: 'pending',\n      result: null,\n      error: null\n    })),\n    options,\n    progress: {\n      total: files.length,\n      completed: 0,\n      failed: 0,\n      percentage: 0\n    },\n    results: [],\n    errors: []\n  };\n\n  batchJobs.set(jobId, job);\n  console.log(`[SPLICE] Batch job ${jobId} created with ${files.length} files`);\n\n  // Start processing in background\n  processBatchJob(jobId);\n\n  res.json({\n    success: true,\n    jobId,\n    message: `Batch job created with ${files.length} files`,\n    statusUrl: `/batch/status/${jobId}`\n  });\n});\n\n/**\n * Process a batch job (runs in background)\n */\nasync function processBatchJob(jobId) {\n  const job = batchJobs.get(jobId);\n  if (!job) return;\n\n  const { sensitivity, ...manualOptions } = job.options;\n\n  // Build detection options\n  let detectionOptions = {};\n  if (typeof sensitivity === 'number') {\n    detectionOptions = sensitivityToParams(sensitivity);\n  } else {\n    detectionOptions = {\n      threshold: manualOptions.threshold ?? -30,\n      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n      paddingStart: manualOptions.paddingStart ?? 0.1,\n      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n      autoThreshold: manualOptions.autoThreshold ?? false\n    };\n  }\n\n  // Process files sequentially to avoid overwhelming the system\n  for (let i = 0; i < job.files.length; i++) {\n    const fileEntry = job.files[i];\n    fileEntry.status = 'processing';\n\n    try {\n      const result = await detectSilencesRMS(fileEntry.path, detectionOptions);\n\n      fileEntry.status = 'completed';\n      fileEntry.result = {\n        silences: result.silences,\n        count: result.silences.length,\n        totalSilenceDuration: result.metadata.totalSilenceDuration,\n        audioDuration: result.metadata.audioDuration\n      };\n\n      job.results.push({\n        file: fileEntry.path,\n        ...fileEntry.result\n      });\n\n      job.progress.completed++;\n      console.log(`[SPLICE] Batch ${jobId}: ${i + 1}/${job.files.length} completed`);\n    } catch (err) {\n      fileEntry.status = 'failed';\n      fileEntry.error = err.message;\n\n      job.errors.push({\n        file: fileEntry.path,\n        error: err.message\n      });\n\n      job.progress.failed++;\n      console.error(`[SPLICE] Batch ${jobId}: ${fileEntry.path} failed:`, err.message);\n    }\n\n    // Update progress\n    job.progress.percentage = Math.round(\n      ((job.progress.completed + job.progress.failed) / job.progress.total) * 100\n    );\n  }\n\n  // Mark job as complete\n  job.status = job.progress.failed === job.progress.total ? 'failed' :\n               job.progress.failed > 0 ? 'completed_with_errors' : 'completed';\n  job.completedAt = new Date().toISOString();\n\n  console.log(`[SPLICE] Batch job ${jobId} ${job.status}`);\n}\n\n/**\n * GET /batch/status/:jobId - Get batch job status and results\n */\napp.get('/batch/status/:jobId', (req, res) => {\n  const { jobId } = req.params;\n  const job = batchJobs.get(jobId);\n\n  if (!job) {\n    return res.status(404).json({ error: 'Job not found' });\n  }\n\n  res.json({\n    success: true,\n    job: {\n      id: job.id,\n      type: job.type,\n      status: job.status,\n      createdAt: job.createdAt,\n      completedAt: job.completedAt,\n      progress: job.progress,\n      files: job.files.map(f => ({\n        path: f.path,\n        status: f.status,\n        silenceCount: f.result?.count,\n        error: f.error\n      }))\n    }\n  });\n});\n\n/**\n * GET /batch/results/:jobId - Get full results for a completed batch job\n */\napp.get('/batch/results/:jobId', (req, res) => {\n  const { jobId } = req.params;\n  const job = batchJobs.get(jobId);\n\n  if (!job) {\n    return res.status(404).json({ error: 'Job not found' });\n  }\n\n  if (job.status === 'processing') {\n    return res.status(202).json({\n      success: false,\n      message: 'Job still processing',\n      progress: job.progress\n    });\n  }\n\n  res.json({\n    success: true,\n    jobId: job.id,\n    status: job.status,\n    progress: job.progress,\n    results: job.results,\n    errors: job.errors,\n    summary: {\n      totalFiles: job.progress.total,\n      successful: job.progress.completed,\n      failed: job.progress.failed,\n      totalSilences: job.results.reduce((sum, r) => sum + (r.count || 0), 0),\n      totalSilenceDuration: job.results.reduce((sum, r) => sum + (r.totalSilenceDuration || 0), 0)\n    }\n  });\n});\n\n/**\n * DELETE /batch/:jobId - Cancel or delete a batch job\n */\napp.delete('/batch/:jobId', (req, res) => {\n  const { jobId } = req.params;\n  const job = batchJobs.get(jobId);\n\n  if (!job) {\n    return res.status(404).json({ error: 'Job not found' });\n  }\n\n  // Note: This doesn't actually cancel in-progress processing\n  // but prevents the job from being queried\n  batchJobs.delete(jobId);\n\n  res.json({\n    success: true,\n    message: `Job ${jobId} deleted`\n  });\n});\n\n/**\n * GET /batch/jobs - List all batch jobs\n */\napp.get('/batch/jobs', (req, res) => {\n  const jobs = Array.from(batchJobs.values()).map(job => ({\n    id: job.id,\n    type: job.type,\n    status: job.status,\n    createdAt: job.createdAt,\n    completedAt: job.completedAt,\n    progress: job.progress\n  }));\n\n  // Sort by creation date (newest first)\n  jobs.sort((a, b) => new Date(b.createdAt) - new Date(a.createdAt));\n\n  res.json({\n    success: true,\n    count: jobs.length,\n    jobs\n  });\n});\n\n// =============================================================================\n// Billing & Credits Routes\n// =============================================================================\n\n/**\n * GET /credits - Get user's credit balance\n *\n * Requires x-stripe-customer-id header\n */\napp.get('/credits', async (req, res) => {\n  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n\n  if (!stripeCustomerId) {\n    return res.status(401).json({\n      error: 'Authentication required',\n      message: 'Missing x-stripe-customer-id header'\n    });\n  }\n\n  try {\n    const balance = await usageTracking.getBalance(stripeCustomerId);\n    res.json({\n      success: true,\n      ...balance\n    });\n  } catch (err) {\n    console.error('[SPLICE] Credits error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /usage-history - Get user's usage history\n */\napp.get('/usage-history', async (req, res) => {\n  const stripeCustomerId = req.headers['x-stripe-customer-id'];\n\n  if (!stripeCustomerId) {\n    return res.status(401).json({\n      error: 'Authentication required',\n      message: 'Missing x-stripe-customer-id header'\n    });\n  }\n\n  try {\n    const history = await usageTracking.getUsageHistory(stripeCustomerId);\n    res.json({\n      success: true,\n      history\n    });\n  } catch (err) {\n    console.error('[SPLICE] Usage history error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Start Server\n// =============================================================================\n\n// Initialize database and start server\nasync function startServer() {\n  try {\n    await usageTracking.initDatabase();\n    console.log('[SPLICE] Database initialized');\n\n    if (isProduction || !httpsOptions) {\n      // Production: Railway provides TLS termination, use HTTP\n      http.createServer(app).listen(PORT, () => {\n        console.log(`[SPLICE] Backend running at http://0.0.0.0:${PORT} (production)`);\n      });\n    } else {\n      // Development: Use HTTPS with local certificates\n      https.createServer(httpsOptions, app).listen(PORT, () => {\n        console.log(`[SPLICE] Backend running at https://127.0.0.1:${PORT} (development)`);\n        console.log(`[SPLICE] POST /analyze with { \"wavPath\": \"/path/to/audio.wav\" }`);\n      });\n    }\n  } catch (err) {\n    console.error('[SPLICE] Failed to start server:', err);\n    process.exit(1);\n  }\n}\n\nstartServer();\n","numLines":1806,"startLine":1,"totalLines":1806}}}
{"parentUuid":"b26e540f-4464-4756-a44a-c92816a775dc","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018wn9BfrZZuv4N9ZvpkNCjN","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Plugin Main Entry Point\n     3→ *\n     4→ * Initializes all functionality when the plugin loads.\n     5→ * v3.4 - Timeline Markers + Unified Take-Aware Workflow.\n     6→ */\n     7→\n     8→// Initialize Premiere Pro API\n     9→const ppro = require('premierepro');\n    10→\n    11→// ============================================================================\n    12→// CACHED DOM ELEMENTS (Performance: avoid repeated getElementById calls)\n    13→// ============================================================================\n    14→const ui = {};\n    15→\n    16→function cacheUIElements() {\n    17→  ui.status = document.getElementById('status');\n    18→\n    19→  // Main GO button (unified workflow)\n    20→  ui.goBtn = document.getElementById('goBtn');\n    21→  ui.optionsToggle = document.getElementById('optionsToggle');\n    22→  ui.optionsPanel = document.getElementById('optionsPanel');\n    23→\n    24→  // Sliders and options\n    25→  ui.sensitivitySlider = document.getElementById('sensitivitySlider');\n    26→  ui.sourceOriginal = document.getElementById('sourceOriginal');\n    27→  ui.sourceIsolated = document.getElementById('sourceIsolated');\n    28→\n    29→  // Settings checkboxes\n    30→  ui.enableTakesDetection = document.getElementById('enableTakesDetection');\n    31→  ui.autoMarkBest = document.getElementById('autoMarkBest');\n    32→\n    33→  // Progress\n    34→  ui.progressContainer = document.getElementById('progressContainer');\n    35→  ui.progressText = document.getElementById('progressText');\n    36→  ui.resultsEmpty = document.getElementById('resultsEmpty');\n    37→\n    38→  // Combined Preview\n    39→  ui.combinedPreview = document.getElementById('combinedPreview');\n    40→  ui.previewList = document.getElementById('previewList');\n    41→  ui.silenceCountDisplay = document.getElementById('silenceCount');\n    42→  ui.takeCountDisplay = document.getElementById('takeCount');\n    43→  ui.selectedCount = document.getElementById('selectedCount');\n    44→  ui.selectAllSilences = document.getElementById('selectAllSilences');\n    45→  ui.applyPreviewBtn = document.getElementById('applyPreviewBtn');\n    46→  ui.cancelPreviewBtn = document.getElementById('cancelPreviewBtn');\n    47→  ui.invertSelectionBtn = document.getElementById('invertSelectionBtn');\n    48→  ui.reRunDetectionBtn = document.getElementById('reRunDetectionBtn');\n    49→\n    50→  // Results\n    51→  ui.silenceResults = document.getElementById('silenceResults');\n    52→  ui.resultsCount = document.getElementById('resultsCount');\n    53→  ui.timeSaved = document.getElementById('timeSaved');\n    54→  ui.clipsModified = document.getElementById('clipsModified');\n    55→\n    56→  // Build Sequence button (v3.5)\n    57→  ui.buildSequenceBtn = document.getElementById('buildSequenceBtn');\n    58→\n    59→  // Buttons\n    60→  ui.undoBtn = document.getElementById('undoBtn');\n    61→  ui.advancedSection = document.getElementById('advancedSection');\n    62→\n    63→  // Settings modal\n    64→  ui.settingsBtn = document.getElementById('settingsBtn');\n    65→  ui.settingsModal = document.getElementById('settingsModal');\n    66→  ui.closeSettingsBtn = document.getElementById('closeSettingsBtn');\n    67→\n    68→  // Media folder (v3.5)\n    69→  ui.mediaFolderDisplay = document.getElementById('mediaFolderDisplay');\n    70→  ui.setMediaFolderBtn = document.getElementById('setMediaFolderBtn');\n    71→  ui.clearMediaFolderBtn = document.getElementById('clearMediaFolderBtn');\n    72→}\n    73→\n    74→// ============================================================================\n    75→// STATE MANAGEMENT\n    76→// ============================================================================\n    77→let previewSilences = [];         // All detected silences\n    78→let safeSilences = [];            // Silences that don't overlap takes (safe to remove)\n    79→let protectedSilences = [];       // Silences that overlap takes (protected)\n    80→let previewTakes = [];            // Detected takes (for display)\n    81→let selectedSilenceIndices = new Set();\n    82→let isOperationInProgress = false;\n    83→let pendingUIUpdate = null;\n    84→\n    85→// Export preview state for other modules (Razor integration)\n    86→window.splicePreviewState = {\n    87→  getSelectedSilences: () => safeSilences.filter((_, i) => selectedSilenceIndices.has(i)),\n    88→  getSelectedTakes: () => previewTakes,\n    89→  hasPreview: () => safeSilences.length > 0 || previewTakes.length > 0\n    90→};\n    91→\n    92→// ============================================================================\n    93→// INITIALIZATION\n    94→// ============================================================================\n    95→document.addEventListener('DOMContentLoaded', () => {\n    96→  // Cache all DOM elements first\n    97→  cacheUIElements();\n    98→\n    99→  // Settings & UI\n   100→  initSettingsUI();\n   101→  initSettingsModal();\n   102→  initOptionsToggles();\n   103→  initHelpButton();\n   104→\n   105→  // Credits display\n   106→  initCredits();\n   107→\n   108→  // Unified workflow (silences + takes)\n   109→  initUnifiedWorkflow();\n   110→\n   111→  // Preview handlers (event delegation)\n   112→  initPreviewHandlers();\n   113→\n   114→  // Undo handlers\n   115→  initUndoHandlers();\n   116→\n   117→  // Keyboard shortcuts\n   118→  initKeyboardShortcuts();\n   119→\n   120→  // Advanced features (Razor - legacy)\n   121→  initSlice9();\n   122→\n   123→  // Media folder (v3.5)\n   124→  initMediaFolderHandlers();\n   125→\n   126→  console.log('[SPLICE] Plugin initialized v3.5 (Direct DOM Reconstruction)');\n   127→});\n   128→\n   129→// ============================================================================\n   130→// DEBOUNCED UI UPDATES (Performance: batch DOM updates)\n   131→// ============================================================================\n   132→function scheduleUIUpdate(callback) {\n   133→  if (pendingUIUpdate) {\n   134→    cancelAnimationFrame(pendingUIUpdate);\n   135→  }\n   136→  pendingUIUpdate = requestAnimationFrame(() => {\n   137→    callback();\n   138→    pendingUIUpdate = null;\n   139→  });\n   140→}\n   141→\n   142→function updateSelectionCounts() {\n   143→  scheduleUIUpdate(() => {\n   144→    if (ui.selectedCount) {\n   145→      ui.selectedCount.textContent = selectedSilenceIndices.size;\n   146→    }\n   147→    updateApplyButton();\n   148→    updateSelectAllCheckbox();\n   149→  });\n   150→}\n   151→\n   152→// ============================================================================\n   153→// KEYBOARD SHORTCUTS\n   154→// ============================================================================\n   155→function initKeyboardShortcuts() {\n   156→  document.addEventListener('keydown', (e) => {\n   157→    // Only handle when combined preview is visible\n   158→    const previewVisible = ui.combinedPreview?.style.display !== 'none' &&\n   159→                           ui.combinedPreview?.style.display !== '';\n   160→\n   161→    if (!previewVisible) return;\n   162→\n   163→    const isMac = navigator.platform.toUpperCase().indexOf('MAC') >= 0;\n   164→    const cmdKey = isMac ? e.metaKey : e.ctrlKey;\n   165→\n   166→    // Enter - Apply selected\n   167→    if (e.key === 'Enter' && !e.shiftKey) {\n   168→      e.preventDefault();\n   169→      if (selectedSilenceIndices.size > 0) {\n   170→        ui.applyPreviewBtn?.click();\n   171→      }\n   172→    }\n   173→\n   174→    // Escape - Cancel\n   175→    if (e.key === 'Escape') {\n   176→      e.preventDefault();\n   177→      ui.cancelPreviewBtn?.click();\n   178→    }\n   179→\n   180→    // Cmd/Ctrl+A - Select all\n   181→    if (e.key === 'a' && cmdKey) {\n   182→      e.preventDefault();\n   183→      if (ui.selectAllSilences) {\n   184→        ui.selectAllSilences.checked = true;\n   185→        selectAllSilences(true);\n   186→      }\n   187→    }\n   188→\n   189→    // Cmd/Ctrl+D - Deselect all\n   190→    if (e.key === 'd' && cmdKey) {\n   191→      e.preventDefault();\n   192→      if (ui.selectAllSilences) {\n   193→        ui.selectAllSilences.checked = false;\n   194→        selectAllSilences(false);\n   195→      }\n   196→    }\n   197→\n   198→    // Cmd/Ctrl+I - Invert selection\n   199→    if (e.key === 'i' && cmdKey) {\n   200→      e.preventDefault();\n   201→      invertSilenceSelection();\n   202→    }\n   203→  });\n   204→}\n   205→\n   206→// ============================================================================\n   207→// UNIFIED WORKFLOW (Silence Detection + Takes Detection)\n   208→// ============================================================================\n   209→function initUnifiedWorkflow() {\n   210→  if (!ui.goBtn) return;\n   211→\n   212→  ui.goBtn.addEventListener('click', async () => {\n   213→    // Prevent double-click\n   214→    if (isOperationInProgress) {\n   215→      setStatus('Operation in progress...');\n   216→      return;\n   217→    }\n   218→\n   219→    isOperationInProgress = true;\n   220→    ui.goBtn.disabled = true;\n   221→\n   222→    try {\n   223→      showProgress('Preparing audio...');\n   224→\n   225→      // Step 1: Export audio\n   226→      const exportSuccess = await exportAudioInternal();\n   227→      if (!exportSuccess) {\n   228→        throw new Error('Audio export failed');\n   229→      }\n   230→\n   231→      // Step 2: Get settings\n   232→      const sensitivity = parseInt(ui.sensitivitySlider?.value || 50);\n   233→      const useIsolated = ui.sourceIsolated?.checked ?? false;\n   234→      const detectTakes = ui.enableTakesDetection?.checked ?? true;\n   235→\n   236→      // Step 3: Vocal isolation if needed\n   237→      let audioPath = WAV_PATH;\n   238→      if (useIsolated) {\n   239→        // Check isolation access before proceeding\n   240→        const credits = getCurrentCredits();\n   241→        if (!credits || !credits.hasIsolationAccess) {\n   242→          throw new Error('Vocal isolation requires Pro or Team tier. Upgrade to access this feature.');\n   243→        }\n   244→\n   245→        // Show overage warning if applicable\n   246→        const isolationMins = credits.isolationHoursRemaining * 60;\n   247→        if (isolationMins <= 0) {\n   248→          const overageRate = credits.isolationOverageRate || 0.08;\n   249→          showProgress(`Isolating vocals (overage: $${overageRate}/min)...`);\n   250→        } else {\n   251→          showProgress(`Isolating vocals (${isolationMins.toFixed(0)} min remaining)...`);\n   252→        }\n   253→\n   254→        const isolateResult = await isolateVocals();\n   255→        if (isolateResult.success) {\n   256→          audioPath = isolateResult.outputPath;\n   257→          // Update credits display after isolation\n   258→          await refreshCredits();\n   259→        } else {\n   260→          throw new Error(isolateResult.error || 'Vocal isolation failed');\n   261→        }\n   262→      }\n   263→\n   264→      // Step 4: Run detection (parallel if takes enabled)\n   265→      const params = mapSensitivity(sensitivity);\n   266→      let silences = [];\n   267→      let takes = [];\n   268→\n   269→      if (detectTakes) {\n   270→        // Parallel detection for performance\n   271→        showProgress('Detecting silences and analyzing takes...');\n   272→        const [silenceResult, transcriptResult] = await Promise.all([\n   273→          detectSilences(audioPath, params),\n   274→          transcribeAudio()\n   275→        ]);\n   276→\n   277→        silences = silenceResult;\n   278→\n   279→        // Extract takes from transcript result\n   280→        if (transcriptResult.success && transcriptResult.takes) {\n   281→          if (transcriptResult.takes.takes) {\n   282→            takes = transcriptResult.takes.takes;\n   283→          } else if (Array.isArray(transcriptResult.takes)) {\n   284→            takes = transcriptResult.takes;\n   285→          }\n   286→        }\n   287→\n   288→        // Store transcript for reference\n   289→        window.currentTranscript = transcriptResult.transcript;\n   290→      } else {\n   291→        // Just detect silences\n   292→        showProgress('Detecting silences...');\n   293→        silences = await detectSilences(audioPath, params);\n   294→      }\n   295→\n   296→      if (silences.length === 0) {\n   297→        showEmptyState();\n   298→        setStatus('No silences detected. Try increasing sensitivity.');\n   299→        return;\n   300→      }\n   301→\n   302→      // Step 5: Filter silences by takes (protect speech)\n   303→      const filtered = filterSilencesByTakes(silences, takes);\n   304→\n   305→      // Step 6: Add timeline markers (clear old ones first)\n   306→      showProgress('Adding timeline markers...');\n   307→      await clearSpliceMarkers();\n   308→      const markerResult = await addTimelineMarkers(filtered.safe, filtered.protected, takes);\n   309→\n   310→      // Step 7: Show combined preview\n   311→      showCombinedPreview(filtered.safe, filtered.protected, takes);\n   312→\n   313→      const markerInfo = markerResult.added > 0 ? ` (${markerResult.added} markers added)` : '';\n   314→      const statusMsg = takes.length > 0\n   315→        ? `Found ${silences.length} silences, ${filtered.protected.length} protected by ${takes.length} takes${markerInfo}`\n   316→        : `Found ${silences.length} silence(s) - review and apply${markerInfo}`;\n   317→      setStatus(statusMsg);\n   318→\n   319→    } catch (err) {\n   320→      showEmptyState();\n   321→      setStatus('Error: ' + err.message);\n   322→      console.error('[SPLICE] Unified workflow error:', err);\n   323→    } finally {\n   324→      isOperationInProgress = false;\n   325→      ui.goBtn.disabled = false;\n   326→    }\n   327→  });\n   328→}\n   329→\n   330→// ============================================================================\n   331→// TAKE-AWARE SILENCE FILTERING\n   332→// ============================================================================\n   333→/**\n   334→ * Filter silences to protect those that overlap with takes (speech segments).\n   335→ * Returns { safe: [], protected: [] }\n   336→ */\n   337→function filterSilencesByTakes(silences, takes) {\n   338→  if (!takes || takes.length === 0) {\n   339→    // No takes = all silences are safe to remove\n   340→    return { safe: silences, protected: [] };\n   341→  }\n   342→\n   343→  const safe = [];\n   344→  const protected_ = [];\n   345→\n   346→  silences.forEach(silence => {\n   347→    // Check if this silence overlaps with any take\n   348→    const overlaps = takes.some(take => {\n   349→      const takeStart = take.startTime || take.start;\n   350→      const takeEnd = take.endTime || take.end;\n   351→      // Overlap exists if silence starts before take ends AND silence ends after take starts\n   352→      return silence.start < takeEnd && silence.end > takeStart;\n   353→    });\n   354→\n   355→    if (overlaps) {\n   356→      protected_.push(silence);\n   357→    } else {\n   358→      safe.push(silence);\n   359→    }\n   360→  });\n   361→\n   362→  console.log(`[SPLICE] Filtered silences: ${safe.length} safe, ${protected_.length} protected`);\n   363→  return { safe, protected: protected_ };\n   364→}\n   365→\n   366→// ============================================================================\n   367→// TIMELINE MARKERS (Visual indicators on Premiere Pro timeline)\n   368→// ============================================================================\n   369→/**\n   370→ * Add colored markers to the timeline for silences and takes.\n   371→ * - Red markers: silences safe to remove\n   372→ * - Yellow markers: silences protected by takes\n   373→ * - Blue markers: takes (speech segments)\n   374→ *\n   375→ * @param {Array} safeToRemove - Silences safe to remove\n   376→ * @param {Array} protectedSilences - Silences overlapping speech\n   377→ * @param {Array} takes - Detected speech segments\n   378→ * @returns {Promise<{added: number, errors: number}>}\n   379→ */\n   380→async function addTimelineMarkers(safeToRemove, protectedSilences, takes) {\n   381→  const results = { added: 0, errors: 0 };\n   382→\n   383→  try {\n   384→    const context = await getActiveSequence();\n   385→    if (!context) {\n   386→      console.warn('[SPLICE] No sequence available for markers');\n   387→      return results;\n   388→    }\n   389→\n   390→    const { sequence, project } = context;\n   391→\n   392→    // Get markers object for the sequence\n   393→    const markers = await ppro.Markers.getMarkers(sequence);\n   394→    if (!markers) {\n   395→      console.warn('[SPLICE] Markers API not available');\n   396→      return results;\n   397→    }\n   398→\n   399→    // Collect all marker actions\n   400→    const markerActions = [];\n   401→\n   402→    // Red markers for safe-to-remove silences\n   403→    for (const silence of safeToRemove) {\n   404→      try {\n   405→        const duration = silence.end - silence.start;\n   406→\n   407→        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n   408→        const startTime = ppro.TickTime.createWithSeconds(silence.start);\n   409→        const durationTime = ppro.TickTime.createWithSeconds(duration);\n   410→\n   411→        const action = markers.createAddMarkerAction(\n   412→          `SPLICE: Remove (${duration.toFixed(2)}s)`,\n   413→          ppro.Marker.MARKER_TYPE_COMMENT,\n   414→          startTime,\n   415→          durationTime,\n   416→          `Silence to remove: ${formatTime(silence.start)} - ${formatTime(silence.end)}`\n   417→        );\n   418→        markerActions.push({ action, colorIndex: 1, type: 'silence' }); // Red = index 1\n   419→      } catch (err) {\n   420→        console.error('[SPLICE] Error creating silence marker:', err);\n   421→        results.errors++;\n   422→      }\n   423→    }\n   424→\n   425→    // Yellow markers for protected silences\n   426→    for (const silence of protectedSilences) {\n   427→      try {\n   428→        const duration = silence.end - silence.start;\n   429→\n   430→        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n   431→        const startTime = ppro.TickTime.createWithSeconds(silence.start);\n   432→        const durationTime = ppro.TickTime.createWithSeconds(duration);\n   433→\n   434→        const action = markers.createAddMarkerAction(\n   435→          `SPLICE: Protected (${duration.toFixed(2)}s)`,\n   436→          ppro.Marker.MARKER_TYPE_COMMENT,\n   437→          startTime,\n   438→          durationTime,\n   439→          `Protected silence (overlaps speech): ${formatTime(silence.start)} - ${formatTime(silence.end)}`\n   440→        );\n   441→        markerActions.push({ action, colorIndex: 4, type: 'protected' }); // Yellow = index 4\n   442→      } catch (err) {\n   443→        console.error('[SPLICE] Error creating protected marker:', err);\n   444→        results.errors++;\n   445→      }\n   446→    }\n   447→\n   448→    // Blue markers for takes (speech segments)\n   449→    for (const take of takes) {\n   450→      try {\n   451→        const start = take.startTime || take.start;\n   452→        const end = take.endTime || take.end;\n   453→        const duration = end - start;\n   454→        const textPreview = take.text?.substring(0, 30) || 'Speech';\n   455→\n   456→        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n   457→        const startTime = ppro.TickTime.createWithSeconds(start);\n   458→        const durationTime = ppro.TickTime.createWithSeconds(duration);\n   459→\n   460→        const action = markers.createAddMarkerAction(\n   461→          `SPLICE: Take`,\n   462→          ppro.Marker.MARKER_TYPE_COMMENT,\n   463→          startTime,\n   464→          durationTime,\n   465→          `Speech segment: ${textPreview}${take.text?.length > 30 ? '...' : ''}`\n   466→        );\n   467→        markerActions.push({ action, colorIndex: 5, type: 'take' }); // Blue = index 5\n   468→      } catch (err) {\n   469→        console.error('[SPLICE] Error creating take marker:', err);\n   470→        results.errors++;\n   471→      }\n   472→    }\n   473→\n   474→    // Execute all marker actions in a single transaction\n   475→    if (markerActions.length > 0) {\n   476→      await project.lockedAccess(async () => {\n   477→        await project.executeTransaction((compoundAction) => {\n   478→          for (const { action } of markerActions) {\n   479→            compoundAction.addAction(action);\n   480→          }\n   481→        }, 'SPLICE: Add Timeline Markers');\n   482→      });\n   483→\n   484→      results.added = markerActions.length;\n   485→      console.log(`[SPLICE] Added ${results.added} timeline markers`);\n   486→    }\n   487→\n   488→  } catch (err) {\n   489→    console.error('[SPLICE] Timeline markers error:', err);\n   490→    results.errors++;\n   491→  }\n   492→\n   493→  return results;\n   494→}\n   495→\n   496→/**\n   497→ * Read current SPLICE marker positions from timeline.\n   498→ * This allows users to manually adjust markers before applying.\n   499→ * Returns silences based on current marker positions (not original detection).\n   500→ */\n   501→async function readMarkersAssilences() {\n   502→  const silences = [];\n   503→\n   504→  try {\n   505→    const context = await getActiveSequence();\n   506→    if (!context) return silences;\n   507→\n   508→    const { sequence } = context;\n   509→    const markers = await ppro.Markers.getMarkers(sequence);\n   510→    if (!markers) return silences;\n   511→\n   512→    const allMarkers = await markers.getMarkers();\n   513→    if (!allMarkers || allMarkers.length === 0) return silences;\n   514→\n   515→    for (const marker of allMarkers) {\n   516→      const name = await marker.getName();\n   517→\n   518→      // Only read \"Remove\" markers (red ones that will be cut)\n   519→      if (name && name.startsWith('SPLICE: Remove')) {\n   520→        const startTickTime = await marker.getStart();\n   521→        const durationTickTime = await marker.getDuration();\n   522→\n   523→        // Use TickTime.seconds property to get seconds directly (no TICKS_PER_SECOND needed)\n   524→        const start = startTickTime.seconds;\n   525→        const duration = durationTickTime.seconds;\n   526→        const end = start + duration;\n   527→\n   528→        silences.push({\n   529→          start,\n   530→          end,\n   531→          duration,\n   532→          fromMarker: true  // Flag to indicate this came from marker adjustment\n   533→        });\n   534→      }\n   535→    }\n   536→\n   537→    // Sort by start time\n   538→    silences.sort((a, b) => a.start - b.start);\n   539→    console.log(`[SPLICE] Read ${silences.length} silences from adjusted markers`);\n   540→\n   541→  } catch (err) {\n   542→    console.error('[SPLICE] Error reading markers:', err);\n   543→  }\n   544→\n   545→  return silences;\n   546→}\n   547→\n   548→/**\n   549→ * Remove all SPLICE markers from the timeline.\n   550→ * Call this before adding new markers to avoid duplicates.\n   551→ */\n   552→async function clearSpliceMarkers() {\n   553→  try {\n   554→    const context = await getActiveSequence();\n   555→    if (!context) return 0;\n   556→\n   557→    const { sequence, project } = context;\n   558→    const markers = await ppro.Markers.getMarkers(sequence);\n   559→    if (!markers) return 0;\n   560→\n   561→    // Get all markers\n   562→    const allMarkers = await markers.getMarkers();\n   563→    if (!allMarkers || allMarkers.length === 0) return 0;\n   564→\n   565→    // Filter SPLICE markers\n   566→    const spliceMarkers = [];\n   567→    for (const marker of allMarkers) {\n   568→      const name = await marker.getName();\n   569→      if (name && name.startsWith('SPLICE:')) {\n   570→        spliceMarkers.push(marker);\n   571→      }\n   572→    }\n   573→\n   574→    if (spliceMarkers.length === 0) return 0;\n   575→\n   576→    // Remove all SPLICE markers in a single transaction\n   577→    await project.lockedAccess(async () => {\n   578→      await project.executeTransaction((compoundAction) => {\n   579→        for (const marker of spliceMarkers) {\n   580→          const removeAction = markers.createRemoveMarkerAction(marker);\n   581→          compoundAction.addAction(removeAction);\n   582→        }\n   583→      }, 'SPLICE: Clear Markers');\n   584→    });\n   585→\n   586→    console.log(`[SPLICE] Cleared ${spliceMarkers.length} previous markers`);\n   587→    return spliceMarkers.length;\n   588→\n   589→  } catch (err) {\n   590→    console.error('[SPLICE] Error clearing markers:', err);\n   591→    return 0;\n   592→  }\n   593→}\n   594→\n   595→// ============================================================================\n   596→// COMBINED PREVIEW (Silences + Takes with colored markers)\n   597→// ============================================================================\n   598→/**\n   599→ * Show combined preview with:\n   600→ * - Red markers: silences safe to remove\n   601→ * - Yellow markers: silences protected by takes (won't be removed)\n   602→ * - Blue markers: takes (for reference)\n   603→ */\n   604→function showCombinedPreview(safeToRemove, protectedSilences, takes) {\n   605→  // Update global state\n   606→  safeSilences = safeToRemove;\n   607→  previewSilences = [...safeToRemove, ...protectedSilences];\n   608→  previewTakes = takes;\n   609→  selectedSilenceIndices = new Set(safeToRemove.map((_, i) => i));\n   610→\n   611→  // Hide other views\n   612→  hideAllViews();\n   613→\n   614→  // Update summary counts\n   615→  if (ui.silenceCountDisplay) ui.silenceCountDisplay.textContent = safeToRemove.length;\n   616→  if (ui.takeCountDisplay) ui.takeCountDisplay.textContent = takes.length;\n   617→  if (ui.selectedCount) ui.selectedCount.textContent = safeToRemove.length;\n   618→\n   619→  // Build combined items list (sorted by time)\n   620→  const items = [\n   621→    ...safeToRemove.map((s, i) => ({ type: 'silence', index: i, start: s.start, end: s.end, data: s })),\n   622→    ...protectedSilences.map(s => ({ type: 'protected', start: s.start, end: s.end, data: s })),\n   623→    ...takes.map(t => ({ type: 'take', start: t.startTime || t.start, end: t.endTime || t.end, data: t }))\n   624→  ].sort((a, b) => a.start - b.start);\n   625→\n   626→  // Build preview list using DocumentFragment\n   627→  const fragment = document.createDocumentFragment();\n   628→\n   629→  items.forEach((item) => {\n   630→    const duration = item.end - item.start;\n   631→    const div = document.createElement('div');\n   632→\n   633→    if (item.type === 'silence') {\n   634→      // Safe to remove - red marker with checkbox\n   635→      div.className = 'preview-item silence-marker';\n   636→      div.dataset.index = item.index;\n   637→      div.innerHTML = `\n   638→        <input type=\"checkbox\" class=\"preview-item-check\" checked data-index=\"${item.index}\">\n   639→        <div class=\"preview-item-info\">\n   640→          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n   641→          <div class=\"preview-item-duration\">${duration.toFixed(2)}s silence</div>\n   642→        </div>\n   643→        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek to this silence\">&gt;</button>\n   644→      `;\n   645→    } else if (item.type === 'protected') {\n   646→      // Protected by take - yellow marker, no checkbox\n   647→      div.className = 'preview-item protected-silence';\n   648→      div.innerHTML = `\n   649→        <div style=\"width: 20px; margin-right: 10px;\"></div>\n   650→        <div class=\"preview-item-info\">\n   651→          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n   652→          <div class=\"preview-item-duration\">${duration.toFixed(2)}s (overlaps speech)</div>\n   653→        </div>\n   654→        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek\">&gt;</button>\n   655→      `;\n   656→    } else if (item.type === 'take') {\n   657→      // Take (speech segment) - blue marker, no checkbox\n   658→      div.className = 'preview-item take-marker';\n   659→      const textPreview = item.data.text?.substring(0, 40) || 'Speech segment';\n   660→      div.innerHTML = `\n   661→        <div style=\"width: 20px; margin-right: 10px;\"></div>\n   662→        <div class=\"preview-item-info\">\n   663→          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n   664→          <div class=\"preview-item-duration\">${textPreview}${item.data.text?.length > 40 ? '...' : ''}</div>\n   665→        </div>\n   666→        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek to take\">&gt;</button>\n   667→      `;\n   668→    }\n   669→\n   670→    fragment.appendChild(div);\n   671→  });\n   672→\n   673→  if (ui.previewList) {\n   674→    ui.previewList.textContent = '';\n   675→    ui.previewList.appendChild(fragment);\n   676→  }\n   677→\n   678→  // Reset UI state\n   679→  if (ui.selectAllSilences) ui.selectAllSilences.checked = true;\n   680→  updateApplyButton();\n   681→\n   682→  // Show combined preview section\n   683→  if (ui.combinedPreview) ui.combinedPreview.style.display = 'block';\n   684→}\n   685→\n   686→function initPreviewHandlers() {\n   687→  // EVENT DELEGATION: Single listener on parent instead of many on children\n   688→  if (ui.previewList) {\n   689→    // Handle checkbox changes\n   690→    ui.previewList.addEventListener('change', (e) => {\n   691→      if (e.target.classList.contains('preview-item-check')) {\n   692→        const index = parseInt(e.target.dataset.index);\n   693→        const item = e.target.closest('.preview-item');\n   694→\n   695→        if (e.target.checked) {\n   696→          selectedSilenceIndices.add(index);\n   697→          item?.classList.remove('excluded');\n   698→        } else {\n   699→          selectedSilenceIndices.delete(index);\n   700→          item?.classList.add('excluded');\n   701→        }\n   702→        updateSelectionCounts();\n   703→      }\n   704→    });\n   705→\n   706→    // Handle seek button clicks\n   707→    ui.previewList.addEventListener('click', async (e) => {\n   708→      if (e.target.classList.contains('preview-item-seek')) {\n   709→        e.stopPropagation();\n   710→        const btn = e.target;\n   711→        const time = parseFloat(btn.dataset.time);\n   712→\n   713→        // Show loading state\n   714→        btn.classList.add('loading');\n   715→        btn.textContent = '...';\n   716→\n   717→        try {\n   718→          await seekToTime(time);\n   719→        } finally {\n   720→          btn.classList.remove('loading');\n   721→          btn.textContent = '>';\n   722→        }\n   723→      }\n   724→    });\n   725→  }\n   726→\n   727→  // Apply button\n   728→  if (ui.applyPreviewBtn) {\n   729→    ui.applyPreviewBtn.addEventListener('click', async () => {\n   730→      if (isOperationInProgress) return;\n   731→      isOperationInProgress = true;\n   732→      ui.applyPreviewBtn.disabled = true;\n   733→\n   734→      const originalText = ui.applyPreviewBtn.textContent;\n   735→      ui.applyPreviewBtn.textContent = 'Applying...';\n   736→\n   737→      try {\n   738→        await applySelectedSilences();\n   739→      } catch (err) {\n   740→        setStatus('Error: ' + err.message);\n   741→        console.error('[SPLICE] Apply error:', err);\n   742→      } finally {\n   743→        ui.applyPreviewBtn.textContent = originalText;\n   744→        ui.applyPreviewBtn.disabled = false;\n   745→        isOperationInProgress = false;\n   746→      }\n   747→    });\n   748→  }\n   749→\n   750→  // Cancel button\n   751→  if (ui.cancelPreviewBtn) {\n   752→    ui.cancelPreviewBtn.addEventListener('click', async () => {\n   753→      await cancelPreview();\n   754→    });\n   755→  }\n   756→\n   757→  // Select all checkbox\n   758→  if (ui.selectAllSilences) {\n   759→    ui.selectAllSilences.addEventListener('change', (e) => {\n   760→      selectAllSilences(e.target.checked);\n   761→    });\n   762→  }\n   763→\n   764→  // Invert selection button\n   765→  if (ui.invertSelectionBtn) {\n   766→    ui.invertSelectionBtn.addEventListener('click', () => {\n   767→      invertSilenceSelection();\n   768→    });\n   769→  }\n   770→\n   771→  // Duration filter buttons\n   772→  document.querySelectorAll('.preview-filters .filter-btn[data-min]').forEach(btn => {\n   773→    btn.addEventListener('click', () => {\n   774→      const min = parseFloat(btn.dataset.min);\n   775→      const max = parseFloat(btn.dataset.max);\n   776→      selectByDuration(min, max);\n   777→    });\n   778→  });\n   779→\n   780→  // Re-run detection button\n   781→  if (ui.reRunDetectionBtn) {\n   782→    ui.reRunDetectionBtn.addEventListener('click', () => {\n   783→      // Hide preview, show options, let user adjust sensitivity\n   784→      if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n   785→      if (ui.optionsPanel) ui.optionsPanel.classList.remove('collapsed');\n   786→      if (ui.optionsToggle) ui.optionsToggle.classList.add('expanded');\n   787→      showEmptyState();\n   788→      setStatus('Adjust sensitivity and click GO again');\n   789→    });\n   790→  }\n   791→\n   792→  // Build Sequence button (v3.5 - direct DOM reconstruction)\n   793→  if (ui.buildSequenceBtn) {\n   794→    ui.buildSequenceBtn.addEventListener('click', async () => {\n   795→      if (isOperationInProgress) return;\n   796→\n   797→      // Check if we have detection data\n   798→      if (safeSilences.length === 0 && previewTakes.length === 0) {\n   799→        setStatus('No detection data. Run analysis first.');\n   800→        return;\n   801→      }\n   802→\n   803→      isOperationInProgress = true;\n   804→      ui.buildSequenceBtn.disabled = true;\n   805→      const originalText = ui.buildSequenceBtn.textContent;\n   806→      ui.buildSequenceBtn.textContent = 'Building...';\n   807→\n   808→      try {\n   809→        showProgress('Building new sequence...');\n   810→\n   811→        const result = await buildSequenceWithCutList();\n   812→\n   813→        if (result.success) {\n   814→          // Hide preview and show success\n   815→          if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n   816→          showEmptyState();\n   817→\n   818→          const metadata = result.metadata || {};\n   819→          setStatus(`New sequence created: ${metadata.silencesRemoved || 0} silences removed, ${metadata.takesDetected || 0} takes detected`);\n   820→\n   821→          // Clear markers after successful build\n   822→          await clearSpliceMarkers();\n   823→\n   824→          // Clear preview state\n   825→          previewSilences = [];\n   826→          safeSilences = [];\n   827→          protectedSilences = [];\n   828→          previewTakes = [];\n   829→          selectedSilenceIndices.clear();\n   830→        } else {\n   831→          setStatus('Build failed: ' + (result.error || 'Unknown error'));\n   832→        }\n   833→\n   834→      } catch (err) {\n   835→        console.error('[SPLICE] Build sequence error:', err);\n   836→        setStatus('Build error: ' + err.message);\n   837→      } finally {\n   838→        ui.buildSequenceBtn.textContent = originalText;\n   839→        ui.buildSequenceBtn.disabled = false;\n   840→        isOperationInProgress = false;\n   841→      }\n   842→    });\n   843→  }\n   844→}\n   845→\n   846→function selectAllSilences(checked) {\n   847→  selectedSilenceIndices.clear();\n   848→\n   849→  if (checked) {\n   850→    safeSilences.forEach((_, i) => selectedSilenceIndices.add(i));\n   851→  }\n   852→\n   853→  // Batch update DOM (only silence-marker items have checkboxes)\n   854→  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n   855→    const index = parseInt(checkbox.dataset.index);\n   856→    checkbox.checked = checked;\n   857→    const item = checkbox.closest('.preview-item');\n   858→    if (checked) {\n   859→      item?.classList.remove('excluded');\n   860→    } else {\n   861→      item?.classList.add('excluded');\n   862→    }\n   863→  });\n   864→\n   865→  updateSelectionCounts();\n   866→}\n   867→\n   868→function invertSilenceSelection() {\n   869→  const allIndices = new Set(safeSilences.map((_, i) => i));\n   870→  const newSelection = new Set([...allIndices].filter(i => !selectedSilenceIndices.has(i)));\n   871→  selectedSilenceIndices = newSelection;\n   872→\n   873→  // Update DOM\n   874→  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n   875→    const index = parseInt(checkbox.dataset.index);\n   876→    const isSelected = selectedSilenceIndices.has(index);\n   877→    checkbox.checked = isSelected;\n   878→    const item = checkbox.closest('.preview-item');\n   879→    if (isSelected) {\n   880→      item?.classList.remove('excluded');\n   881→    } else {\n   882→      item?.classList.add('excluded');\n   883→    }\n   884→  });\n   885→\n   886→  updateSelectionCounts();\n   887→}\n   888→\n   889→function selectByDuration(minSec, maxSec) {\n   890→  selectedSilenceIndices.clear();\n   891→\n   892→  safeSilences.forEach((silence, index) => {\n   893→    const duration = silence.end - silence.start;\n   894→    if (duration >= minSec && duration < maxSec) {\n   895→      selectedSilenceIndices.add(index);\n   896→    }\n   897→  });\n   898→\n   899→  // Update DOM\n   900→  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n   901→    const index = parseInt(checkbox.dataset.index);\n   902→    const isSelected = selectedSilenceIndices.has(index);\n   903→    checkbox.checked = isSelected;\n   904→    const item = checkbox.closest('.preview-item');\n   905→    if (isSelected) {\n   906→      item?.classList.remove('excluded');\n   907→    } else {\n   908→      item?.classList.add('excluded');\n   909→    }\n   910→  });\n   911→\n   912→  updateSelectionCounts();\n   913→  setStatus(`Selected ${selectedSilenceIndices.size} silences (${minSec}s - ${maxSec}s)`);\n   914→}\n   915→\n   916→async function cancelPreview() {\n   917→  // Keep silences available for Razor if detected\n   918→  const hadSilences = safeSilences.length > 0;\n   919→\n   920→  previewSilences = [];\n   921→  safeSilences = [];\n   922→  protectedSilences = [];\n   923→  previewTakes = [];\n   924→  selectedSilenceIndices.clear();\n   925→  showEmptyState();\n   926→\n   927→  // Clear timeline markers\n   928→  const cleared = await clearSpliceMarkers();\n   929→  const markerInfo = cleared > 0 ? ` (${cleared} markers removed)` : '';\n   930→  setStatus('Preview cancelled' + markerInfo);\n   931→\n   932→  // Show Razor section if silences were detected\n   933→  if (hadSilences && ui.advancedSection) {\n   934→    ui.advancedSection.classList.remove('hidden');\n   935→  }\n   936→}\n   937→\n   938→async function applySelectedSilences() {\n   939→  if (selectedSilenceIndices.size === 0) {\n   940→    setStatus('No silences selected');\n   941→    return;\n   942→  }\n   943→\n   944→  showProgress('Reading marker positions...');\n   945→\n   946→  // Read current marker positions from timeline\n   947→  // This allows users to manually adjust markers before applying\n   948→  const markerSilences = await readMarkersAssilences();\n   949→\n   950→  let silencesToApply;\n   951→  let adjustedCount = 0;\n   952→\n   953→  if (markerSilences.length > 0) {\n   954→    // Use marker positions (may have been adjusted by user)\n   955→    silencesToApply = markerSilences;\n   956→    adjustedCount = markerSilences.length;\n   957→    console.log(`[SPLICE] Using ${adjustedCount} silences from adjusted markers`);\n   958→  } else {\n   959→    // Fallback to original detection data\n   960→    silencesToApply = safeSilences.filter((_, i) => selectedSilenceIndices.has(i));\n   961→    console.log(`[SPLICE] Using ${silencesToApply.length} silences from original detection`);\n   962→  }\n   963→\n   964→  if (silencesToApply.length === 0) {\n   965→    setStatus('No silences to apply');\n   966→    return;\n   967→  }\n   968→\n   969→  // Update global silences for the apply function\n   970→  setCurrentSilences(silencesToApply);\n   971→\n   972→  showProgress('Applying changes...');\n   973→\n   974→  // Apply to timeline\n   975→  const result = await removeSilencesFromTimeline();\n   976→\n   977→  // Clear markers after applying\n   978→  await clearSpliceMarkers();\n   979→\n   980→  // Hide preview\n   981→  if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n   982→\n   983→  // Show results\n   984→  showSilenceResults({\n   985→    count: silencesToApply.length,\n   986→    timeSaved: silencesToApply.reduce((sum, s) => sum + (s.end - s.start), 0),\n   987→    clipsModified: result\n   988→  });\n   989→\n   990→  // Show advanced section\n   991→  if (ui.advancedSection) ui.advancedSection.classList.remove('hidden');\n   992→\n   993→  const adjustedInfo = adjustedCount > 0 ? ' (from adjusted markers)' : '';\n   994→  setStatus(`Removed ${silencesToApply.length} silence(s) - ${result} clip(s) modified${adjustedInfo}`);\n   995→\n   996→  // Refresh credits (await to prevent race conditions)\n   997→  try {\n   998→    await refreshCredits();\n   999→  } catch (err) {\n  1000→    console.warn('[SPLICE] Credits refresh failed:', err);\n  1001→  }\n  1002→\n  1003→  // Clear preview state\n  1004→  previewSilences = [];\n  1005→  safeSilences = [];\n  1006→  protectedSilences = [];\n  1007→  previewTakes = [];\n  1008→  selectedSilenceIndices.clear();\n  1009→}\n  1010→\n  1011→// ============================================================================\n  1012→// UI HELPERS\n  1013→// ============================================================================\n  1014→function updateApplyButton() {\n  1015→  if (ui.applyPreviewBtn) {\n  1016→    const count = selectedSilenceIndices.size;\n  1017→    ui.applyPreviewBtn.disabled = count === 0;\n  1018→    ui.applyPreviewBtn.textContent = count === 0 ? 'Select silences' : `Apply ${count} Selected`;\n  1019→  }\n  1020→}\n  1021→\n  1022→function updateSelectAllCheckbox() {\n  1023→  if (ui.selectAllSilences) {\n  1024→    ui.selectAllSilences.checked = selectedSilenceIndices.size === safeSilences.length && safeSilences.length > 0;\n  1025→  }\n  1026→}\n  1027→\n  1028→function hideAllViews() {\n  1029→  if (ui.progressContainer) ui.progressContainer.style.display = 'none';\n  1030→  if (ui.resultsEmpty) ui.resultsEmpty.style.display = 'none';\n  1031→  if (ui.silenceResults) ui.silenceResults.style.display = 'none';\n  1032→  if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n  1033→}\n  1034→\n  1035→function showProgress(message) {\n  1036→  hideAllViews();\n  1037→  if (ui.progressContainer) ui.progressContainer.style.display = 'block';\n  1038→  if (ui.progressText) ui.progressText.textContent = message;\n  1039→}\n  1040→\n  1041→function showEmptyState() {\n  1042→  hideAllViews();\n  1043→  if (ui.resultsEmpty) ui.resultsEmpty.style.display = 'block';\n  1044→  if (ui.advancedSection) ui.advancedSection.classList.add('hidden');\n  1045→}\n  1046→\n  1047→function showSilenceResults({ count, timeSaved, clipsModified }) {\n  1048→  hideAllViews();\n  1049→\n  1050→  // Update the results display\n  1051→  if (ui.resultsCount) ui.resultsCount.textContent = count;\n  1052→  if (ui.timeSaved) ui.timeSaved.textContent = timeSaved.toFixed(1) + 's';\n  1053→  if (ui.clipsModified) ui.clipsModified.textContent = clipsModified;\n  1054→\n  1055→  if (ui.silenceResults) ui.silenceResults.style.display = 'block';\n  1056→}\n  1057→\n  1058→function formatTime(seconds) {\n  1059→  if (typeof seconds !== 'number' || isNaN(seconds)) return '0:00';\n  1060→  const mins = Math.floor(seconds / 60);\n  1061→  const secs = Math.floor(seconds % 60);\n  1062→  return `${mins}:${secs.toString().padStart(2, '0')}`;\n  1063→}\n  1064→\n  1065→function setStatus(message) {\n  1066→  if (ui.status) ui.status.textContent = message;\n  1067→}\n  1068→\n  1069→// ============================================================================\n  1070→// UNDO HANDLERS\n  1071→// ============================================================================\n  1072→function initUndoHandlers() {\n  1073→  if (ui.undoBtn) {\n  1074→    ui.undoBtn.addEventListener('click', () => {\n  1075→      try {\n  1076→        ppro.app.executeCommand('Edit.Undo');\n  1077→        showEmptyState();\n  1078→        setStatus('Changes undone');\n  1079→      } catch (e) {\n  1080→        setStatus('Undo failed - use Cmd+Z');\n  1081→      }\n  1082→    });\n  1083→  }\n  1084→}\n  1085→\n  1086→// ============================================================================\n  1087→// MEDIA FOLDER HANDLERS (v3.5)\n  1088→// ============================================================================\n  1089→function initMediaFolderHandlers() {\n  1090→  // Update display on load\n  1091→  updateMediaFolderDisplay();\n  1092→\n  1093→  // Set media folder button\n  1094→  if (ui.setMediaFolderBtn) {\n  1095→    ui.setMediaFolderBtn.addEventListener('click', async () => {\n  1096→      ui.setMediaFolderBtn.disabled = true;\n  1097→      ui.setMediaFolderBtn.textContent = 'Selecting...';\n  1098→\n  1099→      try {\n  1100→        const result = await setupMediaFolder();\n  1101→        if (result.success) {\n  1102→          updateMediaFolderDisplay();\n  1103→          setStatus(`Media folder set: ${result.path}`);\n  1104→        } else {\n  1105→          setStatus(result.error || 'Failed to set media folder');\n  1106→        }\n  1107→      } catch (err) {\n  1108→        setStatus('Error: ' + err.message);\n  1109→      } finally {\n  1110→        ui.setMediaFolderBtn.disabled = false;\n  1111→        ui.setMediaFolderBtn.textContent = 'Set Folder';\n  1112→      }\n  1113→    });\n  1114→  }\n  1115→\n  1116→  // Clear media folder button\n  1117→  if (ui.clearMediaFolderBtn) {\n  1118→    ui.clearMediaFolderBtn.addEventListener('click', () => {\n  1119→      clearMediaFolder();\n  1120→      updateMediaFolderDisplay();\n  1121→      setStatus('Media folder cleared');\n  1122→    });\n  1123→  }\n  1124→}\n  1125→\n  1126→function updateMediaFolderDisplay() {\n  1127→  if (ui.mediaFolderDisplay) {\n  1128→    const path = getMediaFolderPath();\n  1129→    if (path) {\n  1130→      // Truncate long paths for display\n  1131→      const displayPath = path.length > 40\n  1132→        ? '...' + path.slice(-37)\n  1133→        : path;\n  1134→      ui.mediaFolderDisplay.textContent = displayPath;\n  1135→      ui.mediaFolderDisplay.title = path; // Full path on hover\n  1136→      ui.mediaFolderDisplay.style.color = '#4a9eff';\n  1137→    } else {\n  1138→      ui.mediaFolderDisplay.textContent = 'Not configured';\n  1139→      ui.mediaFolderDisplay.title = '';\n  1140→      ui.mediaFolderDisplay.style.color = '#888';\n  1141→    }\n  1142→  }\n  1143→}\n  1144→\n  1145→// ============================================================================\n  1146→// BUILD SEQUENCE (v3.5 - Direct DOM Reconstruction)\n  1147→// ============================================================================\n  1148→\n  1149→// 1. ROBUST CONSTANTS (Fixes 'undefined' error)\n  1150→// We define these integers explicitly because UXP API versions vary on where they store Enums.\n  1151→const ProjectItemType = {\n  1152→    CLIP: 1,\n  1153→    BIN: 2,\n  1154→    ROOT: 3,\n  1155→    FILE: 4\n  1156→};\n  1157→\n  1158→/**\n  1159→ * Find the first media item in the project bin (recursively)\n  1160→ * @param {Object} item - ProjectItem (usually rootItem)\n  1161→ * @returns {Promise<Object|null>} First media item found or null\n  1162→ */\n  1163→async function findFirstMediaItem(item) {\n  1164→  if (!item) return null;\n  1165→\n  1166→  // Fix: Use local Constant instead of app.Constants.ProjectItemType\n  1167→  if (item.type === ProjectItemType.CLIP) {\n  1168→    return item;\n  1169→  }\n  1170→\n  1171→  // Search Children (Bin or Root)\n  1172→  // Note: item.children is a collection, check numItems\n  1173→  if (item.children && item.children.numItems > 0) {\n  1174→    for (let i = 0; i < item.children.numItems; i++) {\n  1175→      const child = item.children[i];\n  1176→      const found = await findFirstMediaItem(child);\n  1177→      if (found) return found;\n  1178→    }\n  1179→  }\n  1180→\n  1181→  return null;\n  1182→}\n  1183→\n  1184→/**\n  1185→ * Build a sequence from detection data locally\n  1186→ * Used for testing via console: await buildSequenceV35(silences)\n  1187→ */\n  1188→async function buildSequenceV35(silences, takes = []) {\n  1189→    if (!window.spliceBuilder) {\n  1190→        throw new Error('Builder not initialized. Check if builder.js is loaded.');\n  1191→    }\n  1192→\n  1193→    const app = require('premierepro');\n  1194→    const project = await app.Project.getActiveProject();\n  1195→    if (!project) throw new Error(\"No active project found.\");\n  1196→\n  1197→    const rootItem = await project.getRootItem();\n  1198→    const sourceProjectItem = await findFirstMediaItem(rootItem);\n  1199→\n  1200→    if (!sourceProjectItem) {\n  1201→        throw new Error('No media items found in project bin.');\n  1202→    }\n  1203→\n  1204→    console.log(`[SPLICE] Found source media: ${sourceProjectItem.name}`);\n  1205→\n  1206→    return await window.spliceBuilder.buildSequenceFromDetection(silences, takes, sourceProjectItem);\n  1207→}\n  1208→\n  1209→/**\n  1210→ * Request a cut list from the backend and build the sequence.\n  1211→ * Full v3.5 workflow.\n  1212→ */\n  1213→async function buildSequenceWithCutList() {\n  1214→  if (!window.spliceBuilder) {\n  1215→    throw new Error('Builder not initialized');\n  1216→  }\n  1217→\n  1218→  // Ensure BACKEND_URL is available\n  1219→  const apiUrl = (typeof BACKEND_URL !== 'undefined') ? BACKEND_URL : 'http://localhost:3000';\n  1220→\n  1221→  if (typeof showProgress === 'function') showProgress('Getting source clip info...');\n  1222→\n  1223→  const app = require('premierepro');\n  1224→  const project = await app.Project.getActiveProject();\n  1225→  if (!project) throw new Error(\"No active project.\");\n  1226→\n  1227→  const rootItem = await project.getRootItem();\n  1228→  const sourceProjectItem = await findFirstMediaItem(rootItem);\n  1229→\n  1230→  if (!sourceProjectItem) {\n  1231→    throw new Error('No media items found in project. Please import a media file first.');\n  1232→  }\n  1233→\n  1234→  // Use properties strictly\n  1235→  const sourceName = sourceProjectItem.name;\n  1236→  const sourcePath = sourceProjectItem.treePath;\n  1237→\n  1238→  const durationObj = await sourceProjectItem.getDuration();\n  1239→  const duration = durationObj ? durationObj.seconds : 0;\n  1240→\n  1241→  // Retrieve Globals\n  1242→  const silences = window.splicePreviewState?.getSelectedSilences() || (typeof safeSilences !== 'undefined' ? safeSilences : []);\n  1243→  const takes = (typeof previewTakes !== 'undefined' ? previewTakes : []);\n  1244→\n  1245→  if (silences.length === 0 && (!takes || takes.length === 0)) {\n  1246→     throw new Error('No detection data found. Run analysis first.');\n  1247→  }\n  1248→\n  1249→  if (typeof showProgress === 'function') showProgress('Generating cut list...');\n  1250→\n  1251→  const response = await fetch(`${apiUrl}/cut-list`, {\n  1252→    method: 'POST',\n  1253→    headers: { 'Content-Type': 'application/json' },\n  1254→    body: JSON.stringify({\n  1255→      sourceName,\n  1256→      sourcePath,\n  1257→      duration,\n  1258→      silences,\n  1259→      takes\n  1260→    })\n  1261→  });\n  1262→\n  1263→  if (!response.ok) {\n  1264→    const err = await response.json();\n  1265→    throw new Error(err.error || 'Failed to generate cut list');\n  1266→  }\n  1267→\n  1268→  const data = await response.json();\n  1269→  const cutList = data.cutList;\n  1270→\n  1271→  if (typeof showProgress === 'function') showProgress('Building sequence...');\n  1272→\n  1273→  const result = await window.spliceBuilder.buildSequenceFromCutList(cutList, sourceProjectItem);\n  1274→\n  1275→  if (!result.success) {\n  1276→    throw new Error(result.error || 'Failed to build sequence');\n  1277→  }\n  1278→\n  1279→  return {\n  1280→    success: true,\n  1281→    clipsAdded: result.clipsAdded,\n  1282→    metadata: cutList.metadata\n  1283→  };\n  1284→}\n  1285→\n  1286→// Expose functions globally for UI and testing\n  1287→if (typeof window !== 'undefined') {\n  1288→    window.buildSequenceV35 = buildSequenceV35;\n  1289→    window.buildSequenceWithCutList = buildSequenceWithCutList;\n  1290→    window.findFirstMediaItem = findFirstMediaItem;\n  1291→}\n  1292→\n  1293→// ============================================================================\n  1294→// TIMELINE SEEK (with proper error handling)\n  1295→// ============================================================================\n  1296→async function seekToTime(seconds) {\n  1297→  // Validate input\n  1298→  if (typeof seconds !== 'number' || isNaN(seconds) || !isFinite(seconds)) {\n  1299→    setStatus('Invalid seek time');\n  1300→    return;\n  1301→  }\n  1302→\n  1303→  if (seconds < 0) {\n  1304→    seconds = 0;\n  1305→  }\n  1306→\n  1307→  try {\n  1308→    const context = await getActiveSequence();\n  1309→    if (!context) {\n  1310→      setStatus('No sequence available');\n  1311→      return;\n  1312→    }\n  1313→\n  1314→    const { sequence } = context;\n  1315→\n  1316→    // Use TickTime API to convert seconds to the format needed\n  1317→    const position = ppro.TickTime.createWithSeconds(seconds);\n  1318→\n  1319→    await sequence.setPlayerPosition(position);\n  1320→    setStatus(`Seeked to ${formatTime(seconds)}`);\n  1321→  } catch (err) {\n  1322→    console.error('[SPLICE] Seek error:', err);\n  1323→    setStatus('Seek failed: ' + (err.message || 'Unknown error'));\n  1324→  }\n  1325→}\n  1326→\n  1327→// ============================================================================\n  1328→// AUDIO EXPORT & BACKEND CALLS\n  1329→// ============================================================================\n  1330→async function exportAudioInternal() {\n  1331→  try {\n  1332→    const context = await getActiveSequence();\n  1333→    if (!context) {\n  1334→      throw new Error('No project or sequence open');\n  1335→    }\n  1336→\n  1337→    const { sequence } = context;\n  1338→    const encoderManager = ppro.EncoderManager.getManager();\n  1339→    if (!encoderManager) {\n  1340→      throw new Error('EncoderManager not available');\n  1341→    }\n  1342→\n  1343→    const result = await encoderManager.exportSequence(\n  1344→      sequence,\n  1345→      ppro.EncoderManager.EXPORT_IMMEDIATELY,\n  1346→      WAV_PATH,\n  1347→      WAV_PRESET_PATH,\n  1348→      true\n  1349→    );\n  1350→\n  1351→    return result;\n  1352→  } catch (err) {\n  1353→    console.error('[SPLICE] Export error:', err);\n  1354→    return false;\n  1355→  }\n  1356→}\n  1357→\n  1358→async function isolateVocals() {\n  1359→  const response = await fetch(`${BACKEND_URL}/isolate-vocals`, {\n  1360→    method: 'POST',\n  1361→    headers: { 'Content-Type': 'application/json' },\n  1362→    body: JSON.stringify({ audioPath: WAV_PATH })\n  1363→  });\n  1364→\n  1365→  if (!response.ok) {\n  1366→    const err = await response.json();\n  1367→    return { success: false, error: err.error || 'Isolation request failed' };\n  1368→  }\n  1369→\n  1370→  return await response.json();\n  1371→}\n  1372→\n  1373→async function detectSilences(audioPath, params) {\n  1374→  const response = await fetch(`${BACKEND_URL}/silences-audio`, {\n  1375→    method: 'POST',\n  1376→    headers: { 'Content-Type': 'application/json' },\n  1377→    body: JSON.stringify({\n  1378→      wavPath: audioPath,\n  1379→      threshold: params.dbThreshold,\n  1380→      minDuration: params.minDuration,\n  1381→      padding: params.padding\n  1382→    })\n  1383→  });\n  1384→\n  1385→  if (!response.ok) {\n  1386→    const err = await response.json();\n  1387→    throw new Error(err.error || 'Silence detection failed');\n  1388→  }\n  1389→\n  1390→  const data = await response.json();\n  1391→  if (!data.success) {\n  1392→    throw new Error(data.error || 'Silence detection failed');\n  1393→  }\n  1394→\n  1395→  // Store silences globally for razor workflow\n  1396→  currentSilences = data.silences;\n  1397→\n  1398→  return data.silences;\n  1399→}\n  1400→\n  1401→async function transcribeAudio() {\n  1402→  const response = await fetch(`${BACKEND_URL}/analyze`, {\n  1403→    method: 'POST',\n  1404→    headers: { 'Content-Type': 'application/json' },\n  1405→    body: JSON.stringify({ wavPath: WAV_PATH })\n  1406→  });\n  1407→\n  1408→  if (!response.ok) {\n  1409→    const err = await response.json();\n  1410→    return { success: false, error: err.error || 'Transcription failed' };\n  1411→  }\n  1412→\n  1413→  const data = await response.json();\n  1414→  return {\n  1415→    success: data.success,\n  1416→    transcript: data.transcript,\n  1417→    takes: data.takes,\n  1418→    error: data.error\n  1419→  };\n  1420→}\n  1421→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"b7a078ae-5d24-4a7f-ae87-a46f2a81a28f","timestamp":"2025-12-25T20:25:13.823Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-plugin/js/main.js","content":"/**\n * SPLICE Plugin Main Entry Point\n *\n * Initializes all functionality when the plugin loads.\n * v3.4 - Timeline Markers + Unified Take-Aware Workflow.\n */\n\n// Initialize Premiere Pro API\nconst ppro = require('premierepro');\n\n// ============================================================================\n// CACHED DOM ELEMENTS (Performance: avoid repeated getElementById calls)\n// ============================================================================\nconst ui = {};\n\nfunction cacheUIElements() {\n  ui.status = document.getElementById('status');\n\n  // Main GO button (unified workflow)\n  ui.goBtn = document.getElementById('goBtn');\n  ui.optionsToggle = document.getElementById('optionsToggle');\n  ui.optionsPanel = document.getElementById('optionsPanel');\n\n  // Sliders and options\n  ui.sensitivitySlider = document.getElementById('sensitivitySlider');\n  ui.sourceOriginal = document.getElementById('sourceOriginal');\n  ui.sourceIsolated = document.getElementById('sourceIsolated');\n\n  // Settings checkboxes\n  ui.enableTakesDetection = document.getElementById('enableTakesDetection');\n  ui.autoMarkBest = document.getElementById('autoMarkBest');\n\n  // Progress\n  ui.progressContainer = document.getElementById('progressContainer');\n  ui.progressText = document.getElementById('progressText');\n  ui.resultsEmpty = document.getElementById('resultsEmpty');\n\n  // Combined Preview\n  ui.combinedPreview = document.getElementById('combinedPreview');\n  ui.previewList = document.getElementById('previewList');\n  ui.silenceCountDisplay = document.getElementById('silenceCount');\n  ui.takeCountDisplay = document.getElementById('takeCount');\n  ui.selectedCount = document.getElementById('selectedCount');\n  ui.selectAllSilences = document.getElementById('selectAllSilences');\n  ui.applyPreviewBtn = document.getElementById('applyPreviewBtn');\n  ui.cancelPreviewBtn = document.getElementById('cancelPreviewBtn');\n  ui.invertSelectionBtn = document.getElementById('invertSelectionBtn');\n  ui.reRunDetectionBtn = document.getElementById('reRunDetectionBtn');\n\n  // Results\n  ui.silenceResults = document.getElementById('silenceResults');\n  ui.resultsCount = document.getElementById('resultsCount');\n  ui.timeSaved = document.getElementById('timeSaved');\n  ui.clipsModified = document.getElementById('clipsModified');\n\n  // Build Sequence button (v3.5)\n  ui.buildSequenceBtn = document.getElementById('buildSequenceBtn');\n\n  // Buttons\n  ui.undoBtn = document.getElementById('undoBtn');\n  ui.advancedSection = document.getElementById('advancedSection');\n\n  // Settings modal\n  ui.settingsBtn = document.getElementById('settingsBtn');\n  ui.settingsModal = document.getElementById('settingsModal');\n  ui.closeSettingsBtn = document.getElementById('closeSettingsBtn');\n\n  // Media folder (v3.5)\n  ui.mediaFolderDisplay = document.getElementById('mediaFolderDisplay');\n  ui.setMediaFolderBtn = document.getElementById('setMediaFolderBtn');\n  ui.clearMediaFolderBtn = document.getElementById('clearMediaFolderBtn');\n}\n\n// ============================================================================\n// STATE MANAGEMENT\n// ============================================================================\nlet previewSilences = [];         // All detected silences\nlet safeSilences = [];            // Silences that don't overlap takes (safe to remove)\nlet protectedSilences = [];       // Silences that overlap takes (protected)\nlet previewTakes = [];            // Detected takes (for display)\nlet selectedSilenceIndices = new Set();\nlet isOperationInProgress = false;\nlet pendingUIUpdate = null;\n\n// Export preview state for other modules (Razor integration)\nwindow.splicePreviewState = {\n  getSelectedSilences: () => safeSilences.filter((_, i) => selectedSilenceIndices.has(i)),\n  getSelectedTakes: () => previewTakes,\n  hasPreview: () => safeSilences.length > 0 || previewTakes.length > 0\n};\n\n// ============================================================================\n// INITIALIZATION\n// ============================================================================\ndocument.addEventListener('DOMContentLoaded', () => {\n  // Cache all DOM elements first\n  cacheUIElements();\n\n  // Settings & UI\n  initSettingsUI();\n  initSettingsModal();\n  initOptionsToggles();\n  initHelpButton();\n\n  // Credits display\n  initCredits();\n\n  // Unified workflow (silences + takes)\n  initUnifiedWorkflow();\n\n  // Preview handlers (event delegation)\n  initPreviewHandlers();\n\n  // Undo handlers\n  initUndoHandlers();\n\n  // Keyboard shortcuts\n  initKeyboardShortcuts();\n\n  // Advanced features (Razor - legacy)\n  initSlice9();\n\n  // Media folder (v3.5)\n  initMediaFolderHandlers();\n\n  console.log('[SPLICE] Plugin initialized v3.5 (Direct DOM Reconstruction)');\n});\n\n// ============================================================================\n// DEBOUNCED UI UPDATES (Performance: batch DOM updates)\n// ============================================================================\nfunction scheduleUIUpdate(callback) {\n  if (pendingUIUpdate) {\n    cancelAnimationFrame(pendingUIUpdate);\n  }\n  pendingUIUpdate = requestAnimationFrame(() => {\n    callback();\n    pendingUIUpdate = null;\n  });\n}\n\nfunction updateSelectionCounts() {\n  scheduleUIUpdate(() => {\n    if (ui.selectedCount) {\n      ui.selectedCount.textContent = selectedSilenceIndices.size;\n    }\n    updateApplyButton();\n    updateSelectAllCheckbox();\n  });\n}\n\n// ============================================================================\n// KEYBOARD SHORTCUTS\n// ============================================================================\nfunction initKeyboardShortcuts() {\n  document.addEventListener('keydown', (e) => {\n    // Only handle when combined preview is visible\n    const previewVisible = ui.combinedPreview?.style.display !== 'none' &&\n                           ui.combinedPreview?.style.display !== '';\n\n    if (!previewVisible) return;\n\n    const isMac = navigator.platform.toUpperCase().indexOf('MAC') >= 0;\n    const cmdKey = isMac ? e.metaKey : e.ctrlKey;\n\n    // Enter - Apply selected\n    if (e.key === 'Enter' && !e.shiftKey) {\n      e.preventDefault();\n      if (selectedSilenceIndices.size > 0) {\n        ui.applyPreviewBtn?.click();\n      }\n    }\n\n    // Escape - Cancel\n    if (e.key === 'Escape') {\n      e.preventDefault();\n      ui.cancelPreviewBtn?.click();\n    }\n\n    // Cmd/Ctrl+A - Select all\n    if (e.key === 'a' && cmdKey) {\n      e.preventDefault();\n      if (ui.selectAllSilences) {\n        ui.selectAllSilences.checked = true;\n        selectAllSilences(true);\n      }\n    }\n\n    // Cmd/Ctrl+D - Deselect all\n    if (e.key === 'd' && cmdKey) {\n      e.preventDefault();\n      if (ui.selectAllSilences) {\n        ui.selectAllSilences.checked = false;\n        selectAllSilences(false);\n      }\n    }\n\n    // Cmd/Ctrl+I - Invert selection\n    if (e.key === 'i' && cmdKey) {\n      e.preventDefault();\n      invertSilenceSelection();\n    }\n  });\n}\n\n// ============================================================================\n// UNIFIED WORKFLOW (Silence Detection + Takes Detection)\n// ============================================================================\nfunction initUnifiedWorkflow() {\n  if (!ui.goBtn) return;\n\n  ui.goBtn.addEventListener('click', async () => {\n    // Prevent double-click\n    if (isOperationInProgress) {\n      setStatus('Operation in progress...');\n      return;\n    }\n\n    isOperationInProgress = true;\n    ui.goBtn.disabled = true;\n\n    try {\n      showProgress('Preparing audio...');\n\n      // Step 1: Export audio\n      const exportSuccess = await exportAudioInternal();\n      if (!exportSuccess) {\n        throw new Error('Audio export failed');\n      }\n\n      // Step 2: Get settings\n      const sensitivity = parseInt(ui.sensitivitySlider?.value || 50);\n      const useIsolated = ui.sourceIsolated?.checked ?? false;\n      const detectTakes = ui.enableTakesDetection?.checked ?? true;\n\n      // Step 3: Vocal isolation if needed\n      let audioPath = WAV_PATH;\n      if (useIsolated) {\n        // Check isolation access before proceeding\n        const credits = getCurrentCredits();\n        if (!credits || !credits.hasIsolationAccess) {\n          throw new Error('Vocal isolation requires Pro or Team tier. Upgrade to access this feature.');\n        }\n\n        // Show overage warning if applicable\n        const isolationMins = credits.isolationHoursRemaining * 60;\n        if (isolationMins <= 0) {\n          const overageRate = credits.isolationOverageRate || 0.08;\n          showProgress(`Isolating vocals (overage: $${overageRate}/min)...`);\n        } else {\n          showProgress(`Isolating vocals (${isolationMins.toFixed(0)} min remaining)...`);\n        }\n\n        const isolateResult = await isolateVocals();\n        if (isolateResult.success) {\n          audioPath = isolateResult.outputPath;\n          // Update credits display after isolation\n          await refreshCredits();\n        } else {\n          throw new Error(isolateResult.error || 'Vocal isolation failed');\n        }\n      }\n\n      // Step 4: Run detection (parallel if takes enabled)\n      const params = mapSensitivity(sensitivity);\n      let silences = [];\n      let takes = [];\n\n      if (detectTakes) {\n        // Parallel detection for performance\n        showProgress('Detecting silences and analyzing takes...');\n        const [silenceResult, transcriptResult] = await Promise.all([\n          detectSilences(audioPath, params),\n          transcribeAudio()\n        ]);\n\n        silences = silenceResult;\n\n        // Extract takes from transcript result\n        if (transcriptResult.success && transcriptResult.takes) {\n          if (transcriptResult.takes.takes) {\n            takes = transcriptResult.takes.takes;\n          } else if (Array.isArray(transcriptResult.takes)) {\n            takes = transcriptResult.takes;\n          }\n        }\n\n        // Store transcript for reference\n        window.currentTranscript = transcriptResult.transcript;\n      } else {\n        // Just detect silences\n        showProgress('Detecting silences...');\n        silences = await detectSilences(audioPath, params);\n      }\n\n      if (silences.length === 0) {\n        showEmptyState();\n        setStatus('No silences detected. Try increasing sensitivity.');\n        return;\n      }\n\n      // Step 5: Filter silences by takes (protect speech)\n      const filtered = filterSilencesByTakes(silences, takes);\n\n      // Step 6: Add timeline markers (clear old ones first)\n      showProgress('Adding timeline markers...');\n      await clearSpliceMarkers();\n      const markerResult = await addTimelineMarkers(filtered.safe, filtered.protected, takes);\n\n      // Step 7: Show combined preview\n      showCombinedPreview(filtered.safe, filtered.protected, takes);\n\n      const markerInfo = markerResult.added > 0 ? ` (${markerResult.added} markers added)` : '';\n      const statusMsg = takes.length > 0\n        ? `Found ${silences.length} silences, ${filtered.protected.length} protected by ${takes.length} takes${markerInfo}`\n        : `Found ${silences.length} silence(s) - review and apply${markerInfo}`;\n      setStatus(statusMsg);\n\n    } catch (err) {\n      showEmptyState();\n      setStatus('Error: ' + err.message);\n      console.error('[SPLICE] Unified workflow error:', err);\n    } finally {\n      isOperationInProgress = false;\n      ui.goBtn.disabled = false;\n    }\n  });\n}\n\n// ============================================================================\n// TAKE-AWARE SILENCE FILTERING\n// ============================================================================\n/**\n * Filter silences to protect those that overlap with takes (speech segments).\n * Returns { safe: [], protected: [] }\n */\nfunction filterSilencesByTakes(silences, takes) {\n  if (!takes || takes.length === 0) {\n    // No takes = all silences are safe to remove\n    return { safe: silences, protected: [] };\n  }\n\n  const safe = [];\n  const protected_ = [];\n\n  silences.forEach(silence => {\n    // Check if this silence overlaps with any take\n    const overlaps = takes.some(take => {\n      const takeStart = take.startTime || take.start;\n      const takeEnd = take.endTime || take.end;\n      // Overlap exists if silence starts before take ends AND silence ends after take starts\n      return silence.start < takeEnd && silence.end > takeStart;\n    });\n\n    if (overlaps) {\n      protected_.push(silence);\n    } else {\n      safe.push(silence);\n    }\n  });\n\n  console.log(`[SPLICE] Filtered silences: ${safe.length} safe, ${protected_.length} protected`);\n  return { safe, protected: protected_ };\n}\n\n// ============================================================================\n// TIMELINE MARKERS (Visual indicators on Premiere Pro timeline)\n// ============================================================================\n/**\n * Add colored markers to the timeline for silences and takes.\n * - Red markers: silences safe to remove\n * - Yellow markers: silences protected by takes\n * - Blue markers: takes (speech segments)\n *\n * @param {Array} safeToRemove - Silences safe to remove\n * @param {Array} protectedSilences - Silences overlapping speech\n * @param {Array} takes - Detected speech segments\n * @returns {Promise<{added: number, errors: number}>}\n */\nasync function addTimelineMarkers(safeToRemove, protectedSilences, takes) {\n  const results = { added: 0, errors: 0 };\n\n  try {\n    const context = await getActiveSequence();\n    if (!context) {\n      console.warn('[SPLICE] No sequence available for markers');\n      return results;\n    }\n\n    const { sequence, project } = context;\n\n    // Get markers object for the sequence\n    const markers = await ppro.Markers.getMarkers(sequence);\n    if (!markers) {\n      console.warn('[SPLICE] Markers API not available');\n      return results;\n    }\n\n    // Collect all marker actions\n    const markerActions = [];\n\n    // Red markers for safe-to-remove silences\n    for (const silence of safeToRemove) {\n      try {\n        const duration = silence.end - silence.start;\n\n        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n        const startTime = ppro.TickTime.createWithSeconds(silence.start);\n        const durationTime = ppro.TickTime.createWithSeconds(duration);\n\n        const action = markers.createAddMarkerAction(\n          `SPLICE: Remove (${duration.toFixed(2)}s)`,\n          ppro.Marker.MARKER_TYPE_COMMENT,\n          startTime,\n          durationTime,\n          `Silence to remove: ${formatTime(silence.start)} - ${formatTime(silence.end)}`\n        );\n        markerActions.push({ action, colorIndex: 1, type: 'silence' }); // Red = index 1\n      } catch (err) {\n        console.error('[SPLICE] Error creating silence marker:', err);\n        results.errors++;\n      }\n    }\n\n    // Yellow markers for protected silences\n    for (const silence of protectedSilences) {\n      try {\n        const duration = silence.end - silence.start;\n\n        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n        const startTime = ppro.TickTime.createWithSeconds(silence.start);\n        const durationTime = ppro.TickTime.createWithSeconds(duration);\n\n        const action = markers.createAddMarkerAction(\n          `SPLICE: Protected (${duration.toFixed(2)}s)`,\n          ppro.Marker.MARKER_TYPE_COMMENT,\n          startTime,\n          durationTime,\n          `Protected silence (overlaps speech): ${formatTime(silence.start)} - ${formatTime(silence.end)}`\n        );\n        markerActions.push({ action, colorIndex: 4, type: 'protected' }); // Yellow = index 4\n      } catch (err) {\n        console.error('[SPLICE] Error creating protected marker:', err);\n        results.errors++;\n      }\n    }\n\n    // Blue markers for takes (speech segments)\n    for (const take of takes) {\n      try {\n        const start = take.startTime || take.start;\n        const end = take.endTime || take.end;\n        const duration = end - start;\n        const textPreview = take.text?.substring(0, 30) || 'Speech';\n\n        // Use TickTime API directly (no TICKS_PER_SECOND needed)\n        const startTime = ppro.TickTime.createWithSeconds(start);\n        const durationTime = ppro.TickTime.createWithSeconds(duration);\n\n        const action = markers.createAddMarkerAction(\n          `SPLICE: Take`,\n          ppro.Marker.MARKER_TYPE_COMMENT,\n          startTime,\n          durationTime,\n          `Speech segment: ${textPreview}${take.text?.length > 30 ? '...' : ''}`\n        );\n        markerActions.push({ action, colorIndex: 5, type: 'take' }); // Blue = index 5\n      } catch (err) {\n        console.error('[SPLICE] Error creating take marker:', err);\n        results.errors++;\n      }\n    }\n\n    // Execute all marker actions in a single transaction\n    if (markerActions.length > 0) {\n      await project.lockedAccess(async () => {\n        await project.executeTransaction((compoundAction) => {\n          for (const { action } of markerActions) {\n            compoundAction.addAction(action);\n          }\n        }, 'SPLICE: Add Timeline Markers');\n      });\n\n      results.added = markerActions.length;\n      console.log(`[SPLICE] Added ${results.added} timeline markers`);\n    }\n\n  } catch (err) {\n    console.error('[SPLICE] Timeline markers error:', err);\n    results.errors++;\n  }\n\n  return results;\n}\n\n/**\n * Read current SPLICE marker positions from timeline.\n * This allows users to manually adjust markers before applying.\n * Returns silences based on current marker positions (not original detection).\n */\nasync function readMarkersAssilences() {\n  const silences = [];\n\n  try {\n    const context = await getActiveSequence();\n    if (!context) return silences;\n\n    const { sequence } = context;\n    const markers = await ppro.Markers.getMarkers(sequence);\n    if (!markers) return silences;\n\n    const allMarkers = await markers.getMarkers();\n    if (!allMarkers || allMarkers.length === 0) return silences;\n\n    for (const marker of allMarkers) {\n      const name = await marker.getName();\n\n      // Only read \"Remove\" markers (red ones that will be cut)\n      if (name && name.startsWith('SPLICE: Remove')) {\n        const startTickTime = await marker.getStart();\n        const durationTickTime = await marker.getDuration();\n\n        // Use TickTime.seconds property to get seconds directly (no TICKS_PER_SECOND needed)\n        const start = startTickTime.seconds;\n        const duration = durationTickTime.seconds;\n        const end = start + duration;\n\n        silences.push({\n          start,\n          end,\n          duration,\n          fromMarker: true  // Flag to indicate this came from marker adjustment\n        });\n      }\n    }\n\n    // Sort by start time\n    silences.sort((a, b) => a.start - b.start);\n    console.log(`[SPLICE] Read ${silences.length} silences from adjusted markers`);\n\n  } catch (err) {\n    console.error('[SPLICE] Error reading markers:', err);\n  }\n\n  return silences;\n}\n\n/**\n * Remove all SPLICE markers from the timeline.\n * Call this before adding new markers to avoid duplicates.\n */\nasync function clearSpliceMarkers() {\n  try {\n    const context = await getActiveSequence();\n    if (!context) return 0;\n\n    const { sequence, project } = context;\n    const markers = await ppro.Markers.getMarkers(sequence);\n    if (!markers) return 0;\n\n    // Get all markers\n    const allMarkers = await markers.getMarkers();\n    if (!allMarkers || allMarkers.length === 0) return 0;\n\n    // Filter SPLICE markers\n    const spliceMarkers = [];\n    for (const marker of allMarkers) {\n      const name = await marker.getName();\n      if (name && name.startsWith('SPLICE:')) {\n        spliceMarkers.push(marker);\n      }\n    }\n\n    if (spliceMarkers.length === 0) return 0;\n\n    // Remove all SPLICE markers in a single transaction\n    await project.lockedAccess(async () => {\n      await project.executeTransaction((compoundAction) => {\n        for (const marker of spliceMarkers) {\n          const removeAction = markers.createRemoveMarkerAction(marker);\n          compoundAction.addAction(removeAction);\n        }\n      }, 'SPLICE: Clear Markers');\n    });\n\n    console.log(`[SPLICE] Cleared ${spliceMarkers.length} previous markers`);\n    return spliceMarkers.length;\n\n  } catch (err) {\n    console.error('[SPLICE] Error clearing markers:', err);\n    return 0;\n  }\n}\n\n// ============================================================================\n// COMBINED PREVIEW (Silences + Takes with colored markers)\n// ============================================================================\n/**\n * Show combined preview with:\n * - Red markers: silences safe to remove\n * - Yellow markers: silences protected by takes (won't be removed)\n * - Blue markers: takes (for reference)\n */\nfunction showCombinedPreview(safeToRemove, protectedSilences, takes) {\n  // Update global state\n  safeSilences = safeToRemove;\n  previewSilences = [...safeToRemove, ...protectedSilences];\n  previewTakes = takes;\n  selectedSilenceIndices = new Set(safeToRemove.map((_, i) => i));\n\n  // Hide other views\n  hideAllViews();\n\n  // Update summary counts\n  if (ui.silenceCountDisplay) ui.silenceCountDisplay.textContent = safeToRemove.length;\n  if (ui.takeCountDisplay) ui.takeCountDisplay.textContent = takes.length;\n  if (ui.selectedCount) ui.selectedCount.textContent = safeToRemove.length;\n\n  // Build combined items list (sorted by time)\n  const items = [\n    ...safeToRemove.map((s, i) => ({ type: 'silence', index: i, start: s.start, end: s.end, data: s })),\n    ...protectedSilences.map(s => ({ type: 'protected', start: s.start, end: s.end, data: s })),\n    ...takes.map(t => ({ type: 'take', start: t.startTime || t.start, end: t.endTime || t.end, data: t }))\n  ].sort((a, b) => a.start - b.start);\n\n  // Build preview list using DocumentFragment\n  const fragment = document.createDocumentFragment();\n\n  items.forEach((item) => {\n    const duration = item.end - item.start;\n    const div = document.createElement('div');\n\n    if (item.type === 'silence') {\n      // Safe to remove - red marker with checkbox\n      div.className = 'preview-item silence-marker';\n      div.dataset.index = item.index;\n      div.innerHTML = `\n        <input type=\"checkbox\" class=\"preview-item-check\" checked data-index=\"${item.index}\">\n        <div class=\"preview-item-info\">\n          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n          <div class=\"preview-item-duration\">${duration.toFixed(2)}s silence</div>\n        </div>\n        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek to this silence\">&gt;</button>\n      `;\n    } else if (item.type === 'protected') {\n      // Protected by take - yellow marker, no checkbox\n      div.className = 'preview-item protected-silence';\n      div.innerHTML = `\n        <div style=\"width: 20px; margin-right: 10px;\"></div>\n        <div class=\"preview-item-info\">\n          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n          <div class=\"preview-item-duration\">${duration.toFixed(2)}s (overlaps speech)</div>\n        </div>\n        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek\">&gt;</button>\n      `;\n    } else if (item.type === 'take') {\n      // Take (speech segment) - blue marker, no checkbox\n      div.className = 'preview-item take-marker';\n      const textPreview = item.data.text?.substring(0, 40) || 'Speech segment';\n      div.innerHTML = `\n        <div style=\"width: 20px; margin-right: 10px;\"></div>\n        <div class=\"preview-item-info\">\n          <div class=\"preview-item-time\">${formatTime(item.start)} - ${formatTime(item.end)}</div>\n          <div class=\"preview-item-duration\">${textPreview}${item.data.text?.length > 40 ? '...' : ''}</div>\n        </div>\n        <button class=\"preview-item-seek\" data-time=\"${item.start}\" title=\"Seek to take\">&gt;</button>\n      `;\n    }\n\n    fragment.appendChild(div);\n  });\n\n  if (ui.previewList) {\n    ui.previewList.textContent = '';\n    ui.previewList.appendChild(fragment);\n  }\n\n  // Reset UI state\n  if (ui.selectAllSilences) ui.selectAllSilences.checked = true;\n  updateApplyButton();\n\n  // Show combined preview section\n  if (ui.combinedPreview) ui.combinedPreview.style.display = 'block';\n}\n\nfunction initPreviewHandlers() {\n  // EVENT DELEGATION: Single listener on parent instead of many on children\n  if (ui.previewList) {\n    // Handle checkbox changes\n    ui.previewList.addEventListener('change', (e) => {\n      if (e.target.classList.contains('preview-item-check')) {\n        const index = parseInt(e.target.dataset.index);\n        const item = e.target.closest('.preview-item');\n\n        if (e.target.checked) {\n          selectedSilenceIndices.add(index);\n          item?.classList.remove('excluded');\n        } else {\n          selectedSilenceIndices.delete(index);\n          item?.classList.add('excluded');\n        }\n        updateSelectionCounts();\n      }\n    });\n\n    // Handle seek button clicks\n    ui.previewList.addEventListener('click', async (e) => {\n      if (e.target.classList.contains('preview-item-seek')) {\n        e.stopPropagation();\n        const btn = e.target;\n        const time = parseFloat(btn.dataset.time);\n\n        // Show loading state\n        btn.classList.add('loading');\n        btn.textContent = '...';\n\n        try {\n          await seekToTime(time);\n        } finally {\n          btn.classList.remove('loading');\n          btn.textContent = '>';\n        }\n      }\n    });\n  }\n\n  // Apply button\n  if (ui.applyPreviewBtn) {\n    ui.applyPreviewBtn.addEventListener('click', async () => {\n      if (isOperationInProgress) return;\n      isOperationInProgress = true;\n      ui.applyPreviewBtn.disabled = true;\n\n      const originalText = ui.applyPreviewBtn.textContent;\n      ui.applyPreviewBtn.textContent = 'Applying...';\n\n      try {\n        await applySelectedSilences();\n      } catch (err) {\n        setStatus('Error: ' + err.message);\n        console.error('[SPLICE] Apply error:', err);\n      } finally {\n        ui.applyPreviewBtn.textContent = originalText;\n        ui.applyPreviewBtn.disabled = false;\n        isOperationInProgress = false;\n      }\n    });\n  }\n\n  // Cancel button\n  if (ui.cancelPreviewBtn) {\n    ui.cancelPreviewBtn.addEventListener('click', async () => {\n      await cancelPreview();\n    });\n  }\n\n  // Select all checkbox\n  if (ui.selectAllSilences) {\n    ui.selectAllSilences.addEventListener('change', (e) => {\n      selectAllSilences(e.target.checked);\n    });\n  }\n\n  // Invert selection button\n  if (ui.invertSelectionBtn) {\n    ui.invertSelectionBtn.addEventListener('click', () => {\n      invertSilenceSelection();\n    });\n  }\n\n  // Duration filter buttons\n  document.querySelectorAll('.preview-filters .filter-btn[data-min]').forEach(btn => {\n    btn.addEventListener('click', () => {\n      const min = parseFloat(btn.dataset.min);\n      const max = parseFloat(btn.dataset.max);\n      selectByDuration(min, max);\n    });\n  });\n\n  // Re-run detection button\n  if (ui.reRunDetectionBtn) {\n    ui.reRunDetectionBtn.addEventListener('click', () => {\n      // Hide preview, show options, let user adjust sensitivity\n      if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n      if (ui.optionsPanel) ui.optionsPanel.classList.remove('collapsed');\n      if (ui.optionsToggle) ui.optionsToggle.classList.add('expanded');\n      showEmptyState();\n      setStatus('Adjust sensitivity and click GO again');\n    });\n  }\n\n  // Build Sequence button (v3.5 - direct DOM reconstruction)\n  if (ui.buildSequenceBtn) {\n    ui.buildSequenceBtn.addEventListener('click', async () => {\n      if (isOperationInProgress) return;\n\n      // Check if we have detection data\n      if (safeSilences.length === 0 && previewTakes.length === 0) {\n        setStatus('No detection data. Run analysis first.');\n        return;\n      }\n\n      isOperationInProgress = true;\n      ui.buildSequenceBtn.disabled = true;\n      const originalText = ui.buildSequenceBtn.textContent;\n      ui.buildSequenceBtn.textContent = 'Building...';\n\n      try {\n        showProgress('Building new sequence...');\n\n        const result = await buildSequenceWithCutList();\n\n        if (result.success) {\n          // Hide preview and show success\n          if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n          showEmptyState();\n\n          const metadata = result.metadata || {};\n          setStatus(`New sequence created: ${metadata.silencesRemoved || 0} silences removed, ${metadata.takesDetected || 0} takes detected`);\n\n          // Clear markers after successful build\n          await clearSpliceMarkers();\n\n          // Clear preview state\n          previewSilences = [];\n          safeSilences = [];\n          protectedSilences = [];\n          previewTakes = [];\n          selectedSilenceIndices.clear();\n        } else {\n          setStatus('Build failed: ' + (result.error || 'Unknown error'));\n        }\n\n      } catch (err) {\n        console.error('[SPLICE] Build sequence error:', err);\n        setStatus('Build error: ' + err.message);\n      } finally {\n        ui.buildSequenceBtn.textContent = originalText;\n        ui.buildSequenceBtn.disabled = false;\n        isOperationInProgress = false;\n      }\n    });\n  }\n}\n\nfunction selectAllSilences(checked) {\n  selectedSilenceIndices.clear();\n\n  if (checked) {\n    safeSilences.forEach((_, i) => selectedSilenceIndices.add(i));\n  }\n\n  // Batch update DOM (only silence-marker items have checkboxes)\n  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n    const index = parseInt(checkbox.dataset.index);\n    checkbox.checked = checked;\n    const item = checkbox.closest('.preview-item');\n    if (checked) {\n      item?.classList.remove('excluded');\n    } else {\n      item?.classList.add('excluded');\n    }\n  });\n\n  updateSelectionCounts();\n}\n\nfunction invertSilenceSelection() {\n  const allIndices = new Set(safeSilences.map((_, i) => i));\n  const newSelection = new Set([...allIndices].filter(i => !selectedSilenceIndices.has(i)));\n  selectedSilenceIndices = newSelection;\n\n  // Update DOM\n  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n    const index = parseInt(checkbox.dataset.index);\n    const isSelected = selectedSilenceIndices.has(index);\n    checkbox.checked = isSelected;\n    const item = checkbox.closest('.preview-item');\n    if (isSelected) {\n      item?.classList.remove('excluded');\n    } else {\n      item?.classList.add('excluded');\n    }\n  });\n\n  updateSelectionCounts();\n}\n\nfunction selectByDuration(minSec, maxSec) {\n  selectedSilenceIndices.clear();\n\n  safeSilences.forEach((silence, index) => {\n    const duration = silence.end - silence.start;\n    if (duration >= minSec && duration < maxSec) {\n      selectedSilenceIndices.add(index);\n    }\n  });\n\n  // Update DOM\n  document.querySelectorAll('.preview-item.silence-marker .preview-item-check').forEach((checkbox) => {\n    const index = parseInt(checkbox.dataset.index);\n    const isSelected = selectedSilenceIndices.has(index);\n    checkbox.checked = isSelected;\n    const item = checkbox.closest('.preview-item');\n    if (isSelected) {\n      item?.classList.remove('excluded');\n    } else {\n      item?.classList.add('excluded');\n    }\n  });\n\n  updateSelectionCounts();\n  setStatus(`Selected ${selectedSilenceIndices.size} silences (${minSec}s - ${maxSec}s)`);\n}\n\nasync function cancelPreview() {\n  // Keep silences available for Razor if detected\n  const hadSilences = safeSilences.length > 0;\n\n  previewSilences = [];\n  safeSilences = [];\n  protectedSilences = [];\n  previewTakes = [];\n  selectedSilenceIndices.clear();\n  showEmptyState();\n\n  // Clear timeline markers\n  const cleared = await clearSpliceMarkers();\n  const markerInfo = cleared > 0 ? ` (${cleared} markers removed)` : '';\n  setStatus('Preview cancelled' + markerInfo);\n\n  // Show Razor section if silences were detected\n  if (hadSilences && ui.advancedSection) {\n    ui.advancedSection.classList.remove('hidden');\n  }\n}\n\nasync function applySelectedSilences() {\n  if (selectedSilenceIndices.size === 0) {\n    setStatus('No silences selected');\n    return;\n  }\n\n  showProgress('Reading marker positions...');\n\n  // Read current marker positions from timeline\n  // This allows users to manually adjust markers before applying\n  const markerSilences = await readMarkersAssilences();\n\n  let silencesToApply;\n  let adjustedCount = 0;\n\n  if (markerSilences.length > 0) {\n    // Use marker positions (may have been adjusted by user)\n    silencesToApply = markerSilences;\n    adjustedCount = markerSilences.length;\n    console.log(`[SPLICE] Using ${adjustedCount} silences from adjusted markers`);\n  } else {\n    // Fallback to original detection data\n    silencesToApply = safeSilences.filter((_, i) => selectedSilenceIndices.has(i));\n    console.log(`[SPLICE] Using ${silencesToApply.length} silences from original detection`);\n  }\n\n  if (silencesToApply.length === 0) {\n    setStatus('No silences to apply');\n    return;\n  }\n\n  // Update global silences for the apply function\n  setCurrentSilences(silencesToApply);\n\n  showProgress('Applying changes...');\n\n  // Apply to timeline\n  const result = await removeSilencesFromTimeline();\n\n  // Clear markers after applying\n  await clearSpliceMarkers();\n\n  // Hide preview\n  if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n\n  // Show results\n  showSilenceResults({\n    count: silencesToApply.length,\n    timeSaved: silencesToApply.reduce((sum, s) => sum + (s.end - s.start), 0),\n    clipsModified: result\n  });\n\n  // Show advanced section\n  if (ui.advancedSection) ui.advancedSection.classList.remove('hidden');\n\n  const adjustedInfo = adjustedCount > 0 ? ' (from adjusted markers)' : '';\n  setStatus(`Removed ${silencesToApply.length} silence(s) - ${result} clip(s) modified${adjustedInfo}`);\n\n  // Refresh credits (await to prevent race conditions)\n  try {\n    await refreshCredits();\n  } catch (err) {\n    console.warn('[SPLICE] Credits refresh failed:', err);\n  }\n\n  // Clear preview state\n  previewSilences = [];\n  safeSilences = [];\n  protectedSilences = [];\n  previewTakes = [];\n  selectedSilenceIndices.clear();\n}\n\n// ============================================================================\n// UI HELPERS\n// ============================================================================\nfunction updateApplyButton() {\n  if (ui.applyPreviewBtn) {\n    const count = selectedSilenceIndices.size;\n    ui.applyPreviewBtn.disabled = count === 0;\n    ui.applyPreviewBtn.textContent = count === 0 ? 'Select silences' : `Apply ${count} Selected`;\n  }\n}\n\nfunction updateSelectAllCheckbox() {\n  if (ui.selectAllSilences) {\n    ui.selectAllSilences.checked = selectedSilenceIndices.size === safeSilences.length && safeSilences.length > 0;\n  }\n}\n\nfunction hideAllViews() {\n  if (ui.progressContainer) ui.progressContainer.style.display = 'none';\n  if (ui.resultsEmpty) ui.resultsEmpty.style.display = 'none';\n  if (ui.silenceResults) ui.silenceResults.style.display = 'none';\n  if (ui.combinedPreview) ui.combinedPreview.style.display = 'none';\n}\n\nfunction showProgress(message) {\n  hideAllViews();\n  if (ui.progressContainer) ui.progressContainer.style.display = 'block';\n  if (ui.progressText) ui.progressText.textContent = message;\n}\n\nfunction showEmptyState() {\n  hideAllViews();\n  if (ui.resultsEmpty) ui.resultsEmpty.style.display = 'block';\n  if (ui.advancedSection) ui.advancedSection.classList.add('hidden');\n}\n\nfunction showSilenceResults({ count, timeSaved, clipsModified }) {\n  hideAllViews();\n\n  // Update the results display\n  if (ui.resultsCount) ui.resultsCount.textContent = count;\n  if (ui.timeSaved) ui.timeSaved.textContent = timeSaved.toFixed(1) + 's';\n  if (ui.clipsModified) ui.clipsModified.textContent = clipsModified;\n\n  if (ui.silenceResults) ui.silenceResults.style.display = 'block';\n}\n\nfunction formatTime(seconds) {\n  if (typeof seconds !== 'number' || isNaN(seconds)) return '0:00';\n  const mins = Math.floor(seconds / 60);\n  const secs = Math.floor(seconds % 60);\n  return `${mins}:${secs.toString().padStart(2, '0')}`;\n}\n\nfunction setStatus(message) {\n  if (ui.status) ui.status.textContent = message;\n}\n\n// ============================================================================\n// UNDO HANDLERS\n// ============================================================================\nfunction initUndoHandlers() {\n  if (ui.undoBtn) {\n    ui.undoBtn.addEventListener('click', () => {\n      try {\n        ppro.app.executeCommand('Edit.Undo');\n        showEmptyState();\n        setStatus('Changes undone');\n      } catch (e) {\n        setStatus('Undo failed - use Cmd+Z');\n      }\n    });\n  }\n}\n\n// ============================================================================\n// MEDIA FOLDER HANDLERS (v3.5)\n// ============================================================================\nfunction initMediaFolderHandlers() {\n  // Update display on load\n  updateMediaFolderDisplay();\n\n  // Set media folder button\n  if (ui.setMediaFolderBtn) {\n    ui.setMediaFolderBtn.addEventListener('click', async () => {\n      ui.setMediaFolderBtn.disabled = true;\n      ui.setMediaFolderBtn.textContent = 'Selecting...';\n\n      try {\n        const result = await setupMediaFolder();\n        if (result.success) {\n          updateMediaFolderDisplay();\n          setStatus(`Media folder set: ${result.path}`);\n        } else {\n          setStatus(result.error || 'Failed to set media folder');\n        }\n      } catch (err) {\n        setStatus('Error: ' + err.message);\n      } finally {\n        ui.setMediaFolderBtn.disabled = false;\n        ui.setMediaFolderBtn.textContent = 'Set Folder';\n      }\n    });\n  }\n\n  // Clear media folder button\n  if (ui.clearMediaFolderBtn) {\n    ui.clearMediaFolderBtn.addEventListener('click', () => {\n      clearMediaFolder();\n      updateMediaFolderDisplay();\n      setStatus('Media folder cleared');\n    });\n  }\n}\n\nfunction updateMediaFolderDisplay() {\n  if (ui.mediaFolderDisplay) {\n    const path = getMediaFolderPath();\n    if (path) {\n      // Truncate long paths for display\n      const displayPath = path.length > 40\n        ? '...' + path.slice(-37)\n        : path;\n      ui.mediaFolderDisplay.textContent = displayPath;\n      ui.mediaFolderDisplay.title = path; // Full path on hover\n      ui.mediaFolderDisplay.style.color = '#4a9eff';\n    } else {\n      ui.mediaFolderDisplay.textContent = 'Not configured';\n      ui.mediaFolderDisplay.title = '';\n      ui.mediaFolderDisplay.style.color = '#888';\n    }\n  }\n}\n\n// ============================================================================\n// BUILD SEQUENCE (v3.5 - Direct DOM Reconstruction)\n// ============================================================================\n\n// 1. ROBUST CONSTANTS (Fixes 'undefined' error)\n// We define these integers explicitly because UXP API versions vary on where they store Enums.\nconst ProjectItemType = {\n    CLIP: 1,\n    BIN: 2,\n    ROOT: 3,\n    FILE: 4\n};\n\n/**\n * Find the first media item in the project bin (recursively)\n * @param {Object} item - ProjectItem (usually rootItem)\n * @returns {Promise<Object|null>} First media item found or null\n */\nasync function findFirstMediaItem(item) {\n  if (!item) return null;\n\n  // Fix: Use local Constant instead of app.Constants.ProjectItemType\n  if (item.type === ProjectItemType.CLIP) {\n    return item;\n  }\n\n  // Search Children (Bin or Root)\n  // Note: item.children is a collection, check numItems\n  if (item.children && item.children.numItems > 0) {\n    for (let i = 0; i < item.children.numItems; i++) {\n      const child = item.children[i];\n      const found = await findFirstMediaItem(child);\n      if (found) return found;\n    }\n  }\n\n  return null;\n}\n\n/**\n * Build a sequence from detection data locally\n * Used for testing via console: await buildSequenceV35(silences)\n */\nasync function buildSequenceV35(silences, takes = []) {\n    if (!window.spliceBuilder) {\n        throw new Error('Builder not initialized. Check if builder.js is loaded.');\n    }\n\n    const app = require('premierepro');\n    const project = await app.Project.getActiveProject();\n    if (!project) throw new Error(\"No active project found.\");\n\n    const rootItem = await project.getRootItem();\n    const sourceProjectItem = await findFirstMediaItem(rootItem);\n\n    if (!sourceProjectItem) {\n        throw new Error('No media items found in project bin.');\n    }\n\n    console.log(`[SPLICE] Found source media: ${sourceProjectItem.name}`);\n\n    return await window.spliceBuilder.buildSequenceFromDetection(silences, takes, sourceProjectItem);\n}\n\n/**\n * Request a cut list from the backend and build the sequence.\n * Full v3.5 workflow.\n */\nasync function buildSequenceWithCutList() {\n  if (!window.spliceBuilder) {\n    throw new Error('Builder not initialized');\n  }\n\n  // Ensure BACKEND_URL is available\n  const apiUrl = (typeof BACKEND_URL !== 'undefined') ? BACKEND_URL : 'http://localhost:3000';\n\n  if (typeof showProgress === 'function') showProgress('Getting source clip info...');\n\n  const app = require('premierepro');\n  const project = await app.Project.getActiveProject();\n  if (!project) throw new Error(\"No active project.\");\n\n  const rootItem = await project.getRootItem();\n  const sourceProjectItem = await findFirstMediaItem(rootItem);\n\n  if (!sourceProjectItem) {\n    throw new Error('No media items found in project. Please import a media file first.');\n  }\n\n  // Use properties strictly\n  const sourceName = sourceProjectItem.name;\n  const sourcePath = sourceProjectItem.treePath;\n\n  const durationObj = await sourceProjectItem.getDuration();\n  const duration = durationObj ? durationObj.seconds : 0;\n\n  // Retrieve Globals\n  const silences = window.splicePreviewState?.getSelectedSilences() || (typeof safeSilences !== 'undefined' ? safeSilences : []);\n  const takes = (typeof previewTakes !== 'undefined' ? previewTakes : []);\n\n  if (silences.length === 0 && (!takes || takes.length === 0)) {\n     throw new Error('No detection data found. Run analysis first.');\n  }\n\n  if (typeof showProgress === 'function') showProgress('Generating cut list...');\n\n  const response = await fetch(`${apiUrl}/cut-list`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      sourceName,\n      sourcePath,\n      duration,\n      silences,\n      takes\n    })\n  });\n\n  if (!response.ok) {\n    const err = await response.json();\n    throw new Error(err.error || 'Failed to generate cut list');\n  }\n\n  const data = await response.json();\n  const cutList = data.cutList;\n\n  if (typeof showProgress === 'function') showProgress('Building sequence...');\n\n  const result = await window.spliceBuilder.buildSequenceFromCutList(cutList, sourceProjectItem);\n\n  if (!result.success) {\n    throw new Error(result.error || 'Failed to build sequence');\n  }\n\n  return {\n    success: true,\n    clipsAdded: result.clipsAdded,\n    metadata: cutList.metadata\n  };\n}\n\n// Expose functions globally for UI and testing\nif (typeof window !== 'undefined') {\n    window.buildSequenceV35 = buildSequenceV35;\n    window.buildSequenceWithCutList = buildSequenceWithCutList;\n    window.findFirstMediaItem = findFirstMediaItem;\n}\n\n// ============================================================================\n// TIMELINE SEEK (with proper error handling)\n// ============================================================================\nasync function seekToTime(seconds) {\n  // Validate input\n  if (typeof seconds !== 'number' || isNaN(seconds) || !isFinite(seconds)) {\n    setStatus('Invalid seek time');\n    return;\n  }\n\n  if (seconds < 0) {\n    seconds = 0;\n  }\n\n  try {\n    const context = await getActiveSequence();\n    if (!context) {\n      setStatus('No sequence available');\n      return;\n    }\n\n    const { sequence } = context;\n\n    // Use TickTime API to convert seconds to the format needed\n    const position = ppro.TickTime.createWithSeconds(seconds);\n\n    await sequence.setPlayerPosition(position);\n    setStatus(`Seeked to ${formatTime(seconds)}`);\n  } catch (err) {\n    console.error('[SPLICE] Seek error:', err);\n    setStatus('Seek failed: ' + (err.message || 'Unknown error'));\n  }\n}\n\n// ============================================================================\n// AUDIO EXPORT & BACKEND CALLS\n// ============================================================================\nasync function exportAudioInternal() {\n  try {\n    const context = await getActiveSequence();\n    if (!context) {\n      throw new Error('No project or sequence open');\n    }\n\n    const { sequence } = context;\n    const encoderManager = ppro.EncoderManager.getManager();\n    if (!encoderManager) {\n      throw new Error('EncoderManager not available');\n    }\n\n    const result = await encoderManager.exportSequence(\n      sequence,\n      ppro.EncoderManager.EXPORT_IMMEDIATELY,\n      WAV_PATH,\n      WAV_PRESET_PATH,\n      true\n    );\n\n    return result;\n  } catch (err) {\n    console.error('[SPLICE] Export error:', err);\n    return false;\n  }\n}\n\nasync function isolateVocals() {\n  const response = await fetch(`${BACKEND_URL}/isolate-vocals`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ audioPath: WAV_PATH })\n  });\n\n  if (!response.ok) {\n    const err = await response.json();\n    return { success: false, error: err.error || 'Isolation request failed' };\n  }\n\n  return await response.json();\n}\n\nasync function detectSilences(audioPath, params) {\n  const response = await fetch(`${BACKEND_URL}/silences-audio`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({\n      wavPath: audioPath,\n      threshold: params.dbThreshold,\n      minDuration: params.minDuration,\n      padding: params.padding\n    })\n  });\n\n  if (!response.ok) {\n    const err = await response.json();\n    throw new Error(err.error || 'Silence detection failed');\n  }\n\n  const data = await response.json();\n  if (!data.success) {\n    throw new Error(data.error || 'Silence detection failed');\n  }\n\n  // Store silences globally for razor workflow\n  currentSilences = data.silences;\n\n  return data.silences;\n}\n\nasync function transcribeAudio() {\n  const response = await fetch(`${BACKEND_URL}/analyze`, {\n    method: 'POST',\n    headers: { 'Content-Type': 'application/json' },\n    body: JSON.stringify({ wavPath: WAV_PATH })\n  });\n\n  if (!response.ok) {\n    const err = await response.json();\n    return { success: false, error: err.error || 'Transcription failed' };\n  }\n\n  const data = await response.json();\n  return {\n    success: data.success,\n    transcript: data.transcript,\n    takes: data.takes,\n    error: data.error\n  };\n}\n","numLines":1421,"startLine":1,"totalLines":1421}}}
{"parentUuid":"b7a078ae-5d24-4a7f-ae87-a46f2a81a28f","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01TkmqbfHuQsKSwrTUd1Hr6h","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me check for a few more specific patterns that could indicate issues:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6683,"cache_creation_input_tokens":58561,"cache_read_input_tokens":16420,"cache_creation":{"ephemeral_5m_input_tokens":58561,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CWU2pnNBSP4AQYbQLqqYE","type":"assistant","uuid":"95009596-cd75-4977-b08e-9df26d2aedf4","timestamp":"2025-12-25T20:25:18.290Z"}
{"parentUuid":"95009596-cd75-4977-b08e-9df26d2aedf4","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01TkmqbfHuQsKSwrTUd1Hr6h","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015zvWfcSTra3tkctHLfbJH2","name":"Grep","input":{"pattern":"new Map\\(\\)|Map\\(\\)","path":"/Users/imorgado/SPLICE/splice-backend","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6683,"cache_creation_input_tokens":58561,"cache_read_input_tokens":16420,"cache_creation":{"ephemeral_5m_input_tokens":58561,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CWU2pnNBSP4AQYbQLqqYE","type":"assistant","uuid":"fd05eef9-1efc-46c5-a19f-8a3da106b935","timestamp":"2025-12-25T20:25:19.841Z"}
{"parentUuid":"fd05eef9-1efc-46c5-a19f-8a3da106b935","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01TkmqbfHuQsKSwrTUd1Hr6h","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VGYeitBQGKQ44YcHLkTiEm","name":"Grep","input":{"pattern":"pool\\.connect|await pool\\.","path":"/Users/imorgado/SPLICE/splice-backend","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6683,"cache_creation_input_tokens":58561,"cache_read_input_tokens":16420,"cache_creation":{"ephemeral_5m_input_tokens":58561,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CWU2pnNBSP4AQYbQLqqYE","type":"assistant","uuid":"7dc1caf9-1cef-4e10-aaaf-258c86b03477","timestamp":"2025-12-25T20:25:21.141Z"}
{"parentUuid":"7dc1caf9-1cef-4e10-aaaf-258c86b03477","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01TkmqbfHuQsKSwrTUd1Hr6h","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01748xxudoyR5Mx454JyP5rf","name":"Grep","input":{"pattern":"fs\\.readFileSync|fs\\.writeFileSync","path":"/Users/imorgado/SPLICE/splice-backend","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6683,"cache_creation_input_tokens":58561,"cache_read_input_tokens":16420,"cache_creation":{"ephemeral_5m_input_tokens":58561,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CWU2pnNBSP4AQYbQLqqYE","type":"assistant","uuid":"8d9a3f0c-cdd5-4b26-8ff5-70c26b35be8d","timestamp":"2025-12-25T20:25:22.939Z"}
{"parentUuid":"8d9a3f0c-cdd5-4b26-8ff5-70c26b35be8d","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VGYeitBQGKQ44YcHLkTiEm","type":"tool_result","content":"services/usageTracking.js:47:  const client = await pool.connect();\nservices/usageTracking.js:121:  const client = await pool.connect();\nservices/usageTracking.js:247:  const client = await pool.connect();\nservices/usageTracking.js:314:  const client = await pool.connect();\nservices/usageTracking.js:358:  await pool.query(\nservices/usageTracking.js:377:  const client = await pool.connect();\nservices/usageTracking.js:414:  const result = await pool.query(\nservices/usageTracking.js:428:  const result = await pool.query(\nservices/usageTracking.js:440:    await pool.query("}]},"uuid":"7b26a711-9f36-4479-822d-57794447e73e","timestamp":"2025-12-25T20:25:23.578Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"services/usageTracking.js:47:  const client = await pool.connect();\nservices/usageTracking.js:121:  const client = await pool.connect();\nservices/usageTracking.js:247:  const client = await pool.connect();\nservices/usageTracking.js:314:  const client = await pool.connect();\nservices/usageTracking.js:358:  await pool.query(\nservices/usageTracking.js:377:  const client = await pool.connect();\nservices/usageTracking.js:414:  const result = await pool.query(\nservices/usageTracking.js:428:  const result = await pool.query(\nservices/usageTracking.js:440:    await pool.query(","numLines":9}}
{"parentUuid":"7b26a711-9f36-4479-822d-57794447e73e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015zvWfcSTra3tkctHLfbJH2","type":"tool_result","content":"services/transcription.js:17:const transcriptCache = new Map();\ntests/phase5-e2e.test.js:360:  const cache = new Map();\ntests/transcription-e2e.test.js:213:  const cache = new Map();\nserver.js:1413:const batchJobs = new Map();"}]},"uuid":"0d2863e5-27fe-4327-a572-12fe57eb9f45","timestamp":"2025-12-25T20:25:23.705Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"services/transcription.js:17:const transcriptCache = new Map();\ntests/phase5-e2e.test.js:360:  const cache = new Map();\ntests/transcription-e2e.test.js:213:  const cache = new Map();\nserver.js:1413:const batchJobs = new Map();","numLines":4}}
{"parentUuid":"0d2863e5-27fe-4327-a572-12fe57eb9f45","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01748xxudoyR5Mx454JyP5rf","type":"tool_result","content":"services/rmsSilenceDetection.js:173:    const rawData = fs.readFileSync(tempFile);\nservices/vocalIsolation.js:47:  const audioBuffer = fs.readFileSync(audioPath);\nservices/captionExporter.js:240:  fs.writeFileSync(outputPath, content, 'utf8');\nservices/multitrackAnalysis.js:195:    const rawData = fs.readFileSync(tempFile);\ntests/phase5-e2e.test.js:326:  const content = fs.readFileSync(transcriptionPath, 'utf8');\ntests/phase5-e2e.test.js:411:  const settingsContent = fs.readFileSync(settingsPath, 'utf8');\nserver.js:82:      key: fs.readFileSync(keyPath),\nserver.js:83:      cert: fs.readFileSync(certPath)"}]},"uuid":"03643257-0930-4c36-bd65-3ac46c98709e","timestamp":"2025-12-25T20:25:23.839Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"services/rmsSilenceDetection.js:173:    const rawData = fs.readFileSync(tempFile);\nservices/vocalIsolation.js:47:  const audioBuffer = fs.readFileSync(audioPath);\nservices/captionExporter.js:240:  fs.writeFileSync(outputPath, content, 'utf8');\nservices/multitrackAnalysis.js:195:    const rawData = fs.readFileSync(tempFile);\ntests/phase5-e2e.test.js:326:  const content = fs.readFileSync(transcriptionPath, 'utf8');\ntests/phase5-e2e.test.js:411:  const settingsContent = fs.readFileSync(settingsPath, 'utf8');\nserver.js:82:      key: fs.readFileSync(keyPath),\nserver.js:83:      cert: fs.readFileSync(certPath)","numLines":8}}
{"parentUuid":"03643257-0930-4c36-bd65-3ac46c98709e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01JpMA9MRB6tEWgdnRbDu6XT","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01DBByRa75sbJW3mBXhchwA6","name":"Grep","input":{"pattern":"catch \\(err\\)|\\.catch\\(","path":"/Users/imorgado/SPLICE/splice-backend/services","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":7657,"cache_read_input_tokens":74981,"cache_creation":{"ephemeral_5m_input_tokens":7657,"ephemeral_1h_input_tokens":0},"output_tokens":21,"service_tier":"standard"}},"requestId":"req_011CWU2qWpa6QWNjPDxiKPzQ","type":"assistant","uuid":"469d4e00-d1b6-4559-8451-bac739a5bfbd","timestamp":"2025-12-25T20:25:28.015Z"}
{"parentUuid":"469d4e00-d1b6-4559-8451-bac739a5bfbd","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01JpMA9MRB6tEWgdnRbDu6XT","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017MLSH8FtAGg98JfszBhYC2","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/middleware/rateLimiter.js"}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":7657,"cache_read_input_tokens":74981,"cache_creation":{"ephemeral_5m_input_tokens":7657,"ephemeral_1h_input_tokens":0},"output_tokens":168,"service_tier":"standard"}},"requestId":"req_011CWU2qWpa6QWNjPDxiKPzQ","type":"assistant","uuid":"54c45cf8-c25f-4b1e-acd3-ba1579d73af5","timestamp":"2025-12-25T20:25:29.130Z"}
{"parentUuid":"54c45cf8-c25f-4b1e-acd3-ba1579d73af5","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_017MLSH8FtAGg98JfszBhYC2","type":"tool_result","content":"     1→/**\n     2→ * Rate Limiter Middleware\n     3→ *\n     4→ * Checks if user has sufficient credits before processing.\n     5→ * Returns 402 Payment Required if insufficient credits.\n     6→ */\n     7→\n     8→const { hasCredits, deductUsage, getBalance } = require('../services/usageTracking');\n     9→\n    10→/**\n    11→ * Middleware to check credits before processing\n    12→ *\n    13→ * Expects req.stripeCustomerId to be set by auth middleware\n    14→ * Optionally uses req.estimatedDuration (in seconds) for pre-check\n    15→ */\n    16→function requireCredits(options = {}) {\n    17→  const { endpoint = 'unknown' } = options;\n    18→\n    19→  return async (req, res, next) => {\n    20→    const stripeCustomerId = req.stripeCustomerId || req.headers['x-stripe-customer-id'];\n    21→\n    22→    if (!stripeCustomerId) {\n    23→      return res.status(401).json({\n    24→        error: 'Authentication required',\n    25→        message: 'Missing Stripe customer ID'\n    26→      });\n    27→    }\n    28→\n    29→    // Store for later use\n    30→    req.stripeCustomerId = stripeCustomerId;\n    31→\n    32→    try {\n    33→      // Check if user has any credits\n    34→      const hasCreds = await hasCredits(stripeCustomerId, req.estimatedDuration || 0);\n    35→\n    36→      if (!hasCreds) {\n    37→        const balance = await getBalance(stripeCustomerId);\n    38→        return res.status(402).json({\n    39→          error: 'Insufficient credits',\n    40→          message: 'Please upgrade your plan or wait for your next billing cycle',\n    41→          balance: {\n    42→            hoursRemaining: balance.hoursRemaining,\n    43→            tier: balance.tier\n    44→          }\n    45→        });\n    46→      }\n    47→\n    48→      // Attach helper function to deduct after processing\n    49→      req.deductUsage = async (audioDurationSeconds) => {\n    50→        return await deductUsage(stripeCustomerId, audioDurationSeconds, endpoint);\n    51→      };\n    52→\n    53→      next();\n    54→    } catch (err) {\n    55→      console.error('[RateLimiter] Error checking credits:', err);\n    56→      return res.status(500).json({\n    57→        error: 'Credit check failed',\n    58→        message: err.message\n    59→      });\n    60→    }\n    61→  };\n    62→}\n    63→\n    64→/**\n    65→ * Middleware to track usage after successful response\n    66→ *\n    67→ * Call this after processing to deduct actual usage\n    68→ */\n    69→function trackUsage(endpoint = 'unknown') {\n    70→  return async (req, res, next) => {\n    71→    // Store original json method\n    72→    const originalJson = res.json.bind(res);\n    73→\n    74→    // Override json to track usage on success\n    75→    res.json = async (data) => {\n    76→      // Only deduct on successful responses\n    77→      if (res.statusCode >= 200 && res.statusCode < 300 && req.audioDuration) {\n    78→        try {\n    79→          const balance = await deductUsage(\n    80→            req.stripeCustomerId,\n    81→            req.audioDuration,\n    82→            endpoint\n    83→          );\n    84→          // Attach updated balance to response\n    85→          data.balance = balance;\n    86→        } catch (err) {\n    87→          console.error('[RateLimiter] Error deducting usage:', err);\n    88→        }\n    89→      }\n    90→      return originalJson(data);\n    91→    };\n    92→\n    93→    next();\n    94→  };\n    95→}\n    96→\n    97→module.exports = {\n    98→  requireCredits,\n    99→  trackUsage\n   100→};\n   101→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"061a437b-3550-453b-a5de-7f67f35ede8e","timestamp":"2025-12-25T20:25:29.183Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/middleware/rateLimiter.js","content":"/**\n * Rate Limiter Middleware\n *\n * Checks if user has sufficient credits before processing.\n * Returns 402 Payment Required if insufficient credits.\n */\n\nconst { hasCredits, deductUsage, getBalance } = require('../services/usageTracking');\n\n/**\n * Middleware to check credits before processing\n *\n * Expects req.stripeCustomerId to be set by auth middleware\n * Optionally uses req.estimatedDuration (in seconds) for pre-check\n */\nfunction requireCredits(options = {}) {\n  const { endpoint = 'unknown' } = options;\n\n  return async (req, res, next) => {\n    const stripeCustomerId = req.stripeCustomerId || req.headers['x-stripe-customer-id'];\n\n    if (!stripeCustomerId) {\n      return res.status(401).json({\n        error: 'Authentication required',\n        message: 'Missing Stripe customer ID'\n      });\n    }\n\n    // Store for later use\n    req.stripeCustomerId = stripeCustomerId;\n\n    try {\n      // Check if user has any credits\n      const hasCreds = await hasCredits(stripeCustomerId, req.estimatedDuration || 0);\n\n      if (!hasCreds) {\n        const balance = await getBalance(stripeCustomerId);\n        return res.status(402).json({\n          error: 'Insufficient credits',\n          message: 'Please upgrade your plan or wait for your next billing cycle',\n          balance: {\n            hoursRemaining: balance.hoursRemaining,\n            tier: balance.tier\n          }\n        });\n      }\n\n      // Attach helper function to deduct after processing\n      req.deductUsage = async (audioDurationSeconds) => {\n        return await deductUsage(stripeCustomerId, audioDurationSeconds, endpoint);\n      };\n\n      next();\n    } catch (err) {\n      console.error('[RateLimiter] Error checking credits:', err);\n      return res.status(500).json({\n        error: 'Credit check failed',\n        message: err.message\n      });\n    }\n  };\n}\n\n/**\n * Middleware to track usage after successful response\n *\n * Call this after processing to deduct actual usage\n */\nfunction trackUsage(endpoint = 'unknown') {\n  return async (req, res, next) => {\n    // Store original json method\n    const originalJson = res.json.bind(res);\n\n    // Override json to track usage on success\n    res.json = async (data) => {\n      // Only deduct on successful responses\n      if (res.statusCode >= 200 && res.statusCode < 300 && req.audioDuration) {\n        try {\n          const balance = await deductUsage(\n            req.stripeCustomerId,\n            req.audioDuration,\n            endpoint\n          );\n          // Attach updated balance to response\n          data.balance = balance;\n        } catch (err) {\n          console.error('[RateLimiter] Error deducting usage:', err);\n        }\n      }\n      return originalJson(data);\n    };\n\n    next();\n  };\n}\n\nmodule.exports = {\n  requireCredits,\n  trackUsage\n};\n","numLines":101,"startLine":1,"totalLines":101}}}
{"parentUuid":"061a437b-3550-453b-a5de-7f67f35ede8e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01DBByRa75sbJW3mBXhchwA6","type":"tool_result","content":"services/rmsSilenceDetection.js:149:  } catch (err) {\nservices/rmsSilenceDetection.js:159:    } catch (err) {\nservices/rmsSilenceDetection.js:192:  } catch (err) {\nservices/usageTracking.js:444:  } catch (err) {\nservices/vocalIsolation.js:204:          .catch(reject);\nservices/transcription.js:35:    } catch (err) {\nservices/multitrackAnalysis.js:172:  } catch (err) {\nservices/multitrackAnalysis.js:182:    } catch (err) {\nservices/multitrackAnalysis.js:228:  } catch (err) {\nservices/repetitionDetection.js:200:    } catch (err) {"}]},"uuid":"92475335-4f13-4399-bb77-bc778ecfcc76","timestamp":"2025-12-25T20:25:29.519Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"services/rmsSilenceDetection.js:149:  } catch (err) {\nservices/rmsSilenceDetection.js:159:    } catch (err) {\nservices/rmsSilenceDetection.js:192:  } catch (err) {\nservices/usageTracking.js:444:  } catch (err) {\nservices/vocalIsolation.js:204:          .catch(reject);\nservices/transcription.js:35:    } catch (err) {\nservices/multitrackAnalysis.js:172:  } catch (err) {\nservices/multitrackAnalysis.js:182:    } catch (err) {\nservices/multitrackAnalysis.js:228:  } catch (err) {\nservices/repetitionDetection.js:200:    } catch (err) {","numLines":10}}
{"parentUuid":"92475335-4f13-4399-bb77-bc778ecfcc76","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE/splice-backend","sessionId":"1fbeafd4-6f35-4615-a353-457301132de4","version":"2.0.76","gitBranch":"main","agentId":"a16d587","slug":"lovely-tinkering-turing","message":{"model":"claude-opus-4-5-20251101","id":"msg_01W7PzgMpBz82UkxRBRmmyE9","type":"message","role":"assistant","content":[{"type":"text","text":"Based on my comprehensive analysis of the SPLICE codebase, here is my performance bottleneck report:\n\n---\n\n# SPLICE Performance Analysis Report\n\n## CRITICAL ISSUES\n\n### 1. Memory Leaks - Unbounded Growth\n\n**[PERF-001] Batch Jobs Map - Potential Memory Leak**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/server.js:1413`\n- **Issue:** `const batchJobs = new Map()` grows unbounded. While cleanup runs every hour (line 1468), jobs younger than 24 hours accumulate. Under high load (1000+ jobs/day), memory will grow significantly.\n- **Impact:** Memory exhaustion over time; server crash on long-running instances\n- **Current mitigation:** MAX_BATCH_JOBS=10000 limit exists (line 1416), but still allows ~10K jobs in memory\n\n**[PERF-002] Transcript Cache - Weak LRU Implementation**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/services/transcription.js:17-20`\n- **Issue:** Cache limited to 50 entries (MAX_CACHE_SIZE), but cache stores full transcription results. With word-level timing data, each entry can be 100KB+.\n- **Impact:** Up to 5MB memory for cache alone; no size-based eviction\n\n### 2. Blocking I/O Operations\n\n**[PERF-003] Synchronous File Reads in Hot Path**\n- **Locations:**\n  - `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:173` - `fs.readFileSync(tempFile)`\n  - `/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js:195` - `fs.readFileSync(tempFile)`\n  - `/Users/imorgado/SPLICE/splice-backend/services/vocalIsolation.js:47` - `fs.readFileSync(audioPath)`\n- **Issue:** Synchronous reads block the event loop while processing audio files (potentially 100MB+)\n- **Impact:** All concurrent requests blocked during file read; P99 latency spikes\n\n**[PERF-004] HTTPS Certs Read Synchronously at Startup**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/server.js:82-83`\n- **Issue:** `fs.readFileSync` for cert loading - acceptable at startup but worth noting\n\n### 3. Database Connection Handling\n\n**[PERF-005] Excessive Pool.connect() Calls**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js` (lines 47, 121, 247, 314, 377)\n- **Issue:** Each function calls `pool.connect()` and releases. Under load, connection pool exhaustion possible.\n- **Current pool:** Default pg Pool (10 connections max)\n- **Pattern:** Functions like `getBalance()` -> `getOrCreateUser()` results in 2 connections per operation\n\n**[PERF-006] No Connection Pool Monitoring**\n- **Issue:** No visibility into pool exhaustion, waiting connections, or query timeouts\n- **Impact:** Silent failures under load; hard to diagnose production issues\n\n### 4. Missing Error Handling\n\n**[PERF-007] FFmpeg Process Not Killed on Error**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:169`\n- **Issue:** `execAsync` spawns FFmpeg process; if request is cancelled, process continues running\n- **Impact:** Zombie processes; resource exhaustion\n\n**[PERF-008] Temp File Cleanup Race Condition**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:184,194`\n- **Issue:** Temp file at `/tmp/splice_rms_${Date.now()}.raw` - if two requests at same millisecond, collision\n- **Impact:** Rare but possible data corruption\n\n### 5. API Call Inefficiencies\n\n**[PERF-009] Sequential Batch Processing**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/server.js:1562` in `processBatchJob()`\n- **Issue:** Files processed sequentially in loop: `for (let i = 0; i < job.files.length; i++)`\n- **Impact:** 10 files at 30s each = 5 minutes vs ~30s with parallel (bounded concurrency)\n\n**[PERF-010] Rate Limiting Not Enforced (Documented Production Blocker)**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/middleware/rateLimiter.js`\n- **Issue:** Credit check exists but no actual rate limiting (requests per second/minute). Any user can flood endpoints.\n- **Impact:** DoS vulnerability; API cost explosion\n\n### 6. Large File Processing Concerns\n\n**[PERF-011] Full File Load into Memory**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:173`\n- **Issue:** `const rawData = fs.readFileSync(tempFile)` loads entire audio into memory\n- **For 1-hour audio at 16kHz 16-bit:** ~115MB per request\n- **Impact:** 10 concurrent requests = 1.15GB RAM just for audio buffers\n\n**[PERF-012] No File Size Validation**\n- **Issue:** No checks for maximum file size before processing\n- **Impact:** User uploads 10GB file -> server OOM crash\n\n**[PERF-013] Float32Array Allocation for RMS**\n- **Location:** `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:176`\n- **Issue:** `const samples = new Float32Array(rawData.length / 2)` - allocates new array for every request\n- **Impact:** Additional ~230MB allocation per 1-hour file\n\n---\n\n## MEDIUM PRIORITY ISSUES\n\n### 7. Builder Error Handling (Plugin Side)\n\n**[PERF-014] No Retry on UXP API Failures**\n- **Location:** `/Users/imorgado/SPLICE/splice-plugin/js/builder.js:287-290`\n- **Issue:** Single try-catch, no retry logic for flaky Premiere Pro API calls\n- **Impact:** User must manually retry failed operations\n\n**[PERF-015] Missing getActiveSequence() Definition**\n- **Location:** `/Users/imorgado/SPLICE/splice-plugin/js/builder.js:141`\n- **Issue:** Function `getActiveSequence()` called but not defined in builder.js (likely in another file)\n- **Impact:** If not imported correctly, runtime errors\n\n### 8. Network/IO Patterns\n\n**[PERF-016] No Request Timeout Configuration**\n- **Issue:** fetch() calls in main.js have no timeout\n- **Impact:** Stuck requests hold UI indefinitely\n\n**[PERF-017] No Backend Health Check from Plugin**\n- **Issue:** Plugin assumes backend is available; no graceful degradation\n- **Impact:** Poor UX when backend is down\n\n---\n\n## QUICK WINS\n\n| # | Fix | Location | Effort | Impact |\n|---|-----|----------|--------|--------|\n| 1 | Add file size check (reject >500MB) | server.js POST handlers | 30 min | Prevents OOM |\n| 2 | Add request-level rate limit (100/min) | rateLimiter.js | 1 hr | Prevents abuse |\n| 3 | Use `fs.promises.readFile` instead of sync | rmsSilenceDetection.js:173 | 15 min | Unblocks event loop |\n| 4 | Add unique suffix to temp files | rmsSilenceDetection.js:165 | 10 min | Prevents collision |\n| 5 | Reduce MAX_CACHE_SIZE to 20 | transcription.js:20 | 5 min | Reduce memory |\n\n---\n\n## PERFORMANCE SUMMARY\n\n| Metric | Current State | Risk Level |\n|--------|---------------|------------|\n| Memory Growth | Unbounded (batch jobs + cache) | HIGH |\n| Connection Pool | 10 max, no monitoring | MEDIUM |\n| File I/O | Synchronous blocking | HIGH |\n| Rate Limiting | Not enforced | HIGH |\n| Error Recovery | Partial (no retry, no cleanup) | MEDIUM |\n| Large Files | No size limits | HIGH |\n\n---\n\n## ARCHITECTURE NOTES\n\n- The codebase is well-structured with clear separation of concerns\n- LRU cache implementation is correct but could benefit from size-based eviction\n- Retry logic in transcription.js is well-implemented with exponential backoff\n- Database schema has proper indexes (lines 90-108 in usageTracking.js)\n- Webhook idempotency is correctly implemented"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1210,"cache_creation_input_tokens":395,"cache_read_input_tokens":82638,"cache_creation":{"ephemeral_5m_input_tokens":395,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWU2qvsvhV5QdM6Bdn7GV","type":"assistant","uuid":"96e26efd-0f26-4bec-8c89-943dc8f5d73f","timestamp":"2025-12-25T20:26:17.495Z"}
