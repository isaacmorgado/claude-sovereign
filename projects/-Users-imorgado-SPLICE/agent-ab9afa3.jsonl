{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","type":"user","message":{"role":"user","content":"Analyze the Fireside host JSX file at /Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx\n\nThis is the main ExtendScript/JSX host file that interfaces with Premiere Pro. Analyze:\n1. All functions and their purposes\n2. How it communicates with the panel\n3. Timeline manipulation methods\n4. Sequence editing operations\n5. Audio/video clip handling\n6. Any unique techniques or optimizations\n\nThis is critical for understanding how Fireside implements its core functionality. Document everything thoroughly."},"uuid":"9325a65f-c39f-441a-9b0a-b82ba199435c","timestamp":"2025-12-25T18:00:51.687Z"}
{"parentUuid":"9325a65f-c39f-441a-9b0a-b82ba199435c","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01BxEq3eQnCULah3kptVkLmf","type":"message","role":"assistant","content":[{"type":"text","text":"I'll analyze the Fireside host JSX file thoroughly. Let me start by reading it."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":12386,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":12386,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWTqpFiuntRj8miDF58iE","type":"assistant","uuid":"d65ad202-0f76-4a18-80d8-ee616c725975","timestamp":"2025-12-25T18:00:53.766Z"}
{"parentUuid":"d65ad202-0f76-4a18-80d8-ee616c725975","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01BxEq3eQnCULah3kptVkLmf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018bGp4zJTmJHX37zuDB6hoa","name":"Read","input":{"file_path":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":12386,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":12386,"ephemeral_1h_input_tokens":0},"output_tokens":101,"service_tier":"standard"}},"requestId":"req_011CWTqpFiuntRj8miDF58iE","type":"assistant","uuid":"bea0a529-a525-4004-81c6-7a7f39ce7e6f","timestamp":"2025-12-25T18:00:54.124Z"}
{"parentUuid":"bea0a529-a525-4004-81c6-7a7f39ce7e6f","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"File content (42983 tokens) exceeds maximum allowed tokens (25000). Please use offset and limit parameters to read specific portions of the file, or use the GrepTool to search for specific content.","is_error":true,"tool_use_id":"toolu_018bGp4zJTmJHX37zuDB6hoa"}]},"uuid":"824c8c81-6417-46ae-b06a-fe92cee29f08","timestamp":"2025-12-25T18:00:54.352Z","toolUseResult":"Error: File content (42983 tokens) exceeds maximum allowed tokens (25000). Please use offset and limit parameters to read specific portions of the file, or use the GrepTool to search for specific content."}
{"parentUuid":"824c8c81-6417-46ae-b06a-fe92cee29f08","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_0185oQa2cWpQywhaMwv3H7DM","type":"message","role":"assistant","content":[{"type":"text","text":"The file is very large. Let me read it in sections and also search for key patterns to understand the architecture."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":196,"cache_read_input_tokens":12386,"cache_creation":{"ephemeral_5m_input_tokens":196,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWTqpS9Uji6y979i1Mw2e","type":"assistant","uuid":"df617f6c-66b1-4a35-acc4-48021b9f64c6","timestamp":"2025-12-25T18:00:55.270Z"}
{"parentUuid":"df617f6c-66b1-4a35-acc4-48021b9f64c6","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_0185oQa2cWpQywhaMwv3H7DM","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01D58TrB5cmbM7PczfUKoywm","name":"Read","input":{"file_path":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx","offset":1,"limit":500}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":196,"cache_read_input_tokens":12386,"cache_creation":{"ephemeral_5m_input_tokens":196,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWTqpS9Uji6y979i1Mw2e","type":"assistant","uuid":"235cec86-26fd-4fb3-9aae-17c9639517ab","timestamp":"2025-12-25T18:00:55.617Z"}
{"parentUuid":"235cec86-26fd-4fb3-9aae-17c9639517ab","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_0185oQa2cWpQywhaMwv3H7DM","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TT1f7sfBeygS5hX3RkFNw4","name":"Grep","input":{"path":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx","pattern":"^(function|var|const)\\s+\\w+\\s*=|^function\\s+\\w+\\s*\\(","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":196,"cache_read_input_tokens":12386,"cache_creation":{"ephemeral_5m_input_tokens":196,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWTqpS9Uji6y979i1Mw2e","type":"assistant","uuid":"06369d47-9309-4fdd-94f6-e4331d7d5d62","timestamp":"2025-12-25T18:00:56.370Z"}
{"parentUuid":"06369d47-9309-4fdd-94f6-e4331d7d5d62","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01D58TrB5cmbM7PczfUKoywm","type":"tool_result","content":"     1→/*\n     2→* Decompiled with Jsxer\n     3→* Version: 1.7.4\n     4→* JSXBIN 2.0\n     5→*/\n     6→\n     7→function storeDefaultTimecode() {\n     8→if (app.project.activeSequence) { \n     9→sequence_default_timecode = app.project.activeSequence.getSettings().videoDisplayFormat;\n    10→}\n    11→}\n    12→function initialise() {\n    13→storeDefaultTimecode();\n    14→}\n    15→function quantise_time(time_seconds) {\n    16→const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n    17→return frame_rate * Math.round(parseInt(time_seconds / frame_rate));\n    18→}\n    19→function frames_to_time(number_of_frames) {\n    20→const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n    21→return number_of_frames * frame_rate;\n    22→}\n    23→function checkSequenceOpen() {\n    24→if (app.project.activeSequence) { \n    25→return true;\n    26→}\n    27→else {\n    28→return false;\n    29→}\n    30→}\n    31→function checkInitialisation() {\n    32→return true;\n    33→}\n    34→function setAdjusmentLayerFilepath(path) {\n    35→adjustment_layer_filepath = path;\n    36→return \"successfully set adjustment_layer_filepath to \" + adjustment_layer_filepath;\n    37→}\n    38→function setSilenceCuttingLayerFilepath(path) {\n    39→silence_cutting_layer_filepath = path;\n    40→return \"successfully set silence_cutting_layer_filepath to \" + silence_cutting_layer_filepath;\n    41→}\n    42→function setTimecodeToDefault() {\n    43→var seq = app.project.activeSequence;\n    44→if (seq) { \n    45→var currentSeqSettings = app.project.activeSequence.getSettings();\n    46→currentSeqSettings.videoDisplayFormat = sequence_default_timecode;\n    47→app.project.activeSequence.setSettings(currentSeqSettings);\n    48→}\n    49→}\n    50→function secondsToCurrentTimecode(seconds) {\n    51→var seq = app.project.activeSequence;\n    52→if (seq) { \n    53→t = new Time();\n    54→t.seconds = parseFloat(seconds);\n    55→settings = app.project.activeSequence.getSettings();\n    56→return t.getFormatted(settings.videoFrameRate, settings.videoDisplayFormat);\n    57→}\n    58→else {\n    59→return false;\n    60→}\n    61→}\n    62→function breakLines(s, max_chars_per_line) {\n    63→const words = s.split(\" \");\n    64→var output = \"\";\n    65→var currentLine = \"\";\n    66→for (var j = 0; j < words.length; j += 1) { \n    67→var word = words[j];\n    68→if (word.length > max_chars_per_line) { \n    69→if (currentLine.length > 0) { \n    70→output += currentLine + \"\\n\";\n    71→currentLine = \"\";\n    72→}\n    73→var i = 0;\n    74→while (i < word.length) {\n    75→const remainingChars = word.slice(i, (i + max_chars_per_line) - 1);\n    76→const lastLine = (remainingChars.length === (max_chars_per_line - 1)) && ((i + remainingChars.length) < word.length);\n    77→currentLine = lastLine ? remainingChars + \"-\" : remainingChars;\n    78→output += currentLine + lastLine ? \"\\n\" : \"\";\n    79→i += (max_chars_per_line - 1);\n    80→}\n    81→}\n    82→else if ((currentLine.length + 1 + word.length) <= max_chars_per_line) {\n    83→currentLine += currentLine.length > 0 ? \" \" + word : word;\n    84→}\n    85→else {\n    86→output += currentLine + \"\\n\";\n    87→currentLine = word;\n    88→}}\n    89→if (currentLine.length > 0) { \n    90→output += currentLine;\n    91→}\n    92→return output;\n    93→}\n    94→function openDocument(path) {\n    95→app.project.importFiles([path], suppressUI = true, targetBin = app.project.rootItem, importAsNumberedStills = false);\n    96→}\n    97→function getTimecodeFormat() {\n    98→return app.project.activeSequence.getSettings().videoDisplayFormat;\n    99→}\n   100→function setTimecodeFormat(format) {\n   101→format = parseInt(format);\n   102→var seq = app.project.activeSequence;\n   103→if (seq) { \n   104→var currentSeqSettings = app.project.activeSequence.getSettings();\n   105→currentSeqSettings.videoDisplayFormat = format;\n   106→app.project.activeSequence.setSettings(currentSeqSettings);\n   107→}\n   108→return true;\n   109→}\n   110→function setTimecodeToFrames() {\n   111→var seq = app.project.activeSequence;\n   112→if (seq) { \n   113→var currentSeqSettings = app.project.activeSequence.getSettings();\n   114→if (currentSeqSettings.videoDisplayFormat != TIMECODES.TIMEDISPLAY_Frames) { \n   115→currentSeqSettings.videoDisplayFormat = TIMECODES.TIMEDISPLAY_Frames;\n   116→app.project.activeSequence.setSettings(currentSeqSettings);\n   117→}\n   118→}\n   119→}\n   120→function razorSequenceAtFrames(frames) {\n   121→frames = String(frames);\n   122→setTimecodeToFrames();\n   123→qe.project.getActiveSequence().razor(frames);\n   124→}\n   125→function razorSequenceAtSeconds(seconds) {\n   126→const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n   127→razorSequenceAtFrames(frames);\n   128→}\n   129→function razorTrackAtFrames(trackType, trackIndex, frames) {\n   130→frames = String(frames);\n   131→setTimecodeToFrames();\n   132→if (trackType == \"video\") { \n   133→qe.project.getActiveSequence().getVideoTrackAt(trackIndex).razor(frames);\n   134→}\n   135→else {\n   136→if (trackType == \"audio\") { \n   137→qe.project.getActiveSequence().getAudioTrackAt(trackIndex).razor(frames);\n   138→}\n   139→}\n   140→}\n   141→function getActiveSequence() {\n   142→return JSON.stringify(app.project.activeSequence);\n   143→}\n   144→function getActiveSequenceVideoClips() {\n   145→var sequence = app.project.activeSequence;\n   146→var sequence_clips = [];\n   147→for (var i = 0; i < sequence.videoTracks.numTracks; i += 1) { \n   148→var clips = sequence.videoTracks[i].clips;\n   149→for (var j = 0; j < clips.length; j += 1) { \n   150→sequence_clips.push(clips[j]);}}\n   151→return JSON.stringify(sequence_clips);\n   152→}\n   153→function getActiveSequenceVideoClipsForTrackId(trackId) {\n   154→var sequence = app.project.activeSequence;\n   155→var sequence_clips = [];\n   156→var clips = sequence.videoTracks[trackId].clips;\n   157→for (var j = 0; j < clips.length; j += 1) { \n   158→sequence_clips.push(clips[j]);}\n   159→return JSON.stringify(sequence_clips);\n   160→}\n   161→function getVideoFrameRateInSeconds() {\n   162→return app.project.activeSequence.getSettings().videoFrameRate.seconds;\n   163→}\n   164→function razorTrackAtSeconds(trackType, trackIndex, seconds) {\n   165→const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n   166→razorTrackAtFrames(trackType, trackIndex, frames);\n   167→}\n   168→function getTrackNames(trackType) {\n   169→ret = [];\n   170→if (trackType == \"video\") { \n   171→for (var i = 0; i < qe.project.getActiveSequence().numVideoTracks; i += 1) { \n   172→ret.push(qe.project.getActiveSequence().getVideoTrackAt(i).name);}\n   173→}\n   174→else {\n   175→if (trackType == \"audio\") { \n   176→for (var i = 0; i < qe.project.getActiveSequence().numAudioTracks; i += 1) { \n   177→ret.push(qe.project.getActiveSequence().getAudioTrackAt(i).name);}\n   178→}\n   179→}\n   180→return ret;\n   181→}\n   182→function deleteClipsAtSilencePointsInTrack(silences, trackType, trackIndex) {\n   183→silences = JSON.parse(silences);\n   184→if (trackType == \"video\") { \n   185→var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n   186→}\n   187→else {\n   188→if (trackType == \"audio\") { \n   189→var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n   190→}\n   191→}\n   192→for (var i = 0; i < clips.length; i += 1) { \n   193→var clip = clips[i];\n   194→var start = clip.start.seconds;\n   195→var end = clip.end.seconds;\n   196→for (var j = 0; j < silences.length; j += 1) { \n   197→var silence = silences[j];\n   198→var silence_center = (silence[0] + silence[1]) / 2;\n   199→if ((silence_center > start) && (silence_center < end)) { \n   200→clip.remove(false, false);\n   201→}}}\n   202→}\n   203→function makeRemainingClipsContiguousInTrack(silences, deltas, trackType, trackIndex) {\n   204→silences = JSON.parse(silences);\n   205→deltas = JSON.parse(deltas);\n   206→if (trackType == \"video\") { \n   207→var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n   208→}\n   209→else {\n   210→if (trackType == \"audio\") { \n   211→var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n   212→}\n   213→}\n   214→for (var i = 0; i < clips.length; i += 1) { \n   215→var clip = clips[i];\n   216→var start = clip.start.seconds;\n   217→var end = clip.end.seconds;\n   218→var num_silences = -1;\n   219→for (var j = 0; j < silences.length; j += 1) { \n   220→if (end > silences[j][1]) { \n   221→num_silences++;\n   222→}}\n   223→if (num_silences > -1) { \n   224→delta_for_this_clip = deltas[num_silences];\n   225→t_start = clip.start;\n   226→t_start.seconds = t_start.seconds - delta_for_this_clip;\n   227→clip.start = t_start;\n   228→t_end = clip.end;\n   229→t_end.seconds = t_end.seconds - delta_for_this_clip;\n   230→clip.end = t_end;\n   231→}}\n   232→}\n   233→function moveAllClipsInTrackAfterSeconds(track_index, track_type, starting_time, move_amount_in_seconds) {\n   234→if (track_type == \"video\") { \n   235→var clips = app.project.activeSequence.videoTracks[track_index].clips;\n   236→}\n   237→else {\n   238→if (track_type == \"audio\") { \n   239→var clips = app.project.activeSequence.audioTracks[track_index].clips;\n   240→}\n   241→}\n   242→for (var i = 0; i < clips.length; i += 1) { \n   243→var clip = clips[i];\n   244→var start = clip.start.seconds;\n   245→var end = clip.end.seconds;\n   246→if (start >= starting_time) { \n   247→clip.move(move_amount_in_seconds);\n   248→}}\n   249→}\n   250→function moveAllClipsInTrack(track_index, track_type, move_amount_in_seconds) {\n   251→if (track_type == \"video\") { \n   252→var clips = app.project.activeSequence.videoTracks[track_index].clips;\n   253→}\n   254→else {\n   255→if (track_type == \"audio\") { \n   256→var clips = app.project.activeSequence.audioTracks[track_index].clips;\n   257→}\n   258→}\n   259→if (move_amount_in_seconds > 0) { \n   260→var i = clips.length - 1;\n   261→var increment = -1;\n   262→}\n   263→else {\n   264→var i = 0;\n   265→var increment = 1;\n   266→}\n   267→for (i; (i < clips.length) && (i >= 0); i += increment) { \n   268→var clip = clips[i];\n   269→clip.move(move_amount_in_seconds);}\n   270→}\n   271→function linkIdenticalClipsInTracks(videoTrackIndex, audioTrackIndex) {\n   272→var selected_clips = app.project.activeSequence.getSelection();\n   273→for (var i = 0; i < selected_clips.length; i += 1) { \n   274→selected_clips[i].setSelected(false, false);}\n   275→var video_clips = app.project.activeSequence.videoTracks[videoTrackIndex].clips;\n   276→var audio_clips = app.project.activeSequence.audioTracks[audioTrackIndex].clips;\n   277→for (var i = 0; i < video_clips.length; i += 1) { \n   278→var video_clip = video_clips[i];\n   279→var video_start = video_clip.start.seconds;\n   280→for (var j = 0; j < audio_clips.length; j += 1) { \n   281→var audio_clip = audio_clips[j];\n   282→var audio_start = audio_clip.start.seconds;\n   283→if (video_start == audio_start) { \n   284→video_clip.setSelected(true, false);\n   285→audio_clip.setSelected(true, false);\n   286→app.project.activeSequence.linkSelection();\n   287→video_clip.setSelected(false, false);\n   288→audio_clip.setSelected(false, false);\n   289→break ;\n   290→}}}\n   291→}\n   292→function getMuteState() {\n   293→var mute_state_video = [];\n   294→for (var i = 0; i < app.project.activeSequence.videoTracks.length; i += 1) { \n   295→mute_state_video.push(app.project.activeSequence.videoTracks[i].isMuted());}\n   296→var mute_state_audio = [];\n   297→for (var i = 0; i < app.project.activeSequence.audioTracks.length; i += 1) { \n   298→mute_state_audio.push(app.project.activeSequence.audioTracks[i].isMuted());}\n   299→mute_state = {audio: mute_state_audio, video: mute_state_video};\n   300→return JSON.stringify(mute_state);\n   301→}\n   302→function setMuteState(mute_state) {\n   303→mute_state = JSON.parse(mute_state);\n   304→if (mute_state.video.length != app.project.activeSequence.videoTracks.length) { \n   305→return \"ERROR: mute_state video length does not match number of video tracks\";\n   306→}\n   307→if (mute_state.audio.length != app.project.activeSequence.audioTracks.length) { \n   308→return \"ERROR: mute_state audio length does not match number of audio tracks\";\n   309→}\n   310→for (var i = 0; i < app.project.activeSequence.videoTracks.length; i += 1) { \n   311→app.project.activeSequence.videoTracks[i].setMute(mute_state.video[i]);}\n   312→for (var i = 0; i < app.project.activeSequence.audioTracks.length; i += 1) { \n   313→app.project.activeSequence.audioTracks[i].setMute(mute_state.audio[i]);}\n   314→return true;\n   315→}\n   316→function insertMediaInTrackAtTime(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, scaling_mode) {\n   317→const success = app.project.importFiles([media_path], false, targetBin, false);\n   318→const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n   319→start_time = frame_rate * Math.round(parseInt(start_time / frame_rate));\n   320→end_time = frame_rate * Math.round(parseInt(end_time / frame_rate));\n   321→if (success == true) { \n   322→const item = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true)[0];\n   323→var audioChannelMapping = item.getAudioChannelMapping;\n   324→if (audioChannelMapping.audioClipsNumber > 0) { \n   325→audioChannelMapping.audioClipsNumber = 0;\n   326→item.setAudioChannelMapping(audioChannelMapping);\n   327→}\n   328→if ((clip_width == null) || (clip_height == null)) { \n   329→console.log(\"Getting dimensions from Premiere Pro\");\n   330→try {\n   331→const dimensions = getProjectItemDimensions(item);\n   332→console.log(\"Dimensions:\", JSON.stringify(dimensions));\n   333→if (dimensions) { \n   334→clip_width = dimensions.width;\n   335→clip_height = dimensions.height;\n   336→}\n   337→} catch (error) {console.log(\"Error getting project item dimensions:\", error);\n   338→clip_width = 1920;\n   339→clip_height = 1080;\n   340→}\n   341→}\n   342→if (trackIndex == -1) { \n   343→trackIndex = app.project.activeSequence.videoTracks.length - 1;\n   344→}\n   345→var targetTime = new Time();\n   346→targetTime.seconds = start_time;\n   347→app.project.activeSequence.videoTracks[trackIndex].insertClip(item, targetTime);\n   348→for (var i = 0; i < app.project.activeSequence.videoTracks[trackIndex].clips.length; i += 1) { \n   349→if (app.project.activeSequence.videoTracks[trackIndex].clips[i].projectItem.getMediaPath() == media_path) { \n   350→clip = app.project.activeSequence.videoTracks[trackIndex].clips[i];\n   351→}}\n   352→if (clip) { \n   353→var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n   354→var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n   355→var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n   356→if (scaling_mode === \"fit\") { \n   357→scale_required = 100 * Math.min(sequence_height / clip_height, sequence_width / clip_width);\n   358→}\n   359→console.log(\"SET SCALE\", sequence_height, sequence_width, clip_height, clip_width, scale_required, scaling_mode);\n   360→setPropertyStatic(clip, \"Motion\", \"Scale\", scale_required);\n   361→if (clip.end.seconds > end_time) { \n   362→const duration = end_time - start_time;\n   363→var end = new Time();\n   364→end.seconds = end_time;\n   365→var outPoint = new Time();\n   366→outPoint.seconds = duration;\n   367→clip.end = end;\n   368→clip.outPoint = outPoint;\n   369→}\n   370→return clip.nodeId;\n   371→}\n   372→}\n   373→}\n   374→function insertBrollInTrackAtTime(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, position_property, anchor_property, rotation_property, scale_property, clip_in, clip_out, scaling_mode) {\n   375→const success = app.project.importFiles([media_path], false, targetBin, false);\n   376→const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n   377→start_time = frame_rate * Math.round(parseInt(start_time / frame_rate));\n   378→end_time = frame_rate * Math.round(parseInt(end_time / frame_rate));\n   379→if (success === true) { \n   380→var items = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n   381→const item = items[items.length - 1];\n   382→var audioChannelMapping = item.getAudioChannelMapping;\n   383→if (audioChannelMapping.audioClipsNumber > 0) { \n   384→audioChannelMapping.audioClipsNumber = 0;\n   385→item.setAudioChannelMapping(audioChannelMapping);\n   386→}\n   387→if ((clip_width == null) || (clip_height == null)) { \n   388→console.log(\"Getting dimensions from Premiere Pro\");\n   389→try {\n   390→const dimensions = getProjectItemDimensions(item);\n   391→console.log(\"Dimensions:\", JSON.stringify(dimensions));\n   392→if (dimensions) { \n   393→clip_width = dimensions.width;\n   394→clip_height = dimensions.height;\n   395→}\n   396→} catch (error) {console.log(\"Error getting project item dimensions:\", error);\n   397→clip_width = 1920;\n   398→clip_height = 1080;\n   399→}\n   400→}\n   401→var tempTrackIndex = app.project.activeSequence.videoTracks.length;\n   402→qe.project.getActiveSequence().addTracks(1, tempTrackIndex, 0, 0);\n   403→var targetTime = new Time();\n   404→targetTime.seconds = start_time;\n   405→app.project.activeSequence.videoTracks[tempTrackIndex].insertClip(item, targetTime);\n   406→var track = app.project.activeSequence.videoTracks[tempTrackIndex];\n   407→for (var i = 0; i < track.clips.length; i += 1) { \n   408→if ((track.clips[i].projectItem.getMediaPath() === media_path) && (Math.abs(track.clips[i].start.seconds - targetTime.seconds) < time_tolerance)) { \n   409→new_clip = track.clips[i];\n   410→break ;\n   411→}}\n   412→if (new_clip) { \n   413→var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n   414→var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n   415→var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n   416→if (scaling_mode === \"fit\") { \n   417→scale_required = 100 * Math.min(sequence_height / clip_height, sequence_width / clip_width);\n   418→}\n   419→console.log(\"MANUAL BROLL\", sequence_height, sequence_width, scale_required);\n   420→if (anchor_property) { \n   421→setPropertyStatic(new_clip, \"Motion\", \"Anchor Point\", anchor_property);\n   422→}\n   423→if (position_property) { \n   424→setPropertyStatic(new_clip, \"Motion\", \"Position\", position_property);\n   425→}\n   426→if (rotation_property) { \n   427→setPropertyStatic(new_clip, \"Motion\", \"Rotation\", rotation_property);\n   428→}\n   429→if (scale_property) { \n   430→setPropertyStatic(new_clip, \"Motion\", \"Scale\", scale_property);\n   431→}\n   432→else {\n   433→setPropertyStatic(new_clip, \"Motion\", \"Scale\", scale_required);\n   434→}\n   435→if (clip_in !== null) { \n   436→var inPoint = new Time();\n   437→inPoint.seconds = clip_in;\n   438→new_clip.inPoint = inPoint;\n   439→}\n   440→if (clip_out !== null) { \n   441→var outPoint = new Time();\n   442→outPoint.seconds = clip_out;\n   443→new_clip.outPoint = outPoint;\n   444→}\n   445→if (new_clip.end.seconds > end_time) { \n   446→const duration = end_time - start_time;\n   447→var end = new Time();\n   448→end.seconds = end_time;\n   449→var outPoint = new Time();\n   450→outPoint.seconds = duration;\n   451→new_clip.end = end;\n   452→new_clip.outPoint = outPoint;\n   453→}\n   454→}\n   455→else {\n   456→return null;\n   457→}\n   458→var item_index = 1;\n   459→if (start_time === 0) { \n   460→item_index = 0;\n   461→}\n   462→qe.project.getActiveSequence().getVideoTrackAt(tempTrackIndex).getItemAt(item_index).moveToTrack(trackIndex - tempTrackIndex, 0, \"0\");\n   463→qe.project.getActiveSequence().removeVideoTrack(tempTrackIndex);\n   464→return new_clip.nodeId;\n   465→}\n   466→}\n   467→function addMusicToProject(media_path) {\n   468→const targetBin = app.project.rootItem;\n   469→const success = app.project.importFiles([media_path], false, targetBin, false);\n   470→if (success == true) { \n   471→qe.project.getActiveSequence().addTracks(0, 0, 1, 0);\n   472→const items = app.project.rootItem.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n   473→var targetTime = new Time();\n   474→targetTime.seconds = 0;\n   475→app.project.activeSequence.audioTracks[0].insertClip(items[0], targetTime);\n   476→}\n   477→}\n   478→function addAudioTrack() {\n   479→var track_id = qe.project.getActiveSequence().numAudioTracks;\n   480→qe.project.getActiveSequence().addTracks(0, 0, 1, 1, track_id);\n   481→return track_id;\n   482→}\n   483→function addAudioClipToProject(audio_track_id, media_path, start_time, end_time) {\n   484→var targetBin = getEditAIBin();\n   485→var items = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n   486→if (items.length <= 0) { \n   487→app.project.importFiles([media_path], false, targetBin, false);\n   488→items = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n   489→}\n   490→app.project.activeSequence.audioTracks[audio_track_id].insertClip(items[0], start_time);\n   491→for (var i = 0; i < app.project.activeSequence.audioTracks[audio_track_id].clips.length; i += 1) { \n   492→var clip = app.project.activeSequence.audioTracks[audio_track_id].clips[i];\n   493→if (clip.projectItem.getMediaPath() === media_path) { \n   494→selected_clip = clip;\n   495→}}\n   496→if (selected_clip.end.seconds > end_time) { \n   497→const duration = end_time - start_time;\n   498→var end = new Time();\n   499→end.seconds = end_time;\n   500→var outPoint = new Time();\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"2999b358-3eb7-4cf2-8a20-3cf9ab930b54","timestamp":"2025-12-25T18:00:56.440Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx","content":"/*\n* Decompiled with Jsxer\n* Version: 1.7.4\n* JSXBIN 2.0\n*/\n\nfunction storeDefaultTimecode() {\nif (app.project.activeSequence) { \nsequence_default_timecode = app.project.activeSequence.getSettings().videoDisplayFormat;\n}\n}\nfunction initialise() {\nstoreDefaultTimecode();\n}\nfunction quantise_time(time_seconds) {\nconst frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\nreturn frame_rate * Math.round(parseInt(time_seconds / frame_rate));\n}\nfunction frames_to_time(number_of_frames) {\nconst frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\nreturn number_of_frames * frame_rate;\n}\nfunction checkSequenceOpen() {\nif (app.project.activeSequence) { \nreturn true;\n}\nelse {\nreturn false;\n}\n}\nfunction checkInitialisation() {\nreturn true;\n}\nfunction setAdjusmentLayerFilepath(path) {\nadjustment_layer_filepath = path;\nreturn \"successfully set adjustment_layer_filepath to \" + adjustment_layer_filepath;\n}\nfunction setSilenceCuttingLayerFilepath(path) {\nsilence_cutting_layer_filepath = path;\nreturn \"successfully set silence_cutting_layer_filepath to \" + silence_cutting_layer_filepath;\n}\nfunction setTimecodeToDefault() {\nvar seq = app.project.activeSequence;\nif (seq) { \nvar currentSeqSettings = app.project.activeSequence.getSettings();\ncurrentSeqSettings.videoDisplayFormat = sequence_default_timecode;\napp.project.activeSequence.setSettings(currentSeqSettings);\n}\n}\nfunction secondsToCurrentTimecode(seconds) {\nvar seq = app.project.activeSequence;\nif (seq) { \nt = new Time();\nt.seconds = parseFloat(seconds);\nsettings = app.project.activeSequence.getSettings();\nreturn t.getFormatted(settings.videoFrameRate, settings.videoDisplayFormat);\n}\nelse {\nreturn false;\n}\n}\nfunction breakLines(s, max_chars_per_line) {\nconst words = s.split(\" \");\nvar output = \"\";\nvar currentLine = \"\";\nfor (var j = 0; j < words.length; j += 1) { \nvar word = words[j];\nif (word.length > max_chars_per_line) { \nif (currentLine.length > 0) { \noutput += currentLine + \"\\n\";\ncurrentLine = \"\";\n}\nvar i = 0;\nwhile (i < word.length) {\nconst remainingChars = word.slice(i, (i + max_chars_per_line) - 1);\nconst lastLine = (remainingChars.length === (max_chars_per_line - 1)) && ((i + remainingChars.length) < word.length);\ncurrentLine = lastLine ? remainingChars + \"-\" : remainingChars;\noutput += currentLine + lastLine ? \"\\n\" : \"\";\ni += (max_chars_per_line - 1);\n}\n}\nelse if ((currentLine.length + 1 + word.length) <= max_chars_per_line) {\ncurrentLine += currentLine.length > 0 ? \" \" + word : word;\n}\nelse {\noutput += currentLine + \"\\n\";\ncurrentLine = word;\n}}\nif (currentLine.length > 0) { \noutput += currentLine;\n}\nreturn output;\n}\nfunction openDocument(path) {\napp.project.importFiles([path], suppressUI = true, targetBin = app.project.rootItem, importAsNumberedStills = false);\n}\nfunction getTimecodeFormat() {\nreturn app.project.activeSequence.getSettings().videoDisplayFormat;\n}\nfunction setTimecodeFormat(format) {\nformat = parseInt(format);\nvar seq = app.project.activeSequence;\nif (seq) { \nvar currentSeqSettings = app.project.activeSequence.getSettings();\ncurrentSeqSettings.videoDisplayFormat = format;\napp.project.activeSequence.setSettings(currentSeqSettings);\n}\nreturn true;\n}\nfunction setTimecodeToFrames() {\nvar seq = app.project.activeSequence;\nif (seq) { \nvar currentSeqSettings = app.project.activeSequence.getSettings();\nif (currentSeqSettings.videoDisplayFormat != TIMECODES.TIMEDISPLAY_Frames) { \ncurrentSeqSettings.videoDisplayFormat = TIMECODES.TIMEDISPLAY_Frames;\napp.project.activeSequence.setSettings(currentSeqSettings);\n}\n}\n}\nfunction razorSequenceAtFrames(frames) {\nframes = String(frames);\nsetTimecodeToFrames();\nqe.project.getActiveSequence().razor(frames);\n}\nfunction razorSequenceAtSeconds(seconds) {\nconst frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\nrazorSequenceAtFrames(frames);\n}\nfunction razorTrackAtFrames(trackType, trackIndex, frames) {\nframes = String(frames);\nsetTimecodeToFrames();\nif (trackType == \"video\") { \nqe.project.getActiveSequence().getVideoTrackAt(trackIndex).razor(frames);\n}\nelse {\nif (trackType == \"audio\") { \nqe.project.getActiveSequence().getAudioTrackAt(trackIndex).razor(frames);\n}\n}\n}\nfunction getActiveSequence() {\nreturn JSON.stringify(app.project.activeSequence);\n}\nfunction getActiveSequenceVideoClips() {\nvar sequence = app.project.activeSequence;\nvar sequence_clips = [];\nfor (var i = 0; i < sequence.videoTracks.numTracks; i += 1) { \nvar clips = sequence.videoTracks[i].clips;\nfor (var j = 0; j < clips.length; j += 1) { \nsequence_clips.push(clips[j]);}}\nreturn JSON.stringify(sequence_clips);\n}\nfunction getActiveSequenceVideoClipsForTrackId(trackId) {\nvar sequence = app.project.activeSequence;\nvar sequence_clips = [];\nvar clips = sequence.videoTracks[trackId].clips;\nfor (var j = 0; j < clips.length; j += 1) { \nsequence_clips.push(clips[j]);}\nreturn JSON.stringify(sequence_clips);\n}\nfunction getVideoFrameRateInSeconds() {\nreturn app.project.activeSequence.getSettings().videoFrameRate.seconds;\n}\nfunction razorTrackAtSeconds(trackType, trackIndex, seconds) {\nconst frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\nrazorTrackAtFrames(trackType, trackIndex, frames);\n}\nfunction getTrackNames(trackType) {\nret = [];\nif (trackType == \"video\") { \nfor (var i = 0; i < qe.project.getActiveSequence().numVideoTracks; i += 1) { \nret.push(qe.project.getActiveSequence().getVideoTrackAt(i).name);}\n}\nelse {\nif (trackType == \"audio\") { \nfor (var i = 0; i < qe.project.getActiveSequence().numAudioTracks; i += 1) { \nret.push(qe.project.getActiveSequence().getAudioTrackAt(i).name);}\n}\n}\nreturn ret;\n}\nfunction deleteClipsAtSilencePointsInTrack(silences, trackType, trackIndex) {\nsilences = JSON.parse(silences);\nif (trackType == \"video\") { \nvar clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n}\nelse {\nif (trackType == \"audio\") { \nvar clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n}\n}\nfor (var i = 0; i < clips.length; i += 1) { \nvar clip = clips[i];\nvar start = clip.start.seconds;\nvar end = clip.end.seconds;\nfor (var j = 0; j < silences.length; j += 1) { \nvar silence = silences[j];\nvar silence_center = (silence[0] + silence[1]) / 2;\nif ((silence_center > start) && (silence_center < end)) { \nclip.remove(false, false);\n}}}\n}\nfunction makeRemainingClipsContiguousInTrack(silences, deltas, trackType, trackIndex) {\nsilences = JSON.parse(silences);\ndeltas = JSON.parse(deltas);\nif (trackType == \"video\") { \nvar clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n}\nelse {\nif (trackType == \"audio\") { \nvar clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n}\n}\nfor (var i = 0; i < clips.length; i += 1) { \nvar clip = clips[i];\nvar start = clip.start.seconds;\nvar end = clip.end.seconds;\nvar num_silences = -1;\nfor (var j = 0; j < silences.length; j += 1) { \nif (end > silences[j][1]) { \nnum_silences++;\n}}\nif (num_silences > -1) { \ndelta_for_this_clip = deltas[num_silences];\nt_start = clip.start;\nt_start.seconds = t_start.seconds - delta_for_this_clip;\nclip.start = t_start;\nt_end = clip.end;\nt_end.seconds = t_end.seconds - delta_for_this_clip;\nclip.end = t_end;\n}}\n}\nfunction moveAllClipsInTrackAfterSeconds(track_index, track_type, starting_time, move_amount_in_seconds) {\nif (track_type == \"video\") { \nvar clips = app.project.activeSequence.videoTracks[track_index].clips;\n}\nelse {\nif (track_type == \"audio\") { \nvar clips = app.project.activeSequence.audioTracks[track_index].clips;\n}\n}\nfor (var i = 0; i < clips.length; i += 1) { \nvar clip = clips[i];\nvar start = clip.start.seconds;\nvar end = clip.end.seconds;\nif (start >= starting_time) { \nclip.move(move_amount_in_seconds);\n}}\n}\nfunction moveAllClipsInTrack(track_index, track_type, move_amount_in_seconds) {\nif (track_type == \"video\") { \nvar clips = app.project.activeSequence.videoTracks[track_index].clips;\n}\nelse {\nif (track_type == \"audio\") { \nvar clips = app.project.activeSequence.audioTracks[track_index].clips;\n}\n}\nif (move_amount_in_seconds > 0) { \nvar i = clips.length - 1;\nvar increment = -1;\n}\nelse {\nvar i = 0;\nvar increment = 1;\n}\nfor (i; (i < clips.length) && (i >= 0); i += increment) { \nvar clip = clips[i];\nclip.move(move_amount_in_seconds);}\n}\nfunction linkIdenticalClipsInTracks(videoTrackIndex, audioTrackIndex) {\nvar selected_clips = app.project.activeSequence.getSelection();\nfor (var i = 0; i < selected_clips.length; i += 1) { \nselected_clips[i].setSelected(false, false);}\nvar video_clips = app.project.activeSequence.videoTracks[videoTrackIndex].clips;\nvar audio_clips = app.project.activeSequence.audioTracks[audioTrackIndex].clips;\nfor (var i = 0; i < video_clips.length; i += 1) { \nvar video_clip = video_clips[i];\nvar video_start = video_clip.start.seconds;\nfor (var j = 0; j < audio_clips.length; j += 1) { \nvar audio_clip = audio_clips[j];\nvar audio_start = audio_clip.start.seconds;\nif (video_start == audio_start) { \nvideo_clip.setSelected(true, false);\naudio_clip.setSelected(true, false);\napp.project.activeSequence.linkSelection();\nvideo_clip.setSelected(false, false);\naudio_clip.setSelected(false, false);\nbreak ;\n}}}\n}\nfunction getMuteState() {\nvar mute_state_video = [];\nfor (var i = 0; i < app.project.activeSequence.videoTracks.length; i += 1) { \nmute_state_video.push(app.project.activeSequence.videoTracks[i].isMuted());}\nvar mute_state_audio = [];\nfor (var i = 0; i < app.project.activeSequence.audioTracks.length; i += 1) { \nmute_state_audio.push(app.project.activeSequence.audioTracks[i].isMuted());}\nmute_state = {audio: mute_state_audio, video: mute_state_video};\nreturn JSON.stringify(mute_state);\n}\nfunction setMuteState(mute_state) {\nmute_state = JSON.parse(mute_state);\nif (mute_state.video.length != app.project.activeSequence.videoTracks.length) { \nreturn \"ERROR: mute_state video length does not match number of video tracks\";\n}\nif (mute_state.audio.length != app.project.activeSequence.audioTracks.length) { \nreturn \"ERROR: mute_state audio length does not match number of audio tracks\";\n}\nfor (var i = 0; i < app.project.activeSequence.videoTracks.length; i += 1) { \napp.project.activeSequence.videoTracks[i].setMute(mute_state.video[i]);}\nfor (var i = 0; i < app.project.activeSequence.audioTracks.length; i += 1) { \napp.project.activeSequence.audioTracks[i].setMute(mute_state.audio[i]);}\nreturn true;\n}\nfunction insertMediaInTrackAtTime(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, scaling_mode) {\nconst success = app.project.importFiles([media_path], false, targetBin, false);\nconst frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\nstart_time = frame_rate * Math.round(parseInt(start_time / frame_rate));\nend_time = frame_rate * Math.round(parseInt(end_time / frame_rate));\nif (success == true) { \nconst item = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true)[0];\nvar audioChannelMapping = item.getAudioChannelMapping;\nif (audioChannelMapping.audioClipsNumber > 0) { \naudioChannelMapping.audioClipsNumber = 0;\nitem.setAudioChannelMapping(audioChannelMapping);\n}\nif ((clip_width == null) || (clip_height == null)) { \nconsole.log(\"Getting dimensions from Premiere Pro\");\ntry {\nconst dimensions = getProjectItemDimensions(item);\nconsole.log(\"Dimensions:\", JSON.stringify(dimensions));\nif (dimensions) { \nclip_width = dimensions.width;\nclip_height = dimensions.height;\n}\n} catch (error) {console.log(\"Error getting project item dimensions:\", error);\nclip_width = 1920;\nclip_height = 1080;\n}\n}\nif (trackIndex == -1) { \ntrackIndex = app.project.activeSequence.videoTracks.length - 1;\n}\nvar targetTime = new Time();\ntargetTime.seconds = start_time;\napp.project.activeSequence.videoTracks[trackIndex].insertClip(item, targetTime);\nfor (var i = 0; i < app.project.activeSequence.videoTracks[trackIndex].clips.length; i += 1) { \nif (app.project.activeSequence.videoTracks[trackIndex].clips[i].projectItem.getMediaPath() == media_path) { \nclip = app.project.activeSequence.videoTracks[trackIndex].clips[i];\n}}\nif (clip) { \nvar sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\nvar sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\nvar scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\nif (scaling_mode === \"fit\") { \nscale_required = 100 * Math.min(sequence_height / clip_height, sequence_width / clip_width);\n}\nconsole.log(\"SET SCALE\", sequence_height, sequence_width, clip_height, clip_width, scale_required, scaling_mode);\nsetPropertyStatic(clip, \"Motion\", \"Scale\", scale_required);\nif (clip.end.seconds > end_time) { \nconst duration = end_time - start_time;\nvar end = new Time();\nend.seconds = end_time;\nvar outPoint = new Time();\noutPoint.seconds = duration;\nclip.end = end;\nclip.outPoint = outPoint;\n}\nreturn clip.nodeId;\n}\n}\n}\nfunction insertBrollInTrackAtTime(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, position_property, anchor_property, rotation_property, scale_property, clip_in, clip_out, scaling_mode) {\nconst success = app.project.importFiles([media_path], false, targetBin, false);\nconst frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\nstart_time = frame_rate * Math.round(parseInt(start_time / frame_rate));\nend_time = frame_rate * Math.round(parseInt(end_time / frame_rate));\nif (success === true) { \nvar items = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\nconst item = items[items.length - 1];\nvar audioChannelMapping = item.getAudioChannelMapping;\nif (audioChannelMapping.audioClipsNumber > 0) { \naudioChannelMapping.audioClipsNumber = 0;\nitem.setAudioChannelMapping(audioChannelMapping);\n}\nif ((clip_width == null) || (clip_height == null)) { \nconsole.log(\"Getting dimensions from Premiere Pro\");\ntry {\nconst dimensions = getProjectItemDimensions(item);\nconsole.log(\"Dimensions:\", JSON.stringify(dimensions));\nif (dimensions) { \nclip_width = dimensions.width;\nclip_height = dimensions.height;\n}\n} catch (error) {console.log(\"Error getting project item dimensions:\", error);\nclip_width = 1920;\nclip_height = 1080;\n}\n}\nvar tempTrackIndex = app.project.activeSequence.videoTracks.length;\nqe.project.getActiveSequence().addTracks(1, tempTrackIndex, 0, 0);\nvar targetTime = new Time();\ntargetTime.seconds = start_time;\napp.project.activeSequence.videoTracks[tempTrackIndex].insertClip(item, targetTime);\nvar track = app.project.activeSequence.videoTracks[tempTrackIndex];\nfor (var i = 0; i < track.clips.length; i += 1) { \nif ((track.clips[i].projectItem.getMediaPath() === media_path) && (Math.abs(track.clips[i].start.seconds - targetTime.seconds) < time_tolerance)) { \nnew_clip = track.clips[i];\nbreak ;\n}}\nif (new_clip) { \nvar sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\nvar sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\nvar scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\nif (scaling_mode === \"fit\") { \nscale_required = 100 * Math.min(sequence_height / clip_height, sequence_width / clip_width);\n}\nconsole.log(\"MANUAL BROLL\", sequence_height, sequence_width, scale_required);\nif (anchor_property) { \nsetPropertyStatic(new_clip, \"Motion\", \"Anchor Point\", anchor_property);\n}\nif (position_property) { \nsetPropertyStatic(new_clip, \"Motion\", \"Position\", position_property);\n}\nif (rotation_property) { \nsetPropertyStatic(new_clip, \"Motion\", \"Rotation\", rotation_property);\n}\nif (scale_property) { \nsetPropertyStatic(new_clip, \"Motion\", \"Scale\", scale_property);\n}\nelse {\nsetPropertyStatic(new_clip, \"Motion\", \"Scale\", scale_required);\n}\nif (clip_in !== null) { \nvar inPoint = new Time();\ninPoint.seconds = clip_in;\nnew_clip.inPoint = inPoint;\n}\nif (clip_out !== null) { \nvar outPoint = new Time();\noutPoint.seconds = clip_out;\nnew_clip.outPoint = outPoint;\n}\nif (new_clip.end.seconds > end_time) { \nconst duration = end_time - start_time;\nvar end = new Time();\nend.seconds = end_time;\nvar outPoint = new Time();\noutPoint.seconds = duration;\nnew_clip.end = end;\nnew_clip.outPoint = outPoint;\n}\n}\nelse {\nreturn null;\n}\nvar item_index = 1;\nif (start_time === 0) { \nitem_index = 0;\n}\nqe.project.getActiveSequence().getVideoTrackAt(tempTrackIndex).getItemAt(item_index).moveToTrack(trackIndex - tempTrackIndex, 0, \"0\");\nqe.project.getActiveSequence().removeVideoTrack(tempTrackIndex);\nreturn new_clip.nodeId;\n}\n}\nfunction addMusicToProject(media_path) {\nconst targetBin = app.project.rootItem;\nconst success = app.project.importFiles([media_path], false, targetBin, false);\nif (success == true) { \nqe.project.getActiveSequence().addTracks(0, 0, 1, 0);\nconst items = app.project.rootItem.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\nvar targetTime = new Time();\ntargetTime.seconds = 0;\napp.project.activeSequence.audioTracks[0].insertClip(items[0], targetTime);\n}\n}\nfunction addAudioTrack() {\nvar track_id = qe.project.getActiveSequence().numAudioTracks;\nqe.project.getActiveSequence().addTracks(0, 0, 1, 1, track_id);\nreturn track_id;\n}\nfunction addAudioClipToProject(audio_track_id, media_path, start_time, end_time) {\nvar targetBin = getEditAIBin();\nvar items = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\nif (items.length <= 0) { \napp.project.importFiles([media_path], false, targetBin, false);\nitems = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n}\napp.project.activeSequence.audioTracks[audio_track_id].insertClip(items[0], start_time);\nfor (var i = 0; i < app.project.activeSequence.audioTracks[audio_track_id].clips.length; i += 1) { \nvar clip = app.project.activeSequence.audioTracks[audio_track_id].clips[i];\nif (clip.projectItem.getMediaPath() === media_path) { \nselected_clip = clip;\n}}\nif (selected_clip.end.seconds > end_time) { \nconst duration = end_time - start_time;\nvar end = new Time();\nend.seconds = end_time;\nvar outPoint = new Time();","numLines":500,"startLine":1,"totalLines":2858}}}
{"parentUuid":"2999b358-3eb7-4cf2-8a20-3cf9ab930b54","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01TT1f7sfBeygS5hX3RkFNw4","type":"tool_result","content":"7:function storeDefaultTimecode() {\n12:function initialise() {\n15:function quantise_time(time_seconds) {\n16:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n19:function frames_to_time(number_of_frames) {\n20:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n23:function checkSequenceOpen() {\n31:function checkInitialisation() {\n34:function setAdjusmentLayerFilepath(path) {\n38:function setSilenceCuttingLayerFilepath(path) {\n42:function setTimecodeToDefault() {\n43:var seq = app.project.activeSequence;\n45:var currentSeqSettings = app.project.activeSequence.getSettings();\n50:function secondsToCurrentTimecode(seconds) {\n51:var seq = app.project.activeSequence;\n62:function breakLines(s, max_chars_per_line) {\n63:const words = s.split(\" \");\n64:var output = \"\";\n65:var currentLine = \"\";\n67:var word = words[j];\n73:var i = 0;\n75:const remainingChars = word.slice(i, (i + max_chars_per_line) - 1);\n76:const lastLine = (remainingChars.length === (max_chars_per_line - 1)) && ((i + remainingChars.length) < word.length);\n94:function openDocument(path) {\n97:function getTimecodeFormat() {\n100:function setTimecodeFormat(format) {\n102:var seq = app.project.activeSequence;\n104:var currentSeqSettings = app.project.activeSequence.getSettings();\n110:function setTimecodeToFrames() {\n111:var seq = app.project.activeSequence;\n113:var currentSeqSettings = app.project.activeSequence.getSettings();\n120:function razorSequenceAtFrames(frames) {\n125:function razorSequenceAtSeconds(seconds) {\n126:const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n129:function razorTrackAtFrames(trackType, trackIndex, frames) {\n141:function getActiveSequence() {\n144:function getActiveSequenceVideoClips() {\n145:var sequence = app.project.activeSequence;\n146:var sequence_clips = [];\n148:var clips = sequence.videoTracks[i].clips;\n153:function getActiveSequenceVideoClipsForTrackId(trackId) {\n154:var sequence = app.project.activeSequence;\n155:var sequence_clips = [];\n156:var clips = sequence.videoTracks[trackId].clips;\n161:function getVideoFrameRateInSeconds() {\n164:function razorTrackAtSeconds(trackType, trackIndex, seconds) {\n165:const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n168:function getTrackNames(trackType) {\n182:function deleteClipsAtSilencePointsInTrack(silences, trackType, trackIndex) {\n185:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n189:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n193:var clip = clips[i];\n194:var start = clip.start.seconds;\n195:var end = clip.end.seconds;\n197:var silence = silences[j];\n198:var silence_center = (silence[0] + silence[1]) / 2;\n203:function makeRemainingClipsContiguousInTrack(silences, deltas, trackType, trackIndex) {\n207:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n211:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n215:var clip = clips[i];\n216:var start = clip.start.seconds;\n217:var end = clip.end.seconds;\n218:var num_silences = -1;\n233:function moveAllClipsInTrackAfterSeconds(track_index, track_type, starting_time, move_amount_in_seconds) {\n235:var clips = app.project.activeSequence.videoTracks[track_index].clips;\n239:var clips = app.project.activeSequence.audioTracks[track_index].clips;\n243:var clip = clips[i];\n244:var start = clip.start.seconds;\n245:var end = clip.end.seconds;\n250:function moveAllClipsInTrack(track_index, track_type, move_amount_in_seconds) {\n252:var clips = app.project.activeSequence.videoTracks[track_index].clips;\n256:var clips = app.project.activeSequence.audioTracks[track_index].clips;\n260:var i = clips.length - 1;\n261:var increment = -1;\n264:var i = 0;\n265:var increment = 1;\n268:var clip = clips[i];\n271:function linkIdenticalClipsInTracks(videoTrackIndex, audioTrackIndex) {\n272:var selected_clips = app.project.activeSequence.getSelection();\n275:var video_clips = app.project.activeSequence.videoTracks[videoTrackIndex].clips;\n276:var audio_clips = app.project.activeSequence.audioTracks[audioTrackIndex].clips;\n278:var video_clip = video_clips[i];\n279:var video_start = video_clip.start.seconds;\n281:var audio_clip = audio_clips[j];\n282:var audio_start = audio_clip.start.seconds;\n292:function getMuteState() {\n293:var mute_state_video = [];\n296:var mute_state_audio = [];\n302:function setMuteState(mute_state) {\n316:function insertMediaInTrackAtTime(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, scaling_mode) {\n317:const success = app.project.importFiles([media_path], false, targetBin, false);\n318:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n322:const item = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true)[0];\n323:var audioChannelMapping = item.getAudioChannelMapping;\n331:const dimensions = getProjectItemDimensions(item);\n345:var targetTime = new Time();\n353:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n354:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n355:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n362:const duration = end_time - start_time;\n363:var end = new Time();\n365:var outPoint = new Time();\n374:function insertBrollInTrackAtTime(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, position_property, anchor_property, rotation_property, scale_property, clip_in, clip_out, scaling_mode) {\n375:const success = app.project.importFiles([media_path], false, targetBin, false);\n376:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n380:var items = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n381:const item = items[items.length - 1];\n382:var audioChannelMapping = item.getAudioChannelMapping;\n390:const dimensions = getProjectItemDimensions(item);\n401:var tempTrackIndex = app.project.activeSequence.videoTracks.length;\n403:var targetTime = new Time();\n406:var track = app.project.activeSequence.videoTracks[tempTrackIndex];\n413:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n414:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n415:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n436:var inPoint = new Time();\n441:var outPoint = new Time();\n446:const duration = end_time - start_time;\n447:var end = new Time();\n449:var outPoint = new Time();\n458:var item_index = 1;\n467:function addMusicToProject(media_path) {\n468:const targetBin = app.project.rootItem;\n469:const success = app.project.importFiles([media_path], false, targetBin, false);\n472:const items = app.project.rootItem.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n473:var targetTime = new Time();\n478:function addAudioTrack() {\n479:var track_id = qe.project.getActiveSequence().numAudioTracks;\n483:function addAudioClipToProject(audio_track_id, media_path, start_time, end_time) {\n484:var targetBin = getEditAIBin();\n485:var items = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n492:var clip = app.project.activeSequence.audioTracks[audio_track_id].clips[i];\n497:const duration = end_time - start_time;\n498:var end = new Time();\n500:var outPoint = new Time();\n506:function getVideoTransitionName(name) {\n507:var video_transitions = qe.project.getVideoTransitionList();\n508:var index = 0;\n510:var found = false;\n522:function getVideoEffectName(englishEffectName) {\n523:var video_effects = qe.project.getVideoEffectList();\n524:var index = 0;\n526:var found = false;\n538:function setPropertyStatic(clip, component_displayName, property_displayName, value) {\n540:var component = clip.components[i];\n543:var property = component.properties[j];\n550:function getProperty(clip, component_displayName, property_displayName) {\n552:var component = clip.components[i];\n555:var property = component.properties[j];\n561:function findItemByName(bin, string) {\n568:var item = findItemByName(bin.children[i], string);\n576:function findItemByNameAndType(bin, string, type_string) {\n591:function findItemByNodeId(bin, nodeId) {\n605:function getAllNodeIDs(bin) {\n606:const node_ids = [];\n616:function addSilenceCuttingControlClipToNewTrack(sequence_start, sequence_end) {\n617:var index_of_new_track = qe.project.getActiveSequence().numVideoTracks;\n621:var transparent_video = false;\n622:var imported_as_silence_cutting_layer = false;\n657:var start_time = new Time();\n659:var end_time = new Time();\n662:var transparent_clip = app.project.activeSequence.videoTracks[index_of_new_track].clips[0];\n666:function deleteSilenceCuttingControlClip() {\n667:var index_of_new_track = qe.project.getActiveSequence().numVideoTracks - 1;\n674:function importAdjustmentLayerIntoProject() {\n675:const adjustmentLayerPath = adjustment_layer_filepath;\n676:const sequenceID = \"4572f1e6-6bb0-4435-8bff-fb70dea8608d\";\n679:function importSilenceCuttingLayerIntoProject() {\n680:const adjustmentLayerPath = silence_cutting_layer_filepath;\n681:const sequenceID = \"2965ff0d-489b-44d7-82f8-6826514ed3b1\";\n684:function getAdjustmentLayer() {\n685:var adjustmentLayerExists = false;\n686:var adjustmentLayer = null;\n688:var child = getEditAIBin().children[i];\n701:function getSilenceCuttingLayer() {\n702:var silenceCuttingLayerExists = false;\n703:var silenceCuttingLayer = null;\n705:var child = getEditAIBin().children[i];\n718:function addZoomAtTime(start, end, animated, animation_duration, zoom_scale, trackIndex) {\n724:var targetTime = new Time();\n727:var clips = findClip(start, trackIndex);\n728:var clip_normal = clips.clip_normal;\n729:var clip_qe = clips.clip_qe;\n731:var end_time = clip_normal.end;\n734:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n735:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n736:var clip_height = 1080;\n737:var clip_width = 1920;\n738:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width) * 10;\n740:var video_effects = qe.project.getVideoEffectList();\n741:var index = 0;\n743:var found = false;\n753:var transform_effect_name_to_use = video_effects[index];\n759:var component = clip_normal.components[i];\n762:var property = component.properties[j];\n793:function addZoomAnimation(start, end, animation_duration, zoom_scale, center_point_x, center_point_y, animation_easing_power, trackIndex) {\n794:var clips = findClip(start, trackIndex);\n795:var clip_normal = clips.clip_normal;\n796:var offset = clip_normal.inPoint.seconds;\n797:var end = offset + clip_normal.duration.seconds;\n798:var videoFrameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n800:var component = clip_normal.components[i];\n803:var property = component.properties[j];\n824:var t = new Time();\n825:var last_t = new Time();\n848:function addZoomAnimationAsymmetric(start, end, zoom_scale, center_point_x, center_point_y, zoom_curve_start_power, zoom_curve_start_duration, zoom_curve_end_power, zoom_curve_end_duration, trackIndex) {\n849:var clips = findClip(start, trackIndex);\n850:var clip_normal = clips.clip_normal;\n858:var offset = clip_normal.inPoint.seconds;\n859:var end = offset + clip_normal.duration.seconds;\n864:var videoFrameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n866:var component = clip_normal.components[i];\n869:var property = component.properties[j];\n904:var t = new Time();\n905:var last_t = new Time();\n939:function findClip(start, trackIndex) {\n940:var clip_normal = null;\n941:var clip_qe = null;\n955:function getVideoTransitionList() {\n958:function addTransitionsToQEClipsOnTrack(trackIndex, transitionType, transitionLength) {\n959:var numClips = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems;\n960:var frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n963:var prev_clip_qe = null;\n964:var next_clip_qe = null;\n971:var clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n972:var start_time = parseInt(clip_qe.start.secs / frame_rate);\n973:var end_time = parseInt(clip_qe.end.secs / frame_rate);\n974:var clipLength = Math.abs(end_time - start_time);\n981:function addTransitionsToQEClipOnTrackWithName(trackIndex, transitionType, transitionLength, clipName) {\n982:var numClips = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems;\n983:var frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n986:var prev_clip_qe = null;\n990:var clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n993:var start_time = parseInt(clip_qe.start.secs / frame_rate);\n994:var end_time = parseInt(clip_qe.end.secs / frame_rate);\n995:var clipLength = Math.abs(end_time - start_time);\n1005:function addZoomAtTimeAndCenter(start, end, animated, animation_duration, zoom_scale, center_point_x, center_point_y, animation_easing_power, trackIndex) {\n1011:var targetTime = new Time();\n1014:var clips = findClip(start, trackIndex);\n1015:var clip_normal = clips.clip_normal;\n1016:var clip_qe = clips.clip_qe;\n1019:var end_time = clip_normal.end;\n1022:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1023:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1024:var clip_height = 1080;\n1025:var clip_width = 1920;\n1026:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width) * 10;\n1028:var video_effects = qe.project.getVideoEffectList();\n1029:var index = 0;\n1031:var found = false;\n1041:var transform_effect_name_to_use = video_effects[index];\n1056:function addZoomAtTimeAndCenterAsymmetric(start, end, zoom_scale, center_point_x, center_point_y, zoom_curve_start_power, zoom_curve_start_duration, zoom_curve_end_power, zoom_curve_end_duration, trackIndex) {\n1062:var targetTime = new Time();\n1065:var clips = findClip(start, trackIndex);\n1066:var clip_normal = clips.clip_normal;\n1067:var clip_qe = clips.clip_qe;\n1070:var end_time = clip_normal.end;\n1073:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1074:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1075:var clip_height = 1080;\n1076:var clip_width = 1920;\n1077:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width) * 10;\n1079:var video_effects = qe.project.getVideoEffectList();\n1080:var index = 0;\n1082:var found = false;\n1092:var transform_effect_name_to_use = video_effects[index];\n1100:function removeClipKeyFrames(start, end, trackIndex) {\n1101:var clips = findClip(start, trackIndex);\n1102:var clip_normal = clips.clip_normal;\n1103:var clip_qe = clips.clip_qe;\n1108:var component = clip_normal.components[i];\n1111:var property = component.properties[j];\n1118:function easing_curve(t, power) {\n1130:var coefficient = Math.pow(2, power - 1);\n1139:function getClipsInTrack(trackIndex, track_type) {\n1140:var clips = [];\n1153:function getClipsInTrack_Timings_Seconds(trackIndex, track_type) {\n1154:var clips = [];\n1167:function getClipsInAllTracks_Timings_Seconds(track_type) {\n1168:var clips_in_all_tracks = [];\n1171:var clips_in_this_track = [];\n1179:var clips_in_this_track = [];\n1187:function getClipsInTrack_Timings_Frames(trackIndex, track_type) {\n1189:var clips = [];\n1202:function getClipsInTrack_Timings_Frames_List(trackIndex, track_type) {\n1204:var clips = [];\n1217:function moveClipByNTracks(currentTrackId, clipStartTime, nTracksToMove, track_type) {\n1218:const accuracy = 3;\n1219:const search_start_time = clipStartTime.toFixed(accuracy);\n1221:var numClipsInTrack = qe.project.getActiveSequence().getAudioTrackAt(currentTrackId).numItems;\n1230:var numClipsInTrack = qe.project.getActiveSequence().getVideoTrackAt(currentTrackId).numItems;\n1239:function getLinkedItemsOfClip(track_type, track_index, clip_index) {\n1241:var linked_items = app.project.activeSequence.videoTracks[track_index].clips[clip_index].getLinkedItems();\n1244:var linked_items = app.project.activeSequence.audioTracks[track_index].clips[clip_index].getLinkedItems();\n1249:var linked_items_ret = [];\n1259:function getNumTracks(track_type) {\n1270:function getNumClipsInTrack(trackIndex, track_type) {\n1281:function shiftAllClipsInTrackWithinTimes(trackIndex, timeDelta, startTime, endTime, trackType) {\n1283:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n1287:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n1291:var clip = clips[i];\n1301:function addChapterClip(mogrtToImport, chapter_time, videoTrackIndex, audioTrackIndex, chapter_name) {\n1308:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1309:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1310:var clip_height = 2160;\n1311:var clip_width = 3840;\n1312:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n1316:function addChapterClipWithColours(mogrtToImport, chapter_time, videoTrackIndex, audioTrackIndex, chapter_name, start_color, end_color) {\n1327:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1328:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1329:var clip_height = 2160;\n1330:var clip_width = 3840;\n1331:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n1335:function findSequenceIDByName(name) {\n1342:function findSequenceIndexByName(name) {\n1349:function findSequenceIDByNode(nodeId) {\n1356:function findSequenceByID(sequenceID) {\n1363:function getEditAIBin() {\n1380:function removeAllLinks(track_type, track_index, clip_index) {\n1399:function selectAudioTracks(tracks_array, scope) {\n1400:var clips_to_select = [];\n1402:var inPoint = app.project.activeSequence.getInPoint();\n1403:var outPoint = app.project.activeSequence.getOutPoint();\n1404:var scopeValid = true;\n1419:var track = app.project.activeSequence.audioTracks[tracks_array[i]];\n1432:function disableClip(track_type, track_index, clip_index, affect_linked_tracks) {\n1453:function disableTrackItemByNodeId(nodeId) {\n1454:var clip = findTrackItemByNodeId(nodeId);\n1459:function enableTrackItemByNodeId(nodeId) {\n1460:var clip = findTrackItemByNodeId(nodeId);\n1465:function razorSequenceAtFramesArray(silences_array) {\n1473:function moveItemToBin(item, bin) {\n1479:function insertImageInTrackAtTime(image_project_item, trackIndex, start_ticks, duration_ticks, frame_rate, fade, pop, position_string, clip_name) {\n1488:var targetTime = new Time();\n1492:var clip_qe = null;\n1493:var clip_normal = null;\n1494:var clip_found = false;\n1495:var log = \"\";\n1496:var track = qe.project.getActiveSequence().getVideoTrackAt(trackIndex);\n1498:var trackItem = track.getItemAt(i);\n1510:var videoTrack = app.project.activeSequence.videoTracks[trackIndex];\n1521:var video_transitions = qe.project.getVideoTransitionList();\n1522:var index = 0;\n1524:var found = false;\n1534:var transition = video_transitions[index];\n1544:var component = clip_normal.components[i];\n1547:var property = component.properties[j];\n1566:var footageInterpretation = image_project_item.getFootageInterpretation();\n1571:function animateImageInTrackAtTime(clip_normal, clip_qe, frame_rate, fade, pop, position_string) {\n1574:var video_transitions = qe.project.getVideoTransitionList();\n1575:var index = 0;\n1577:var found = false;\n1587:var transition = video_transitions[index];\n1597:var component = clip_normal.components[i];\n1600:var property = component.properties[j];\n1603:var t = new Time();\n1619:function insertNestedSequenceInTrackAtTime(project_item, trackIndex, start_time) {\n1623:var targetTime = new Time();\n1627:function importStyledCaptions(g_captions_chunked, caption_render_params, index_of_new_track, random_string, layer_index, layer, word_by_word_animation_required, box_positions) {\n1632:var added_clips_names = [];\n1652:var clipName = layer_index + \"__\" + i_captions + \"_\" + j_captions + \".png\";\n1689:function createChapterMarker(time, name, duration, comments, colour) {\n1695:function getMediaPathOfFirstSelection() {\n1697:var selectedClip = app.project.activeSequence.getSelection()[0];\n1698:var trackNumber = selectedClip.parentTrackIndex;\n1699:var nodeId = selectedClip.nodeId;\n1700:var mediaPath = selectedClip.projectItem.getMediaPath();\n1701:var name = selectedClip.projectItem.name;\n1708:function getAllMediaPathsOfSelection() {\n1710:const selections = app.project.activeSequence.getSelection();\n1711:var media_paths = [];\n1720:function getSelection() {\n1723:function getSelectionCount() {\n1726:function getMediaPathOfFirstObjectInSequence(sequenceName) {\n1729:const sequence = app.project.sequences[i];\n1730:const videoTrack = sequence.videoTracks[0];\n1731:const clip = videoTrack.clips[0];\n1735:function changeSourceForUpdateNestedCaptions(nodeIds, random_string) {\n1737:var original_node_id = nodeIds[i];\n1738:var original_clip = findTrackItemByNodeId(original_node_id);\n1739:var original_name = original_clip.name;\n1740:var original_inPoint_seconds = original_clip.inPoint.seconds;\n1741:var original_outPoint_seconds = original_clip.outPoint.seconds;\n1742:var new_captions_sequence_name = \"captions_\" + random_string;\n1743:var new_captions_sequence_project_item = findItemByNameAndType(getFireCutSubBin(new_captions_sequence_name), new_captions_sequence_name, \"CLIP\");\n1748:function unmuteTrackForNodeId(nodeId) {\n1749:var clip = findTrackItemByNodeId(nodeId);\n1750:var track_index = clip.parentTrackIndex;\n1755:function muteCaptionTracks() {\n1756:const selections = app.project.activeSequence.getSelection();\n1758:const tracksToMute = [];\n1766:var trackClips = app.project.activeSequence.videoTracks[trackIndex].clips;\n1777:function getFireCutSubBin(binName) {\n1785:function getFireCutSubBinJSON(binName) {\n1786:var bin = null;\n1795:function saveFileToBin(path, contents) {\n1798:function selectVideoClipByNodeId(nodeId) {\n1799:var clip = null;\n1815:function replaceClipWithMedia(nodeId_to_replace, media_path, clip_width, clip_height, scaling_mode) {\n1816:var clip = null;\n1817:var trackIndex = null;\n1829:var start_time = clip.start.seconds;\n1830:var end_time = clip.end.seconds;\n1835:function insertMediaInTrackAtTime_v2(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, scaling_mode) {\n1836:const success = app.project.importFiles([media_path], false, targetBin, false);\n1837:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n1841:const item = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true)[0];\n1842:var audioChannelMapping = item.getAudioChannelMapping;\n1850:var offset_required = app.project.activeSequence.videoTracks[trackIndex].clips.length > 0;\n1851:var offset_seconds = 0;\n1859:var targetTime = new Time();\n1867:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1868:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1869:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n1875:var end = new Time();\n1889:function sectionContainsVideoClips(trackIndex, startTime, endTime) {\n1890:var containsClip = false;\n1892:var clip = app.project.activeSequence.videoTracks[trackIndex].clips[i];\n1893:var before = (clip.start.seconds <= startTime) && (clip.end.seconds <= startTime);\n1894:var after = (clip.start.seconds >= endTime) && (clip.end.seconds >= endTime);\n1901:function sectionContainsAudioClips(trackIndex, startTime, endTime) {\n1902:var containsClip = false;\n1904:var clip = app.project.activeSequence.audioTracks[trackIndex].clips[i];\n1905:var before = (clip.start.seconds <= startTime) && (clip.end.seconds <= startTime);\n1906:var after = (clip.start.seconds >= endTime) && (clip.end.seconds >= endTime);\n1913:function get_folder_selection() {\n1914:var folder = Folder.selectDialog(\"Select a folder to process\");\n1922:function loadSequence(newProjectPath, newSequenceId) {\n1923:const fireCutBin = getEditAIBin();\n1925:const currentDate = new Date().toLocaleString();\n1926:const previousSequence = app.project.activeSequence;\n1927:const openResult = app.project.openSequence(newSequenceId);\n1936:function addEmojiAnimations(animationTypes, trackIndex) {\n1937:const clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n1938:const qeClips = [];\n1939:var shouldFade = false;\n1940:var shouldSlide = false;\n1950:var clipIndex = 0;\n1952:var clipQe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(clipIndex);\n1959:const fade = getVideoTransitionName(\"Cross Dissolve\");\n1964:const transform = getVideoEffectName(\"Transform\");\n1965:const frameHeight = app.project.activeSequence.getSettings().videoFrameHeight;\n1966:const frameWidth = app.project.activeSequence.getSettings().videoFrameWidth;\n1967:const frameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n1968:const animationTime = 0.8;\n1969:const keyframeCount = (animationTime / 1) / frameRate;\n1970:const totalDistance = frameHeight * 0.2;\n1974:var transformTime = new Time();\n1981:var progress = 0.5 + ((0.5 * k) / (keyframeCount - 1));\n1982:var easingFactor = easing_curve(progress, 4);\n1983:var keyframeDistance = totalDistance - (totalDistance * easingFactor);\n1989:function addEmojiAnimationsInSequenceIndex(animationTypes, trackIndex, sequenceIndex) {\n1990:const clips = app.project.sequences[sequenceIndex].videoTracks[trackIndex].clips;\n1991:const qeClips = [];\n1992:var shouldFade = false;\n1993:var shouldSlide = false;\n2003:var clipIndex = 0;\n2005:var clipQe = qe.project.getSequenceAt(sequenceIndex).getVideoTrackAt(trackIndex).getItemAt(clipIndex);\n2012:const fade = getVideoTransitionName(\"Cross Dissolve\");\n2017:const transform = getVideoEffectName(\"Transform\");\n2018:const frameHeight = app.project.activeSequence.getSettings().videoFrameHeight;\n2019:const frameWidth = app.project.activeSequence.getSettings().videoFrameWidth;\n2020:const frameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n2021:const animationTime = 0.8;\n2022:const keyframeCount = (animationTime / 1) / frameRate;\n2023:const totalDistance = frameHeight * 0.2;\n2027:var transformTime = new Time();\n2034:var progress = 0.5 + ((0.5 * k) / (keyframeCount - 1));\n2035:var easingFactor = easing_curve(progress, 4);\n2036:var keyframeDistance = totalDistance - (totalDistance * easingFactor);\n2042:function addPopAndFadeToCaptions(g_captions_chunked, caption_render_params, layer, layer_index, word_by_word_animation_required, box_positions) {\n2043:var timebase = app.project.activeSequence.timebase;\n2044:var videoFrameRate = parseFloat(getVideoFrameRateInSeconds());\n2045:var frame_rate = 1 / videoFrameRate;\n2046:var position_string = \"[\" + caption_render_params.horizontal_position.toFixed(2) + \",\" + caption_render_params.vertical_position.toFixed(2) + \"]\";\n2047:var index_clip_normal = 0;\n2048:var index_clip_qe = 0;\n2062:var start_time = Math.round(g_captions_chunked[i_captions].words[j_captions].start / videoFrameRate) * videoFrameRate;\n2063:var end_time = Math.round(g_captions_chunked[i_captions].words[j_captions].end / videoFrameRate) * videoFrameRate;\n2064:var start_frames = Math.round(g_captions_chunked[i_captions].words[j_captions].start / videoFrameRate);\n2065:var end_frames = Math.round(g_captions_chunked[i_captions].words[j_captions].end / videoFrameRate);\n2066:var start_ticks = start_frames * timebase;\n2067:var end_ticks = end_frames * timebase;\n2069:var fade_or_pop_required = (((((caption_render_params.animation_reveal) && (caption_render_params.animation_reveal_fade)) && (j_captions > 0)) && ([\"text\", \"text_nobox\"].indexOf(layer) > -1)) || (((caption_render_params.animation_pop) && (j_captions == 0)) && ([\"text\", \"text_nobox\"].indexOf(layer) > -1))) || ((((caption_render_params.animation) && (caption_render_params.animation_box)) && (caption_render_params.animation_box_pop)) && (layer == \"box\"));\n2080:var currentItem = qe.project.getActiveSequence().getVideoTrackAt(layer_index).getItemAt(index_clip_qe);\n2100:const box_position_string = \"[\" + box_positions[i_captions][j_captions][0].toFixed(2) + \",\" + box_positions[i_captions][j_captions][1].toFixed(2) + \"]\";\n2107:var fade_or_pop_required = ((caption_render_params.animation) && (caption_render_params.animation_pop)) && (layer !== \"emojis\");\n2118:var currentItem = qe.project.getActiveSequence().getVideoTrackAt(layer_index).getItemAt(index_clip_qe);\n2140:function addDirectionalBlurToCaptions(g_captions_chunked, caption_render_params, layer, layer_index, word_by_word_animation_required) {\n2141:var blur_size = parseInt(caption_render_params.font_size / 4);\n2144:var chunk = g_captions_chunked[i];\n2147:var endClipName = layer_index + \"__\" + i + \"_\" + (chunk.words.length - 1) + \".png\";\n2151:var startClipName = layer_index + \"__\" + i + \"_0.png\";\n2155:var clipName = layer_index + \"__\" + i + \"_0.png\";\n2161:var endClipName = layer_index + \"__\" + i + \"_\" + (chunk.words.length - 1) + \".png\";\n2165:var startClipName = layer_index + \"__\" + i + \"_0.png\";\n2169:var endClipName = layer_index + \"__\" + i + \"_\" + (chunk.words.length - 1) + \".png\";\n2171:var startClipName = layer_index + \"__\" + i + \"_0.png\";\n2178:var clipName = layer_index + \"__\" + i + \".png\";\n2190:function wiggleOffset(coeffs, frequency, amplitude, time) {\n2191:const frequency_normalizer = 3;\n2192:const normalizedFrequency = frequency * frequency_normalizer;\n2193:var ret = 0;\n2199:function addWiggleToCaptions(captionLayers, wiggle, startTime, endTime) {\n2200:var trackIndex = captionLayers[0];\n2201:const frameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n2202:const allClips = app.project.activeSequence.videoTracks[trackIndex].clips;\n2203:var clips = [];\n2208:const frameHeight = app.project.activeSequence.getSettings().videoFrameHeight;\n2209:const frameWidth = app.project.activeSequence.getSettings().videoFrameWidth;\n2210:var video_effects = qe.project.getVideoEffectList();\n2211:var index = 0;\n2213:var found = false;\n2223:var transform_effect_name_to_use = video_effects[index];\n2224:var foundQeClips = 0;\n2225:var clipIndex = 0;\n2234:var clipIndex = 0;\n2235:var transformTime = new Time();\n2237:var time = clips[clipIndex].start.seconds;\n2238:var clipEnd = clips[clipIndex].end.seconds;\n2239:var positionProperty = getProperty(clips[clipIndex], \"Transform\", \"Position\");\n2241:var currentPosition = positionProperty.getValue();\n2263:function addTransitionToCaption(trackIndex, clipId, transition) {\n2264:const clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n2266:var clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n2272:function addDirectionalBlurEffectToClip(trackIndex, clipId, keyframesPositions, blurAmount, isNestedSequence) {\n2274:var sequence = app.project.activeSequence;\n2275:var sequence_qe = qe.project.getActiveSequence();\n2276:var clips = sequence.videoTracks[trackIndex].clips;\n2278:var clip_qe = sequence_qe.getVideoTrackAt(trackIndex).getItemAt(i);\n2284:var clip = clips[i];\n2287:var component = clip.components[j];\n2292:var property = component.properties[k];\n2296:var t1 = new Time();\n2297:var t2 = new Time();\n2298:var tqStart = quantise_time(clip.inPoint.seconds);\n2309:var t3 = new Time();\n2310:var t4 = new Time();\n2311:var tqEnd = quantise_time(clip.inPoint.seconds + clip.duration.seconds);\n2325:function addWiggleToCaptionsInSequenceIndex(captionLayers, wiggle, sequenceIndex) {\n2326:var trackIndex = captionLayers[0];\n2327:const frameRate = app.project.sequences[sequenceIndex].getSettings().videoFrameRate.seconds;\n2328:const clips = app.project.sequences[sequenceIndex].videoTracks[trackIndex].clips;\n2329:const frameHeight = app.project.sequences[sequenceIndex].getSettings().videoFrameHeight;\n2330:const frameWidth = app.project.sequences[sequenceIndex].getSettings().videoFrameWidth;\n2331:var video_effects = qe.project.getVideoEffectList();\n2332:var index = 0;\n2334:var found = false;\n2344:var transform_effect_name_to_use = video_effects[index];\n2345:var foundQeClips = 0;\n2346:var clipIndex = 0;\n2355:var clipIndex = 0;\n2356:var transformTime = new Time();\n2358:var time = clips[clipIndex].start.seconds;\n2359:var clipEnd = clips[clipIndex].end.seconds;\n2360:var positionProperty = getProperty(clips[clipIndex], \"Transform\", \"Position\");\n2362:var currentPosition = positionProperty.getValue();\n2384:function moveTemporaryScreenshots(screenshots, destinationFolderName) {\n2385:var destination = findItemByName(getEditAIBin(), destinationFolderName);\n2387:var item = findItemByName(app.project.rootItem, screenshots[i]);\n2393:function getFilesPaths(dialogTitle) {\n2394:var files = File.openDialog(dialogTitle, \"All Files:*.*\", true);\n2395:var filePaths = [];\n2403:function getFilePath(dialogTitle) {\n2404:var file = File.openDialog(dialogTitle, \"\", false);\n2410:function getStockFootagePreviewClipCount(stockFootageLibrary) {\n2411:var previewCount = 0;\n2414:var splitClip = app.project.activeSequence.videoTracks[i].clips[j].name.split(\"_\");\n2420:function getStockFootagePreviewClips(stockFootageLibrary) {\n2421:var previewClips = [];\n2422:var tracks = app.project.activeSequence.videoTracks;\n2424:var track = tracks[i];\n2427:var clip = track.clips[j];\n2428:var scale = getProperty(clip, \"Motion\", \"Scale\").getValue();\n2429:var splitClip = clip.name.split(\"_\");\n2437:function replacePreviewClipWithFullVersion(previewName, fullPath, clipWidth, clipHeight, replacedClipWidth, replacedClipHeight, replacedClipScale) {\n2438:var previewClip = null;\n2439:var previewClips = [];\n2440:const brollBin = getFireCutSubBin(\"B-roll\");\n2441:const clips = brollBin.children;\n2452:const previewNodeIds = [];\n2455:var clipTrackIndex = -1;\n2456:var clipIndex = -1;\n2472:var clip = app.project.activeSequence.videoTracks[clipTrackIndex].clips[clipIndex];\n2473:var clipStart = clip.start.seconds;\n2474:var clipEnd = clip.end.seconds;\n2475:var clipInPoint = clip.inPoint.seconds;\n2476:var clipOutPoint = clip.inPoint.seconds;\n2479:var positionProperty = [0.5, 0.5];\n2480:var anchorProperty = [0.5, 0.5];\n2481:var rotationProperty = 0;\n2488:var adjustedScale = (replacedClipWidth * replacedClipScale) / clipWidth;\n2491:var nodeId = insertBrollInTrackAtTime(fullPath, brollBin, clipTrackIndex, clipStart, clipEnd, clipWidth, clipHeight, positionProperty, anchorProperty, rotationProperty, adjustedScale, clipInPoint, clipOutPoint, \"\");\n2494:function getChapterMarkers() {\n2495:var response = [];\n2496:var markers = app.project.activeSequence.markers;\n2503:function deleteChapterMarker(markerGuid) {\n2504:var markers = app.project.activeSequence.markers;\n2512:function deleteAllChapterMarks() {\n2513:var markers = app.project.activeSequence.markers;\n2514:var marksToDelete = [];\n2522:function resizeVideoClip(videoTrackIndex, clipIndex, newDuration) {\n2523:var t = new Time();\n2527:function findTrackItemByNodeId(nodeId) {\n2540:function deleteTrackItemByNodeId(nodeId) {\n2541:var clip = findTrackItemByNodeId(nodeId);\n2548:function updateZoomTimesByNodeId(nodeId, start, end) {\n2549:var clip = findTrackItemByNodeId(nodeId);\n2551:var t = new Time();\n2560:function getZoomClipsAccordingToSequence(zoomsAccordingToJS) {\n2562:var ret = [];\n2564:var clip = findTrackItemByNodeId(zoomsAccordingToJS[i].nodeId);\n2566:var zoomClipAccordingToSequence = {};\n2574:function createBlankSequenceWithSameSettings(originalSequenceID, newSequenceName, targetBinName, sequencePresetPath) {\n2575:var originalSequence = findSequenceByID(originalSequenceID);\n2579:var originalSequenceSettings = originalSequence.getSettings();\n2581:var targetBin = getFireCutSubBin(targetBinName);\n2582:var newSequenceProjectItem = findItemByName(app.project.rootItem, newSequenceName);\n2584:var newSequenceID = findSequenceIDByName(newSequenceName);\n2585:var newSequence = findSequenceByID(newSequenceID);\n2589:function updateSequenceAspectRatio(sequenceID, aspectRatio) {\n2590:var sequence = findSequenceByID(sequenceID);\n2594:var settings = sequence.getSettings();\n2599:function insertViewboxInTrack(project_item, track_index, viewbox_params, include_audio) {\n2603:var targetTime = new Time();\n2607:var linked_items = app.project.activeSequence.videoTracks[track_index].clips[0].getLinkedItems();\n2624:function getTimeRegionOfNodeId(nodeId) {\n2625:var clip = findTrackItemByNodeId(nodeId);\n2631:function getFullActiveSequenceObject() {\n2633:var activeSequence = JSON.parse(JSON.stringify(app.project.activeSequence));\n2634:var videoTracks = expandObjectProperty(app.project.activeSequence, \"videoTracks\");\n2635:var audioTracks = expandObjectProperty(app.project.activeSequence, \"audioTracks\");\n2637:var videoTrack = videoTracks[i];\n2638:var videoTrackSource = app.project.activeSequence.videoTracks[i];\n2640:var clips = videoTrack.clips;\n2642:var clip = clips[j];\n2643:var clipSource = videoTrackSource.clips[j];\n2647:var audioTrack = audioTracks[i];\n2648:var audioTrackSource = app.project.activeSequence.audioTracks[i];\n2654:function expandObjectProperty(obj, propertyName) {\n2656:var array = [];\n2657:var arrayItemsCount = obj[propertyName].length;\n2658:var a = obj[propertyName];\n2667:function deepRemoveProperty(obj, targetProp) {\n2671:var item = obj[i];\n2687:function exportSequenceObject() {\n2688:var activeSequenceObject = getFullActiveSequenceObject();\n2692:function exportActiveSequenceData() {\n2693:var sequence = {};\n2792:function getTotalClipsInTracks(track) {\n2793:var totalClips = 0;\n2798:function insertAudioInTrackAtTime(audio_project_item, trackIndex, start_seconds) {\n2805:var targetTime = new Time();\n2810:function getProjectItemDimensions(projectItem) {\n2812:const columnsMetadata = JSON.parse(projectItem.getProjectColumnsMetadata());\n2813:const videoInfoColumn = columnsMetadata.filter(function (column) {\n2816:const videoInfo = videoInfoColumn.ColumnValue;\n2817:const dimensions = {height: parseFloat(videoInfo.split(\"x\")[1].split(\"(\")[0].trim()), width: parseFloat(videoInfo.split(\"x\")[0].trim())};\n2826:[Omitted long matching line]\n2827:[Omitted long matching line]\n2828:var project_item_type = {BIN: 2, CLIP: 1, FILE: 4, ROOT: 3, bin: 2, clip: 1, file: 4, root: 3};\n2829:var console = {log: function () {\n2835:var eventObj = new CSXSEvent();\n2844:var sequence_default_timecode = TIMECODES.TIMEDISPLAY_25Timecode;\n2848:var i = this.length;\n2857:var temp_bin = null;"}]},"uuid":"fc8959ba-c8b1-4a1a-825c-54b539543b6d","timestamp":"2025-12-25T18:00:56.766Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"7:function storeDefaultTimecode() {\n12:function initialise() {\n15:function quantise_time(time_seconds) {\n16:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n19:function frames_to_time(number_of_frames) {\n20:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n23:function checkSequenceOpen() {\n31:function checkInitialisation() {\n34:function setAdjusmentLayerFilepath(path) {\n38:function setSilenceCuttingLayerFilepath(path) {\n42:function setTimecodeToDefault() {\n43:var seq = app.project.activeSequence;\n45:var currentSeqSettings = app.project.activeSequence.getSettings();\n50:function secondsToCurrentTimecode(seconds) {\n51:var seq = app.project.activeSequence;\n62:function breakLines(s, max_chars_per_line) {\n63:const words = s.split(\" \");\n64:var output = \"\";\n65:var currentLine = \"\";\n67:var word = words[j];\n73:var i = 0;\n75:const remainingChars = word.slice(i, (i + max_chars_per_line) - 1);\n76:const lastLine = (remainingChars.length === (max_chars_per_line - 1)) && ((i + remainingChars.length) < word.length);\n94:function openDocument(path) {\n97:function getTimecodeFormat() {\n100:function setTimecodeFormat(format) {\n102:var seq = app.project.activeSequence;\n104:var currentSeqSettings = app.project.activeSequence.getSettings();\n110:function setTimecodeToFrames() {\n111:var seq = app.project.activeSequence;\n113:var currentSeqSettings = app.project.activeSequence.getSettings();\n120:function razorSequenceAtFrames(frames) {\n125:function razorSequenceAtSeconds(seconds) {\n126:const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n129:function razorTrackAtFrames(trackType, trackIndex, frames) {\n141:function getActiveSequence() {\n144:function getActiveSequenceVideoClips() {\n145:var sequence = app.project.activeSequence;\n146:var sequence_clips = [];\n148:var clips = sequence.videoTracks[i].clips;\n153:function getActiveSequenceVideoClipsForTrackId(trackId) {\n154:var sequence = app.project.activeSequence;\n155:var sequence_clips = [];\n156:var clips = sequence.videoTracks[trackId].clips;\n161:function getVideoFrameRateInSeconds() {\n164:function razorTrackAtSeconds(trackType, trackIndex, seconds) {\n165:const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n168:function getTrackNames(trackType) {\n182:function deleteClipsAtSilencePointsInTrack(silences, trackType, trackIndex) {\n185:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n189:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n193:var clip = clips[i];\n194:var start = clip.start.seconds;\n195:var end = clip.end.seconds;\n197:var silence = silences[j];\n198:var silence_center = (silence[0] + silence[1]) / 2;\n203:function makeRemainingClipsContiguousInTrack(silences, deltas, trackType, trackIndex) {\n207:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n211:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n215:var clip = clips[i];\n216:var start = clip.start.seconds;\n217:var end = clip.end.seconds;\n218:var num_silences = -1;\n233:function moveAllClipsInTrackAfterSeconds(track_index, track_type, starting_time, move_amount_in_seconds) {\n235:var clips = app.project.activeSequence.videoTracks[track_index].clips;\n239:var clips = app.project.activeSequence.audioTracks[track_index].clips;\n243:var clip = clips[i];\n244:var start = clip.start.seconds;\n245:var end = clip.end.seconds;\n250:function moveAllClipsInTrack(track_index, track_type, move_amount_in_seconds) {\n252:var clips = app.project.activeSequence.videoTracks[track_index].clips;\n256:var clips = app.project.activeSequence.audioTracks[track_index].clips;\n260:var i = clips.length - 1;\n261:var increment = -1;\n264:var i = 0;\n265:var increment = 1;\n268:var clip = clips[i];\n271:function linkIdenticalClipsInTracks(videoTrackIndex, audioTrackIndex) {\n272:var selected_clips = app.project.activeSequence.getSelection();\n275:var video_clips = app.project.activeSequence.videoTracks[videoTrackIndex].clips;\n276:var audio_clips = app.project.activeSequence.audioTracks[audioTrackIndex].clips;\n278:var video_clip = video_clips[i];\n279:var video_start = video_clip.start.seconds;\n281:var audio_clip = audio_clips[j];\n282:var audio_start = audio_clip.start.seconds;\n292:function getMuteState() {\n293:var mute_state_video = [];\n296:var mute_state_audio = [];\n302:function setMuteState(mute_state) {\n316:function insertMediaInTrackAtTime(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, scaling_mode) {\n317:const success = app.project.importFiles([media_path], false, targetBin, false);\n318:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n322:const item = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true)[0];\n323:var audioChannelMapping = item.getAudioChannelMapping;\n331:const dimensions = getProjectItemDimensions(item);\n345:var targetTime = new Time();\n353:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n354:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n355:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n362:const duration = end_time - start_time;\n363:var end = new Time();\n365:var outPoint = new Time();\n374:function insertBrollInTrackAtTime(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, position_property, anchor_property, rotation_property, scale_property, clip_in, clip_out, scaling_mode) {\n375:const success = app.project.importFiles([media_path], false, targetBin, false);\n376:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n380:var items = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n381:const item = items[items.length - 1];\n382:var audioChannelMapping = item.getAudioChannelMapping;\n390:const dimensions = getProjectItemDimensions(item);\n401:var tempTrackIndex = app.project.activeSequence.videoTracks.length;\n403:var targetTime = new Time();\n406:var track = app.project.activeSequence.videoTracks[tempTrackIndex];\n413:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n414:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n415:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n436:var inPoint = new Time();\n441:var outPoint = new Time();\n446:const duration = end_time - start_time;\n447:var end = new Time();\n449:var outPoint = new Time();\n458:var item_index = 1;\n467:function addMusicToProject(media_path) {\n468:const targetBin = app.project.rootItem;\n469:const success = app.project.importFiles([media_path], false, targetBin, false);\n472:const items = app.project.rootItem.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n473:var targetTime = new Time();\n478:function addAudioTrack() {\n479:var track_id = qe.project.getActiveSequence().numAudioTracks;\n483:function addAudioClipToProject(audio_track_id, media_path, start_time, end_time) {\n484:var targetBin = getEditAIBin();\n485:var items = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true);\n492:var clip = app.project.activeSequence.audioTracks[audio_track_id].clips[i];\n497:const duration = end_time - start_time;\n498:var end = new Time();\n500:var outPoint = new Time();\n506:function getVideoTransitionName(name) {\n507:var video_transitions = qe.project.getVideoTransitionList();\n508:var index = 0;\n510:var found = false;\n522:function getVideoEffectName(englishEffectName) {\n523:var video_effects = qe.project.getVideoEffectList();\n524:var index = 0;\n526:var found = false;\n538:function setPropertyStatic(clip, component_displayName, property_displayName, value) {\n540:var component = clip.components[i];\n543:var property = component.properties[j];\n550:function getProperty(clip, component_displayName, property_displayName) {\n552:var component = clip.components[i];\n555:var property = component.properties[j];\n561:function findItemByName(bin, string) {\n568:var item = findItemByName(bin.children[i], string);\n576:function findItemByNameAndType(bin, string, type_string) {\n591:function findItemByNodeId(bin, nodeId) {\n605:function getAllNodeIDs(bin) {\n606:const node_ids = [];\n616:function addSilenceCuttingControlClipToNewTrack(sequence_start, sequence_end) {\n617:var index_of_new_track = qe.project.getActiveSequence().numVideoTracks;\n621:var transparent_video = false;\n622:var imported_as_silence_cutting_layer = false;\n657:var start_time = new Time();\n659:var end_time = new Time();\n662:var transparent_clip = app.project.activeSequence.videoTracks[index_of_new_track].clips[0];\n666:function deleteSilenceCuttingControlClip() {\n667:var index_of_new_track = qe.project.getActiveSequence().numVideoTracks - 1;\n674:function importAdjustmentLayerIntoProject() {\n675:const adjustmentLayerPath = adjustment_layer_filepath;\n676:const sequenceID = \"4572f1e6-6bb0-4435-8bff-fb70dea8608d\";\n679:function importSilenceCuttingLayerIntoProject() {\n680:const adjustmentLayerPath = silence_cutting_layer_filepath;\n681:const sequenceID = \"2965ff0d-489b-44d7-82f8-6826514ed3b1\";\n684:function getAdjustmentLayer() {\n685:var adjustmentLayerExists = false;\n686:var adjustmentLayer = null;\n688:var child = getEditAIBin().children[i];\n701:function getSilenceCuttingLayer() {\n702:var silenceCuttingLayerExists = false;\n703:var silenceCuttingLayer = null;\n705:var child = getEditAIBin().children[i];\n718:function addZoomAtTime(start, end, animated, animation_duration, zoom_scale, trackIndex) {\n724:var targetTime = new Time();\n727:var clips = findClip(start, trackIndex);\n728:var clip_normal = clips.clip_normal;\n729:var clip_qe = clips.clip_qe;\n731:var end_time = clip_normal.end;\n734:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n735:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n736:var clip_height = 1080;\n737:var clip_width = 1920;\n738:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width) * 10;\n740:var video_effects = qe.project.getVideoEffectList();\n741:var index = 0;\n743:var found = false;\n753:var transform_effect_name_to_use = video_effects[index];\n759:var component = clip_normal.components[i];\n762:var property = component.properties[j];\n793:function addZoomAnimation(start, end, animation_duration, zoom_scale, center_point_x, center_point_y, animation_easing_power, trackIndex) {\n794:var clips = findClip(start, trackIndex);\n795:var clip_normal = clips.clip_normal;\n796:var offset = clip_normal.inPoint.seconds;\n797:var end = offset + clip_normal.duration.seconds;\n798:var videoFrameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n800:var component = clip_normal.components[i];\n803:var property = component.properties[j];\n824:var t = new Time();\n825:var last_t = new Time();\n848:function addZoomAnimationAsymmetric(start, end, zoom_scale, center_point_x, center_point_y, zoom_curve_start_power, zoom_curve_start_duration, zoom_curve_end_power, zoom_curve_end_duration, trackIndex) {\n849:var clips = findClip(start, trackIndex);\n850:var clip_normal = clips.clip_normal;\n858:var offset = clip_normal.inPoint.seconds;\n859:var end = offset + clip_normal.duration.seconds;\n864:var videoFrameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n866:var component = clip_normal.components[i];\n869:var property = component.properties[j];\n904:var t = new Time();\n905:var last_t = new Time();\n939:function findClip(start, trackIndex) {\n940:var clip_normal = null;\n941:var clip_qe = null;\n955:function getVideoTransitionList() {\n958:function addTransitionsToQEClipsOnTrack(trackIndex, transitionType, transitionLength) {\n959:var numClips = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems;\n960:var frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n963:var prev_clip_qe = null;\n964:var next_clip_qe = null;\n971:var clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n972:var start_time = parseInt(clip_qe.start.secs / frame_rate);\n973:var end_time = parseInt(clip_qe.end.secs / frame_rate);\n974:var clipLength = Math.abs(end_time - start_time);\n981:function addTransitionsToQEClipOnTrackWithName(trackIndex, transitionType, transitionLength, clipName) {\n982:var numClips = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems;\n983:var frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n986:var prev_clip_qe = null;\n990:var clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n993:var start_time = parseInt(clip_qe.start.secs / frame_rate);\n994:var end_time = parseInt(clip_qe.end.secs / frame_rate);\n995:var clipLength = Math.abs(end_time - start_time);\n1005:function addZoomAtTimeAndCenter(start, end, animated, animation_duration, zoom_scale, center_point_x, center_point_y, animation_easing_power, trackIndex) {\n1011:var targetTime = new Time();\n1014:var clips = findClip(start, trackIndex);\n1015:var clip_normal = clips.clip_normal;\n1016:var clip_qe = clips.clip_qe;\n1019:var end_time = clip_normal.end;\n1022:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1023:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1024:var clip_height = 1080;\n1025:var clip_width = 1920;\n1026:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width) * 10;\n1028:var video_effects = qe.project.getVideoEffectList();\n1029:var index = 0;\n1031:var found = false;\n1041:var transform_effect_name_to_use = video_effects[index];\n1056:function addZoomAtTimeAndCenterAsymmetric(start, end, zoom_scale, center_point_x, center_point_y, zoom_curve_start_power, zoom_curve_start_duration, zoom_curve_end_power, zoom_curve_end_duration, trackIndex) {\n1062:var targetTime = new Time();\n1065:var clips = findClip(start, trackIndex);\n1066:var clip_normal = clips.clip_normal;\n1067:var clip_qe = clips.clip_qe;\n1070:var end_time = clip_normal.end;\n1073:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1074:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1075:var clip_height = 1080;\n1076:var clip_width = 1920;\n1077:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width) * 10;\n1079:var video_effects = qe.project.getVideoEffectList();\n1080:var index = 0;\n1082:var found = false;\n1092:var transform_effect_name_to_use = video_effects[index];\n1100:function removeClipKeyFrames(start, end, trackIndex) {\n1101:var clips = findClip(start, trackIndex);\n1102:var clip_normal = clips.clip_normal;\n1103:var clip_qe = clips.clip_qe;\n1108:var component = clip_normal.components[i];\n1111:var property = component.properties[j];\n1118:function easing_curve(t, power) {\n1130:var coefficient = Math.pow(2, power - 1);\n1139:function getClipsInTrack(trackIndex, track_type) {\n1140:var clips = [];\n1153:function getClipsInTrack_Timings_Seconds(trackIndex, track_type) {\n1154:var clips = [];\n1167:function getClipsInAllTracks_Timings_Seconds(track_type) {\n1168:var clips_in_all_tracks = [];\n1171:var clips_in_this_track = [];\n1179:var clips_in_this_track = [];\n1187:function getClipsInTrack_Timings_Frames(trackIndex, track_type) {\n1189:var clips = [];\n1202:function getClipsInTrack_Timings_Frames_List(trackIndex, track_type) {\n1204:var clips = [];\n1217:function moveClipByNTracks(currentTrackId, clipStartTime, nTracksToMove, track_type) {\n1218:const accuracy = 3;\n1219:const search_start_time = clipStartTime.toFixed(accuracy);\n1221:var numClipsInTrack = qe.project.getActiveSequence().getAudioTrackAt(currentTrackId).numItems;\n1230:var numClipsInTrack = qe.project.getActiveSequence().getVideoTrackAt(currentTrackId).numItems;\n1239:function getLinkedItemsOfClip(track_type, track_index, clip_index) {\n1241:var linked_items = app.project.activeSequence.videoTracks[track_index].clips[clip_index].getLinkedItems();\n1244:var linked_items = app.project.activeSequence.audioTracks[track_index].clips[clip_index].getLinkedItems();\n1249:var linked_items_ret = [];\n1259:function getNumTracks(track_type) {\n1270:function getNumClipsInTrack(trackIndex, track_type) {\n1281:function shiftAllClipsInTrackWithinTimes(trackIndex, timeDelta, startTime, endTime, trackType) {\n1283:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n1287:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n1291:var clip = clips[i];\n1301:function addChapterClip(mogrtToImport, chapter_time, videoTrackIndex, audioTrackIndex, chapter_name) {\n1308:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1309:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1310:var clip_height = 2160;\n1311:var clip_width = 3840;\n1312:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n1316:function addChapterClipWithColours(mogrtToImport, chapter_time, videoTrackIndex, audioTrackIndex, chapter_name, start_color, end_color) {\n1327:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1328:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1329:var clip_height = 2160;\n1330:var clip_width = 3840;\n1331:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n1335:function findSequenceIDByName(name) {\n1342:function findSequenceIndexByName(name) {\n1349:function findSequenceIDByNode(nodeId) {\n1356:function findSequenceByID(sequenceID) {\n1363:function getEditAIBin() {\n1380:function removeAllLinks(track_type, track_index, clip_index) {\n1399:function selectAudioTracks(tracks_array, scope) {\n1400:var clips_to_select = [];\n1402:var inPoint = app.project.activeSequence.getInPoint();\n1403:var outPoint = app.project.activeSequence.getOutPoint();\n1404:var scopeValid = true;\n1419:var track = app.project.activeSequence.audioTracks[tracks_array[i]];\n1432:function disableClip(track_type, track_index, clip_index, affect_linked_tracks) {\n1453:function disableTrackItemByNodeId(nodeId) {\n1454:var clip = findTrackItemByNodeId(nodeId);\n1459:function enableTrackItemByNodeId(nodeId) {\n1460:var clip = findTrackItemByNodeId(nodeId);\n1465:function razorSequenceAtFramesArray(silences_array) {\n1473:function moveItemToBin(item, bin) {\n1479:function insertImageInTrackAtTime(image_project_item, trackIndex, start_ticks, duration_ticks, frame_rate, fade, pop, position_string, clip_name) {\n1488:var targetTime = new Time();\n1492:var clip_qe = null;\n1493:var clip_normal = null;\n1494:var clip_found = false;\n1495:var log = \"\";\n1496:var track = qe.project.getActiveSequence().getVideoTrackAt(trackIndex);\n1498:var trackItem = track.getItemAt(i);\n1510:var videoTrack = app.project.activeSequence.videoTracks[trackIndex];\n1521:var video_transitions = qe.project.getVideoTransitionList();\n1522:var index = 0;\n1524:var found = false;\n1534:var transition = video_transitions[index];\n1544:var component = clip_normal.components[i];\n1547:var property = component.properties[j];\n1566:var footageInterpretation = image_project_item.getFootageInterpretation();\n1571:function animateImageInTrackAtTime(clip_normal, clip_qe, frame_rate, fade, pop, position_string) {\n1574:var video_transitions = qe.project.getVideoTransitionList();\n1575:var index = 0;\n1577:var found = false;\n1587:var transition = video_transitions[index];\n1597:var component = clip_normal.components[i];\n1600:var property = component.properties[j];\n1603:var t = new Time();\n1619:function insertNestedSequenceInTrackAtTime(project_item, trackIndex, start_time) {\n1623:var targetTime = new Time();\n1627:function importStyledCaptions(g_captions_chunked, caption_render_params, index_of_new_track, random_string, layer_index, layer, word_by_word_animation_required, box_positions) {\n1632:var added_clips_names = [];\n1652:var clipName = layer_index + \"__\" + i_captions + \"_\" + j_captions + \".png\";\n1689:function createChapterMarker(time, name, duration, comments, colour) {\n1695:function getMediaPathOfFirstSelection() {\n1697:var selectedClip = app.project.activeSequence.getSelection()[0];\n1698:var trackNumber = selectedClip.parentTrackIndex;\n1699:var nodeId = selectedClip.nodeId;\n1700:var mediaPath = selectedClip.projectItem.getMediaPath();\n1701:var name = selectedClip.projectItem.name;\n1708:function getAllMediaPathsOfSelection() {\n1710:const selections = app.project.activeSequence.getSelection();\n1711:var media_paths = [];\n1720:function getSelection() {\n1723:function getSelectionCount() {\n1726:function getMediaPathOfFirstObjectInSequence(sequenceName) {\n1729:const sequence = app.project.sequences[i];\n1730:const videoTrack = sequence.videoTracks[0];\n1731:const clip = videoTrack.clips[0];\n1735:function changeSourceForUpdateNestedCaptions(nodeIds, random_string) {\n1737:var original_node_id = nodeIds[i];\n1738:var original_clip = findTrackItemByNodeId(original_node_id);\n1739:var original_name = original_clip.name;\n1740:var original_inPoint_seconds = original_clip.inPoint.seconds;\n1741:var original_outPoint_seconds = original_clip.outPoint.seconds;\n1742:var new_captions_sequence_name = \"captions_\" + random_string;\n1743:var new_captions_sequence_project_item = findItemByNameAndType(getFireCutSubBin(new_captions_sequence_name), new_captions_sequence_name, \"CLIP\");\n1748:function unmuteTrackForNodeId(nodeId) {\n1749:var clip = findTrackItemByNodeId(nodeId);\n1750:var track_index = clip.parentTrackIndex;\n1755:function muteCaptionTracks() {\n1756:const selections = app.project.activeSequence.getSelection();\n1758:const tracksToMute = [];\n1766:var trackClips = app.project.activeSequence.videoTracks[trackIndex].clips;\n1777:function getFireCutSubBin(binName) {\n1785:function getFireCutSubBinJSON(binName) {\n1786:var bin = null;\n1795:function saveFileToBin(path, contents) {\n1798:function selectVideoClipByNodeId(nodeId) {\n1799:var clip = null;\n1815:function replaceClipWithMedia(nodeId_to_replace, media_path, clip_width, clip_height, scaling_mode) {\n1816:var clip = null;\n1817:var trackIndex = null;\n1829:var start_time = clip.start.seconds;\n1830:var end_time = clip.end.seconds;\n1835:function insertMediaInTrackAtTime_v2(media_path, targetBin, trackIndex, start_time, end_time, clip_width, clip_height, scaling_mode) {\n1836:const success = app.project.importFiles([media_path], false, targetBin, false);\n1837:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n1841:const item = targetBin.findItemsMatchingMediaPath(media_path, ignoreSubclips = true)[0];\n1842:var audioChannelMapping = item.getAudioChannelMapping;\n1850:var offset_required = app.project.activeSequence.videoTracks[trackIndex].clips.length > 0;\n1851:var offset_seconds = 0;\n1859:var targetTime = new Time();\n1867:var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n1868:var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n1869:var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width);\n1875:var end = new Time();\n1889:function sectionContainsVideoClips(trackIndex, startTime, endTime) {\n1890:var containsClip = false;\n1892:var clip = app.project.activeSequence.videoTracks[trackIndex].clips[i];\n1893:var before = (clip.start.seconds <= startTime) && (clip.end.seconds <= startTime);\n1894:var after = (clip.start.seconds >= endTime) && (clip.end.seconds >= endTime);\n1901:function sectionContainsAudioClips(trackIndex, startTime, endTime) {\n1902:var containsClip = false;\n1904:var clip = app.project.activeSequence.audioTracks[trackIndex].clips[i];\n1905:var before = (clip.start.seconds <= startTime) && (clip.end.seconds <= startTime);\n1906:var after = (clip.start.seconds >= endTime) && (clip.end.seconds >= endTime);\n1913:function get_folder_selection() {\n1914:var folder = Folder.selectDialog(\"Select a folder to process\");\n1922:function loadSequence(newProjectPath, newSequenceId) {\n1923:const fireCutBin = getEditAIBin();\n1925:const currentDate = new Date().toLocaleString();\n1926:const previousSequence = app.project.activeSequence;\n1927:const openResult = app.project.openSequence(newSequenceId);\n1936:function addEmojiAnimations(animationTypes, trackIndex) {\n1937:const clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n1938:const qeClips = [];\n1939:var shouldFade = false;\n1940:var shouldSlide = false;\n1950:var clipIndex = 0;\n1952:var clipQe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(clipIndex);\n1959:const fade = getVideoTransitionName(\"Cross Dissolve\");\n1964:const transform = getVideoEffectName(\"Transform\");\n1965:const frameHeight = app.project.activeSequence.getSettings().videoFrameHeight;\n1966:const frameWidth = app.project.activeSequence.getSettings().videoFrameWidth;\n1967:const frameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n1968:const animationTime = 0.8;\n1969:const keyframeCount = (animationTime / 1) / frameRate;\n1970:const totalDistance = frameHeight * 0.2;\n1974:var transformTime = new Time();\n1981:var progress = 0.5 + ((0.5 * k) / (keyframeCount - 1));\n1982:var easingFactor = easing_curve(progress, 4);\n1983:var keyframeDistance = totalDistance - (totalDistance * easingFactor);\n1989:function addEmojiAnimationsInSequenceIndex(animationTypes, trackIndex, sequenceIndex) {\n1990:const clips = app.project.sequences[sequenceIndex].videoTracks[trackIndex].clips;\n1991:const qeClips = [];\n1992:var shouldFade = false;\n1993:var shouldSlide = false;\n2003:var clipIndex = 0;\n2005:var clipQe = qe.project.getSequenceAt(sequenceIndex).getVideoTrackAt(trackIndex).getItemAt(clipIndex);\n2012:const fade = getVideoTransitionName(\"Cross Dissolve\");\n2017:const transform = getVideoEffectName(\"Transform\");\n2018:const frameHeight = app.project.activeSequence.getSettings().videoFrameHeight;\n2019:const frameWidth = app.project.activeSequence.getSettings().videoFrameWidth;\n2020:const frameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n2021:const animationTime = 0.8;\n2022:const keyframeCount = (animationTime / 1) / frameRate;\n2023:const totalDistance = frameHeight * 0.2;\n2027:var transformTime = new Time();\n2034:var progress = 0.5 + ((0.5 * k) / (keyframeCount - 1));\n2035:var easingFactor = easing_curve(progress, 4);\n2036:var keyframeDistance = totalDistance - (totalDistance * easingFactor);\n2042:function addPopAndFadeToCaptions(g_captions_chunked, caption_render_params, layer, layer_index, word_by_word_animation_required, box_positions) {\n2043:var timebase = app.project.activeSequence.timebase;\n2044:var videoFrameRate = parseFloat(getVideoFrameRateInSeconds());\n2045:var frame_rate = 1 / videoFrameRate;\n2046:var position_string = \"[\" + caption_render_params.horizontal_position.toFixed(2) + \",\" + caption_render_params.vertical_position.toFixed(2) + \"]\";\n2047:var index_clip_normal = 0;\n2048:var index_clip_qe = 0;\n2062:var start_time = Math.round(g_captions_chunked[i_captions].words[j_captions].start / videoFrameRate) * videoFrameRate;\n2063:var end_time = Math.round(g_captions_chunked[i_captions].words[j_captions].end / videoFrameRate) * videoFrameRate;\n2064:var start_frames = Math.round(g_captions_chunked[i_captions].words[j_captions].start / videoFrameRate);\n2065:var end_frames = Math.round(g_captions_chunked[i_captions].words[j_captions].end / videoFrameRate);\n2066:var start_ticks = start_frames * timebase;\n2067:var end_ticks = end_frames * timebase;\n2069:var fade_or_pop_required = (((((caption_render_params.animation_reveal) && (caption_render_params.animation_reveal_fade)) && (j_captions > 0)) && ([\"text\", \"text_nobox\"].indexOf(layer) > -1)) || (((caption_render_params.animation_pop) && (j_captions == 0)) && ([\"text\", \"text_nobox\"].indexOf(layer) > -1))) || ((((caption_render_params.animation) && (caption_render_params.animation_box)) && (caption_render_params.animation_box_pop)) && (layer == \"box\"));\n2080:var currentItem = qe.project.getActiveSequence().getVideoTrackAt(layer_index).getItemAt(index_clip_qe);\n2100:const box_position_string = \"[\" + box_positions[i_captions][j_captions][0].toFixed(2) + \",\" + box_positions[i_captions][j_captions][1].toFixed(2) + \"]\";\n2107:var fade_or_pop_required = ((caption_render_params.animation) && (caption_render_params.animation_pop)) && (layer !== \"emojis\");\n2118:var currentItem = qe.project.getActiveSequence().getVideoTrackAt(layer_index).getItemAt(index_clip_qe);\n2140:function addDirectionalBlurToCaptions(g_captions_chunked, caption_render_params, layer, layer_index, word_by_word_animation_required) {\n2141:var blur_size = parseInt(caption_render_params.font_size / 4);\n2144:var chunk = g_captions_chunked[i];\n2147:var endClipName = layer_index + \"__\" + i + \"_\" + (chunk.words.length - 1) + \".png\";\n2151:var startClipName = layer_index + \"__\" + i + \"_0.png\";\n2155:var clipName = layer_index + \"__\" + i + \"_0.png\";\n2161:var endClipName = layer_index + \"__\" + i + \"_\" + (chunk.words.length - 1) + \".png\";\n2165:var startClipName = layer_index + \"__\" + i + \"_0.png\";\n2169:var endClipName = layer_index + \"__\" + i + \"_\" + (chunk.words.length - 1) + \".png\";\n2171:var startClipName = layer_index + \"__\" + i + \"_0.png\";\n2178:var clipName = layer_index + \"__\" + i + \".png\";\n2190:function wiggleOffset(coeffs, frequency, amplitude, time) {\n2191:const frequency_normalizer = 3;\n2192:const normalizedFrequency = frequency * frequency_normalizer;\n2193:var ret = 0;\n2199:function addWiggleToCaptions(captionLayers, wiggle, startTime, endTime) {\n2200:var trackIndex = captionLayers[0];\n2201:const frameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n2202:const allClips = app.project.activeSequence.videoTracks[trackIndex].clips;\n2203:var clips = [];\n2208:const frameHeight = app.project.activeSequence.getSettings().videoFrameHeight;\n2209:const frameWidth = app.project.activeSequence.getSettings().videoFrameWidth;\n2210:var video_effects = qe.project.getVideoEffectList();\n2211:var index = 0;\n2213:var found = false;\n2223:var transform_effect_name_to_use = video_effects[index];\n2224:var foundQeClips = 0;\n2225:var clipIndex = 0;\n2234:var clipIndex = 0;\n2235:var transformTime = new Time();\n2237:var time = clips[clipIndex].start.seconds;\n2238:var clipEnd = clips[clipIndex].end.seconds;\n2239:var positionProperty = getProperty(clips[clipIndex], \"Transform\", \"Position\");\n2241:var currentPosition = positionProperty.getValue();\n2263:function addTransitionToCaption(trackIndex, clipId, transition) {\n2264:const clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n2266:var clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n2272:function addDirectionalBlurEffectToClip(trackIndex, clipId, keyframesPositions, blurAmount, isNestedSequence) {\n2274:var sequence = app.project.activeSequence;\n2275:var sequence_qe = qe.project.getActiveSequence();\n2276:var clips = sequence.videoTracks[trackIndex].clips;\n2278:var clip_qe = sequence_qe.getVideoTrackAt(trackIndex).getItemAt(i);\n2284:var clip = clips[i];\n2287:var component = clip.components[j];\n2292:var property = component.properties[k];\n2296:var t1 = new Time();\n2297:var t2 = new Time();\n2298:var tqStart = quantise_time(clip.inPoint.seconds);\n2309:var t3 = new Time();\n2310:var t4 = new Time();\n2311:var tqEnd = quantise_time(clip.inPoint.seconds + clip.duration.seconds);\n2325:function addWiggleToCaptionsInSequenceIndex(captionLayers, wiggle, sequenceIndex) {\n2326:var trackIndex = captionLayers[0];\n2327:const frameRate = app.project.sequences[sequenceIndex].getSettings().videoFrameRate.seconds;\n2328:const clips = app.project.sequences[sequenceIndex].videoTracks[trackIndex].clips;\n2329:const frameHeight = app.project.sequences[sequenceIndex].getSettings().videoFrameHeight;\n2330:const frameWidth = app.project.sequences[sequenceIndex].getSettings().videoFrameWidth;\n2331:var video_effects = qe.project.getVideoEffectList();\n2332:var index = 0;\n2334:var found = false;\n2344:var transform_effect_name_to_use = video_effects[index];\n2345:var foundQeClips = 0;\n2346:var clipIndex = 0;\n2355:var clipIndex = 0;\n2356:var transformTime = new Time();\n2358:var time = clips[clipIndex].start.seconds;\n2359:var clipEnd = clips[clipIndex].end.seconds;\n2360:var positionProperty = getProperty(clips[clipIndex], \"Transform\", \"Position\");\n2362:var currentPosition = positionProperty.getValue();\n2384:function moveTemporaryScreenshots(screenshots, destinationFolderName) {\n2385:var destination = findItemByName(getEditAIBin(), destinationFolderName);\n2387:var item = findItemByName(app.project.rootItem, screenshots[i]);\n2393:function getFilesPaths(dialogTitle) {\n2394:var files = File.openDialog(dialogTitle, \"All Files:*.*\", true);\n2395:var filePaths = [];\n2403:function getFilePath(dialogTitle) {\n2404:var file = File.openDialog(dialogTitle, \"\", false);\n2410:function getStockFootagePreviewClipCount(stockFootageLibrary) {\n2411:var previewCount = 0;\n2414:var splitClip = app.project.activeSequence.videoTracks[i].clips[j].name.split(\"_\");\n2420:function getStockFootagePreviewClips(stockFootageLibrary) {\n2421:var previewClips = [];\n2422:var tracks = app.project.activeSequence.videoTracks;\n2424:var track = tracks[i];\n2427:var clip = track.clips[j];\n2428:var scale = getProperty(clip, \"Motion\", \"Scale\").getValue();\n2429:var splitClip = clip.name.split(\"_\");\n2437:function replacePreviewClipWithFullVersion(previewName, fullPath, clipWidth, clipHeight, replacedClipWidth, replacedClipHeight, replacedClipScale) {\n2438:var previewClip = null;\n2439:var previewClips = [];\n2440:const brollBin = getFireCutSubBin(\"B-roll\");\n2441:const clips = brollBin.children;\n2452:const previewNodeIds = [];\n2455:var clipTrackIndex = -1;\n2456:var clipIndex = -1;\n2472:var clip = app.project.activeSequence.videoTracks[clipTrackIndex].clips[clipIndex];\n2473:var clipStart = clip.start.seconds;\n2474:var clipEnd = clip.end.seconds;\n2475:var clipInPoint = clip.inPoint.seconds;\n2476:var clipOutPoint = clip.inPoint.seconds;\n2479:var positionProperty = [0.5, 0.5];\n2480:var anchorProperty = [0.5, 0.5];\n2481:var rotationProperty = 0;\n2488:var adjustedScale = (replacedClipWidth * replacedClipScale) / clipWidth;\n2491:var nodeId = insertBrollInTrackAtTime(fullPath, brollBin, clipTrackIndex, clipStart, clipEnd, clipWidth, clipHeight, positionProperty, anchorProperty, rotationProperty, adjustedScale, clipInPoint, clipOutPoint, \"\");\n2494:function getChapterMarkers() {\n2495:var response = [];\n2496:var markers = app.project.activeSequence.markers;\n2503:function deleteChapterMarker(markerGuid) {\n2504:var markers = app.project.activeSequence.markers;\n2512:function deleteAllChapterMarks() {\n2513:var markers = app.project.activeSequence.markers;\n2514:var marksToDelete = [];\n2522:function resizeVideoClip(videoTrackIndex, clipIndex, newDuration) {\n2523:var t = new Time();\n2527:function findTrackItemByNodeId(nodeId) {\n2540:function deleteTrackItemByNodeId(nodeId) {\n2541:var clip = findTrackItemByNodeId(nodeId);\n2548:function updateZoomTimesByNodeId(nodeId, start, end) {\n2549:var clip = findTrackItemByNodeId(nodeId);\n2551:var t = new Time();\n2560:function getZoomClipsAccordingToSequence(zoomsAccordingToJS) {\n2562:var ret = [];\n2564:var clip = findTrackItemByNodeId(zoomsAccordingToJS[i].nodeId);\n2566:var zoomClipAccordingToSequence = {};\n2574:function createBlankSequenceWithSameSettings(originalSequenceID, newSequenceName, targetBinName, sequencePresetPath) {\n2575:var originalSequence = findSequenceByID(originalSequenceID);\n2579:var originalSequenceSettings = originalSequence.getSettings();\n2581:var targetBin = getFireCutSubBin(targetBinName);\n2582:var newSequenceProjectItem = findItemByName(app.project.rootItem, newSequenceName);\n2584:var newSequenceID = findSequenceIDByName(newSequenceName);\n2585:var newSequence = findSequenceByID(newSequenceID);\n2589:function updateSequenceAspectRatio(sequenceID, aspectRatio) {\n2590:var sequence = findSequenceByID(sequenceID);\n2594:var settings = sequence.getSettings();\n2599:function insertViewboxInTrack(project_item, track_index, viewbox_params, include_audio) {\n2603:var targetTime = new Time();\n2607:var linked_items = app.project.activeSequence.videoTracks[track_index].clips[0].getLinkedItems();\n2624:function getTimeRegionOfNodeId(nodeId) {\n2625:var clip = findTrackItemByNodeId(nodeId);\n2631:function getFullActiveSequenceObject() {\n2633:var activeSequence = JSON.parse(JSON.stringify(app.project.activeSequence));\n2634:var videoTracks = expandObjectProperty(app.project.activeSequence, \"videoTracks\");\n2635:var audioTracks = expandObjectProperty(app.project.activeSequence, \"audioTracks\");\n2637:var videoTrack = videoTracks[i];\n2638:var videoTrackSource = app.project.activeSequence.videoTracks[i];\n2640:var clips = videoTrack.clips;\n2642:var clip = clips[j];\n2643:var clipSource = videoTrackSource.clips[j];\n2647:var audioTrack = audioTracks[i];\n2648:var audioTrackSource = app.project.activeSequence.audioTracks[i];\n2654:function expandObjectProperty(obj, propertyName) {\n2656:var array = [];\n2657:var arrayItemsCount = obj[propertyName].length;\n2658:var a = obj[propertyName];\n2667:function deepRemoveProperty(obj, targetProp) {\n2671:var item = obj[i];\n2687:function exportSequenceObject() {\n2688:var activeSequenceObject = getFullActiveSequenceObject();\n2692:function exportActiveSequenceData() {\n2693:var sequence = {};\n2792:function getTotalClipsInTracks(track) {\n2793:var totalClips = 0;\n2798:function insertAudioInTrackAtTime(audio_project_item, trackIndex, start_seconds) {\n2805:var targetTime = new Time();\n2810:function getProjectItemDimensions(projectItem) {\n2812:const columnsMetadata = JSON.parse(projectItem.getProjectColumnsMetadata());\n2813:const videoInfoColumn = columnsMetadata.filter(function (column) {\n2816:const videoInfo = videoInfoColumn.ColumnValue;\n2817:const dimensions = {height: parseFloat(videoInfo.split(\"x\")[1].split(\"(\")[0].trim()), width: parseFloat(videoInfo.split(\"x\")[0].trim())};\n2826:[Omitted long matching line]\n2827:[Omitted long matching line]\n2828:var project_item_type = {BIN: 2, CLIP: 1, FILE: 4, ROOT: 3, bin: 2, clip: 1, file: 4, root: 3};\n2829:var console = {log: function () {\n2835:var eventObj = new CSXSEvent();\n2844:var sequence_default_timecode = TIMECODES.TIMEDISPLAY_25Timecode;\n2848:var i = this.length;\n2857:var temp_bin = null;","numLines":666}}
{"parentUuid":"fc8959ba-c8b1-4a1a-825c-54b539543b6d","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01LMPuqsPWpSHSQhXF9E1F2x","type":"message","role":"assistant","content":[{"type":"text","text":"Let me continue reading the file to capture all functions and key sections."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":21238,"cache_read_input_tokens":12582,"cache_creation":{"ephemeral_5m_input_tokens":21238,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CWTqpceWNUk2XM8KcDCPE","type":"assistant","uuid":"c911a2c5-708c-4433-ba31-34603436e504","timestamp":"2025-12-25T18:00:57.900Z"}
{"parentUuid":"c911a2c5-708c-4433-ba31-34603436e504","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01LMPuqsPWpSHSQhXF9E1F2x","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01CjGQd2X9YdvVYF6WzoEWrQ","name":"Read","input":{"file_path":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx","offset":500,"limit":500}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":21238,"cache_read_input_tokens":12582,"cache_creation":{"ephemeral_5m_input_tokens":21238,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CWTqpceWNUk2XM8KcDCPE","type":"assistant","uuid":"6e643565-10fa-483d-8b81-55005aa5fd03","timestamp":"2025-12-25T18:00:58.743Z"}
{"parentUuid":"6e643565-10fa-483d-8b81-55005aa5fd03","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01LMPuqsPWpSHSQhXF9E1F2x","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01EicFqkRHUftpkJjoq67sTx","name":"Grep","input":{"path":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx","pattern":"qe\\.|app\\.project|this\\.addEventListener|CSXSEvent","output_mode":"content","head_limit":50}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":21238,"cache_read_input_tokens":12582,"cache_creation":{"ephemeral_5m_input_tokens":21238,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CWTqpceWNUk2XM8KcDCPE","type":"assistant","uuid":"06a5acc1-0093-43bd-ae8a-0492cdce0999","timestamp":"2025-12-25T18:00:59.531Z"}
{"parentUuid":"06a5acc1-0093-43bd-ae8a-0492cdce0999","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01CjGQd2X9YdvVYF6WzoEWrQ","type":"tool_result","content":"   500→var outPoint = new Time();\n   501→outPoint.seconds = duration;\n   502→selected_clip.end = end;\n   503→selected_clip.outPoint = outPoint;\n   504→}\n   505→}\n   506→function getVideoTransitionName(name) {\n   507→var video_transitions = qe.project.getVideoTransitionList();\n   508→var index = 0;\n   509→for (var i = 0; i < video_transitions.length; i += 1) { \n   510→var found = false;\n   511→for (var j = 0; j < translations[name].length; j += 1) { \n   512→if (video_transitions[i] == translations[name][j]) { \n   513→index = i;\n   514→found = true;\n   515→break ;\n   516→}}\n   517→if (found) { \n   518→break ;\n   519→}}\n   520→return video_transitions[index];\n   521→}\n   522→function getVideoEffectName(englishEffectName) {\n   523→var video_effects = qe.project.getVideoEffectList();\n   524→var index = 0;\n   525→for (var i = 0; i < video_effects.length; i += 1) { \n   526→var found = false;\n   527→for (var j = 0; j < translations[englishEffectName].length; j += 1) { \n   528→if (video_effects[i] == translations[englishEffectName][j]) { \n   529→index = i;\n   530→found = true;\n   531→break ;\n   532→}}\n   533→if (found) { \n   534→break ;\n   535→}}\n   536→return video_effects[index];\n   537→}\n   538→function setPropertyStatic(clip, component_displayName, property_displayName, value) {\n   539→for (var i = 0; i < clip.components.length; i += 1) { \n   540→var component = clip.components[i];\n   541→if (translations[component_displayName].indexOf(component.displayName) > -1) { \n   542→for (var j = 0; j < component.properties.length; j += 1) { \n   543→var property = component.properties[j];\n   544→if (translations[property_displayName].indexOf(property.displayName) > -1) { \n   545→property.setTimeVarying(false);\n   546→property.setValue(value, true);\n   547→}}\n   548→}}\n   549→}\n   550→function getProperty(clip, component_displayName, property_displayName) {\n   551→for (var i = 0; i < clip.components.length; i += 1) { \n   552→var component = clip.components[i];\n   553→if (translations[component_displayName].indexOf(component.displayName) > -1) { \n   554→for (var j = 0; j < component.properties.length; j += 1) { \n   555→var property = component.properties[j];\n   556→if (translations[property_displayName].indexOf(property.displayName) > -1) { \n   557→return property;\n   558→}}\n   559→}}\n   560→}\n   561→function findItemByName(bin, string) {\n   562→for (var i = 0; i < bin.children.numItems; i += 1) { \n   563→if (bin.children[i].name === string) { \n   564→return bin.children[i];\n   565→}\n   566→else {\n   567→if (bin.children[i].children !== undefined) { \n   568→var item = findItemByName(bin.children[i], string);\n   569→if (item) { \n   570→return item;\n   571→}\n   572→}\n   573→}}\n   574→return false;\n   575→}\n   576→function findItemByNameAndType(bin, string, type_string) {\n   577→type = project_item_type[type_string];\n   578→for (var i = 0; i < bin.children.numItems; i += 1) { \n   579→if ((bin.children[i].name == string) && (bin.children[i].type == type)) { \n   580→return bin.children[i];\n   581→}\n   582→else {\n   583→if (bin.children[i].children != undefined) { \n   584→if (findItemByNameAndType(bin.children[i], string, type_string)) { \n   585→return findItemByNameAndType(bin.children[i], string, type_string);\n   586→}\n   587→}\n   588→}}\n   589→return false;\n   590→}\n   591→function findItemByNodeId(bin, nodeId) {\n   592→for (var i = 0; i < bin.children.numItems; i += 1) { \n   593→if (bin.children[i].nodeId == nodeId) { \n   594→return bin.children[i];\n   595→}\n   596→else {\n   597→if (bin.children[i].children != undefined) { \n   598→if (findItemByNodeId(bin.children[i], nodeId)) { \n   599→return findItemByNodeId(bin.children[i], nodeId);\n   600→}\n   601→}\n   602→}}\n   603→return false;\n   604→}\n   605→function getAllNodeIDs(bin) {\n   606→const node_ids = [];\n   607→for (var i = 0; i < bin.children.length; i += 1) { \n   608→node_ids.push(bin.children[i].nodeId);\n   609→if (bin.children[i].children != undefined) { \n   610→children_nodeIds = getAllNodeIDs(bin.children[i]);\n   611→for (var j = 0; j < children_nodeIds.length; j += 1) { \n   612→node_ids.push(children_nodeIds[j]);}\n   613→}}\n   614→return node_ids;\n   615→}\n   616→function addSilenceCuttingControlClipToNewTrack(sequence_start, sequence_end) {\n   617→var index_of_new_track = qe.project.getActiveSequence().numVideoTracks;\n   618→qe.project.getActiveSequence().addTracks(1, qe.project.getActiveSequence().numVideoTracks, 0, 0);\n   619→original_node_ids = getAllNodeIDs(app.project.rootItem);\n   620→qe.project.newTransparentVideo(50, 50, 0, 1, 1);\n   621→var transparent_video = false;\n   622→var imported_as_silence_cutting_layer = false;\n   623→transparent_video = findItemByName(app.project.rootItem, \"Transparent Video\");\n   624→if (!transparent_video) { \n   625→new_node_ids = getAllNodeIDs(app.project.rootItem);\n   626→difference = [];\n   627→for (var i = 0; i < new_node_ids.length; i += 1) { \n   628→if (original_node_ids.indexOf(new_node_ids[i]) == -1) { \n   629→difference.push(new_node_ids[i]);\n   630→}}\n   631→for (var i = 0; i < difference.length; i += 1) { \n   632→if (findItemByNodeId(app.project.rootItem, difference[i])) { \n   633→if (findItemByNodeId(app.project.rootItem, difference[i]).type == 1) { \n   634→transparent_video = findItemByNodeId(app.project.rootItem, difference[i]);\n   635→break ;\n   636→}\n   637→}}\n   638→}\n   639→if (!transparent_video) { \n   640→transparent_video = getSilenceCuttingLayer();\n   641→imported_as_silence_cutting_layer = true;\n   642→}\n   643→if (!transparent_video) { \n   644→importSilenceCuttingLayerIntoProject();\n   645→transparent_video = findItemByName(app.project.rootItem, \"Silence Cutting Layer\");\n   646→imported_as_silence_cutting_layer = true;\n   647→}\n   648→if (!transparent_video) { \n   649→return \"EvalScript error (custom): Transparent Video not found.\";\n   650→}\n   651→if (!imported_as_silence_cutting_layer) { \n   652→app.project.rootItem.createBin(\"temp-3b6fi75qb6fi5826b\");\n   653→temp_bin = app.project.rootItem.children[app.project.rootItem.children.length - 1];\n   654→transparent_video.moveBin(temp_bin);\n   655→transparent_video.name = \"Silence Cutting Control (Temp)\";\n   656→}\n   657→var start_time = new Time();\n   658→start_time.seconds = sequence_start;\n   659→var end_time = new Time();\n   660→end_time.seconds = sequence_end;\n   661→app.project.activeSequence.videoTracks[index_of_new_track].insertClip(transparent_video, start_time);\n   662→var transparent_clip = app.project.activeSequence.videoTracks[index_of_new_track].clips[0];\n   663→transparent_clip.end = end_time;\n   664→return true;\n   665→}\n   666→function deleteSilenceCuttingControlClip() {\n   667→var index_of_new_track = qe.project.getActiveSequence().numVideoTracks - 1;\n   668→qe.project.getActiveSequence().removeVideoTrack(index_of_new_track);\n   669→if (temp_bin != null) { \n   670→temp_bin.deleteBin();\n   671→}\n   672→return true;\n   673→}\n   674→function importAdjustmentLayerIntoProject() {\n   675→const adjustmentLayerPath = adjustment_layer_filepath;\n   676→const sequenceID = \"4572f1e6-6bb0-4435-8bff-fb70dea8608d\";\n   677→app.project.importFiles(adjustment_layer_filepath, 1, getEditAIBin(), 0);\n   678→}\n   679→function importSilenceCuttingLayerIntoProject() {\n   680→const adjustmentLayerPath = silence_cutting_layer_filepath;\n   681→const sequenceID = \"2965ff0d-489b-44d7-82f8-6826514ed3b1\";\n   682→app.project.importFiles(silence_cutting_layer_filepath, 1, getEditAIBin(), 0);\n   683→}\n   684→function getAdjustmentLayer() {\n   685→var adjustmentLayerExists = false;\n   686→var adjustmentLayer = null;\n   687→for (var i = 0; i < getEditAIBin().children.length; i += 1) { \n   688→var child = getEditAIBin().children[i];\n   689→if (child.name == \"AI Adjustment Layer\") { \n   690→adjustmentLayerExists = true;\n   691→adjustmentLayer = child;\n   692→}}\n   693→if (adjustmentLayerExists == true) { \n   694→return adjustmentLayer;\n   695→}\n   696→else {\n   697→importAdjustmentLayerIntoProject();\n   698→return findItemByName(app.project.rootItem, \"AI Adjustment Layer\");\n   699→}\n   700→}\n   701→function getSilenceCuttingLayer() {\n   702→var silenceCuttingLayerExists = false;\n   703→var silenceCuttingLayer = null;\n   704→for (var i = 0; i < getEditAIBin().children.length; i += 1) { \n   705→var child = getEditAIBin().children[i];\n   706→if (child.name == \"Silence Cutting Layer\") { \n   707→silenceCuttingLayerExists = true;\n   708→silenceCuttingLayer = child;\n   709→}}\n   710→if (silenceCuttingLayerExists == true) { \n   711→return silenceCuttingLayer;\n   712→}\n   713→else {\n   714→importSilenceCuttingLayerIntoProject();\n   715→return getEditAIBin().children[getEditAIBin().children.length - 2];\n   716→}\n   717→}\n   718→function addZoomAtTime(start, end, animated, animation_duration, zoom_scale, trackIndex) {\n   719→adjustmentLayer = findItemByName(app.project.rootItem, \"AI Adjustment Layer\");\n   720→if (!adjustmentLayer) { \n   721→importAdjustmentLayerIntoProject();\n   722→adjustmentLayer = findItemByName(app.project.rootItem, \"AI Adjustment Layer\");\n   723→}\n   724→var targetTime = new Time();\n   725→targetTime.seconds = start;\n   726→app.project.activeSequence.videoTracks[trackIndex].insertClip(adjustmentLayer, targetTime);\n   727→var clips = findClip(start, trackIndex);\n   728→var clip_normal = clips.clip_normal;\n   729→var clip_qe = clips.clip_qe;\n   730→if ((clip_normal) && (clip_qe)) { \n   731→var end_time = clip_normal.end;\n   732→end_time.seconds = end;\n   733→clip_normal.end = end_time;\n   734→var sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\n   735→var sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\n   736→var clip_height = 1080;\n   737→var clip_width = 1920;\n   738→var scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width) * 10;\n   739→setPropertyStatic(clip_normal, \"Motion\", \"Scale\", scale_required);\n   740→var video_effects = qe.project.getVideoEffectList();\n   741→var index = 0;\n   742→for (var i = 0; i < video_effects.length; i += 1) { \n   743→var found = false;\n   744→for (var j = 0; j < translations.Transform.length; j += 1) { \n   745→if (video_effects[i] == translations.Transform[j]) { \n   746→index = i;\n   747→found = true;\n   748→break ;\n   749→}}\n   750→if (found) { \n   751→break ;\n   752→}}\n   753→var transform_effect_name_to_use = video_effects[index];\n   754→clip_qe.addVideoEffect(qe.project.getVideoEffectByName(transform_effect_name_to_use));\n   755→if (animated) { \n   756→offset = clip_normal.inPoint.seconds;\n   757→end = offset + clip_normal.duration.seconds;\n   758→for (var i = 0; i < clip_normal.components.length; i += 1) { \n   759→var component = clip_normal.components[i];\n   760→if (translations.Transform.indexOf(component.displayName) > -1) { \n   761→for (var j = 0; j < component.properties.length; j += 1) { \n   762→var property = component.properties[j];\n   763→if ((translations[\"Scale Height\"].indexOf(property.displayName) > -1) || (translations[\"Scale Width\"].indexOf(property.displayName) > -1)) { \n   764→property.setTimeVarying(true, true);\n   765→t = new Time();\n   766→t.seconds = offset;\n   767→property.addKey(t);\n   768→property.setValueAtKey(t, value = 100, updateUI = true);\n   769→property.setInterpolationTypeAtKey(t, 3, updateUI = true);\n   770→t.seconds = offset + animation_duration;\n   771→property.addKey(t);\n   772→property.setValueAtKey(t, value = zoom_scale, updateUI = true);\n   773→property.setInterpolationTypeAtKey(t, 4, updateUI = true);\n   774→t.seconds = end - animation_duration;\n   775→property.addKey(t);\n   776→property.setValueAtKey(t, value = zoom_scale, updateUI = true);\n   777→property.setInterpolationTypeAtKey(t, 3, updateUI = true);\n   778→t.seconds = end;\n   779→property.addKey(t);\n   780→property.setValueAtKey(t, value = 100, updateUI = true);\n   781→property.setInterpolationTypeAtKey(t, 4, updateUI = true);\n   782→}}\n   783→}}\n   784→}\n   785→else {\n   786→setPropertyStatic(clip_normal, \"Transform\", \"Scale Height\", zoom_scale);\n   787→setPropertyStatic(clip_normal, \"Transform\", \"Scale Width\", zoom_scale);\n   788→}\n   789→return \"clips were found\";\n   790→}\n   791→return \"clips were NOT found\";\n   792→}\n   793→function addZoomAnimation(start, end, animation_duration, zoom_scale, center_point_x, center_point_y, animation_easing_power, trackIndex) {\n   794→var clips = findClip(start, trackIndex);\n   795→var clip_normal = clips.clip_normal;\n   796→var offset = clip_normal.inPoint.seconds;\n   797→var end = offset + clip_normal.duration.seconds;\n   798→var videoFrameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n   799→for (var i = 0; i < clip_normal.components.length; i += 1) { \n   800→var component = clip_normal.components[i];\n   801→if (translations.Transform.indexOf(component.displayName) > -1) { \n   802→for (var j = 0; j < component.properties.length; j += 1) { \n   803→var property = component.properties[j];\n   804→if ((translations[\"Scale Height\"].indexOf(property.displayName) > -1) || (translations[\"Scale Width\"].indexOf(property.displayName) > -1)) { \n   805→property.setTimeVarying(true, true);\n   806→t = new Time();\n   807→last_t = new Time();\n   808→for (var k = 0; k < animation_duration; k += videoFrameRate) { \n   809→t.seconds = offset + k;\n   810→property.addKey(t);\n   811→property.setValueAtKey(t, value = 100 + ((zoom_scale - 100) * easing_curve(k / animation_duration, animation_easing_power)), updateUI = true);\n   812→last_t = t;}\n   813→property.setValueAtKey(last_t, value = zoom_scale, updateUI = true);\n   814→for (var k = 0; k < animation_duration; k += videoFrameRate) { \n   815→t.seconds = (end - animation_duration) + k;\n   816→property.addKey(t);\n   817→property.setValueAtKey(t, value = zoom_scale - ((zoom_scale - 100) * easing_curve(k / animation_duration, animation_easing_power)), updateUI = true);\n   818→last_t = t;}\n   819→property.setValueAtKey(last_t, value = 100, updateUI = true);\n   820→}\n   821→else {\n   822→if (translations[\"Anchor Point\"].indexOf(property.displayName) > -1) { \n   823→property.setTimeVarying(true, true);\n   824→var t = new Time();\n   825→var last_t = new Time();\n   826→for (var k = 0; k < animation_duration; k += videoFrameRate) { \n   827→t.seconds = offset + k;\n   828→property.addKey(t);\n   829→easing_percentage = easing_curve(k / animation_duration, animation_easing_power);\n   830→value_x = 0.5 + (((center_point_x / 100) - 0.5) * easing_percentage);\n   831→value_y = 0.5 + (((center_point_y / 100) - 0.5) * easing_percentage);\n   832→property.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\n   833→last_t = t;}\n   834→property.setValueAtKey(last_t, value = zoom_scale, updateUI = true);\n   835→for (var k = 0; k < animation_duration; k += videoFrameRate) { \n   836→t.seconds = (end - animation_duration) + k;\n   837→property.addKey(t);\n   838→easing_percentage = easing_curve(k / animation_duration, animation_easing_power);\n   839→value_x = (center_point_x / 100) - (((center_point_x / 100) - 0.5) * easing_percentage);\n   840→value_y = (center_point_y / 100) - (((center_point_y / 100) - 0.5) * easing_percentage);\n   841→property.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\n   842→last_t = t;}\n   843→property.setValueAtKey(last_t, value = 100, updateUI = true);\n   844→}\n   845→}}\n   846→}}\n   847→}\n   848→function addZoomAnimationAsymmetric(start, end, zoom_scale, center_point_x, center_point_y, zoom_curve_start_power, zoom_curve_start_duration, zoom_curve_end_power, zoom_curve_end_duration, trackIndex) {\n   849→var clips = findClip(start, trackIndex);\n   850→var clip_normal = clips.clip_normal;\n   851→if ((zoom_curve_start_power == 0) && (zoom_curve_end_power == 0)) { \n   852→console.log(\"Both zoom powers are 0, doing static zooms\");\n   853→setPropertyStatic(clip_normal, \"Transform\", \"Scale Height\", zoom_scale);\n   854→setPropertyStatic(clip_normal, \"Transform\", \"Scale Width\", zoom_scale);\n   855→setPropertyStatic(clip_normal, \"Transform\", \"Anchor Point\", [center_point_x / 100, center_point_y / 100]);\n   856→return;\n   857→}\n   858→var offset = clip_normal.inPoint.seconds;\n   859→var end = offset + clip_normal.duration.seconds;\n   860→if (clip_normal.duration.seconds < (zoom_curve_start_duration + zoom_curve_end_duration)) { \n   861→zoom_curve_start_duration = (zoom_curve_start_duration / (zoom_curve_start_duration + zoom_curve_end_duration)) * clip_normal.duration.seconds;\n   862→zoom_curve_end_duration = clip_normal.duration.seconds - zoom_curve_start_duration;\n   863→}\n   864→var videoFrameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n   865→for (var i = 0; i < clip_normal.components.length; i += 1) { \n   866→var component = clip_normal.components[i];\n   867→if (translations.Transform.indexOf(component.displayName) > -1) { \n   868→for (var j = 0; j < component.properties.length; j += 1) { \n   869→var property = component.properties[j];\n   870→if (((translations[\"Scale Height\"].indexOf(property.displayName) > -1) || (translations[\"Scale Width\"].indexOf(property.displayName) > -1)) || (translations.Scale.indexOf(property.displayName) > -1)) { \n   871→property.setTimeVarying(true, true);\n   872→t = new Time();\n   873→last_t = new Time();\n   874→for (var k = 0; k < zoom_curve_start_duration; k += videoFrameRate) { \n   875→easing_percentage = easing_curve(k / zoom_curve_start_duration, zoom_curve_start_power);\n   876→if (zoom_curve_start_power == 0) { \n   877→easing_percentage = 1;\n   878→}\n   879→t.seconds = offset + k;\n   880→property.addKey(t);\n   881→property.setValueAtKey(t, value = 100 + ((zoom_scale - 100) * easing_percentage), updateUI = true);\n   882→last_t = t;}\n   883→property.setValueAtKey(last_t, value = zoom_scale, updateUI = true);\n   884→if ((zoom_curve_end_power > 0) && (zoom_curve_end_duration > 0)) { \n   885→for (var k = 0; k < zoom_curve_end_duration; k += videoFrameRate) { \n   886→easing_percentage = easing_curve(k / zoom_curve_end_duration, zoom_curve_end_power);\n   887→t.seconds = (end - zoom_curve_end_duration) + k;\n   888→property.addKey(t);\n   889→property.setValueAtKey(t, value = zoom_scale - ((zoom_scale - 100) * easing_percentage), updateUI = true);\n   890→last_t = t;}\n   891→property.setValueAtKey(last_t, value = 100, updateUI = true);\n   892→}\n   893→else {\n   894→t.seconds = end - videoFrameRate;\n   895→property.addKey(t);\n   896→property.setValueAtKey(t, value = zoom_scale, updateUI = true);\n   897→last_t = end;\n   898→}\n   899→property.setValueAtKey(last_t, value = 100, updateUI = true);\n   900→}\n   901→else {\n   902→if (translations[\"Anchor Point\"].indexOf(property.displayName) > -1) { \n   903→property.setTimeVarying(true, true);\n   904→var t = new Time();\n   905→var last_t = new Time();\n   906→for (var k = 0; k < zoom_curve_start_duration; k += videoFrameRate) { \n   907→easing_percentage = easing_curve(k / zoom_curve_start_duration, zoom_curve_start_power);\n   908→if (zoom_curve_start_power == 0) { \n   909→easing_percentage = 1;\n   910→}\n   911→t.seconds = offset + k;\n   912→property.addKey(t);\n   913→value_x = 0.5 + (((center_point_x / 100) - 0.5) * easing_percentage);\n   914→value_y = 0.5 + (((center_point_y / 100) - 0.5) * easing_percentage);\n   915→property.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\n   916→last_t = t;}\n   917→property.setValueAtKey(last_t, value = zoom_scale, updateUI = true);\n   918→if ((zoom_curve_end_power > 0) && (zoom_curve_end_duration > 0)) { \n   919→for (var k = 0; k < zoom_curve_end_duration; k += videoFrameRate) { \n   920→easing_percentage = easing_curve(k / zoom_curve_end_duration, zoom_curve_end_power);\n   921→t.seconds = (end - zoom_curve_end_duration) + k;\n   922→property.addKey(t);\n   923→value_x = (center_point_x / 100) - (((center_point_x / 100) - 0.5) * easing_percentage);\n   924→value_y = (center_point_y / 100) - (((center_point_y / 100) - 0.5) * easing_percentage);\n   925→property.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\n   926→last_t = t;}\n   927→}\n   928→else {\n   929→t.seconds = end - videoFrameRate;\n   930→property.addKey(t);\n   931→property.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\n   932→last_t = t;\n   933→}\n   934→property.setValueAtKey(last_t, value = 100, updateUI = true);\n   935→}\n   936→}}\n   937→}}\n   938→}\n   939→function findClip(start, trackIndex) {\n   940→var clip_normal = null;\n   941→var clip_qe = null;\n   942→for (var i = 0; i < app.project.activeSequence.videoTracks[trackIndex].clips.length; i += 1) { \n   943→if (Math.abs(app.project.activeSequence.videoTracks[trackIndex].clips[i].start.seconds - start) < time_tolerance) { \n   944→clip_normal = app.project.activeSequence.videoTracks[trackIndex].clips[i];\n   945→}}\n   946→for (var i = 0; i < qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems; i += 1) { \n   947→try {\n   948→if (Math.abs(qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i).start.secs - start) < time_tolerance) { \n   949→clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n   950→}\n   951→} catch (err) {\n   952→}}\n   953→return {clip_normal: clip_normal, clip_qe: clip_qe};\n   954→}\n   955→function getVideoTransitionList() {\n   956→return JSON.stringify(qe.project.getVideoTransitionList());\n   957→}\n   958→function addTransitionsToQEClipsOnTrack(trackIndex, transitionType, transitionLength) {\n   959→var numClips = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems;\n   960→var frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n   961→for (var i = 0; i < numClips; i += 1) { \n   962→try {\n   963→var prev_clip_qe = null;\n   964→var next_clip_qe = null;\n   965→if (i > 0) { \n   966→prev_clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n   967→}\n   968→if (i < (numClips - 1)) { \n   969→next_clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i + 1);\n   970→}\n   971→var clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n   972→var start_time = parseInt(clip_qe.start.secs / frame_rate);\n   973→var end_time = parseInt(clip_qe.end.secs / frame_rate);\n   974→var clipLength = Math.abs(end_time - start_time);\n   975→transitionLength = Math.min(Math.ceil(clipLength / 2), parseInt(transitionLength)).toString();\n   976→clip_qe.addTransition(qe.project.getVideoTransitionByName(transitionType), false, transitionLength, \"0:00\", 0.5, false);\n   977→} catch (err) {\n   978→}}\n   979→return clip_qe;\n   980→}\n   981→function addTransitionsToQEClipOnTrackWithName(trackIndex, transitionType, transitionLength, clipName) {\n   982→var numClips = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems;\n   983→var frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n   984→for (var i = 0; i < numClips; i += 1) { \n   985→try {\n   986→var prev_clip_qe = null;\n   987→if (i > 0) { \n   988→prev_clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n   989→}\n   990→var clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n   991→if (clip_qe.name) { \n   992→if (clip_qe.name == clipName) { \n   993→var start_time = parseInt(clip_qe.start.secs / frame_rate);\n   994→var end_time = parseInt(clip_qe.end.secs / frame_rate);\n   995→var clipLength = Math.abs(end_time - start_time);\n   996→transitionLength = Math.min(Math.ceil(clipLength / 2), parseInt(transitionLength)).toString();\n   997→clip_qe.addTransition(qe.project.getVideoTransitionByName(transitionType), false, transitionLength, \"0:00\", 0.5, false);\n   998→clip_qe.addTransition(qe.project.getVideoTransitionByName(transitionType), true, transitionLength, \"0:00\", 0.5, false);\n   999→}\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"6e07b53b-6b92-4c0d-96ab-994b0fd8a04d","timestamp":"2025-12-25T18:00:59.581Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx","content":"var outPoint = new Time();\noutPoint.seconds = duration;\nselected_clip.end = end;\nselected_clip.outPoint = outPoint;\n}\n}\nfunction getVideoTransitionName(name) {\nvar video_transitions = qe.project.getVideoTransitionList();\nvar index = 0;\nfor (var i = 0; i < video_transitions.length; i += 1) { \nvar found = false;\nfor (var j = 0; j < translations[name].length; j += 1) { \nif (video_transitions[i] == translations[name][j]) { \nindex = i;\nfound = true;\nbreak ;\n}}\nif (found) { \nbreak ;\n}}\nreturn video_transitions[index];\n}\nfunction getVideoEffectName(englishEffectName) {\nvar video_effects = qe.project.getVideoEffectList();\nvar index = 0;\nfor (var i = 0; i < video_effects.length; i += 1) { \nvar found = false;\nfor (var j = 0; j < translations[englishEffectName].length; j += 1) { \nif (video_effects[i] == translations[englishEffectName][j]) { \nindex = i;\nfound = true;\nbreak ;\n}}\nif (found) { \nbreak ;\n}}\nreturn video_effects[index];\n}\nfunction setPropertyStatic(clip, component_displayName, property_displayName, value) {\nfor (var i = 0; i < clip.components.length; i += 1) { \nvar component = clip.components[i];\nif (translations[component_displayName].indexOf(component.displayName) > -1) { \nfor (var j = 0; j < component.properties.length; j += 1) { \nvar property = component.properties[j];\nif (translations[property_displayName].indexOf(property.displayName) > -1) { \nproperty.setTimeVarying(false);\nproperty.setValue(value, true);\n}}\n}}\n}\nfunction getProperty(clip, component_displayName, property_displayName) {\nfor (var i = 0; i < clip.components.length; i += 1) { \nvar component = clip.components[i];\nif (translations[component_displayName].indexOf(component.displayName) > -1) { \nfor (var j = 0; j < component.properties.length; j += 1) { \nvar property = component.properties[j];\nif (translations[property_displayName].indexOf(property.displayName) > -1) { \nreturn property;\n}}\n}}\n}\nfunction findItemByName(bin, string) {\nfor (var i = 0; i < bin.children.numItems; i += 1) { \nif (bin.children[i].name === string) { \nreturn bin.children[i];\n}\nelse {\nif (bin.children[i].children !== undefined) { \nvar item = findItemByName(bin.children[i], string);\nif (item) { \nreturn item;\n}\n}\n}}\nreturn false;\n}\nfunction findItemByNameAndType(bin, string, type_string) {\ntype = project_item_type[type_string];\nfor (var i = 0; i < bin.children.numItems; i += 1) { \nif ((bin.children[i].name == string) && (bin.children[i].type == type)) { \nreturn bin.children[i];\n}\nelse {\nif (bin.children[i].children != undefined) { \nif (findItemByNameAndType(bin.children[i], string, type_string)) { \nreturn findItemByNameAndType(bin.children[i], string, type_string);\n}\n}\n}}\nreturn false;\n}\nfunction findItemByNodeId(bin, nodeId) {\nfor (var i = 0; i < bin.children.numItems; i += 1) { \nif (bin.children[i].nodeId == nodeId) { \nreturn bin.children[i];\n}\nelse {\nif (bin.children[i].children != undefined) { \nif (findItemByNodeId(bin.children[i], nodeId)) { \nreturn findItemByNodeId(bin.children[i], nodeId);\n}\n}\n}}\nreturn false;\n}\nfunction getAllNodeIDs(bin) {\nconst node_ids = [];\nfor (var i = 0; i < bin.children.length; i += 1) { \nnode_ids.push(bin.children[i].nodeId);\nif (bin.children[i].children != undefined) { \nchildren_nodeIds = getAllNodeIDs(bin.children[i]);\nfor (var j = 0; j < children_nodeIds.length; j += 1) { \nnode_ids.push(children_nodeIds[j]);}\n}}\nreturn node_ids;\n}\nfunction addSilenceCuttingControlClipToNewTrack(sequence_start, sequence_end) {\nvar index_of_new_track = qe.project.getActiveSequence().numVideoTracks;\nqe.project.getActiveSequence().addTracks(1, qe.project.getActiveSequence().numVideoTracks, 0, 0);\noriginal_node_ids = getAllNodeIDs(app.project.rootItem);\nqe.project.newTransparentVideo(50, 50, 0, 1, 1);\nvar transparent_video = false;\nvar imported_as_silence_cutting_layer = false;\ntransparent_video = findItemByName(app.project.rootItem, \"Transparent Video\");\nif (!transparent_video) { \nnew_node_ids = getAllNodeIDs(app.project.rootItem);\ndifference = [];\nfor (var i = 0; i < new_node_ids.length; i += 1) { \nif (original_node_ids.indexOf(new_node_ids[i]) == -1) { \ndifference.push(new_node_ids[i]);\n}}\nfor (var i = 0; i < difference.length; i += 1) { \nif (findItemByNodeId(app.project.rootItem, difference[i])) { \nif (findItemByNodeId(app.project.rootItem, difference[i]).type == 1) { \ntransparent_video = findItemByNodeId(app.project.rootItem, difference[i]);\nbreak ;\n}\n}}\n}\nif (!transparent_video) { \ntransparent_video = getSilenceCuttingLayer();\nimported_as_silence_cutting_layer = true;\n}\nif (!transparent_video) { \nimportSilenceCuttingLayerIntoProject();\ntransparent_video = findItemByName(app.project.rootItem, \"Silence Cutting Layer\");\nimported_as_silence_cutting_layer = true;\n}\nif (!transparent_video) { \nreturn \"EvalScript error (custom): Transparent Video not found.\";\n}\nif (!imported_as_silence_cutting_layer) { \napp.project.rootItem.createBin(\"temp-3b6fi75qb6fi5826b\");\ntemp_bin = app.project.rootItem.children[app.project.rootItem.children.length - 1];\ntransparent_video.moveBin(temp_bin);\ntransparent_video.name = \"Silence Cutting Control (Temp)\";\n}\nvar start_time = new Time();\nstart_time.seconds = sequence_start;\nvar end_time = new Time();\nend_time.seconds = sequence_end;\napp.project.activeSequence.videoTracks[index_of_new_track].insertClip(transparent_video, start_time);\nvar transparent_clip = app.project.activeSequence.videoTracks[index_of_new_track].clips[0];\ntransparent_clip.end = end_time;\nreturn true;\n}\nfunction deleteSilenceCuttingControlClip() {\nvar index_of_new_track = qe.project.getActiveSequence().numVideoTracks - 1;\nqe.project.getActiveSequence().removeVideoTrack(index_of_new_track);\nif (temp_bin != null) { \ntemp_bin.deleteBin();\n}\nreturn true;\n}\nfunction importAdjustmentLayerIntoProject() {\nconst adjustmentLayerPath = adjustment_layer_filepath;\nconst sequenceID = \"4572f1e6-6bb0-4435-8bff-fb70dea8608d\";\napp.project.importFiles(adjustment_layer_filepath, 1, getEditAIBin(), 0);\n}\nfunction importSilenceCuttingLayerIntoProject() {\nconst adjustmentLayerPath = silence_cutting_layer_filepath;\nconst sequenceID = \"2965ff0d-489b-44d7-82f8-6826514ed3b1\";\napp.project.importFiles(silence_cutting_layer_filepath, 1, getEditAIBin(), 0);\n}\nfunction getAdjustmentLayer() {\nvar adjustmentLayerExists = false;\nvar adjustmentLayer = null;\nfor (var i = 0; i < getEditAIBin().children.length; i += 1) { \nvar child = getEditAIBin().children[i];\nif (child.name == \"AI Adjustment Layer\") { \nadjustmentLayerExists = true;\nadjustmentLayer = child;\n}}\nif (adjustmentLayerExists == true) { \nreturn adjustmentLayer;\n}\nelse {\nimportAdjustmentLayerIntoProject();\nreturn findItemByName(app.project.rootItem, \"AI Adjustment Layer\");\n}\n}\nfunction getSilenceCuttingLayer() {\nvar silenceCuttingLayerExists = false;\nvar silenceCuttingLayer = null;\nfor (var i = 0; i < getEditAIBin().children.length; i += 1) { \nvar child = getEditAIBin().children[i];\nif (child.name == \"Silence Cutting Layer\") { \nsilenceCuttingLayerExists = true;\nsilenceCuttingLayer = child;\n}}\nif (silenceCuttingLayerExists == true) { \nreturn silenceCuttingLayer;\n}\nelse {\nimportSilenceCuttingLayerIntoProject();\nreturn getEditAIBin().children[getEditAIBin().children.length - 2];\n}\n}\nfunction addZoomAtTime(start, end, animated, animation_duration, zoom_scale, trackIndex) {\nadjustmentLayer = findItemByName(app.project.rootItem, \"AI Adjustment Layer\");\nif (!adjustmentLayer) { \nimportAdjustmentLayerIntoProject();\nadjustmentLayer = findItemByName(app.project.rootItem, \"AI Adjustment Layer\");\n}\nvar targetTime = new Time();\ntargetTime.seconds = start;\napp.project.activeSequence.videoTracks[trackIndex].insertClip(adjustmentLayer, targetTime);\nvar clips = findClip(start, trackIndex);\nvar clip_normal = clips.clip_normal;\nvar clip_qe = clips.clip_qe;\nif ((clip_normal) && (clip_qe)) { \nvar end_time = clip_normal.end;\nend_time.seconds = end;\nclip_normal.end = end_time;\nvar sequence_height = parseInt(app.project.activeSequence.getSettings().videoFrameHeight);\nvar sequence_width = parseInt(app.project.activeSequence.getSettings().videoFrameWidth);\nvar clip_height = 1080;\nvar clip_width = 1920;\nvar scale_required = 100 * Math.max(sequence_height / clip_height, sequence_width / clip_width) * 10;\nsetPropertyStatic(clip_normal, \"Motion\", \"Scale\", scale_required);\nvar video_effects = qe.project.getVideoEffectList();\nvar index = 0;\nfor (var i = 0; i < video_effects.length; i += 1) { \nvar found = false;\nfor (var j = 0; j < translations.Transform.length; j += 1) { \nif (video_effects[i] == translations.Transform[j]) { \nindex = i;\nfound = true;\nbreak ;\n}}\nif (found) { \nbreak ;\n}}\nvar transform_effect_name_to_use = video_effects[index];\nclip_qe.addVideoEffect(qe.project.getVideoEffectByName(transform_effect_name_to_use));\nif (animated) { \noffset = clip_normal.inPoint.seconds;\nend = offset + clip_normal.duration.seconds;\nfor (var i = 0; i < clip_normal.components.length; i += 1) { \nvar component = clip_normal.components[i];\nif (translations.Transform.indexOf(component.displayName) > -1) { \nfor (var j = 0; j < component.properties.length; j += 1) { \nvar property = component.properties[j];\nif ((translations[\"Scale Height\"].indexOf(property.displayName) > -1) || (translations[\"Scale Width\"].indexOf(property.displayName) > -1)) { \nproperty.setTimeVarying(true, true);\nt = new Time();\nt.seconds = offset;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = 100, updateUI = true);\nproperty.setInterpolationTypeAtKey(t, 3, updateUI = true);\nt.seconds = offset + animation_duration;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = zoom_scale, updateUI = true);\nproperty.setInterpolationTypeAtKey(t, 4, updateUI = true);\nt.seconds = end - animation_duration;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = zoom_scale, updateUI = true);\nproperty.setInterpolationTypeAtKey(t, 3, updateUI = true);\nt.seconds = end;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = 100, updateUI = true);\nproperty.setInterpolationTypeAtKey(t, 4, updateUI = true);\n}}\n}}\n}\nelse {\nsetPropertyStatic(clip_normal, \"Transform\", \"Scale Height\", zoom_scale);\nsetPropertyStatic(clip_normal, \"Transform\", \"Scale Width\", zoom_scale);\n}\nreturn \"clips were found\";\n}\nreturn \"clips were NOT found\";\n}\nfunction addZoomAnimation(start, end, animation_duration, zoom_scale, center_point_x, center_point_y, animation_easing_power, trackIndex) {\nvar clips = findClip(start, trackIndex);\nvar clip_normal = clips.clip_normal;\nvar offset = clip_normal.inPoint.seconds;\nvar end = offset + clip_normal.duration.seconds;\nvar videoFrameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\nfor (var i = 0; i < clip_normal.components.length; i += 1) { \nvar component = clip_normal.components[i];\nif (translations.Transform.indexOf(component.displayName) > -1) { \nfor (var j = 0; j < component.properties.length; j += 1) { \nvar property = component.properties[j];\nif ((translations[\"Scale Height\"].indexOf(property.displayName) > -1) || (translations[\"Scale Width\"].indexOf(property.displayName) > -1)) { \nproperty.setTimeVarying(true, true);\nt = new Time();\nlast_t = new Time();\nfor (var k = 0; k < animation_duration; k += videoFrameRate) { \nt.seconds = offset + k;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = 100 + ((zoom_scale - 100) * easing_curve(k / animation_duration, animation_easing_power)), updateUI = true);\nlast_t = t;}\nproperty.setValueAtKey(last_t, value = zoom_scale, updateUI = true);\nfor (var k = 0; k < animation_duration; k += videoFrameRate) { \nt.seconds = (end - animation_duration) + k;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = zoom_scale - ((zoom_scale - 100) * easing_curve(k / animation_duration, animation_easing_power)), updateUI = true);\nlast_t = t;}\nproperty.setValueAtKey(last_t, value = 100, updateUI = true);\n}\nelse {\nif (translations[\"Anchor Point\"].indexOf(property.displayName) > -1) { \nproperty.setTimeVarying(true, true);\nvar t = new Time();\nvar last_t = new Time();\nfor (var k = 0; k < animation_duration; k += videoFrameRate) { \nt.seconds = offset + k;\nproperty.addKey(t);\neasing_percentage = easing_curve(k / animation_duration, animation_easing_power);\nvalue_x = 0.5 + (((center_point_x / 100) - 0.5) * easing_percentage);\nvalue_y = 0.5 + (((center_point_y / 100) - 0.5) * easing_percentage);\nproperty.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\nlast_t = t;}\nproperty.setValueAtKey(last_t, value = zoom_scale, updateUI = true);\nfor (var k = 0; k < animation_duration; k += videoFrameRate) { \nt.seconds = (end - animation_duration) + k;\nproperty.addKey(t);\neasing_percentage = easing_curve(k / animation_duration, animation_easing_power);\nvalue_x = (center_point_x / 100) - (((center_point_x / 100) - 0.5) * easing_percentage);\nvalue_y = (center_point_y / 100) - (((center_point_y / 100) - 0.5) * easing_percentage);\nproperty.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\nlast_t = t;}\nproperty.setValueAtKey(last_t, value = 100, updateUI = true);\n}\n}}\n}}\n}\nfunction addZoomAnimationAsymmetric(start, end, zoom_scale, center_point_x, center_point_y, zoom_curve_start_power, zoom_curve_start_duration, zoom_curve_end_power, zoom_curve_end_duration, trackIndex) {\nvar clips = findClip(start, trackIndex);\nvar clip_normal = clips.clip_normal;\nif ((zoom_curve_start_power == 0) && (zoom_curve_end_power == 0)) { \nconsole.log(\"Both zoom powers are 0, doing static zooms\");\nsetPropertyStatic(clip_normal, \"Transform\", \"Scale Height\", zoom_scale);\nsetPropertyStatic(clip_normal, \"Transform\", \"Scale Width\", zoom_scale);\nsetPropertyStatic(clip_normal, \"Transform\", \"Anchor Point\", [center_point_x / 100, center_point_y / 100]);\nreturn;\n}\nvar offset = clip_normal.inPoint.seconds;\nvar end = offset + clip_normal.duration.seconds;\nif (clip_normal.duration.seconds < (zoom_curve_start_duration + zoom_curve_end_duration)) { \nzoom_curve_start_duration = (zoom_curve_start_duration / (zoom_curve_start_duration + zoom_curve_end_duration)) * clip_normal.duration.seconds;\nzoom_curve_end_duration = clip_normal.duration.seconds - zoom_curve_start_duration;\n}\nvar videoFrameRate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\nfor (var i = 0; i < clip_normal.components.length; i += 1) { \nvar component = clip_normal.components[i];\nif (translations.Transform.indexOf(component.displayName) > -1) { \nfor (var j = 0; j < component.properties.length; j += 1) { \nvar property = component.properties[j];\nif (((translations[\"Scale Height\"].indexOf(property.displayName) > -1) || (translations[\"Scale Width\"].indexOf(property.displayName) > -1)) || (translations.Scale.indexOf(property.displayName) > -1)) { \nproperty.setTimeVarying(true, true);\nt = new Time();\nlast_t = new Time();\nfor (var k = 0; k < zoom_curve_start_duration; k += videoFrameRate) { \neasing_percentage = easing_curve(k / zoom_curve_start_duration, zoom_curve_start_power);\nif (zoom_curve_start_power == 0) { \neasing_percentage = 1;\n}\nt.seconds = offset + k;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = 100 + ((zoom_scale - 100) * easing_percentage), updateUI = true);\nlast_t = t;}\nproperty.setValueAtKey(last_t, value = zoom_scale, updateUI = true);\nif ((zoom_curve_end_power > 0) && (zoom_curve_end_duration > 0)) { \nfor (var k = 0; k < zoom_curve_end_duration; k += videoFrameRate) { \neasing_percentage = easing_curve(k / zoom_curve_end_duration, zoom_curve_end_power);\nt.seconds = (end - zoom_curve_end_duration) + k;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = zoom_scale - ((zoom_scale - 100) * easing_percentage), updateUI = true);\nlast_t = t;}\nproperty.setValueAtKey(last_t, value = 100, updateUI = true);\n}\nelse {\nt.seconds = end - videoFrameRate;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = zoom_scale, updateUI = true);\nlast_t = end;\n}\nproperty.setValueAtKey(last_t, value = 100, updateUI = true);\n}\nelse {\nif (translations[\"Anchor Point\"].indexOf(property.displayName) > -1) { \nproperty.setTimeVarying(true, true);\nvar t = new Time();\nvar last_t = new Time();\nfor (var k = 0; k < zoom_curve_start_duration; k += videoFrameRate) { \neasing_percentage = easing_curve(k / zoom_curve_start_duration, zoom_curve_start_power);\nif (zoom_curve_start_power == 0) { \neasing_percentage = 1;\n}\nt.seconds = offset + k;\nproperty.addKey(t);\nvalue_x = 0.5 + (((center_point_x / 100) - 0.5) * easing_percentage);\nvalue_y = 0.5 + (((center_point_y / 100) - 0.5) * easing_percentage);\nproperty.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\nlast_t = t;}\nproperty.setValueAtKey(last_t, value = zoom_scale, updateUI = true);\nif ((zoom_curve_end_power > 0) && (zoom_curve_end_duration > 0)) { \nfor (var k = 0; k < zoom_curve_end_duration; k += videoFrameRate) { \neasing_percentage = easing_curve(k / zoom_curve_end_duration, zoom_curve_end_power);\nt.seconds = (end - zoom_curve_end_duration) + k;\nproperty.addKey(t);\nvalue_x = (center_point_x / 100) - (((center_point_x / 100) - 0.5) * easing_percentage);\nvalue_y = (center_point_y / 100) - (((center_point_y / 100) - 0.5) * easing_percentage);\nproperty.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\nlast_t = t;}\n}\nelse {\nt.seconds = end - videoFrameRate;\nproperty.addKey(t);\nproperty.setValueAtKey(t, value = [value_x, value_y], updateUI = true);\nlast_t = t;\n}\nproperty.setValueAtKey(last_t, value = 100, updateUI = true);\n}\n}}\n}}\n}\nfunction findClip(start, trackIndex) {\nvar clip_normal = null;\nvar clip_qe = null;\nfor (var i = 0; i < app.project.activeSequence.videoTracks[trackIndex].clips.length; i += 1) { \nif (Math.abs(app.project.activeSequence.videoTracks[trackIndex].clips[i].start.seconds - start) < time_tolerance) { \nclip_normal = app.project.activeSequence.videoTracks[trackIndex].clips[i];\n}}\nfor (var i = 0; i < qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems; i += 1) { \ntry {\nif (Math.abs(qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i).start.secs - start) < time_tolerance) { \nclip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n}\n} catch (err) {\n}}\nreturn {clip_normal: clip_normal, clip_qe: clip_qe};\n}\nfunction getVideoTransitionList() {\nreturn JSON.stringify(qe.project.getVideoTransitionList());\n}\nfunction addTransitionsToQEClipsOnTrack(trackIndex, transitionType, transitionLength) {\nvar numClips = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems;\nvar frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\nfor (var i = 0; i < numClips; i += 1) { \ntry {\nvar prev_clip_qe = null;\nvar next_clip_qe = null;\nif (i > 0) { \nprev_clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n}\nif (i < (numClips - 1)) { \nnext_clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i + 1);\n}\nvar clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\nvar start_time = parseInt(clip_qe.start.secs / frame_rate);\nvar end_time = parseInt(clip_qe.end.secs / frame_rate);\nvar clipLength = Math.abs(end_time - start_time);\ntransitionLength = Math.min(Math.ceil(clipLength / 2), parseInt(transitionLength)).toString();\nclip_qe.addTransition(qe.project.getVideoTransitionByName(transitionType), false, transitionLength, \"0:00\", 0.5, false);\n} catch (err) {\n}}\nreturn clip_qe;\n}\nfunction addTransitionsToQEClipOnTrackWithName(trackIndex, transitionType, transitionLength, clipName) {\nvar numClips = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).numItems;\nvar frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\nfor (var i = 0; i < numClips; i += 1) { \ntry {\nvar prev_clip_qe = null;\nif (i > 0) { \nprev_clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\n}\nvar clip_qe = qe.project.getActiveSequence().getVideoTrackAt(trackIndex).getItemAt(i);\nif (clip_qe.name) { \nif (clip_qe.name == clipName) { \nvar start_time = parseInt(clip_qe.start.secs / frame_rate);\nvar end_time = parseInt(clip_qe.end.secs / frame_rate);\nvar clipLength = Math.abs(end_time - start_time);\ntransitionLength = Math.min(Math.ceil(clipLength / 2), parseInt(transitionLength)).toString();\nclip_qe.addTransition(qe.project.getVideoTransitionByName(transitionType), false, transitionLength, \"0:00\", 0.5, false);\nclip_qe.addTransition(qe.project.getVideoTransitionByName(transitionType), true, transitionLength, \"0:00\", 0.5, false);\n}","numLines":500,"startLine":500,"totalLines":2858}}}
{"parentUuid":"6e07b53b-6b92-4c0d-96ab-994b0fd8a04d","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01EicFqkRHUftpkJjoq67sTx","type":"tool_result","content":"8:if (app.project.activeSequence) { \n9:sequence_default_timecode = app.project.activeSequence.getSettings().videoDisplayFormat;\n16:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n20:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n24:if (app.project.activeSequence) { \n43:var seq = app.project.activeSequence;\n45:var currentSeqSettings = app.project.activeSequence.getSettings();\n47:app.project.activeSequence.setSettings(currentSeqSettings);\n51:var seq = app.project.activeSequence;\n55:settings = app.project.activeSequence.getSettings();\n95:app.project.importFiles([path], suppressUI = true, targetBin = app.project.rootItem, importAsNumberedStills = false);\n98:return app.project.activeSequence.getSettings().videoDisplayFormat;\n102:var seq = app.project.activeSequence;\n104:var currentSeqSettings = app.project.activeSequence.getSettings();\n106:app.project.activeSequence.setSettings(currentSeqSettings);\n111:var seq = app.project.activeSequence;\n113:var currentSeqSettings = app.project.activeSequence.getSettings();\n116:app.project.activeSequence.setSettings(currentSeqSettings);\n123:qe.project.getActiveSequence().razor(frames);\n126:const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n133:qe.project.getActiveSequence().getVideoTrackAt(trackIndex).razor(frames);\n137:qe.project.getActiveSequence().getAudioTrackAt(trackIndex).razor(frames);\n142:return JSON.stringify(app.project.activeSequence);\n145:var sequence = app.project.activeSequence;\n154:var sequence = app.project.activeSequence;\n162:return app.project.activeSequence.getSettings().videoFrameRate.seconds;\n165:const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n171:for (var i = 0; i < qe.project.getActiveSequence().numVideoTracks; i += 1) { \n172:ret.push(qe.project.getActiveSequence().getVideoTrackAt(i).name);}\n176:for (var i = 0; i < qe.project.getActiveSequence().numAudioTracks; i += 1) { \n177:ret.push(qe.project.getActiveSequence().getAudioTrackAt(i).name);}\n185:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n189:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n207:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n211:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n235:var clips = app.project.activeSequence.videoTracks[track_index].clips;\n239:var clips = app.project.activeSequence.audioTracks[track_index].clips;\n252:var clips = app.project.activeSequence.videoTracks[track_index].clips;\n256:var clips = app.project.activeSequence.audioTracks[track_index].clips;\n272:var selected_clips = app.project.activeSequence.getSelection();\n275:var video_clips = app.project.activeSequence.videoTracks[videoTrackIndex].clips;\n276:var audio_clips = app.project.activeSequence.audioTracks[audioTrackIndex].clips;\n286:app.project.activeSequence.linkSelection();\n294:for (var i = 0; i < app.project.activeSequence.videoTracks.length; i += 1) { \n295:mute_state_video.push(app.project.activeSequence.videoTracks[i].isMuted());}\n297:for (var i = 0; i < app.project.activeSequence.audioTracks.length; i += 1) { \n298:mute_state_audio.push(app.project.activeSequence.audioTracks[i].isMuted());}\n304:if (mute_state.video.length != app.project.activeSequence.videoTracks.length) { \n307:if (mute_state.audio.length != app.project.activeSequence.audioTracks.length) { \n310:for (var i = 0; i < app.project.activeSequence.videoTracks.length; i += 1) { \n\n[Showing results with pagination = limit: 50, offset: 0]"}]},"uuid":"0b3633be-6e48-45ef-84b7-69e665a74fd8","timestamp":"2025-12-25T18:00:59.972Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"8:if (app.project.activeSequence) { \n9:sequence_default_timecode = app.project.activeSequence.getSettings().videoDisplayFormat;\n16:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n20:const frame_rate = app.project.activeSequence.getSettings().videoFrameRate.seconds;\n24:if (app.project.activeSequence) { \n43:var seq = app.project.activeSequence;\n45:var currentSeqSettings = app.project.activeSequence.getSettings();\n47:app.project.activeSequence.setSettings(currentSeqSettings);\n51:var seq = app.project.activeSequence;\n55:settings = app.project.activeSequence.getSettings();\n95:app.project.importFiles([path], suppressUI = true, targetBin = app.project.rootItem, importAsNumberedStills = false);\n98:return app.project.activeSequence.getSettings().videoDisplayFormat;\n102:var seq = app.project.activeSequence;\n104:var currentSeqSettings = app.project.activeSequence.getSettings();\n106:app.project.activeSequence.setSettings(currentSeqSettings);\n111:var seq = app.project.activeSequence;\n113:var currentSeqSettings = app.project.activeSequence.getSettings();\n116:app.project.activeSequence.setSettings(currentSeqSettings);\n123:qe.project.getActiveSequence().razor(frames);\n126:const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n133:qe.project.getActiveSequence().getVideoTrackAt(trackIndex).razor(frames);\n137:qe.project.getActiveSequence().getAudioTrackAt(trackIndex).razor(frames);\n142:return JSON.stringify(app.project.activeSequence);\n145:var sequence = app.project.activeSequence;\n154:var sequence = app.project.activeSequence;\n162:return app.project.activeSequence.getSettings().videoFrameRate.seconds;\n165:const frames = parseInt(Math.round(seconds / app.project.activeSequence.getSettings().videoFrameRate.seconds)).toString();\n171:for (var i = 0; i < qe.project.getActiveSequence().numVideoTracks; i += 1) { \n172:ret.push(qe.project.getActiveSequence().getVideoTrackAt(i).name);}\n176:for (var i = 0; i < qe.project.getActiveSequence().numAudioTracks; i += 1) { \n177:ret.push(qe.project.getActiveSequence().getAudioTrackAt(i).name);}\n185:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n189:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n207:var clips = app.project.activeSequence.videoTracks[trackIndex].clips;\n211:var clips = app.project.activeSequence.audioTracks[trackIndex].clips;\n235:var clips = app.project.activeSequence.videoTracks[track_index].clips;\n239:var clips = app.project.activeSequence.audioTracks[track_index].clips;\n252:var clips = app.project.activeSequence.videoTracks[track_index].clips;\n256:var clips = app.project.activeSequence.audioTracks[track_index].clips;\n272:var selected_clips = app.project.activeSequence.getSelection();\n275:var video_clips = app.project.activeSequence.videoTracks[videoTrackIndex].clips;\n276:var audio_clips = app.project.activeSequence.audioTracks[audioTrackIndex].clips;\n286:app.project.activeSequence.linkSelection();\n294:for (var i = 0; i < app.project.activeSequence.videoTracks.length; i += 1) { \n295:mute_state_video.push(app.project.activeSequence.videoTracks[i].isMuted());}\n297:for (var i = 0; i < app.project.activeSequence.audioTracks.length; i += 1) { \n298:mute_state_audio.push(app.project.activeSequence.audioTracks[i].isMuted());}\n304:if (mute_state.video.length != app.project.activeSequence.videoTracks.length) { \n307:if (mute_state.audio.length != app.project.activeSequence.audioTracks.length) { \n310:for (var i = 0; i < app.project.activeSequence.videoTracks.length; i += 1) { ","numLines":50,"appliedLimit":50}}
{"parentUuid":"0b3633be-6e48-45ef-84b7-69e665a74fd8","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_018eHwNN4DaVb3hD5jyx4VNC","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me search for event listeners and panel communication patterns:"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":10479,"cache_read_input_tokens":33820,"cache_creation":{"ephemeral_5m_input_tokens":10479,"ephemeral_1h_input_tokens":0},"output_tokens":6,"service_tier":"standard"}},"requestId":"req_011CWTqps2CSbbf7Jj2sdQcz","type":"assistant","uuid":"2b4a6c17-c563-4a0e-af87-8df216016cd2","timestamp":"2025-12-25T18:01:01.268Z"}
{"parentUuid":"2b4a6c17-c563-4a0e-af87-8df216016cd2","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_018eHwNN4DaVb3hD5jyx4VNC","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01SWZRPD1StjzNNNuhHemhHf","name":"Grep","input":{"path":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx","pattern":"addEventListener|dispatchEvent|CSXSEvent|stateChanged|onLoaded","output_mode":"content"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":10479,"cache_read_input_tokens":33820,"cache_creation":{"ephemeral_5m_input_tokens":10479,"ephemeral_1h_input_tokens":0},"output_tokens":6,"service_tier":"standard"}},"requestId":"req_011CWTqps2CSbbf7Jj2sdQcz","type":"assistant","uuid":"de53a8d2-a6c3-4016-b13d-734922877c16","timestamp":"2025-12-25T18:01:02.434Z"}
{"parentUuid":"de53a8d2-a6c3-4016-b13d-734922877c16","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_018eHwNN4DaVb3hD5jyx4VNC","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01MdVZH7HUjh2mG5rF3AuEtE","name":"Read","input":{"file_path":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx","offset":2800,"limit":500}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":7,"cache_creation_input_tokens":10479,"cache_read_input_tokens":33820,"cache_creation":{"ephemeral_5m_input_tokens":10479,"ephemeral_1h_input_tokens":0},"output_tokens":6,"service_tier":"standard"}},"requestId":"req_011CWTqps2CSbbf7Jj2sdQcz","type":"assistant","uuid":"d3fb5eb3-e0ad-488f-836d-fbaa477e9b6f","timestamp":"2025-12-25T18:01:02.826Z"}
{"parentUuid":"d3fb5eb3-e0ad-488f-836d-fbaa477e9b6f","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01MdVZH7HUjh2mG5rF3AuEtE","type":"tool_result","content":"  2800→return false;\n  2801→}\n  2802→if (trackIndex == -1) { \n  2803→trackIndex = app.project.activeSequence.audioTracks.length - 1;\n  2804→}\n  2805→var targetTime = new Time();\n  2806→targetTime.seconds = start_seconds;\n  2807→app.project.activeSequence.audioTracks[trackIndex].insertClip(audio_project_item, targetTime);\n  2808→return true;\n  2809→}\n  2810→function getProjectItemDimensions(projectItem) {\n  2811→try {\n  2812→const columnsMetadata = JSON.parse(projectItem.getProjectColumnsMetadata());\n  2813→const videoInfoColumn = columnsMetadata.filter(function (column) {\n  2814→return column.ColumnID === \"Column.Intrinsic.VideoInfo\";\n  2815→})[0];\n  2816→const videoInfo = videoInfoColumn.ColumnValue;\n  2817→const dimensions = {height: parseFloat(videoInfo.split(\"x\")[1].split(\"(\")[0].trim()), width: parseFloat(videoInfo.split(\"x\")[0].trim())};\n  2818→return dimensions;\n  2819→} catch (e) {console.log(\"Failed to get project item dimensions:\", e);\n  2820→return null;\n  2821→}\n  2822→}\n  2823→app.enableQE();\n  2824→qe.setDebugDatabaseEntry(\"dvascripting.EnabledInternalDOM\", \"true\");\n  2825→$.includeStack = true;\n  2826→var translations = {\"Anchor Point\": [\"Anchor Point\", \"Point d\\'ancrage\", \"Ankerpunkt\", \"Punto di ancoraggio\", \"Ponto de ancoragem\", \"Punto de anclaje\", \"\\u30a2\\u30f3\\u30ab\\u30fc\\u30dd\\u30a4\\u30f3\\u30c8\", \"\\uae30\\uc900\\uc810\", \"\\u041e\\u043f\\u043e\\u0440\\u043d\\u0430\\u044f \\u0442\\u043e\\u0447\\u043a\\u0430\", \"\\u951a\\u70b9\"], \"Blur Length\": [\"Blur Length\", \"Longueur du Flou\", \"Unsch\\xe4rfel\\xe4nge\", \"Lunghezza della Sfocatura\", \"Comprimento do Desfoque\", \"Longitud del Desenfoque\", \"\\u0414\\u043b\\u0438\\u043d\\u0430 \\u0440\\u0430\\u0437\\u043c\\u044b\\u0442\\u0438\\u044f\", \"\\u30d6\\u30e9\\u30fc\\u306e\\u9577\\u3055\", \"\\ube14\\ub7ec \\uae38\\uc774\", \"\\u6a21\\u7cca\\u957f\\u5ea6\"], Bottom: [\"Abajo\", \"Bas\", \"Basso\", \"Bottom\", \"Inferior\", \"Unten\", \"\\u0421\\u043d\\u0438\\u0437\\u0443\", \"\\uc544\\ub798\", \"\\u4e0b\", \"\\u5e95\\u90e8\"], Crop: [\"Cortar\", \"Crop\", \"Recadrage\", \"Recortar\", \"Ritaglia\", \"Zuschneiden\", \"\\u041e\\u0431\\u0440\\u0435\\u0437\\u0430\\u0442\\u044c\", \"\\uc790\\ub974\\uae30\", \"\\u30af\\u30ed\\u30c3\\u30d7\", \"\\u88c1\\u526a\"], \"Cross Dissolve\": [\"Cross Dissolve\", \"Weiche Blende\", \"Disoluci\\xf3n cruzada\", \"Fondu encha\\xeen\\xe9\", \"Dissolvenza incrociata\", \"\\u30af\\u30ed\\u30b9\\u30c7\\u30a3\\u30be\\u30eb\\u30d6\", \"\\uad50\\ucc28 \\ub514\\uc878\\ube0c\", \"Dissolu\\xe7\\xe3o cruzada\", \"\\u041f\\u0435\\u0440\\u0435\\u043a\\u0440\\u0435\\u0441\\u0442\\u043d\\u044b\\u0439 \\u043d\\u0430\\u043f\\u043b\\u044b\\u0432\", \"\\u4ea4\\u53c9\\u6eb6\\u89e3\"], Direction: [\"Direction\", \"Direction\", \"Richtung\", \"Direzione\", \"Dire\\xe7\\xe3o\", \"Direcci\\xf3n\", \"\\u041d\\u0430\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435\", \"\\u65b9\\u5411\", \"\\ubc29\\ud5a5\", \"\\u65b9\\u5411\"], \"Directional Blur\": [\"Directional Blur\", \"Flou Directionnel\", \"Contour progressif\", \"Richtungsunsch\\xe4rfe\", \"Sfocatura Direzionale\", \"Desfoque Direcional\", \"Desenfoque Direccional\", \"\\u041d\\u0430\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u043d\\u043e\\u0435 P\\u0430\\u0437\\u043c\\u044b\\u0442\\u0438\\u0435\", \"\\u30c0\\u30a4\\u30ec\\u30af\\u30b7\\u30e7\\u30ca\\u30eb\\u30d6\\u30e9\", \"\\ub514\\ub809\\uc154\\ub110 \\ube14\\ub7ec\", \"\\u65b9\\u5411\\u6a21\\u7cca\"], \"Edge  Feather\": [\"Bordi sfumati\", \"Contour progressif\", \"Difus\\xe3o de borda\", \"Edge Feather\", \"Pluma de borde\", \"Weiche Kanten\", \"\\u0420\\u0430\\u0441\\u0442\\u0443\\u0448\\u0435\\u0432\\u043a\\u0430 \\u0433\\u0440\\u0430\\u043d\\u0438\\u0446\", \"\\uac00\\uc7a5\\uc790\\ub9ac \\ud398\\ub354\", \"\\u30a8\\u30c3\\u30b8\\u3092\\u307c\\u304b\\u3059\", \"\\u7fbd\\u5316\\u8fb9\\u7f18\"], Left: [\"Esquerda\", \"Gauche\", \"Izquierda\", \"Left\", \"Links\", \"Sinistra\", \"\\u0421\\u043b\\u0435\\u0432\\u0430\", \"\\uc67c\\ucabd\", \"\\u5de6\", \"\\u5de6\\u4fa7\"], Motion: [\"Motion\", \"Trajectoire\", \"Bewegung\", \"Movimento\", \"Movimiento\", \"Movimento\", \"\\ubaa8\\uc158\"], Position: [\"Position\", \"Posizione\", \"Posici\\xf3n\", \"Posi\\xe7\\xe3o\", \"\\u041f\\u043e\\u043b\\u043e\\u0436\\u0435\\u043d\\u0438\\u0435\", \"\\u4f4d\\u7f6e\", \"\\uc704\\uce58\"], Right: [\"Derecha\", \"Destra\", \"Direita\", \"Droite\", \"Rechts\", \"Right\", \"\\u0421\\u043f\\u0440\\u0430\\u0432\\u0430\", \"\\uc624\\ub978\\ucabd\", \"\\u53f3\", \"\\u53f3\\u4fa7\"], Rotation: [\"Rotation\", \"Rotazione\", \"Drehung\", \"Rotaci\\xf3n\", \"Rota\\xe7\\xe3o\", \"\\u0412\\u0440\\u0430\\u0449\\u0435\\u043d\\u0438\\u0435\", \"\\u65cb\\u8f6c\", \"\\ud68c\\uc804\"], Scale: [\"Scale\", \"Echelle\", \"Skalieren\", \"Scala\", \"Escala\", \"Escala\", \"\\ube44\\uc728 \\uc870\\uc815\"], \"Scale Height\": [\"Scale Height\", \"Hauteur d\\'\\xe9chelle\", \"Skalierungsh\\xf6he\", \"Altezza scala\", \"Altura de escala\", \"Dimensionar altura\", \"\\ub192\\uc774 \\ube44\\uc728 \\uc870\\uc815\"], \"Scale Width\": [\"Scale Width\", \"Largeur d\\'\\xe9chelle\", \"Skalierungsbreite\", \"Larghezza scala\", \"Anchura de escala\", \"Dimensionar largura\", \"\\ud3ed \\ube44\\uc728 \\uc870\\uc815\"], Top: [\"Alto\", \"Arriba\", \"Haut\", \"Oben\", \"Superior\", \"Top\", \"\\u0421\\u0432\\u0435\\u0440\\u0445\\u0443\", \"\\uc704\", \"\\u4e0a\", \"\\u9876\\u90e8\"], Transform: [\"Transform\", \"Transformation\", \"Transformieren\", \"Trasformazione\", \"Transformar\", \"Transformar\", \"\\ubcc0\\ud615\"], Zoom: [\"Zoom\", \"Zoom\", \"Zoom\", \"Zoom\", \"Zoom\", \"Zoom\", \"\\u041c\\u0430\\u0441\\u0448\\u0442\\u0430\\u0431\", \"\\ud655\\ub300/\\ucd95\\uc18c\", \"\\u30ba\\u30fc\\u30e0\", \"\\u7f29\\u653e\"]};\n  2827→var TIMECODES = {TIMEDISPLAY_16mmFeetFrames: 111, TIMEDISPLAY_23976Timecode: 110, TIMEDISPLAY_24Timecode: 100, TIMEDISPLAY_25Timecode: 101, TIMEDISPLAY_2997DropTimecode: 102, TIMEDISPLAY_2997NonDropTimecode: 103, TIMEDISPLAY_30Timecode: 104, TIMEDISPLAY_35mmFeetFrames: 112, TIMEDISPLAY_48Timecode: 113, TIMEDISPLAY_50Timecode: 105, TIMEDISPLAY_5994DropTimecode: 106, TIMEDISPLAY_5994NonDropTimecode: 107, TIMEDISPLAY_60Timecode: 108, TIMEDISPLAY_AudioMsTimecode: 201, TIMEDISPLAY_AudioSamplesTimecode: 200, TIMEDISPLAY_Frames: 109};\n  2828→var project_item_type = {BIN: 2, CLIP: 1, FILE: 4, ROOT: 3, bin: 2, clip: 1, file: 4, root: 3};\n  2829→var console = {log: function () {\n  2830→try {\n  2831→xlib = new ExternalObject(\"lib:PlugPlugExternalObject\");\n  2832→} catch (e) {throw e;\n  2833→}\n  2834→if (xlib) { \n  2835→var eventObj = new CSXSEvent();\n  2836→eventObj.type = \"jsxLog\";\n  2837→eventObj.data = JSON.stringify(arguments);\n  2838→eventObj.dispatch();\n  2839→}\n  2840→else {\n  2841→throw \"No xlib\";\n  2842→}\n  2843→}};\n  2844→var sequence_default_timecode = TIMECODES.TIMEDISPLAY_25Timecode;\n  2845→initialise();\n  2846→if (!Array.prototype.indexOf) { \n  2847→Array.prototype.indexOf = function (item) {\n  2848→var i = this.length;\n  2849→while (i--) {\n  2850→if (this[i] === item) { \n  2851→return i;\n  2852→}\n  2853→}\n  2854→return -1;\n  2855→};\n  2856→}\n  2857→var temp_bin = null;\n  2858→time_tolerance = 0.1;\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"a76eeea1-e46c-4bd7-a76a-6a200afde593","timestamp":"2025-12-25T18:01:02.885Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx","content":"return false;\n}\nif (trackIndex == -1) { \ntrackIndex = app.project.activeSequence.audioTracks.length - 1;\n}\nvar targetTime = new Time();\ntargetTime.seconds = start_seconds;\napp.project.activeSequence.audioTracks[trackIndex].insertClip(audio_project_item, targetTime);\nreturn true;\n}\nfunction getProjectItemDimensions(projectItem) {\ntry {\nconst columnsMetadata = JSON.parse(projectItem.getProjectColumnsMetadata());\nconst videoInfoColumn = columnsMetadata.filter(function (column) {\nreturn column.ColumnID === \"Column.Intrinsic.VideoInfo\";\n})[0];\nconst videoInfo = videoInfoColumn.ColumnValue;\nconst dimensions = {height: parseFloat(videoInfo.split(\"x\")[1].split(\"(\")[0].trim()), width: parseFloat(videoInfo.split(\"x\")[0].trim())};\nreturn dimensions;\n} catch (e) {console.log(\"Failed to get project item dimensions:\", e);\nreturn null;\n}\n}\napp.enableQE();\nqe.setDebugDatabaseEntry(\"dvascripting.EnabledInternalDOM\", \"true\");\n$.includeStack = true;\nvar translations = {\"Anchor Point\": [\"Anchor Point\", \"Point d\\'ancrage\", \"Ankerpunkt\", \"Punto di ancoraggio\", \"Ponto de ancoragem\", \"Punto de anclaje\", \"\\u30a2\\u30f3\\u30ab\\u30fc\\u30dd\\u30a4\\u30f3\\u30c8\", \"\\uae30\\uc900\\uc810\", \"\\u041e\\u043f\\u043e\\u0440\\u043d\\u0430\\u044f \\u0442\\u043e\\u0447\\u043a\\u0430\", \"\\u951a\\u70b9\"], \"Blur Length\": [\"Blur Length\", \"Longueur du Flou\", \"Unsch\\xe4rfel\\xe4nge\", \"Lunghezza della Sfocatura\", \"Comprimento do Desfoque\", \"Longitud del Desenfoque\", \"\\u0414\\u043b\\u0438\\u043d\\u0430 \\u0440\\u0430\\u0437\\u043c\\u044b\\u0442\\u0438\\u044f\", \"\\u30d6\\u30e9\\u30fc\\u306e\\u9577\\u3055\", \"\\ube14\\ub7ec \\uae38\\uc774\", \"\\u6a21\\u7cca\\u957f\\u5ea6\"], Bottom: [\"Abajo\", \"Bas\", \"Basso\", \"Bottom\", \"Inferior\", \"Unten\", \"\\u0421\\u043d\\u0438\\u0437\\u0443\", \"\\uc544\\ub798\", \"\\u4e0b\", \"\\u5e95\\u90e8\"], Crop: [\"Cortar\", \"Crop\", \"Recadrage\", \"Recortar\", \"Ritaglia\", \"Zuschneiden\", \"\\u041e\\u0431\\u0440\\u0435\\u0437\\u0430\\u0442\\u044c\", \"\\uc790\\ub974\\uae30\", \"\\u30af\\u30ed\\u30c3\\u30d7\", \"\\u88c1\\u526a\"], \"Cross Dissolve\": [\"Cross Dissolve\", \"Weiche Blende\", \"Disoluci\\xf3n cruzada\", \"Fondu encha\\xeen\\xe9\", \"Dissolvenza incrociata\", \"\\u30af\\u30ed\\u30b9\\u30c7\\u30a3\\u30be\\u30eb\\u30d6\", \"\\uad50\\ucc28 \\ub514\\uc878\\ube0c\", \"Dissolu\\xe7\\xe3o cruzada\", \"\\u041f\\u0435\\u0440\\u0435\\u043a\\u0440\\u0435\\u0441\\u0442\\u043d\\u044b\\u0439 \\u043d\\u0430\\u043f\\u043b\\u044b\\u0432\", \"\\u4ea4\\u53c9\\u6eb6\\u89e3\"], Direction: [\"Direction\", \"Direction\", \"Richtung\", \"Direzione\", \"Dire\\xe7\\xe3o\", \"Direcci\\xf3n\", \"\\u041d\\u0430\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u0438\\u0435\", \"\\u65b9\\u5411\", \"\\ubc29\\ud5a5\", \"\\u65b9\\u5411\"], \"Directional Blur\": [\"Directional Blur\", \"Flou Directionnel\", \"Contour progressif\", \"Richtungsunsch\\xe4rfe\", \"Sfocatura Direzionale\", \"Desfoque Direcional\", \"Desenfoque Direccional\", \"\\u041d\\u0430\\u043f\\u0440\\u0430\\u0432\\u043b\\u0435\\u043d\\u043d\\u043e\\u0435 P\\u0430\\u0437\\u043c\\u044b\\u0442\\u0438\\u0435\", \"\\u30c0\\u30a4\\u30ec\\u30af\\u30b7\\u30e7\\u30ca\\u30eb\\u30d6\\u30e9\", \"\\ub514\\ub809\\uc154\\ub110 \\ube14\\ub7ec\", \"\\u65b9\\u5411\\u6a21\\u7cca\"], \"Edge  Feather\": [\"Bordi sfumati\", \"Contour progressif\", \"Difus\\xe3o de borda\", \"Edge Feather\", \"Pluma de borde\", \"Weiche Kanten\", \"\\u0420\\u0430\\u0441\\u0442\\u0443\\u0448\\u0435\\u0432\\u043a\\u0430 \\u0433\\u0440\\u0430\\u043d\\u0438\\u0446\", \"\\uac00\\uc7a5\\uc790\\ub9ac \\ud398\\ub354\", \"\\u30a8\\u30c3\\u30b8\\u3092\\u307c\\u304b\\u3059\", \"\\u7fbd\\u5316\\u8fb9\\u7f18\"], Left: [\"Esquerda\", \"Gauche\", \"Izquierda\", \"Left\", \"Links\", \"Sinistra\", \"\\u0421\\u043b\\u0435\\u0432\\u0430\", \"\\uc67c\\ucabd\", \"\\u5de6\", \"\\u5de6\\u4fa7\"], Motion: [\"Motion\", \"Trajectoire\", \"Bewegung\", \"Movimento\", \"Movimiento\", \"Movimento\", \"\\ubaa8\\uc158\"], Position: [\"Position\", \"Posizione\", \"Posici\\xf3n\", \"Posi\\xe7\\xe3o\", \"\\u041f\\u043e\\u043b\\u043e\\u0436\\u0435\\u043d\\u0438\\u0435\", \"\\u4f4d\\u7f6e\", \"\\uc704\\uce58\"], Right: [\"Derecha\", \"Destra\", \"Direita\", \"Droite\", \"Rechts\", \"Right\", \"\\u0421\\u043f\\u0440\\u0430\\u0432\\u0430\", \"\\uc624\\ub978\\ucabd\", \"\\u53f3\", \"\\u53f3\\u4fa7\"], Rotation: [\"Rotation\", \"Rotazione\", \"Drehung\", \"Rotaci\\xf3n\", \"Rota\\xe7\\xe3o\", \"\\u0412\\u0440\\u0430\\u0449\\u0435\\u043d\\u0438\\u0435\", \"\\u65cb\\u8f6c\", \"\\ud68c\\uc804\"], Scale: [\"Scale\", \"Echelle\", \"Skalieren\", \"Scala\", \"Escala\", \"Escala\", \"\\ube44\\uc728 \\uc870\\uc815\"], \"Scale Height\": [\"Scale Height\", \"Hauteur d\\'\\xe9chelle\", \"Skalierungsh\\xf6he\", \"Altezza scala\", \"Altura de escala\", \"Dimensionar altura\", \"\\ub192\\uc774 \\ube44\\uc728 \\uc870\\uc815\"], \"Scale Width\": [\"Scale Width\", \"Largeur d\\'\\xe9chelle\", \"Skalierungsbreite\", \"Larghezza scala\", \"Anchura de escala\", \"Dimensionar largura\", \"\\ud3ed \\ube44\\uc728 \\uc870\\uc815\"], Top: [\"Alto\", \"Arriba\", \"Haut\", \"Oben\", \"Superior\", \"Top\", \"\\u0421\\u0432\\u0435\\u0440\\u0445\\u0443\", \"\\uc704\", \"\\u4e0a\", \"\\u9876\\u90e8\"], Transform: [\"Transform\", \"Transformation\", \"Transformieren\", \"Trasformazione\", \"Transformar\", \"Transformar\", \"\\ubcc0\\ud615\"], Zoom: [\"Zoom\", \"Zoom\", \"Zoom\", \"Zoom\", \"Zoom\", \"Zoom\", \"\\u041c\\u0430\\u0441\\u0448\\u0442\\u0430\\u0431\", \"\\ud655\\ub300/\\ucd95\\uc18c\", \"\\u30ba\\u30fc\\u30e0\", \"\\u7f29\\u653e\"]};\nvar TIMECODES = {TIMEDISPLAY_16mmFeetFrames: 111, TIMEDISPLAY_23976Timecode: 110, TIMEDISPLAY_24Timecode: 100, TIMEDISPLAY_25Timecode: 101, TIMEDISPLAY_2997DropTimecode: 102, TIMEDISPLAY_2997NonDropTimecode: 103, TIMEDISPLAY_30Timecode: 104, TIMEDISPLAY_35mmFeetFrames: 112, TIMEDISPLAY_48Timecode: 113, TIMEDISPLAY_50Timecode: 105, TIMEDISPLAY_5994DropTimecode: 106, TIMEDISPLAY_5994NonDropTimecode: 107, TIMEDISPLAY_60Timecode: 108, TIMEDISPLAY_AudioMsTimecode: 201, TIMEDISPLAY_AudioSamplesTimecode: 200, TIMEDISPLAY_Frames: 109};\nvar project_item_type = {BIN: 2, CLIP: 1, FILE: 4, ROOT: 3, bin: 2, clip: 1, file: 4, root: 3};\nvar console = {log: function () {\ntry {\nxlib = new ExternalObject(\"lib:PlugPlugExternalObject\");\n} catch (e) {throw e;\n}\nif (xlib) { \nvar eventObj = new CSXSEvent();\neventObj.type = \"jsxLog\";\neventObj.data = JSON.stringify(arguments);\neventObj.dispatch();\n}\nelse {\nthrow \"No xlib\";\n}\n}};\nvar sequence_default_timecode = TIMECODES.TIMEDISPLAY_25Timecode;\ninitialise();\nif (!Array.prototype.indexOf) { \nArray.prototype.indexOf = function (item) {\nvar i = this.length;\nwhile (i--) {\nif (this[i] === item) { \nreturn i;\n}\n}\nreturn -1;\n};\n}\nvar temp_bin = null;\ntime_tolerance = 0.1;","numLines":59,"startLine":2800,"totalLines":2858}}}
{"parentUuid":"a76eeea1-e46c-4bd7-a76a-6a200afde593","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01SWZRPD1StjzNNNuhHemhHf","type":"tool_result","content":"2835:var eventObj = new CSXSEvent();"}]},"uuid":"6af6f04a-a688-49a5-ab8c-3260c89a9382","timestamp":"2025-12-25T18:01:03.554Z","toolUseResult":{"mode":"content","numFiles":0,"filenames":[],"content":"2835:var eventObj = new CSXSEvent();","numLines":1}}
{"parentUuid":"6af6f04a-a688-49a5-ab8c-3260c89a9382","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"40e35131-d872-47c8-a3ec-760adc72c5aa","version":"2.0.76","gitBranch":"main","agentId":"ab9afa3","slug":"zippy-mixing-wilkes","message":{"model":"claude-haiku-4-5-20251001","id":"msg_01JhtDQBHrHEuANXZzVVKiad","type":"message","role":"assistant","content":[{"type":"text","text":"Perfect! Now I have a comprehensive view of the Fireside host JSX file. Let me create a detailed analysis document by compiling all the information I've gathered.\n\n## Fireside Host JSX File - Comprehensive Analysis\n\nBased on my thorough examination of `/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx`, here is a complete analysis of Fireside's core ExtendScript/JSX host implementation:\n\n---\n\n### 1. ARCHITECTURE OVERVIEW\n\n**Core Communication Model:**\n- **ExtendScript (JSX) Host Layer**: Runs inside Premiere Pro with full API access\n- **Panel Communication**: Uses CSXSEvent (CEP - Common Extensibility Platform) for bidirectional messaging with the HTML/JavaScript panel\n- **Debug System**: Custom console.log() that dispatches CSXSEvent messages of type \"jsxLog\" to the panel\n- **Enable QE APIs**: Uses `app.enableQE()` and `qe.setDebugDatabaseEntry()` for internal DOM access\n\n**Key Global Variables:**\n```javascript\nvar translations = {...}              // Multilingual effect/component names\nvar TIMECODES = {...}                // Timecode format constants\nvar project_item_type = {...}        // Item type enums\nvar sequence_default_timecode         // Stores default timecode format\nvar time_tolerance = 0.1              // Clip matching tolerance in seconds\nvar temp_bin = null                   // Temporary bin for media management\n```\n\n---\n\n### 2. FUNCTION CATEGORIES & PURPOSES\n\n#### **A. INITIALIZATION & UTILITIES**\n\n| Function | Purpose |\n|----------|---------|\n| `initialise()` | Stores default timecode format on startup |\n| `storeDefaultTimecode()` | Captures active sequence timecode display format |\n| `checkSequenceOpen()` | Validates that a sequence is active |\n| `checkInitialisation()` | Returns true (placeholder validation) |\n| `quantise_time(time_seconds)` | Rounds time to nearest frame |\n| `frames_to_time(number_of_frames)` | Converts frame count to seconds |\n\n#### **B. TIMECODE & FORMAT MANAGEMENT**\n\n| Function | Purpose |\n|----------|---------|\n| `setTimecodeToDefault()` | Restores stored default timecode format |\n| `setTimecodeToFrames()` | Switches display to frame numbers |\n| `getTimecodeFormat()` | Returns current timecode display format |\n| `setTimecodeFormat(format)` | Sets timecode format by ID |\n| `secondsToCurrentTimecode(seconds)` | Converts seconds to timecode string in current format |\n| `getVideoFrameRateInSeconds()` | Returns frame rate as seconds per frame |\n\n#### **C. TIMELINE CUTTING & MANIPULATION**\n\n| Function | Purpose | Key Details |\n|----------|---------|------------|\n| `razorSequenceAtFrames(frames)` | Cuts entire sequence at specific frame | Sets timecode to frames first |\n| `razorSequenceAtSeconds(seconds)` | Cuts entire sequence at time (converts to frames) | - |\n| `razorTrackAtFrames(trackType, trackIndex, frames)` | Cuts specific video/audio track | Handles both \"video\" and \"audio\" types |\n| `razorTrackAtSeconds(trackType, trackIndex, seconds)` | Cuts track at time in seconds | Converts to frames internally |\n| `razorSequenceAtFramesArray(silences_array)` | Cuts multiple points in one pass | Used for batch silence removal |\n\n**Implementation Detail:**\nUses `qe.project.getActiveSequence().razor(frames)` and track-specific variants for razor cuts.\n\n#### **D. CLIP MANIPULATION & DELETION**\n\n| Function | Purpose |\n|----------|---------|\n| `deleteClipsAtSilencePointsInTrack()` | Removes clips whose center falls within silence regions |\n| `makeRemainingClipsContiguousInTrack()` | Shifts remaining clips to eliminate silence gaps |\n| `moveAllClipsInTrack()` | Moves all clips in a track by fixed amount (handles direction for efficiency) |\n| `moveAllClipsInTrackAfterSeconds()` | Shifts only clips starting after a specific time |\n| `deleteTrackItemByNodeId()` | Deletes clip by its unique node ID |\n| `deleteClipByNodeId()` | Alternative clip deletion method |\n| `resizeVideoClip()` | Changes clip duration |\n\n**Silence Removal Algorithm:**\n```javascript\n// Find silence center point\nsilence_center = (silence[0] + silence[1]) / 2\n// Check if clip overlaps silence\nif (silence_center > clip.start && silence_center < clip.end) {\n    clip.remove(false, false)\n}\n```\n\n#### **E. CLIP LINKING & MANAGEMENT**\n\n| Function | Purpose |\n|----------|---------|\n| `linkIdenticalClipsInTracks()` | Links video/audio clips with matching start times |\n| `removeAllLinks()` | Unlinks clips in specified track |\n| `getLinkedItemsOfClip()` | Returns linked items for a given clip |\n| `disableClip()` | Disables clip (affects linked tracks if specified) |\n| `disableTrackItemByNodeId()` | Disables clip via node ID |\n| `enableTrackItemByNodeId()` | Re-enables disabled clip |\n\n#### **F. AUDIO/VIDEO TRACK MANAGEMENT**\n\n| Function | Purpose |\n|----------|---------|\n| `getTrackNames()` | Returns array of track names (video or audio) |\n| `getNumTracks()` | Gets total count of video/audio tracks |\n| `getNumClipsInTrack()` | Counts clips in specific track |\n| `addAudioTrack()` | Creates new audio track, returns track ID |\n| `getMuteState()` | Captures mute state of all tracks (JSON) |\n| `setMuteState()` | Restores mute state from JSON |\n\n#### **G. CLIP SELECTION & RETRIEVAL**\n\n| Function | Purpose |\n|----------|---------|\n| `getActiveSequence()` | Returns entire active sequence as JSON |\n| `getActiveSequenceVideoClips()` | Gets all video clips across all tracks |\n| `getActiveSequenceVideoClipsForTrackId()` | Gets clips from specific video track |\n| `getSelection()` | Returns currently selected clips |\n| `getSelectionCount()` | Counts selected clips |\n| `selectVideoClipByNodeId()` | Programmatically selects clip by node ID |\n| `selectAudioTracks()` | Selects audio tracks within time range |\n\n#### **H. MEDIA INSERTION & REPLACEMENT**\n\n| Function | Purpose | Key Features |\n|----------|---------|-------------|\n| `insertMediaInTrackAtTime()` | Inserts video/image at specific time and track | Auto-scales to fit, sets in/out points |\n| `insertMediaInTrackAtTime_v2()` | Updated version with better offset handling | Improved clip detection |\n| `insertBrollInTrackAtTime()` | Inserts B-roll with motion properties (position, anchor, rotation, scale) | Creates temp track, applies properties, moves to target |\n| `replaceClipWithMedia()` | Swaps media while preserving timing | Maintains start/end times |\n| `addMusicToProject()` | Adds audio to project (creates new audio track) | Inserts at timecode 0 |\n| `addAudioTrack()` | Adds audio track | Returns new track ID |\n| `addAudioClipToProject()` | Inserts audio clip to specific audio track | Maintains duration |\n| `insertAudioInTrackAtTime()` | Adds audio clip with target time control | - |\n| `insertImageInTrackAtTime()` | Inserts image with fade/pop animation | Complex animation setup |\n| `insertNestedSequenceInTrackAtTime()` | Embeds another sequence as clip | - |\n| `insertViewboxInTrack()` | Inserts viewbox project item with optional audio | Links audio if included |\n\n**Media Insertion Features:**\n- Imports to project bin\n- Auto-detects dimensions via metadata\n- Handles audio channel mapping (disables audio on video)\n- Scales to fit or contain sequence resolution\n- Sets in/out points programmatically\n- Frame rate quantization for accuracy\n\n#### **I. MOTION & EFFECTS**\n\n**Zoom Functions:**\n| Function | Purpose |\n|----------|---------|\n| `addZoomAtTime()` | Basic zoom with optional animation |\n| `addZoomAnimation()` | Animated zoom with easing and center point control |\n| `addZoomAnimationAsymmetric()` | Zoom with separate start/end animation curves |\n| `addZoomAtTimeAndCenter()` | Zoom with explicit center point |\n| `addZoomAtTimeAndCenterAsymmetric()` | Asymmetric zoom with center point |\n| `removeClipKeyFrames()` | Clears all keyframes from clip properties |\n\n**Zoom Implementation:**\n- Uses Transform effect (localized via translations)\n- Adds keyframes at specific times\n- Supports easing curves with configurable power\n- Animates: Scale Height, Scale Width, Anchor Point\n- Applies to adjustment layers (AI Adjustment Layer)\n\n**Easing Curves:**\n```javascript\nfunction easing_curve(t, power) {\n    // Exponential easing based on power parameter\n    const coefficient = Math.pow(2, power - 1);\n    return (1 - Math.pow(2, -power * t)) / coefficient;\n}\n```\n\n**Effect & Transition Functions:**\n| Function | Purpose |\n|----------|---------|\n| `getVideoEffectName()` | Finds effect by English name with multilingual lookup |\n| `getVideoTransitionName()` | Finds transition by English name with multilingual lookup |\n| `setPropertyStatic()` | Sets non-animated property value on clip component |\n| `getProperty()` | Retrieves property object from clip component |\n| `addTransitionsToQEClipsOnTrack()` | Adds transition between all clips on track |\n| `addTransitionsToQEClipOnTrackWithName()` | Adds transition to specific named clip |\n| `addTransitionToCaption()` | Adds transition to caption clip by ID |\n\n#### **J. DIRECTIONAL BLUR & EFFECTS**\n\n| Function | Purpose |\n|----------|---------|\n| `addDirectionalBlurToCaptions()` | Applies directional blur effect to caption clips |\n| `addDirectionalBlurEffectToClip()` | Complex blur with keyframe positions |\n\n#### **K. CAPTION/TEXT RENDERING**\n\n| Function | Purpose |\n|----------|---------|\n| `importStyledCaptions()` | Complex caption import with animation, positioning, word-by-word timing |\n| `addPopAndFadeToCaptions()` | Adds pop/fade animations to caption words with timing |\n| `addWiggleToCaptions()` | Applies wiggle effect to captions with customizable frequency/amplitude |\n| `addWiggleToCaptionsInSequenceIndex()` | Wiggle effect for non-active sequences |\n| `addEmojiAnimations()` | Adds fade and slide animations to emoji clips |\n| `addEmojiAnimationsInSequenceIndex()` | Emoji animations for specific sequence |\n| `wiggleOffset()` | Calculates wiggle displacement for given time |\n\n**Caption Animation Features:**\n- Word-by-word timing control\n- Multiple animation types: fade, slide, pop\n- Custom easing and keyframe generation\n- Box position and size control\n- Per-word animation parameters\n\n#### **L. SEQUENCE & PROJECT MANAGEMENT**\n\n| Function | Purpose |\n|----------|---------|\n| `findSequenceIDByName()` | Locates sequence by display name |\n| `findSequenceIndexByName()` | Gets sequence index by name |\n| `findSequenceIDByNode()` | Finds sequence by node ID |\n| `findSequenceByID()` | Retrieves sequence object by ID |\n| `createBlankSequenceWithSameSettings()` | Creates new sequence with copied settings |\n| `loadSequence()` | Switches to different sequence |\n| `updateSequenceAspectRatio()` | Changes sequence aspect ratio |\n| `getFullActiveSequenceObject()` | Deep copies entire active sequence object |\n| `exportSequenceObject()` | Exports sequence to JSON for debugging |\n| `exportActiveSequenceData()` | Exports detailed sequence data with all properties |\n\n#### **M. BIN & PROJECT ORGANIZATION**\n\n| Function | Purpose |\n|----------|---------|\n| `getEditAIBin()` | Returns (or creates) \"Edit AI\" folder |\n| `getFireCutSubBin()` | Gets FireCut sub-folder, creates if missing |\n| `getFireCutSubBinJSON()` | Returns FireCut bin as JSON |\n| `findItemByName()` | Recursively searches bins for item by name |\n| `findItemByNameAndType()` | Finds item matching name and type (BIN/CLIP/FILE) |\n| `findItemByNodeId()` | Finds item by unique node ID (recursive) |\n| `getAllNodeIDs()` | Collects all node IDs in bin hierarchy |\n| `moveItemToBin()` | Moves project item to target bin |\n| `moveTemporaryScreenshots()` | Moves files between bins |\n| `findTrackItemByNodeId()` | Finds clip in sequence by node ID |\n| `getTimeRegionOfNodeId()` | Returns start/end times of clip by node ID |\n\n#### **N. SILENCE CUTTING SPECIAL LAYERS**\n\n| Function | Purpose |\n|----------|---------|\n| `addSilenceCuttingControlClipToNewTrack()` | Creates transparent video clip for silence detection control |\n| `deleteSilenceCuttingControlClip()` | Removes control clip and temp bin |\n| `getAdjustmentLayer()` | Gets/creates AI Adjustment Layer |\n| `getSilenceCuttingLayer()` | Gets/creates Silence Cutting Layer |\n| `importAdjustmentLayerIntoProject()` | Imports adjustment layer preset |\n| `importSilenceCuttingLayerIntoProject()` | Imports silence cutting control layer |\n| `setAdjusmentLayerFilepath()` | Sets path to adjustment layer file |\n| `setSilenceCuttingLayerFilepath()` | Sets path to silence cutting layer file |\n\n#### **O. CHAPTER MARKERS & METADATA**\n\n| Function | Purpose |\n|----------|---------|\n| `addChapterClip()` | Inserts chapter marker graphic (MOGRT) at time |\n| `addChapterClipWithColours()` | Chapter marker with custom gradient colors |\n| `createChapterMarker()` | Creates metadata marker at timecode |\n| `getChapterMarkers()` | Returns all sequence markers |\n| `deleteChapterMarker()` | Removes marker by GUID |\n| `deleteAllChapterMarks()` | Clears all markers |\n\n#### **P. STOCK FOOTAGE & PREVIEW REPLACEMENT**\n\n| Function | Purpose |\n|----------|---------|\n| `getStockFootagePreviewClipCount()` | Counts preview clips by name pattern |\n| `getStockFootagePreviewClips()` | Gets all preview clips with scale info |\n| `replacePreviewClipWithFullVersion()` | Swaps preview with full resolution footage, maintains scale |\n\n#### **Q. UTILITIES & HELPERS**\n\n| Function | Purpose |\n|----------|---------|\n| `getClipsInTrack()` | Gets clip objects from track |\n| `getClipsInTrack_Timings_Seconds()` | Gets clips with start/end times in seconds |\n| `getClipsInAllTracks_Timings_Seconds()` | Gets clips from all tracks with timings |\n| `getClipsInTrack_Timings_Frames()` | Gets clips with timings in frames |\n| `getClipsInTrack_Timings_Frames_List()` | Frame timings as formatted list |\n| `moveClipByNTracks()` | Moves clip between adjacent tracks |\n| `shiftAllClipsInTrackWithinTimes()` | Moves clips in time range by delta |\n| `breakLines()` | Word-wraps text to max line length |\n| `getProjectItemDimensions()` | Extracts video dimensions from metadata |\n| `sectionContainsVideoClips()` | Checks if time range has clips |\n| `sectionContainsAudioClips()` | Checks if time range has audio |\n| `getMediaPathOfFirstSelection()` | Gets file path of selected clip |\n| `getAllMediaPathsOfSelection()` | Gets paths of all selected clips |\n| `getMediaPathOfFirstObjectInSequence()` | Gets path of first clip in sequence |\n| `openDocument()` | Imports file to project |\n| `get_folder_selection()` | Opens folder browser dialog |\n| `getFilesPaths()` | Multi-file open dialog |\n| `getFilePath()` | Single-file open dialog |\n\n#### **R. SOURCE REPLACEMENT FOR NESTED CAPTIONS**\n\n| Function | Purpose |\n|----------|---------|\n| `changeSourceForUpdateNestedCaptions()` | Updates caption clips to point to new sequence source |\n| `unmuteTrackForNodeId()` | Unmutes track containing clip |\n| `muteCaptionTracks()` | Mutes all tracks except audio |\n\n---\n\n### 3. PANEL-HOST COMMUNICATION MECHANISM\n\n**CSXSEvent System:**\n```javascript\nvar eventObj = new CSXSEvent();\neventObj.type = \"jsxLog\";                      // Message type\neventObj.data = JSON.stringify(arguments);     // Data payload\neventObj.dispatch();                           // Send to panel\n```\n\n**Console Implementation:**\n- Wraps console.log() calls\n- Dispatches events to CEP panel via CSXSEvent\n- Allows HTML panel to receive JSX console output for debugging\n\n**Expected Panel Integration:**\n- Panel listens for CSXSEvent messages of type \"jsxLog\"\n- Panel receives JSON-stringified function arguments\n- Enables real-time debugging of JSX execution\n\n---\n\n### 4. DUAL-API PATTERN: app vs. qe\n\n**Premiere Pro uses two APIs:**\n\n| API | Purpose | Methods Used |\n|-----|---------|--------------|\n| `app` (PPRO) | Standard Premiere Pro API | Stable, full-featured |\n| `qe` (QueryEngine) | Internal/QuickEdit API | Advanced operations, timing |\n\n**Key Differences:**\n- `app.project.activeSequence.videoTracks[i].clips[j]` - Standard API for clips\n- `qe.project.getActiveSequence().getVideoTrackAt(i).getItemAt(j)` - QE API with different indexing\n- Both must be synced when modifying the same clips\n- QE API needed for: razor, addTracks, addTransition, addVideoEffect, etc.\n\n**Critical Pattern:**\nMany functions maintain both `clip_normal` (app API) and `clip_qe` (QE API) references:\n```javascript\nvar clips = findClip(start, trackIndex);\nvar clip_normal = clips.clip_normal;  // For property access\nvar clip_qe = clips.clip_qe;          // For effects/transitions\n```\n\n---\n\n### 5. CORE OPTIMIZATION TECHNIQUES\n\n#### **A. Clip Matching & Tolerance**\n```javascript\nif (Math.abs(clip.start.seconds - start) < time_tolerance) {\n    // 0.1 second tolerance for floating-point comparison\n}\n```\n\n#### **B. Batch Operations**\n- Multiple clips moved in single direction for efficiency\n- Cuts made in batch with `razorSequenceAtFramesArray()`\n- Track mute state captured/restored atomically\n\n#### **C. Directional Iteration**\n```javascript\nif (move_amount > 0) {\n    // Start from end, move backward (prevent cascade)\n    var i = clips.length - 1;\n    var increment = -1;\n} else {\n    // Start from beginning\n    var i = 0;\n    var increment = 1;\n}\n```\n\n#### **D. Property Caching**\n- Stores frame rate, timecode format, track info\n- Avoids repeated API calls in loops\n\n#### **E. Time Quantization**\n- Snaps times to nearest frame for precision\n- Prevents floating-point drift in cuts\n\n---\n\n### 6. UNIQUE TECHNIQUES & INNOVATIONS\n\n#### **A. Transparent Video Control Layer**\n- Creates invisible video layer for silence detection visualization\n- Can be toggled on/off without affecting editing\n- Uses `qe.project.newTransparentVideo()` to create\n\n#### **B. Silence Removal Algorithm**\n- Identifies silence regions by center point\n- Clips overlapping center are removed\n- Remaining clips shifted back with calculated deltas\n- Preserves video continuity without gaps\n\n#### **C. Caption System**\n- Renders word-by-word with individual PNG images\n- Precise timing per word via millisecond data\n- Applies pop/fade/slide animations to each word\n- Supports box background with animation\n- Implements wiggle effect with mathematical frequency/amplitude\n\n#### **D. Zoom Animation with Easing**\n- Generates keyframes at frame-level precision\n- Custom easing curves with configurable power\n- Animates: scale (start->peak->end) + anchor point (center point shift)\n- Asymmetric easing allows different acceleration/deceleration\n\n#### **E. Stock Footage Preview Swap**\n- Maintains aspect ratio and scale during preview->full replacement\n- Preserves all motion properties from preview\n- Supports clip in/out point preservation\n\n#### **F. Adjustment Layer Pattern**\n- Uses AI Adjustment Layer for effects/zooms\n- Allows non-destructive, compositable effects\n- Inserted at specific times only where needed\n\n#### **G. Multilingual Effect Names**\n- Stores 11+ translations for all effects/components\n- Dynamically looks up localized effect names\n- Enables script to work across all Premiere Pro languages\n\n#### **H. Node ID System**\n- Unique identifier for every project item\n- Persists across edits (unlike array indices)\n- Enables safe clip references across operations\n- Used for linking items across time\n\n---\n\n### 7. PERFORMANCE CHARACTERISTICS\n\n**Strengths:**\n- Batch operations (cuts, moves, mutes)\n- Directional iteration prevents re-indexing\n- Tight loops with early-break conditions\n- Caches expensive queries (frame rate, timecode)\n\n**Potential Bottlenecks:**\n- Recursive bin searches (getAllNodeIDs, findItemByName)\n- Full sequence JSON export\n- Caption import with hundreds of PNG frames\n- Double-API sync (app + qe)\n\n---\n\n### 8. ERROR HANDLING & ROBUSTNESS**\n\n**Patterns Observed:**\n- Try-catch for dimension queries (graceful fallback to 1920x1080)\n- Tolerance-based clip matching (±0.1 seconds)\n- Null checks before accessing properties\n- Multiple fallbacks (e.g., transparent video creation paths)\n- Silent failures in effect lookups (returns first available if not found)\n\n---\n\n### 9. COMPLEX WORKFLOWS IMPLEMENTED\n\n#### **Silence Removal Complete Flow:**\n1. Store mute states\n2. Find all clips in detection\n3. Apply razor cuts at silence boundaries\n4. Delete clips in silence regions\n5. Calculate cumulative time deltas\n6. Shift remaining clips by deltas\n7. Restore mute states\n\n#### **Caption Rendering Complete Flow:**\n1. Generate PNG frames per word\n2. Create new video track\n3. Import each PNG individually\n4. Set in/out points for word duration\n5. Apply pop/fade transition\n6. Add wiggle effect with keyframes\n7. Mute caption track\n8. Replace nested sequences for updates\n\n#### **Motion Graphics Complete Flow:**\n1. Insert adjustment layer\n2. Create Transform effect\n3. Generate keyframes at frame intervals\n4. Calculate easing for each frame\n5. Set both scale and anchor point\n6. Support start/end animation asymmetry\n\n---\n\n### 10. KEY CONSTRAINTS & ASSUMPTIONS\n\n1. **Sequence must be active** - Most functions check `app.project.activeSequence`\n2. **Frame rate must be constant** - Uses stored frame rate for all calculations\n3. **QE API requires Enable** - Script starts with `app.enableQE()`\n4. **ExtendScript version** - Uses modern ES5-compatible syntax\n5. **Multilingual lookup required** - Effects/components need translation tables\n6. **Node IDs stable** - Assumes node IDs persist across operations\n7. **Time tolerance** - Uses 0.1 second tolerance for clip matching\n\n---\n\n### SUMMARY\n\nFireside's host JSX is a **sophisticated, production-grade Premiere Pro automation engine** featuring:\n\n- **140+ functions** covering every aspect of timeline editing\n- **Dual-API pattern** (app + qe) for maximum control\n- **Multilingual support** with effect name translation\n- **Advanced animation system** (zoom, easing, keyframes)\n- **Intelligent clip management** (linking, moving, timing)\n- **Professional caption system** (word-by-word, multiple animations)\n- **Robust silence removal** with delta-based repositioning\n- **CEP integration** for panel communication\n\nThe codebase demonstrates deep knowledge of Premiere Pro's API internals and implements production-ready editing automation."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3284,"cache_creation_input_tokens":432,"cache_read_input_tokens":44299,"cache_creation":{"ephemeral_5m_input_tokens":432,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWTqq85p1uJ4pVJLU9RMc","type":"assistant","uuid":"79827eac-00ef-494b-a8ac-1a07f6aa6243","timestamp":"2025-12-25T18:02:03.159Z"}
