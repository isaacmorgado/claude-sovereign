{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","type":"user","message":{"role":"user","content":"Analyze the SPLICE backend for performance bottlenecks. Focus on:\n\n1. **Database queries** - Look for N+1 queries, missing indexes, slow queries in `/Users/imorgado/SPLICE/splice-backend/services/`\n2. **Memory leaks** - Check for unbounded caches, unclosed streams, large object allocations\n3. **Blocking operations** - Find synchronous file I/O, blocking network calls that should be async\n4. **Inefficient algorithms** - Look for O(n²) or worse complexity in loops\n5. **Missing parallelization** - Places where Promise.all could speed things up\n\nCheck these files specifically:\n- splice-backend/services/*.js\n- splice-backend/server.js\n\nReport findings with file:line references and severity (critical/high/medium/low)."},"uuid":"a4189c5b-adf3-444c-8c28-d811e87a2f55","timestamp":"2025-12-26T07:39:53.382Z"}
{"parentUuid":"a4189c5b-adf3-444c-8c28-d811e87a2f55","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_013yxgL34diT385tWNVPggm5","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VRBtezGrXpNK8PbUqRDezG","name":"Glob","input":{"pattern":"splice-backend/services/*.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":24355,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":24355,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CWUvGhX9B8TTWvvayJpYz","type":"assistant","uuid":"084adca7-89f1-4989-9e42-78cf6aa2ae72","timestamp":"2025-12-26T07:39:59.045Z"}
{"parentUuid":"084adca7-89f1-4989-9e42-78cf6aa2ae72","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_013yxgL34diT385tWNVPggm5","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_017Jga9UWqWwY8oGDgRko4y8","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/server.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":3,"cache_creation_input_tokens":24355,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":24355,"ephemeral_1h_input_tokens":0},"output_tokens":5,"service_tier":"standard"}},"requestId":"req_011CWUvGhX9B8TTWvvayJpYz","type":"assistant","uuid":"2c2a0a09-8769-4264-bd65-fb1bff6de25e","timestamp":"2025-12-26T07:39:59.493Z"}
{"parentUuid":"2c2a0a09-8769-4264-bd65-fb1bff6de25e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"File content (35771 tokens) exceeds maximum allowed tokens (25000). Please use offset and limit parameters to read specific portions of the file, or use the GrepTool to search for specific content.","is_error":true,"tool_use_id":"toolu_017Jga9UWqWwY8oGDgRko4y8"}]},"uuid":"97e7cb2c-5dd7-40c4-af86-bfbf44ade2d3","timestamp":"2025-12-26T07:39:59.754Z","toolUseResult":"Error: File content (35771 tokens) exceeds maximum allowed tokens (25000). Please use offset and limit parameters to read specific portions of the file, or use the GrepTool to search for specific content."}
{"parentUuid":"97e7cb2c-5dd7-40c4-af86-bfbf44ade2d3","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VRBtezGrXpNK8PbUqRDezG","type":"tool_result","content":"/Users/imorgado/SPLICE/splice-backend/services/takeDetection.js\n/Users/imorgado/SPLICE/splice-backend/services/silenceDetection.js\n/Users/imorgado/SPLICE/splice-backend/services/xmlProcessor.js\n/Users/imorgado/SPLICE/splice-backend/services/ffprobeSilence.js\n/Users/imorgado/SPLICE/splice-backend/services/licenseService.js\n/Users/imorgado/SPLICE/splice-backend/services/referralService.js\n/Users/imorgado/SPLICE/splice-backend/services/repetitionDetection.js\n/Users/imorgado/SPLICE/splice-backend/services/securityUtils.js\n/Users/imorgado/SPLICE/splice-backend/services/transcription.js\n/Users/imorgado/SPLICE/splice-backend/services/cutListGenerator.js\n/Users/imorgado/SPLICE/splice-backend/services/zoomGenerator.js\n/Users/imorgado/SPLICE/splice-backend/services/vocalIsolation.js\n/Users/imorgado/SPLICE/splice-backend/services/captionExporter.js\n/Users/imorgado/SPLICE/splice-backend/services/profanityDetection.js\n/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js\n/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js\n/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js\n/Users/imorgado/SPLICE/splice-backend/services/chapterDetection.js\n/Users/imorgado/SPLICE/splice-backend/services/youtubeGenerator.js\n/Users/imorgado/SPLICE/splice-backend/services/animatedCaptions.js\n/Users/imorgado/SPLICE/splice-backend/services/textBasedEditing.js\n/Users/imorgado/SPLICE/splice-backend/services/faceDetection.js\n/Users/imorgado/SPLICE/splice-backend/services/socialReframe.js"}]},"uuid":"17c06543-f0ea-4da9-a848-be6b8c2dd9da","timestamp":"2025-12-26T07:39:59.886Z","toolUseResult":{"filenames":["/Users/imorgado/SPLICE/splice-backend/services/takeDetection.js","/Users/imorgado/SPLICE/splice-backend/services/silenceDetection.js","/Users/imorgado/SPLICE/splice-backend/services/xmlProcessor.js","/Users/imorgado/SPLICE/splice-backend/services/ffprobeSilence.js","/Users/imorgado/SPLICE/splice-backend/services/licenseService.js","/Users/imorgado/SPLICE/splice-backend/services/referralService.js","/Users/imorgado/SPLICE/splice-backend/services/repetitionDetection.js","/Users/imorgado/SPLICE/splice-backend/services/securityUtils.js","/Users/imorgado/SPLICE/splice-backend/services/transcription.js","/Users/imorgado/SPLICE/splice-backend/services/cutListGenerator.js","/Users/imorgado/SPLICE/splice-backend/services/zoomGenerator.js","/Users/imorgado/SPLICE/splice-backend/services/vocalIsolation.js","/Users/imorgado/SPLICE/splice-backend/services/captionExporter.js","/Users/imorgado/SPLICE/splice-backend/services/profanityDetection.js","/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js","/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js","/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js","/Users/imorgado/SPLICE/splice-backend/services/chapterDetection.js","/Users/imorgado/SPLICE/splice-backend/services/youtubeGenerator.js","/Users/imorgado/SPLICE/splice-backend/services/animatedCaptions.js","/Users/imorgado/SPLICE/splice-backend/services/textBasedEditing.js","/Users/imorgado/SPLICE/splice-backend/services/faceDetection.js","/Users/imorgado/SPLICE/splice-backend/services/socialReframe.js"],"durationMs":344,"numFiles":23,"truncated":false}}
{"parentUuid":"17c06543-f0ea-4da9-a848-be6b8c2dd9da","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01HLLQRxBzgwTXdM8bdPucts","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_013ndnrS1G6JcqShCqHMXtqN","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/server.js","limit":500}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":73,"cache_creation_input_tokens":669,"cache_read_input_tokens":24355,"cache_creation":{"ephemeral_5m_input_tokens":669,"ephemeral_1h_input_tokens":0},"output_tokens":20,"service_tier":"standard"}},"requestId":"req_011CWUvHAU8S8oD19s8jPTpd","type":"assistant","uuid":"8d7b39b2-043f-4008-8dd0-6d06a42c2310","timestamp":"2025-12-26T07:40:04.523Z"}
{"parentUuid":"8d7b39b2-043f-4008-8dd0-6d06a42c2310","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01HLLQRxBzgwTXdM8bdPucts","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01SruASx4AbdbnrdtVGaxPjD","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":73,"cache_creation_input_tokens":669,"cache_read_input_tokens":24355,"cache_creation":{"ephemeral_5m_input_tokens":669,"ephemeral_1h_input_tokens":0},"output_tokens":20,"service_tier":"standard"}},"requestId":"req_011CWUvHAU8S8oD19s8jPTpd","type":"assistant","uuid":"3c5fe7b6-fbf4-4c18-9a51-a7c03d6647fd","timestamp":"2025-12-26T07:40:05.205Z"}
{"parentUuid":"3c5fe7b6-fbf4-4c18-9a51-a7c03d6647fd","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01HLLQRxBzgwTXdM8bdPucts","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_011ZcU3fPNx5DxNqBEKbeGdu","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/transcription.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":73,"cache_creation_input_tokens":669,"cache_read_input_tokens":24355,"cache_creation":{"ephemeral_5m_input_tokens":669,"ephemeral_1h_input_tokens":0},"output_tokens":20,"service_tier":"standard"}},"requestId":"req_011CWUvHAU8S8oD19s8jPTpd","type":"assistant","uuid":"4acc337b-f783-45cd-9fce-75a7e2f44ed4","timestamp":"2025-12-26T07:40:05.747Z"}
{"parentUuid":"4acc337b-f783-45cd-9fce-75a7e2f44ed4","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01HLLQRxBzgwTXdM8bdPucts","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PRfggnmndDJDcvwqQKqk8C","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":73,"cache_creation_input_tokens":669,"cache_read_input_tokens":24355,"cache_creation":{"ephemeral_5m_input_tokens":669,"ephemeral_1h_input_tokens":0},"output_tokens":256,"service_tier":"standard"}},"requestId":"req_011CWUvHAU8S8oD19s8jPTpd","type":"assistant","uuid":"5479e3ac-6377-4ad3-b2d6-3d45964d8668","timestamp":"2025-12-26T07:40:06.438Z"}
{"parentUuid":"5479e3ac-6377-4ad3-b2d6-3d45964d8668","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_013ndnrS1G6JcqShCqHMXtqN","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Backend Server\n     3→ *\n     4→ * Main entry point for the SPLICE backend API.\n     5→ * Orchestrates the audio analysis pipeline.\n     6→ *\n     7→ * Slices:\n     8→ * - Slice 4: Transcription (services/transcription.js)\n     9→ * - Slice 5: Take Detection (services/takeDetection.js)\n    10→ */\n    11→\n    12→/* global setTimeout, setInterval */\n    13→\n    14→require('dotenv').config();\n    15→\n    16→const express = require('express');\n    17→const cors = require('cors');\n    18→const fs = require('fs');\n    19→const fsPromises = require('fs').promises;\n    20→const https = require('https');\n    21→const http = require('http');\n    22→const path = require('path');\n    23→\n    24→// Async file existence check (non-blocking)\n    25→async function fileExists(filePath) {\n    26→  try {\n    27→    await fsPromises.access(filePath, fs.constants.R_OK);\n    28→    return true;\n    29→  } catch {\n    30→    return false;\n    31→  }\n    32→}\n    33→\n    34→// Check if running in production (Railway injects RAILWAY_ENVIRONMENT)\n    35→const isProduction = process.env.NODE_ENV === 'production' || process.env.RAILWAY_ENVIRONMENT;\n    36→\n    37→// Import slice services\n    38→const { transcribeAudio, transcribeWithWords, transcribeFull } = require('./services/transcription');\n    39→const { detectTakes } = require('./services/takeDetection');\n    40→const { detectSilences } = require('./services/silenceDetection');\n    41→const { detectAudioSilences, isFFprobeInstalled, getAudioDuration } = require('./services/ffprobeSilence');\n    42→const { detectSilencesRMS, sensitivityToParams, getWaveformData } = require('./services/rmsSilenceDetection');\n    43→const {\n    44→  detectProfanity,\n    45→  getProfanityList,\n    46→  getSupportedLanguages,\n    47→  getAvailableBleepSounds,\n    48→  parseWordList,\n    49→  generateBleepsForSegments,\n    50→  cleanupBleepFiles\n    51→} = require('./services/profanityDetection');\n    52→const {\n    53→  detectStutters,\n    54→  detectAllRepetitions\n    55→} = require('./services/repetitionDetection');\n    56→const {\n    57→  analyzeMultitrack,\n    58→  autoBalanceMultitrack,\n    59→  advancedBalanceMultitrack\n    60→} = require('./services/multitrackAnalysis');\n    61→const {\n    62→  toSRT,\n    63→  toVTT,\n    64→  toPlainText,\n    65→  toJSON,\n    66→  exportToFile,\n    67→  getSupportedFormats\n    68→} = require('./services/captionExporter');\n    69→const { processXMLFile } = require('./services/xmlProcessor');\n    70→const { isolateVocals, isReplicateConfigured } = require('./services/vocalIsolation');\n    71→const { generateCutList, generateTakesCutList, validateCutList, alignToFrame, alignToFrameFloor, alignToFrameCeil } = require('./services/cutListGenerator');\n    72→const { generateZoomPoints, addZoomsToCutList, ZOOM_PRESETS, ZOOM_FREQUENCIES } = require('./services/zoomGenerator');\n    73→const { detectChapters, detectChaptersFallback, formatYouTubeTimestamps } = require('./services/chapterDetection');\n    74→\n    75→// Usage tracking and billing\n    76→const usageTracking = require('./services/usageTracking');\n    77→// Rate limiter for usage-based endpoints\n    78→const { requireCredits } = require('./middleware/rateLimiter');\n    79→// Referral system\n    80→const referralService = require('./services/referralService');\n    81→// License key system\n    82→const licenseService = require('./services/licenseService');\n    83→\n    84→// Maximum file size for audio processing (500MB)\n    85→const MAX_FILE_SIZE_BYTES = 500 * 1024 * 1024;\n    86→\n    87→/**\n    88→ * Validate file size to prevent OOM crashes\n    89→ * @param {string} filePath - Path to file\n    90→ * @returns {Promise<{valid: boolean, size?: number, error?: string}>}\n    91→ */\n    92→async function validateFileSize(filePath) {\n    93→  try {\n    94→    const stats = await require('fs').promises.stat(filePath);\n    95→    if (stats.size > MAX_FILE_SIZE_BYTES) {\n    96→      return {\n    97→        valid: false,\n    98→        size: stats.size,\n    99→        error: `File too large (${(stats.size / 1024 / 1024).toFixed(1)}MB). Maximum allowed: ${MAX_FILE_SIZE_BYTES / 1024 / 1024}MB`\n   100→      };\n   101→    }\n   102→    return { valid: true, size: stats.size };\n   103→  } catch (err) {\n   104→    return { valid: false, error: `Cannot access file: ${err.message}` };\n   105→  }\n   106→}\n   107→\n   108→// Stripe for webhooks\n   109→const Stripe = require('stripe');\n   110→const stripe = new Stripe(process.env.STRIPE_SECRET_KEY);\n   111→\n   112→// =============================================================================\n   113→// Server Configuration\n   114→// =============================================================================\n   115→\n   116→const app = express();\n   117→const PORT = process.env.PORT || 3847;\n   118→\n   119→// HTTPS certificates (generated by mkcert) - only for local development\n   120→let httpsOptions = null;\n   121→if (!isProduction) {\n   122→  const keyPath = path.join(__dirname, 'localhost+1-key.pem');\n   123→  const certPath = path.join(__dirname, 'localhost+1.pem');\n   124→  if (fs.existsSync(keyPath) && fs.existsSync(certPath)) {\n   125→    httpsOptions = {\n   126→      key: fs.readFileSync(keyPath),\n   127→      cert: fs.readFileSync(certPath)\n   128→    };\n   129→  }\n   130→}\n   131→\n   132→app.use(cors());\n   133→\n   134→// Helper to determine tier from price ID with logging\n   135→function getTierFromPriceId(priceId) {\n   136→  if (priceId === process.env.STRIPE_PRICE_STARTER) return 'starter';\n   137→  if (priceId === process.env.STRIPE_PRICE_PRO) return 'pro';\n   138→  if (priceId === process.env.STRIPE_PRICE_TEAM) return 'team';\n   139→\n   140→  // Log unknown price ID for debugging\n   141→  console.warn(`[SPLICE] Unknown price ID: ${priceId} - defaulting to starter tier`);\n   142→  return 'starter';\n   143→}\n   144→\n   145→// Stripe webhook needs raw body - must be before express.json()\n   146→app.post('/webhooks/stripe', express.raw({ type: 'application/json' }), async (req, res) => {\n   147→  const sig = req.headers['stripe-signature'];\n   148→  const webhookSecret = process.env.STRIPE_WEBHOOK_SECRET;\n   149→\n   150→  let event;\n   151→\n   152→  try {\n   153→    if (webhookSecret) {\n   154→      event = stripe.webhooks.constructEvent(req.body, sig, webhookSecret);\n   155→    } else if (isProduction) {\n   156→      // SECURITY: Reject unsigned webhooks in production\n   157→      console.error('[SPLICE] CRITICAL: STRIPE_WEBHOOK_SECRET not set in production');\n   158→      return res.status(500).json({ error: 'Webhook configuration error: secret not configured' });\n   159→    } else {\n   160→      // For local development testing only\n   161→      // req.body is a Buffer from express.raw(), convert to string for JSON.parse\n   162→      const bodyString = typeof req.body === 'string' ? req.body : req.body.toString('utf8');\n   163→      event = JSON.parse(bodyString);\n   164→      console.warn('[SPLICE] Warning: Processing webhook without signature verification (dev only)');\n   165→    }\n   166→  } catch (err) {\n   167→    console.error('[SPLICE] Webhook signature verification failed:', err.message);\n   168→    return res.status(400).json({ error: 'Webhook signature verification failed' });\n   169→  }\n   170→\n   171→  console.log(`[SPLICE] Webhook received: ${event.type} (${event.id})`);\n   172→\n   173→  // Idempotency check - skip if already processed\n   174→  if (await usageTracking.isEventProcessed(event.id)) {\n   175→    console.log(`[SPLICE] Event ${event.id} already processed, skipping`);\n   176→    return res.json({ received: true, skipped: true });\n   177→  }\n   178→\n   179→  try {\n   180→    switch (event.type) {\n   181→      case 'customer.subscription.created':\n   182→      case 'customer.subscription.updated': {\n   183→        const subscription = event.data.object;\n   184→        const customerId = subscription.customer;\n   185→\n   186→        // Validate customerId\n   187→        if (!customerId) {\n   188→          console.error('[SPLICE] Missing customer ID in subscription event');\n   189→          return res.status(400).json({ error: 'Missing customer ID' });\n   190→        }\n   191→\n   192→        // Get tier from price ID\n   193→        const priceId = subscription.items?.data?.[0]?.price?.id;\n   194→        const tier = getTierFromPriceId(priceId);\n   195→\n   196→        // Update user tier and reset hours\n   197→        await usageTracking.updateTier(customerId, tier);\n   198→        console.log(`[SPLICE] Updated customer ${customerId} to tier: ${tier}`);\n   199→\n   200→        // Generate license key for new subscriptions with retry and delivery\n   201→        if (event.type === 'customer.subscription.created') {\n   202→          let licenseResult = null;\n   203→          let retryCount = 0;\n   204→          const maxRetries = 3;\n   205→\n   206→          // Retry mechanism for license key generation\n   207→          while (retryCount < maxRetries) {\n   208→            licenseResult = await licenseService.generateLicenseKey(customerId);\n   209→            if (licenseResult.success) {\n   210→              break;\n   211→            }\n   212→            retryCount++;\n   213→            console.warn(`[SPLICE] License key generation attempt ${retryCount}/${maxRetries} failed: ${licenseResult.error}`);\n   214→            // Wait before retry (exponential backoff)\n   215→            if (retryCount < maxRetries) {\n   216→              await new Promise(resolve => setTimeout(resolve, 1000 * retryCount));\n   217→            }\n   218→          }\n   219→\n   220→          if (licenseResult && licenseResult.success) {\n   221→            console.log(`[SPLICE] Generated license key for ${customerId}: ${licenseResult.key}`);\n   222→\n   223→            // Store license key in Stripe subscription metadata as backup\n   224→            try {\n   225→              await stripe.subscriptions.update(subscription.id, {\n   226→                metadata: {\n   227→                  license_key: licenseResult.key,\n   228→                  license_generated_at: new Date().toISOString()\n   229→                }\n   230→              });\n   231→              console.log(`[SPLICE] Stored license key in Stripe metadata for subscription ${subscription.id}`);\n   232→            } catch (metaErr) {\n   233→              console.error(`[SPLICE] Failed to store license key in Stripe metadata:`, metaErr.message);\n   234→            }\n   235→\n   236→            // Get customer email and send license key\n   237→            try {\n   238→              const customer = await stripe.customers.retrieve(customerId);\n   239→              if (customer.email) {\n   240→                // Log email delivery (placeholder for actual email service)\n   241→                console.log(`[SPLICE] License key ready for delivery to ${customer.email}: ${licenseResult.key}`);\n   242→                // TODO: Integrate with email service (SendGrid, SES, etc.)\n   243→                // await sendLicenseKeyEmail(customer.email, licenseResult.key, tier);\n   244→\n   245→                // Store email in database for reference\n   246→                await usageTracking.updateTier(customerId, tier, customer.email);\n   247→              } else {\n   248→                console.warn(`[SPLICE] No email found for customer ${customerId}`);\n   249→              }\n   250→            } catch (emailErr) {\n   251→              console.error(`[SPLICE] Error getting customer email:`, emailErr.message);\n   252→            }\n   253→          } else {\n   254→            console.error(`[SPLICE] Failed to generate license key after ${maxRetries} attempts: ${licenseResult?.error || 'Unknown error'}`);\n   255→          }\n   256→        }\n   257→        break;\n   258→      }\n   259→\n   260→      case 'customer.subscription.deleted': {\n   261→        const subscription = event.data.object;\n   262→        const customerId = subscription.customer;\n   263→\n   264→        // Validate customerId\n   265→        if (!customerId) {\n   266→          console.error('[SPLICE] Missing customer ID in subscription.deleted event');\n   267→          return res.status(400).json({ error: 'Missing customer ID' });\n   268→        }\n   269→\n   270→        // Downgrade to cancelled (0 hours)\n   271→        await usageTracking.updateTier(customerId, 'cancelled');\n   272→        console.log(`[SPLICE] Subscription cancelled for customer ${customerId}`);\n   273→        break;\n   274→      }\n   275→\n   276→      case 'invoice.payment_succeeded': {\n   277→        const invoice = event.data.object;\n   278→        const customerId = invoice.customer;\n   279→        const subscriptionId = invoice.subscription;\n   280→\n   281→        // Validate customerId\n   282→        if (!customerId) {\n   283→          console.error('[SPLICE] Missing customer ID in invoice event');\n   284→          return res.status(400).json({ error: 'Missing customer ID' });\n   285→        }\n   286→\n   287→        // Reset hours on successful payment (new billing period)\n   288→        if (subscriptionId) {\n   289→          const subscription = await stripe.subscriptions.retrieve(subscriptionId);\n   290→          const priceId = subscription.items?.data?.[0]?.price?.id;\n   291→          const tier = getTierFromPriceId(priceId);\n   292→\n   293→          await usageTracking.resetHours(customerId, tier);\n   294→          console.log(`[SPLICE] Reset hours for customer ${customerId} (tier: ${tier})`);\n   295→        }\n   296→        break;\n   297→      }\n   298→\n   299→      default:\n   300→        console.log(`[SPLICE] Unhandled event type: ${event.type}`);\n   301→    }\n   302→\n   303→    // Record event as processed (idempotency)\n   304→    await usageTracking.recordWebhookEvent(event.id, event.type);\n   305→\n   306→    res.json({ received: true });\n   307→  } catch (err) {\n   308→    console.error('[SPLICE] Webhook handler error:', err);\n   309→    res.status(500).json({ error: err.message });\n   310→  }\n   311→});\n   312→\n   313→app.use(express.json());\n   314→\n   315→// =============================================================================\n   316→// Routes\n   317→// =============================================================================\n   318→\n   319→/**\n   320→ * GET / - API information\n   321→ */\n   322→app.get('/', (req, res) => {\n   323→  res.json({\n   324→    service: 'splice-backend',\n   325→    version: '0.3.0',\n   326→    endpoints: {\n   327→      'GET /': 'This info',\n   328→      'GET /health': 'Health check',\n   329→      'GET /ffprobe-check': 'Check if FFprobe is installed',\n   330→      'GET /replicate-check': 'Check if Replicate API is configured',\n   331→      'POST /analyze': 'Analyze WAV file { wavPath }',\n   332→      'POST /silences': 'Detect silences via Whisper gaps { wavPath, threshold: 0.5 }',\n   333→      'POST /silences-audio': 'Detect silences via FFprobe { wavPath, threshold: -30, minDuration: 0.5, padding: 0.1 }',\n   334→      'POST /silences-rms': 'Detect silences via RMS analysis { wavPath, threshold: -30, minSilenceLength: 0.5, paddingStart: 0.1, paddingEnd: 0.05, autoThreshold: false, sensitivity: 50 }',\n   335→      'POST /profanity': 'Detect profanity in transcript { wavPath, language: \"en\", customBlocklist: [], customAllowlist: [] }',\n   336→      'GET /profanity/languages': 'Get supported languages for profanity detection',\n   337→      'GET /profanity/bleeps': 'Get available bleep sounds',\n   338→      'POST /repetitions': 'Detect phrase repetitions and stutters { wavPath, phraseSize: 5, tolerance: 0.7, useOpenAI: false }',\n   339→      'POST /fillers': 'Detect filler words (um, uh, like, etc.) { wavPath, customFillers: [] }',\n   340→      'POST /stutters': 'Detect single-word stutters only { wavPath, minRepeats: 2 }',\n   341→      'POST /export/captions': 'Export transcript to caption format { wavPath, format: srt|vtt|txt|json, outputPath? }',\n   342→      'GET /export/formats': 'Get supported caption export formats',\n   343→      'POST /multitrack': 'Analyze multiple audio tracks for multicam { audioPaths: [], speakerNames: [], videoTrackMapping: {} }',\n   344→      'POST /multitrack/auto-balance': 'Auto-balance speaker screentime { audioPaths: [], speakerNames: [] }',\n   345→      'POST /multitrack/advanced-balance': 'Advanced GA-optimized balancing { audioPaths: [], maxConsecutiveSeconds: 30, momentumFactor: 0.7 }',\n   346→      'POST /process-xml': 'Process FCP XML { xmlPath, silences, removeGaps: true }',\n   347→      'POST /cut-list': 'Generate JSON cut list for DOM building (v3.5) { sourceName, sourcePath, duration, silences, takes?, settings? }',\n   348→      'POST /cut-list/takes': 'Generate cut list keeping only takes { sourceName, sourcePath, duration, takes, settings? }',\n   349→      'POST /zoom': 'Generate zoom points from transcript { transcript, settings: { frequency, preset, placement } }',\n   350→      'GET /zoom/presets': 'Get available zoom presets and frequencies',\n   351→      'POST /chapters': 'Detect chapters in transcript using AI { transcript, settings: { maxChapters, minChapterLength } }',\n   352→      'POST /chapters/fallback': 'Detect chapters without AI (gap-based) { transcript, settings }',\n   353→      'POST /isolate-vocals': 'Isolate vocals from audio { audioPath }',\n   354→      'POST /batch/silences': 'Batch process multiple files for silence detection { files: [], options: {} }',\n   355→      'GET /batch/status/:jobId': 'Get batch job status',\n   356→      'GET /batch/results/:jobId': 'Get full batch job results',\n   357→      'GET /batch/jobs': 'List all batch jobs',\n   358→      'DELETE /batch/:jobId': 'Delete a batch job',\n   359→      'GET /credits': 'Get user credit balance (requires x-stripe-customer-id header)',\n   360→      'GET /usage-history': 'Get usage history (requires x-stripe-customer-id header)',\n   361→      'POST /webhooks/stripe': 'Stripe webhook endpoint',\n   362→      'GET /referral/code': 'Get or create referral code for user',\n   363→      'POST /referral/validate': 'Validate a referral code',\n   364→      'POST /referral/apply': 'Apply referral code at signup',\n   365→      'GET /referral/stats': 'Get referral statistics for user',\n   366→      'POST /license/activate': 'Activate license key { key: \"SPLICE-XXXX-XXXX-XXXX\" }',\n   367→      'GET /license/key': 'Get license key for customer (requires x-stripe-customer-id)',\n   368→      'POST /license/resend': 'Resend license key to customer email { customerId? }'\n   369→    }\n   370→  });\n   371→});\n   372→\n   373→/**\n   374→ * GET /health - Health check\n   375→ */\n   376→app.get('/health', (req, res) => {\n   377→  res.json({ status: 'ok', service: 'splice-backend' });\n   378→});\n   379→\n   380→/**\n   381→ * POST /analyze - Main analysis endpoint\n   382→ *\n   383→ * Pipeline:\n   384→ * 1. Validate input (wavPath)\n   385→ * 2. Slice 4: Transcribe audio with Whisper\n   386→ * 3. Slice 5: Detect takes with GPT-4o-mini\n   387→ * 4. Return combined results\n   388→ */\n   389→app.post('/analyze', requireCredits({ endpoint: 'analyze' }), async (req, res) => {\n   390→  const { wavPath } = req.body;\n   391→\n   392→  // Validate input\n   393→  if (!wavPath) {\n   394→    return res.status(400).json({ error: 'wavPath is required' });\n   395→  }\n   396→\n   397→  if (!(await fileExists(wavPath))) {\n   398→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   399→  }\n   400→\n   401→  console.log(`[SPLICE] Analyzing: ${wavPath}`);\n   402→\n   403→  try {\n   404→    // Slice 4 - GPT-4o-mini transcription\n   405→    const transcript = await transcribeAudio(wavPath);\n   406→\n   407→    // Slice 5 - GPT-4o-mini take detection\n   408→    const takes = await detectTakes(transcript);\n   409→\n   410→    // Deduct usage based on audio duration\n   411→    const audioDuration = transcript.duration || 0;\n   412→    let balance = null;\n   413→    if (audioDuration > 0 && req.deductUsage) {\n   414→      balance = await req.deductUsage(audioDuration);\n   415→    }\n   416→\n   417→    res.json({\n   418→      success: true,\n   419→      wavPath,\n   420→      transcript,\n   421→      takes,\n   422→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   423→    });\n   424→  } catch (err) {\n   425→    console.error('[SPLICE] Error:', err);\n   426→    res.status(500).json({ error: err.message });\n   427→  }\n   428→});\n   429→\n   430→/**\n   431→ * POST /transcribe/word-level - Get frame-aligned word-level timestamps\n   432→ *\n   433→ * Returns word-level timestamps with optional frame alignment for precise editing.\n   434→ * Uses the unified transcription cache (same API call as /analyze).\n   435→ *\n   436→ * Body:\n   437→ * - wavPath: Path to audio file\n   438→ * - frameRate: Optional frame rate for alignment (23.976, 24, 29.97, 30, 60)\n   439→ *\n   440→ * Returns:\n   441→ * - words: Array of {word, start, end, startAligned?, endAligned?}\n   442→ * - text: Full transcript text\n   443→ * - duration: Audio duration in seconds\n   444→ */\n   445→app.post('/transcribe/word-level', requireCredits({ endpoint: 'transcribe-word-level' }), async (req, res) => {\n   446→  const { wavPath, frameRate = 0 } = req.body;\n   447→\n   448→  if (!wavPath) {\n   449→    return res.status(400).json({ error: 'wavPath is required' });\n   450→  }\n   451→\n   452→  if (!(await fileExists(wavPath))) {\n   453→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   454→  }\n   455→\n   456→  console.log(`[SPLICE] Word-level transcription: ${wavPath} (frameRate: ${frameRate || 'none'})`);\n   457→\n   458→  try {\n   459→    // Use unified transcription (gets both segments and words in one API call)\n   460→    const full = await transcribeFull(wavPath);\n   461→\n   462→    // Apply frame alignment if requested\n   463→    let words = full.words || [];\n   464→    const hasFrameAlignment = frameRate > 0;\n   465→\n   466→    if (hasFrameAlignment) {\n   467→      words = words.map(w => ({\n   468→        word: w.word,\n   469→        start: w.start,\n   470→        end: w.end,\n   471→        // Add frame-aligned versions\n   472→        startAligned: parseFloat(alignToFrameFloor(w.start, frameRate).toFixed(6)),\n   473→        endAligned: parseFloat(alignToFrameCeil(w.end, frameRate).toFixed(6))\n   474→      }));\n   475→      console.log(`[SPLICE] Applied ${frameRate}fps frame alignment to ${words.length} words`);\n   476→    }\n   477→\n   478→    // Deduct usage based on audio duration\n   479→    const audioDuration = full.duration || 0;\n   480→    let balance = null;\n   481→    if (audioDuration > 0 && req.deductUsage) {\n   482→      balance = await req.deductUsage(audioDuration);\n   483→    }\n   484→\n   485→    res.json({\n   486→      success: true,\n   487→      wavPath,\n   488→      text: full.text,\n   489→      words,\n   490→      wordCount: words.length,\n   491→      duration: full.duration,\n   492→      language: full.language,\n   493→      frameAligned: hasFrameAlignment,\n   494→      frameRate: hasFrameAlignment ? frameRate : null,\n   495→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   496→    });\n   497→  } catch (err) {\n   498→    console.error('[SPLICE] Word-level transcription error:', err);\n   499→    res.status(500).json({ error: err.message });\n   500→  }\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"08f2385e-697d-4c50-9b16-94a0154500c0","timestamp":"2025-12-26T07:40:06.463Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/server.js","content":"/**\n * SPLICE Backend Server\n *\n * Main entry point for the SPLICE backend API.\n * Orchestrates the audio analysis pipeline.\n *\n * Slices:\n * - Slice 4: Transcription (services/transcription.js)\n * - Slice 5: Take Detection (services/takeDetection.js)\n */\n\n/* global setTimeout, setInterval */\n\nrequire('dotenv').config();\n\nconst express = require('express');\nconst cors = require('cors');\nconst fs = require('fs');\nconst fsPromises = require('fs').promises;\nconst https = require('https');\nconst http = require('http');\nconst path = require('path');\n\n// Async file existence check (non-blocking)\nasync function fileExists(filePath) {\n  try {\n    await fsPromises.access(filePath, fs.constants.R_OK);\n    return true;\n  } catch {\n    return false;\n  }\n}\n\n// Check if running in production (Railway injects RAILWAY_ENVIRONMENT)\nconst isProduction = process.env.NODE_ENV === 'production' || process.env.RAILWAY_ENVIRONMENT;\n\n// Import slice services\nconst { transcribeAudio, transcribeWithWords, transcribeFull } = require('./services/transcription');\nconst { detectTakes } = require('./services/takeDetection');\nconst { detectSilences } = require('./services/silenceDetection');\nconst { detectAudioSilences, isFFprobeInstalled, getAudioDuration } = require('./services/ffprobeSilence');\nconst { detectSilencesRMS, sensitivityToParams, getWaveformData } = require('./services/rmsSilenceDetection');\nconst {\n  detectProfanity,\n  getProfanityList,\n  getSupportedLanguages,\n  getAvailableBleepSounds,\n  parseWordList,\n  generateBleepsForSegments,\n  cleanupBleepFiles\n} = require('./services/profanityDetection');\nconst {\n  detectStutters,\n  detectAllRepetitions\n} = require('./services/repetitionDetection');\nconst {\n  analyzeMultitrack,\n  autoBalanceMultitrack,\n  advancedBalanceMultitrack\n} = require('./services/multitrackAnalysis');\nconst {\n  toSRT,\n  toVTT,\n  toPlainText,\n  toJSON,\n  exportToFile,\n  getSupportedFormats\n} = require('./services/captionExporter');\nconst { processXMLFile } = require('./services/xmlProcessor');\nconst { isolateVocals, isReplicateConfigured } = require('./services/vocalIsolation');\nconst { generateCutList, generateTakesCutList, validateCutList, alignToFrame, alignToFrameFloor, alignToFrameCeil } = require('./services/cutListGenerator');\nconst { generateZoomPoints, addZoomsToCutList, ZOOM_PRESETS, ZOOM_FREQUENCIES } = require('./services/zoomGenerator');\nconst { detectChapters, detectChaptersFallback, formatYouTubeTimestamps } = require('./services/chapterDetection');\n\n// Usage tracking and billing\nconst usageTracking = require('./services/usageTracking');\n// Rate limiter for usage-based endpoints\nconst { requireCredits } = require('./middleware/rateLimiter');\n// Referral system\nconst referralService = require('./services/referralService');\n// License key system\nconst licenseService = require('./services/licenseService');\n\n// Maximum file size for audio processing (500MB)\nconst MAX_FILE_SIZE_BYTES = 500 * 1024 * 1024;\n\n/**\n * Validate file size to prevent OOM crashes\n * @param {string} filePath - Path to file\n * @returns {Promise<{valid: boolean, size?: number, error?: string}>}\n */\nasync function validateFileSize(filePath) {\n  try {\n    const stats = await require('fs').promises.stat(filePath);\n    if (stats.size > MAX_FILE_SIZE_BYTES) {\n      return {\n        valid: false,\n        size: stats.size,\n        error: `File too large (${(stats.size / 1024 / 1024).toFixed(1)}MB). Maximum allowed: ${MAX_FILE_SIZE_BYTES / 1024 / 1024}MB`\n      };\n    }\n    return { valid: true, size: stats.size };\n  } catch (err) {\n    return { valid: false, error: `Cannot access file: ${err.message}` };\n  }\n}\n\n// Stripe for webhooks\nconst Stripe = require('stripe');\nconst stripe = new Stripe(process.env.STRIPE_SECRET_KEY);\n\n// =============================================================================\n// Server Configuration\n// =============================================================================\n\nconst app = express();\nconst PORT = process.env.PORT || 3847;\n\n// HTTPS certificates (generated by mkcert) - only for local development\nlet httpsOptions = null;\nif (!isProduction) {\n  const keyPath = path.join(__dirname, 'localhost+1-key.pem');\n  const certPath = path.join(__dirname, 'localhost+1.pem');\n  if (fs.existsSync(keyPath) && fs.existsSync(certPath)) {\n    httpsOptions = {\n      key: fs.readFileSync(keyPath),\n      cert: fs.readFileSync(certPath)\n    };\n  }\n}\n\napp.use(cors());\n\n// Helper to determine tier from price ID with logging\nfunction getTierFromPriceId(priceId) {\n  if (priceId === process.env.STRIPE_PRICE_STARTER) return 'starter';\n  if (priceId === process.env.STRIPE_PRICE_PRO) return 'pro';\n  if (priceId === process.env.STRIPE_PRICE_TEAM) return 'team';\n\n  // Log unknown price ID for debugging\n  console.warn(`[SPLICE] Unknown price ID: ${priceId} - defaulting to starter tier`);\n  return 'starter';\n}\n\n// Stripe webhook needs raw body - must be before express.json()\napp.post('/webhooks/stripe', express.raw({ type: 'application/json' }), async (req, res) => {\n  const sig = req.headers['stripe-signature'];\n  const webhookSecret = process.env.STRIPE_WEBHOOK_SECRET;\n\n  let event;\n\n  try {\n    if (webhookSecret) {\n      event = stripe.webhooks.constructEvent(req.body, sig, webhookSecret);\n    } else if (isProduction) {\n      // SECURITY: Reject unsigned webhooks in production\n      console.error('[SPLICE] CRITICAL: STRIPE_WEBHOOK_SECRET not set in production');\n      return res.status(500).json({ error: 'Webhook configuration error: secret not configured' });\n    } else {\n      // For local development testing only\n      // req.body is a Buffer from express.raw(), convert to string for JSON.parse\n      const bodyString = typeof req.body === 'string' ? req.body : req.body.toString('utf8');\n      event = JSON.parse(bodyString);\n      console.warn('[SPLICE] Warning: Processing webhook without signature verification (dev only)');\n    }\n  } catch (err) {\n    console.error('[SPLICE] Webhook signature verification failed:', err.message);\n    return res.status(400).json({ error: 'Webhook signature verification failed' });\n  }\n\n  console.log(`[SPLICE] Webhook received: ${event.type} (${event.id})`);\n\n  // Idempotency check - skip if already processed\n  if (await usageTracking.isEventProcessed(event.id)) {\n    console.log(`[SPLICE] Event ${event.id} already processed, skipping`);\n    return res.json({ received: true, skipped: true });\n  }\n\n  try {\n    switch (event.type) {\n      case 'customer.subscription.created':\n      case 'customer.subscription.updated': {\n        const subscription = event.data.object;\n        const customerId = subscription.customer;\n\n        // Validate customerId\n        if (!customerId) {\n          console.error('[SPLICE] Missing customer ID in subscription event');\n          return res.status(400).json({ error: 'Missing customer ID' });\n        }\n\n        // Get tier from price ID\n        const priceId = subscription.items?.data?.[0]?.price?.id;\n        const tier = getTierFromPriceId(priceId);\n\n        // Update user tier and reset hours\n        await usageTracking.updateTier(customerId, tier);\n        console.log(`[SPLICE] Updated customer ${customerId} to tier: ${tier}`);\n\n        // Generate license key for new subscriptions with retry and delivery\n        if (event.type === 'customer.subscription.created') {\n          let licenseResult = null;\n          let retryCount = 0;\n          const maxRetries = 3;\n\n          // Retry mechanism for license key generation\n          while (retryCount < maxRetries) {\n            licenseResult = await licenseService.generateLicenseKey(customerId);\n            if (licenseResult.success) {\n              break;\n            }\n            retryCount++;\n            console.warn(`[SPLICE] License key generation attempt ${retryCount}/${maxRetries} failed: ${licenseResult.error}`);\n            // Wait before retry (exponential backoff)\n            if (retryCount < maxRetries) {\n              await new Promise(resolve => setTimeout(resolve, 1000 * retryCount));\n            }\n          }\n\n          if (licenseResult && licenseResult.success) {\n            console.log(`[SPLICE] Generated license key for ${customerId}: ${licenseResult.key}`);\n\n            // Store license key in Stripe subscription metadata as backup\n            try {\n              await stripe.subscriptions.update(subscription.id, {\n                metadata: {\n                  license_key: licenseResult.key,\n                  license_generated_at: new Date().toISOString()\n                }\n              });\n              console.log(`[SPLICE] Stored license key in Stripe metadata for subscription ${subscription.id}`);\n            } catch (metaErr) {\n              console.error(`[SPLICE] Failed to store license key in Stripe metadata:`, metaErr.message);\n            }\n\n            // Get customer email and send license key\n            try {\n              const customer = await stripe.customers.retrieve(customerId);\n              if (customer.email) {\n                // Log email delivery (placeholder for actual email service)\n                console.log(`[SPLICE] License key ready for delivery to ${customer.email}: ${licenseResult.key}`);\n                // TODO: Integrate with email service (SendGrid, SES, etc.)\n                // await sendLicenseKeyEmail(customer.email, licenseResult.key, tier);\n\n                // Store email in database for reference\n                await usageTracking.updateTier(customerId, tier, customer.email);\n              } else {\n                console.warn(`[SPLICE] No email found for customer ${customerId}`);\n              }\n            } catch (emailErr) {\n              console.error(`[SPLICE] Error getting customer email:`, emailErr.message);\n            }\n          } else {\n            console.error(`[SPLICE] Failed to generate license key after ${maxRetries} attempts: ${licenseResult?.error || 'Unknown error'}`);\n          }\n        }\n        break;\n      }\n\n      case 'customer.subscription.deleted': {\n        const subscription = event.data.object;\n        const customerId = subscription.customer;\n\n        // Validate customerId\n        if (!customerId) {\n          console.error('[SPLICE] Missing customer ID in subscription.deleted event');\n          return res.status(400).json({ error: 'Missing customer ID' });\n        }\n\n        // Downgrade to cancelled (0 hours)\n        await usageTracking.updateTier(customerId, 'cancelled');\n        console.log(`[SPLICE] Subscription cancelled for customer ${customerId}`);\n        break;\n      }\n\n      case 'invoice.payment_succeeded': {\n        const invoice = event.data.object;\n        const customerId = invoice.customer;\n        const subscriptionId = invoice.subscription;\n\n        // Validate customerId\n        if (!customerId) {\n          console.error('[SPLICE] Missing customer ID in invoice event');\n          return res.status(400).json({ error: 'Missing customer ID' });\n        }\n\n        // Reset hours on successful payment (new billing period)\n        if (subscriptionId) {\n          const subscription = await stripe.subscriptions.retrieve(subscriptionId);\n          const priceId = subscription.items?.data?.[0]?.price?.id;\n          const tier = getTierFromPriceId(priceId);\n\n          await usageTracking.resetHours(customerId, tier);\n          console.log(`[SPLICE] Reset hours for customer ${customerId} (tier: ${tier})`);\n        }\n        break;\n      }\n\n      default:\n        console.log(`[SPLICE] Unhandled event type: ${event.type}`);\n    }\n\n    // Record event as processed (idempotency)\n    await usageTracking.recordWebhookEvent(event.id, event.type);\n\n    res.json({ received: true });\n  } catch (err) {\n    console.error('[SPLICE] Webhook handler error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\napp.use(express.json());\n\n// =============================================================================\n// Routes\n// =============================================================================\n\n/**\n * GET / - API information\n */\napp.get('/', (req, res) => {\n  res.json({\n    service: 'splice-backend',\n    version: '0.3.0',\n    endpoints: {\n      'GET /': 'This info',\n      'GET /health': 'Health check',\n      'GET /ffprobe-check': 'Check if FFprobe is installed',\n      'GET /replicate-check': 'Check if Replicate API is configured',\n      'POST /analyze': 'Analyze WAV file { wavPath }',\n      'POST /silences': 'Detect silences via Whisper gaps { wavPath, threshold: 0.5 }',\n      'POST /silences-audio': 'Detect silences via FFprobe { wavPath, threshold: -30, minDuration: 0.5, padding: 0.1 }',\n      'POST /silences-rms': 'Detect silences via RMS analysis { wavPath, threshold: -30, minSilenceLength: 0.5, paddingStart: 0.1, paddingEnd: 0.05, autoThreshold: false, sensitivity: 50 }',\n      'POST /profanity': 'Detect profanity in transcript { wavPath, language: \"en\", customBlocklist: [], customAllowlist: [] }',\n      'GET /profanity/languages': 'Get supported languages for profanity detection',\n      'GET /profanity/bleeps': 'Get available bleep sounds',\n      'POST /repetitions': 'Detect phrase repetitions and stutters { wavPath, phraseSize: 5, tolerance: 0.7, useOpenAI: false }',\n      'POST /fillers': 'Detect filler words (um, uh, like, etc.) { wavPath, customFillers: [] }',\n      'POST /stutters': 'Detect single-word stutters only { wavPath, minRepeats: 2 }',\n      'POST /export/captions': 'Export transcript to caption format { wavPath, format: srt|vtt|txt|json, outputPath? }',\n      'GET /export/formats': 'Get supported caption export formats',\n      'POST /multitrack': 'Analyze multiple audio tracks for multicam { audioPaths: [], speakerNames: [], videoTrackMapping: {} }',\n      'POST /multitrack/auto-balance': 'Auto-balance speaker screentime { audioPaths: [], speakerNames: [] }',\n      'POST /multitrack/advanced-balance': 'Advanced GA-optimized balancing { audioPaths: [], maxConsecutiveSeconds: 30, momentumFactor: 0.7 }',\n      'POST /process-xml': 'Process FCP XML { xmlPath, silences, removeGaps: true }',\n      'POST /cut-list': 'Generate JSON cut list for DOM building (v3.5) { sourceName, sourcePath, duration, silences, takes?, settings? }',\n      'POST /cut-list/takes': 'Generate cut list keeping only takes { sourceName, sourcePath, duration, takes, settings? }',\n      'POST /zoom': 'Generate zoom points from transcript { transcript, settings: { frequency, preset, placement } }',\n      'GET /zoom/presets': 'Get available zoom presets and frequencies',\n      'POST /chapters': 'Detect chapters in transcript using AI { transcript, settings: { maxChapters, minChapterLength } }',\n      'POST /chapters/fallback': 'Detect chapters without AI (gap-based) { transcript, settings }',\n      'POST /isolate-vocals': 'Isolate vocals from audio { audioPath }',\n      'POST /batch/silences': 'Batch process multiple files for silence detection { files: [], options: {} }',\n      'GET /batch/status/:jobId': 'Get batch job status',\n      'GET /batch/results/:jobId': 'Get full batch job results',\n      'GET /batch/jobs': 'List all batch jobs',\n      'DELETE /batch/:jobId': 'Delete a batch job',\n      'GET /credits': 'Get user credit balance (requires x-stripe-customer-id header)',\n      'GET /usage-history': 'Get usage history (requires x-stripe-customer-id header)',\n      'POST /webhooks/stripe': 'Stripe webhook endpoint',\n      'GET /referral/code': 'Get or create referral code for user',\n      'POST /referral/validate': 'Validate a referral code',\n      'POST /referral/apply': 'Apply referral code at signup',\n      'GET /referral/stats': 'Get referral statistics for user',\n      'POST /license/activate': 'Activate license key { key: \"SPLICE-XXXX-XXXX-XXXX\" }',\n      'GET /license/key': 'Get license key for customer (requires x-stripe-customer-id)',\n      'POST /license/resend': 'Resend license key to customer email { customerId? }'\n    }\n  });\n});\n\n/**\n * GET /health - Health check\n */\napp.get('/health', (req, res) => {\n  res.json({ status: 'ok', service: 'splice-backend' });\n});\n\n/**\n * POST /analyze - Main analysis endpoint\n *\n * Pipeline:\n * 1. Validate input (wavPath)\n * 2. Slice 4: Transcribe audio with Whisper\n * 3. Slice 5: Detect takes with GPT-4o-mini\n * 4. Return combined results\n */\napp.post('/analyze', requireCredits({ endpoint: 'analyze' }), async (req, res) => {\n  const { wavPath } = req.body;\n\n  // Validate input\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Analyzing: ${wavPath}`);\n\n  try {\n    // Slice 4 - GPT-4o-mini transcription\n    const transcript = await transcribeAudio(wavPath);\n\n    // Slice 5 - GPT-4o-mini take detection\n    const takes = await detectTakes(transcript);\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      transcript,\n      takes,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /transcribe/word-level - Get frame-aligned word-level timestamps\n *\n * Returns word-level timestamps with optional frame alignment for precise editing.\n * Uses the unified transcription cache (same API call as /analyze).\n *\n * Body:\n * - wavPath: Path to audio file\n * - frameRate: Optional frame rate for alignment (23.976, 24, 29.97, 30, 60)\n *\n * Returns:\n * - words: Array of {word, start, end, startAligned?, endAligned?}\n * - text: Full transcript text\n * - duration: Audio duration in seconds\n */\napp.post('/transcribe/word-level', requireCredits({ endpoint: 'transcribe-word-level' }), async (req, res) => {\n  const { wavPath, frameRate = 0 } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Word-level transcription: ${wavPath} (frameRate: ${frameRate || 'none'})`);\n\n  try {\n    // Use unified transcription (gets both segments and words in one API call)\n    const full = await transcribeFull(wavPath);\n\n    // Apply frame alignment if requested\n    let words = full.words || [];\n    const hasFrameAlignment = frameRate > 0;\n\n    if (hasFrameAlignment) {\n      words = words.map(w => ({\n        word: w.word,\n        start: w.start,\n        end: w.end,\n        // Add frame-aligned versions\n        startAligned: parseFloat(alignToFrameFloor(w.start, frameRate).toFixed(6)),\n        endAligned: parseFloat(alignToFrameCeil(w.end, frameRate).toFixed(6))\n      }));\n      console.log(`[SPLICE] Applied ${frameRate}fps frame alignment to ${words.length} words`);\n    }\n\n    // Deduct usage based on audio duration\n    const audioDuration = full.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      text: full.text,\n      words,\n      wordCount: words.length,\n      duration: full.duration,\n      language: full.language,\n      frameAligned: hasFrameAlignment,\n      frameRate: hasFrameAlignment ? frameRate : null,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Word-level transcription error:', err);\n    res.status(500).json({ error: err.message });\n  }","numLines":500,"startLine":1,"totalLines":3719}}}
{"parentUuid":"08f2385e-697d-4c50-9b16-94a0154500c0","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_011ZcU3fPNx5DxNqBEKbeGdu","type":"tool_result","content":"     1→/**\n     2→ * Slice 4: Whisper Transcription Service\n     3→ *\n     4→ * Handles audio transcription using OpenAI's Whisper model.\n     5→ * Returns timestamped segments required for take detection.\n     6→ * Includes caching to avoid repeated API calls.\n     7→ */\n     8→\n     9→/* global setTimeout */\n    10→\n    11→const fs = require('fs');\n    12→const OpenAI = require('openai');\n    13→\n    14→// Initialize OpenAI client\n    15→const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n    16→\n    17→// In-memory cache: { wavPath: { mtime, result, cachedAt } }\n    18→const transcriptCache = new Map();\n    19→\n    20→// Cache configuration\n    21→const MAX_CACHE_SIZE = 50;\n    22→const CACHE_TTL_MS = 60 * 60 * 1000; // 1 hour TTL\n    23→\n    24→/**\n    25→ * Retry function with exponential backoff\n    26→ * @param {Function} fn - Async function to retry\n    27→ * @param {number} retries - Number of retry attempts (default: 3)\n    28→ * @param {number} baseDelay - Base delay in ms (default: 1000)\n    29→ * @returns {Promise<any>} Result of the function\n    30→ */\n    31→async function withRetry(fn, retries = 3, baseDelay = 1000) {\n    32→  let lastError;\n    33→\n    34→  for (let attempt = 0; attempt <= retries; attempt++) {\n    35→    try {\n    36→      return await fn();\n    37→    } catch (err) {\n    38→      lastError = err;\n    39→\n    40→      // Don't retry on quota exceeded - fail immediately\n    41→      if (err.code === 'insufficient_quota' || err.message?.includes('quota')) {\n    42→        throw new Error('OpenAI API quota exceeded. Please check your billing at https://platform.openai.com/account/billing');\n    43→      }\n    44→\n    45→      // Don't retry on the last attempt\n    46→      if (attempt === retries) {\n    47→        break;\n    48→      }\n    49→\n    50→      // Calculate delay with exponential backoff + jitter\n    51→      const delay = baseDelay * Math.pow(2, attempt) + Math.random() * 500;\n    52→\n    53→      // Only retry on retryable errors\n    54→      const isRetryable =\n    55→        err.status === 429 ||                          // Rate limit\n    56→        err.status === 500 ||                          // Server error\n    57→        err.status === 502 ||                          // Bad gateway\n    58→        err.status === 503 ||                          // Service unavailable\n    59→        err.code === 'ECONNRESET' ||                   // Connection reset\n    60→        err.code === 'ETIMEDOUT' ||                    // Timeout\n    61→        err.message?.includes('Connection error') ||   // Connection error\n    62→        err.message?.includes('network');              // Network error\n    63→\n    64→      if (!isRetryable) {\n    65→        throw err;\n    66→      }\n    67→\n    68→      console.log(`[SPLICE] Retry ${attempt + 1}/${retries} after ${Math.round(delay)}ms: ${err.message}`);\n    69→      await new Promise(resolve => setTimeout(resolve, delay));\n    70→    }\n    71→  }\n    72→\n    73→  throw lastError;\n    74→}\n    75→\n    76→/**\n    77→ * LRU cache set with size limit and TTL\n    78→ * Evicts oldest entries when cache exceeds MAX_CACHE_SIZE\n    79→ * @param {string} key - Cache key\n    80→ * @param {any} value - Value to cache (should include mtime, result)\n    81→ */\n    82→function cacheSet(key, value) {\n    83→  // Clean up expired entries first\n    84→  cleanExpiredEntries();\n    85→\n    86→  // Evict oldest entries if at capacity\n    87→  while (transcriptCache.size >= MAX_CACHE_SIZE) {\n    88→    const oldestKey = transcriptCache.keys().next().value;\n    89→    transcriptCache.delete(oldestKey);\n    90→    console.log(`[SPLICE] Cache eviction: removed ${oldestKey}`);\n    91→  }\n    92→\n    93→  // Add cachedAt timestamp\n    94→  transcriptCache.set(key, {\n    95→    ...value,\n    96→    cachedAt: Date.now()\n    97→  });\n    98→}\n    99→\n   100→/**\n   101→ * Get from cache with TTL check\n   102→ * @param {string} key - Cache key\n   103→ * @returns {object|null} Cached value or null if expired/missing\n   104→ */\n   105→function cacheGet(key) {\n   106→  const entry = transcriptCache.get(key);\n   107→  if (!entry) return null;\n   108→\n   109→  // Check TTL\n   110→  if (Date.now() - entry.cachedAt > CACHE_TTL_MS) {\n   111→    transcriptCache.delete(key);\n   112→    console.log(`[SPLICE] Cache entry expired: ${key}`);\n   113→    return null;\n   114→  }\n   115→\n   116→  return entry;\n   117→}\n   118→\n   119→/**\n   120→ * Clean up expired cache entries\n   121→ * Called periodically and before cache set\n   122→ */\n   123→function cleanExpiredEntries() {\n   124→  const now = Date.now();\n   125→  let cleaned = 0;\n   126→\n   127→  for (const [key, entry] of transcriptCache.entries()) {\n   128→    if (now - entry.cachedAt > CACHE_TTL_MS) {\n   129→      transcriptCache.delete(key);\n   130→      cleaned++;\n   131→    }\n   132→  }\n   133→\n   134→  if (cleaned > 0) {\n   135→    console.log(`[SPLICE] Cleaned ${cleaned} expired cache entries`);\n   136→  }\n   137→}\n   138→\n   139→/**\n   140→ * Unified transcription: calls Whisper ONCE with both word and segment granularities.\n   141→ * All other transcription functions should use this cache.\n   142→ *\n   143→ * @param {string} wavPath - Path to the WAV file\n   144→ * @returns {Promise<{text: string, segments: Array, words: Array, duration: number, language: string}>}\n   145→ */\n   146→async function transcribeFull(wavPath) {\n   147→  // Check cache based on file modification time and TTL\n   148→  const stats = fs.statSync(wavPath);\n   149→  const mtime = stats.mtimeMs;\n   150→  const cacheKey = `full:${wavPath}`;\n   151→\n   152→  const cached = cacheGet(cacheKey);  // Uses TTL-aware getter\n   153→  if (cached && cached.mtime === mtime) {\n   154→    console.log('[SPLICE] Using cached full transcription (file unchanged, cache valid)');\n   155→    return cached.result;\n   156→  }\n   157→\n   158→  console.log('[SPLICE] Starting unified Whisper transcription (words + segments)...');\n   159→\n   160→  // Single Whisper call with BOTH granularities\n   161→  const transcription = await withRetry(async () => {\n   162→    return await openai.audio.transcriptions.create({\n   163→      file: fs.createReadStream(wavPath),\n   164→      model: 'whisper-1',\n   165→      response_format: 'verbose_json',\n   166→      timestamp_granularities: ['word', 'segment'],\n   167→      language: 'en',\n   168→    });\n   169→  });\n   170→\n   171→  console.log(`[SPLICE] Unified transcription complete: ${transcription.segments?.length || 0} segments, ${transcription.words?.length || 0} words`);\n   172→\n   173→  // Build unified result with both segments and words\n   174→  const result = {\n   175→    text: transcription.text,\n   176→    segments: (transcription.segments || []).map(s => ({\n   177→      id: s.id,\n   178→      start: s.start,\n   179→      end: s.end,\n   180→      text: s.text\n   181→    })),\n   182→    words: (transcription.words || []).map(w => ({\n   183→      word: w.word,\n   184→      start: w.start,\n   185→      end: w.end\n   186→    })),\n   187→    language: transcription.language || 'en',\n   188→    duration: transcription.duration || 0\n   189→  };\n   190→\n   191→  // Cache the unified result\n   192→  cacheSet(cacheKey, { mtime, result });\n   193→  console.log(`[SPLICE] Full transcription cached (${result.segments.length} segments, ${result.words.length} words)`);\n   194→\n   195→  return result;\n   196→}\n   197→\n   198→/**\n   199→ * Transcribe audio file using Whisper with segment timestamps (with caching)\n   200→ * Uses unified transcribeFull() cache to avoid duplicate API calls.\n   201→ *\n   202→ * @param {string} wavPath - Path to the WAV file\n   203→ * @returns {Promise<{text: string, segments: Array, language: string, duration: number}>}\n   204→ */\n   205→async function transcribeAudio(wavPath) {\n   206→  // Use unified transcription (gets both segments and words)\n   207→  const full = await transcribeFull(wavPath);\n   208→\n   209→  // Return segment-focused result (matches legacy format)\n   210→  return {\n   211→    text: full.text,\n   212→    segments: full.segments,\n   213→    language: full.language,\n   214→    duration: full.duration\n   215→  };\n   216→}\n   217→\n   218→/**\n   219→ * Transcribe audio with word-level timestamps using Whisper\n   220→ * Uses unified transcribeFull() cache to avoid duplicate API calls.\n   221→ *\n   222→ * @param {string} wavPath - Path to the WAV file\n   223→ * @returns {Promise<{text: string, words: Array<{word: string, start: number, end: number}>}>}\n   224→ */\n   225→async function transcribeWithWords(wavPath) {\n   226→  // Use unified transcription (gets both segments and words)\n   227→  const full = await transcribeFull(wavPath);\n   228→\n   229→  // Return word-focused result (matches legacy format)\n   230→  return {\n   231→    text: full.text,\n   232→    words: full.words,\n   233→    language: full.language,\n   234→    duration: full.duration\n   235→  };\n   236→}\n   237→\n   238→/**\n   239→ * Clear transcription cache\n   240→ * Useful for testing or memory management\n   241→ */\n   242→function clearCache() {\n   243→  const size = transcriptCache.size;\n   244→  transcriptCache.clear();\n   245→  console.log(`[SPLICE] Cleared ${size} cached transcription(s)`);\n   246→  return size;\n   247→}\n   248→\n   249→/**\n   250→ * Get cache statistics\n   251→ */\n   252→function getCacheStats() {\n   253→  const now = Date.now();\n   254→  const entries = [];\n   255→\n   256→  for (const [key, entry] of transcriptCache.entries()) {\n   257→    const ageMs = now - entry.cachedAt;\n   258→    entries.push({\n   259→      key,\n   260→      ageMs,\n   261→      ageMinutes: Math.floor(ageMs / 60000),\n   262→      expiresInMs: Math.max(0, CACHE_TTL_MS - ageMs),\n   263→      expired: ageMs > CACHE_TTL_MS\n   264→    });\n   265→  }\n   266→\n   267→  return {\n   268→    entries: transcriptCache.size,\n   269→    ttlMs: CACHE_TTL_MS,\n   270→    ttlMinutes: CACHE_TTL_MS / 60000,\n   271→    maxSize: MAX_CACHE_SIZE,\n   272→    keys: Array.from(transcriptCache.keys()),\n   273→    details: entries\n   274→  };\n   275→}\n   276→\n   277→module.exports = { transcribeFull, transcribeAudio, transcribeWithWords, clearCache, getCacheStats };\n   278→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"096efdb7-5d3a-452f-a690-8e91ff5413f9","timestamp":"2025-12-26T07:40:06.464Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/transcription.js","content":"/**\n * Slice 4: Whisper Transcription Service\n *\n * Handles audio transcription using OpenAI's Whisper model.\n * Returns timestamped segments required for take detection.\n * Includes caching to avoid repeated API calls.\n */\n\n/* global setTimeout */\n\nconst fs = require('fs');\nconst OpenAI = require('openai');\n\n// Initialize OpenAI client\nconst openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n\n// In-memory cache: { wavPath: { mtime, result, cachedAt } }\nconst transcriptCache = new Map();\n\n// Cache configuration\nconst MAX_CACHE_SIZE = 50;\nconst CACHE_TTL_MS = 60 * 60 * 1000; // 1 hour TTL\n\n/**\n * Retry function with exponential backoff\n * @param {Function} fn - Async function to retry\n * @param {number} retries - Number of retry attempts (default: 3)\n * @param {number} baseDelay - Base delay in ms (default: 1000)\n * @returns {Promise<any>} Result of the function\n */\nasync function withRetry(fn, retries = 3, baseDelay = 1000) {\n  let lastError;\n\n  for (let attempt = 0; attempt <= retries; attempt++) {\n    try {\n      return await fn();\n    } catch (err) {\n      lastError = err;\n\n      // Don't retry on quota exceeded - fail immediately\n      if (err.code === 'insufficient_quota' || err.message?.includes('quota')) {\n        throw new Error('OpenAI API quota exceeded. Please check your billing at https://platform.openai.com/account/billing');\n      }\n\n      // Don't retry on the last attempt\n      if (attempt === retries) {\n        break;\n      }\n\n      // Calculate delay with exponential backoff + jitter\n      const delay = baseDelay * Math.pow(2, attempt) + Math.random() * 500;\n\n      // Only retry on retryable errors\n      const isRetryable =\n        err.status === 429 ||                          // Rate limit\n        err.status === 500 ||                          // Server error\n        err.status === 502 ||                          // Bad gateway\n        err.status === 503 ||                          // Service unavailable\n        err.code === 'ECONNRESET' ||                   // Connection reset\n        err.code === 'ETIMEDOUT' ||                    // Timeout\n        err.message?.includes('Connection error') ||   // Connection error\n        err.message?.includes('network');              // Network error\n\n      if (!isRetryable) {\n        throw err;\n      }\n\n      console.log(`[SPLICE] Retry ${attempt + 1}/${retries} after ${Math.round(delay)}ms: ${err.message}`);\n      await new Promise(resolve => setTimeout(resolve, delay));\n    }\n  }\n\n  throw lastError;\n}\n\n/**\n * LRU cache set with size limit and TTL\n * Evicts oldest entries when cache exceeds MAX_CACHE_SIZE\n * @param {string} key - Cache key\n * @param {any} value - Value to cache (should include mtime, result)\n */\nfunction cacheSet(key, value) {\n  // Clean up expired entries first\n  cleanExpiredEntries();\n\n  // Evict oldest entries if at capacity\n  while (transcriptCache.size >= MAX_CACHE_SIZE) {\n    const oldestKey = transcriptCache.keys().next().value;\n    transcriptCache.delete(oldestKey);\n    console.log(`[SPLICE] Cache eviction: removed ${oldestKey}`);\n  }\n\n  // Add cachedAt timestamp\n  transcriptCache.set(key, {\n    ...value,\n    cachedAt: Date.now()\n  });\n}\n\n/**\n * Get from cache with TTL check\n * @param {string} key - Cache key\n * @returns {object|null} Cached value or null if expired/missing\n */\nfunction cacheGet(key) {\n  const entry = transcriptCache.get(key);\n  if (!entry) return null;\n\n  // Check TTL\n  if (Date.now() - entry.cachedAt > CACHE_TTL_MS) {\n    transcriptCache.delete(key);\n    console.log(`[SPLICE] Cache entry expired: ${key}`);\n    return null;\n  }\n\n  return entry;\n}\n\n/**\n * Clean up expired cache entries\n * Called periodically and before cache set\n */\nfunction cleanExpiredEntries() {\n  const now = Date.now();\n  let cleaned = 0;\n\n  for (const [key, entry] of transcriptCache.entries()) {\n    if (now - entry.cachedAt > CACHE_TTL_MS) {\n      transcriptCache.delete(key);\n      cleaned++;\n    }\n  }\n\n  if (cleaned > 0) {\n    console.log(`[SPLICE] Cleaned ${cleaned} expired cache entries`);\n  }\n}\n\n/**\n * Unified transcription: calls Whisper ONCE with both word and segment granularities.\n * All other transcription functions should use this cache.\n *\n * @param {string} wavPath - Path to the WAV file\n * @returns {Promise<{text: string, segments: Array, words: Array, duration: number, language: string}>}\n */\nasync function transcribeFull(wavPath) {\n  // Check cache based on file modification time and TTL\n  const stats = fs.statSync(wavPath);\n  const mtime = stats.mtimeMs;\n  const cacheKey = `full:${wavPath}`;\n\n  const cached = cacheGet(cacheKey);  // Uses TTL-aware getter\n  if (cached && cached.mtime === mtime) {\n    console.log('[SPLICE] Using cached full transcription (file unchanged, cache valid)');\n    return cached.result;\n  }\n\n  console.log('[SPLICE] Starting unified Whisper transcription (words + segments)...');\n\n  // Single Whisper call with BOTH granularities\n  const transcription = await withRetry(async () => {\n    return await openai.audio.transcriptions.create({\n      file: fs.createReadStream(wavPath),\n      model: 'whisper-1',\n      response_format: 'verbose_json',\n      timestamp_granularities: ['word', 'segment'],\n      language: 'en',\n    });\n  });\n\n  console.log(`[SPLICE] Unified transcription complete: ${transcription.segments?.length || 0} segments, ${transcription.words?.length || 0} words`);\n\n  // Build unified result with both segments and words\n  const result = {\n    text: transcription.text,\n    segments: (transcription.segments || []).map(s => ({\n      id: s.id,\n      start: s.start,\n      end: s.end,\n      text: s.text\n    })),\n    words: (transcription.words || []).map(w => ({\n      word: w.word,\n      start: w.start,\n      end: w.end\n    })),\n    language: transcription.language || 'en',\n    duration: transcription.duration || 0\n  };\n\n  // Cache the unified result\n  cacheSet(cacheKey, { mtime, result });\n  console.log(`[SPLICE] Full transcription cached (${result.segments.length} segments, ${result.words.length} words)`);\n\n  return result;\n}\n\n/**\n * Transcribe audio file using Whisper with segment timestamps (with caching)\n * Uses unified transcribeFull() cache to avoid duplicate API calls.\n *\n * @param {string} wavPath - Path to the WAV file\n * @returns {Promise<{text: string, segments: Array, language: string, duration: number}>}\n */\nasync function transcribeAudio(wavPath) {\n  // Use unified transcription (gets both segments and words)\n  const full = await transcribeFull(wavPath);\n\n  // Return segment-focused result (matches legacy format)\n  return {\n    text: full.text,\n    segments: full.segments,\n    language: full.language,\n    duration: full.duration\n  };\n}\n\n/**\n * Transcribe audio with word-level timestamps using Whisper\n * Uses unified transcribeFull() cache to avoid duplicate API calls.\n *\n * @param {string} wavPath - Path to the WAV file\n * @returns {Promise<{text: string, words: Array<{word: string, start: number, end: number}>}>}\n */\nasync function transcribeWithWords(wavPath) {\n  // Use unified transcription (gets both segments and words)\n  const full = await transcribeFull(wavPath);\n\n  // Return word-focused result (matches legacy format)\n  return {\n    text: full.text,\n    words: full.words,\n    language: full.language,\n    duration: full.duration\n  };\n}\n\n/**\n * Clear transcription cache\n * Useful for testing or memory management\n */\nfunction clearCache() {\n  const size = transcriptCache.size;\n  transcriptCache.clear();\n  console.log(`[SPLICE] Cleared ${size} cached transcription(s)`);\n  return size;\n}\n\n/**\n * Get cache statistics\n */\nfunction getCacheStats() {\n  const now = Date.now();\n  const entries = [];\n\n  for (const [key, entry] of transcriptCache.entries()) {\n    const ageMs = now - entry.cachedAt;\n    entries.push({\n      key,\n      ageMs,\n      ageMinutes: Math.floor(ageMs / 60000),\n      expiresInMs: Math.max(0, CACHE_TTL_MS - ageMs),\n      expired: ageMs > CACHE_TTL_MS\n    });\n  }\n\n  return {\n    entries: transcriptCache.size,\n    ttlMs: CACHE_TTL_MS,\n    ttlMinutes: CACHE_TTL_MS / 60000,\n    maxSize: MAX_CACHE_SIZE,\n    keys: Array.from(transcriptCache.keys()),\n    details: entries\n  };\n}\n\nmodule.exports = { transcribeFull, transcribeAudio, transcribeWithWords, clearCache, getCacheStats };\n","numLines":278,"startLine":1,"totalLines":278}}}
{"parentUuid":"096efdb7-5d3a-452f-a690-8e91ff5413f9","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PRfggnmndDJDcvwqQKqk8C","type":"tool_result","content":"     1→/**\n     2→ * Multitrack/Multicam Analysis Service\n     3→ *\n     4→ * Analyzes multiple audio tracks to determine optimal video angle selection.\n     5→ * Based on Fireside's proven multicam editing approach.\n     6→ *\n     7→ * Features:\n     8→ * - Per-speaker RMS audio level analysis\n     9→ * - Noise floor subtraction for cleaner detection\n    10→ * - Automatic video angle selection based on loudest speaker\n    11→ * - Wide shot detection (multiple speakers talking)\n    12→ * - Cutaway insertion for long clips\n    13→ * - Speaker screentime balancing\n    14→ * - Gaussian smoothing to reduce spurious cuts\n    15→ */\n    16→\n    17→const fsPromises = require('fs').promises;\n    18→const { validateAudioPath, safeFFprobe, safeFFmpeg, safeTempPath } = require('./securityUtils');\n    19→\n    20→// =============================================================================\n    21→// Configuration\n    22→// =============================================================================\n    23→\n    24→const DEFAULT_OPTIONS = {\n    25→  // Analysis parameters\n    26→  analysisWindowMs: 100,       // RMS window size in milliseconds\n    27→  smoothingWindow: 5,          // Gaussian blur kernel size (frames)\n    28→\n    29→  // Switching parameters\n    30→  switchingFrequency: 50,      // How often to allow cuts (0-100)\n    31→  minShotDuration: 2.0,        // Minimum seconds before next cut\n    32→  speakerBoosts: {},           // Per-speaker dB adjustments\n    33→\n    34→  // Wide shot parameters\n    35→  wideShotEnabled: true,\n    36→  wideShotPercentage: 20,      // Target % of time in wide shots\n    37→  wideShotTolerance: 6,        // dB tolerance for multiple speaker detection\n    38→\n    39→  // Cutaway parameters\n    40→  cutawayEnabled: false,\n    41→  cutawayMinDuration: 1.5,\n    42→  cutawayMaxDuration: 4.0,\n    43→  cutawayTracks: [],           // Video track indices for cutaways\n    44→\n    45→  // Output\n    46→  frameRate: 30\n    47→};\n    48→\n    49→// =============================================================================\n    50→// Core Analysis\n    51→// =============================================================================\n    52→\n    53→/**\n    54→ * Analyze multiple audio tracks for multicam editing\n    55→ *\n    56→ * @param {Array<string>} audioPaths - Paths to audio files (one per speaker)\n    57→ * @param {Object} options - Analysis options\n    58→ * @returns {Promise<Object>} Analysis results with switching decisions\n    59→ */\n    60→async function analyzeMultitrack(audioPaths, options = {}) {\n    61→  const opts = { ...DEFAULT_OPTIONS, ...options };\n    62→\n    63→  console.log(`[SPLICE Multitrack] Analyzing ${audioPaths.length} audio track(s)`);\n    64→\n    65→  // Step 1: Extract RMS levels for each track\n    66→  const trackLevels = await Promise.all(\n    67→    audioPaths.map((audioPath, i) =>\n    68→      extractTrackLevels(audioPath, {\n    69→        windowMs: opts.analysisWindowMs,\n    70→        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`\n    71→      })\n    72→    )\n    73→  );\n    74→\n    75→  // Validate all tracks have same duration\n    76→  const durations = trackLevels.map(t => t.duration);\n    77→  const maxDuration = Math.max(...durations);\n    78→  const minDuration = Math.min(...durations);\n    79→  if (maxDuration - minDuration > 1) {\n    80→    console.warn('[SPLICE Multitrack] Track durations differ significantly');\n    81→  }\n    82→\n    83→  // Step 2: Calculate noise floor and clean up levels\n    84→  const cleanedLevels = cleanupAudioLevels(trackLevels, opts);\n    85→\n    86→  // Step 3: Apply speaker boosts\n    87→  const boostedLevels = applySpeakerBoosts(cleanedLevels, opts.speakerBoosts);\n    88→\n    89→  // Step 4: Apply Gaussian smoothing\n    90→  const smoothedLevels = opts.smoothingWindow > 0\n    91→    ? smoothAudioLevels(boostedLevels, opts.smoothingWindow)\n    92→    : boostedLevels;\n    93→\n    94→  // Step 5: Generate switching decisions\n    95→  const decisions = generateSwitchingDecisions(smoothedLevels, {\n    96→    videoTrackMapping: opts.videoTrackMapping,\n    97→    minShotDuration: opts.minShotDuration,\n    98→    switchingFrequency: opts.switchingFrequency,\n    99→    wideShotEnabled: opts.wideShotEnabled,\n   100→    wideShotTolerance: opts.wideShotTolerance,\n   101→    wideShotTracks: opts.wideShotTracks,\n   102→    frameRate: opts.frameRate\n   103→  });\n   104→\n   105→  // Step 6: Fine-tune wide shot percentage\n   106→  const tunedDecisions = opts.wideShotEnabled\n   107→    ? tuneWideShotPercentage(decisions, opts.wideShotPercentage)\n   108→    : decisions;\n   109→\n   110→  // Step 7: Insert cutaways if enabled\n   111→  const finalDecisions = opts.cutawayEnabled\n   112→    ? insertCutaways(tunedDecisions, {\n   113→        cutawayTracks: opts.cutawayTracks,\n   114→        minDuration: opts.cutawayMinDuration,\n   115→        maxDuration: opts.cutawayMaxDuration\n   116→      })\n   117→    : tunedDecisions;\n   118→\n   119→  // Step 8: Calculate statistics\n   120→  const stats = calculateMultitrackStats(finalDecisions, trackLevels[0].duration);\n   121→\n   122→  console.log(`[SPLICE Multitrack] Generated ${finalDecisions.length} switching decision(s)`);\n   123→\n   124→  return {\n   125→    decisions: finalDecisions,\n   126→    metadata: {\n   127→      trackCount: audioPaths.length,\n   128→      speakerNames: opts.speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n   129→      duration: maxDuration,\n   130→      frameRate: opts.frameRate,\n   131→      ...stats\n   132→    },\n   133→    levels: {\n   134→      raw: trackLevels.map(t => ({ speaker: t.speakerName, sampleCount: t.levels.length })),\n   135→      // Don't include full level data in response (too large)\n   136→    }\n   137→  };\n   138→}\n   139→\n   140→/**\n   141→ * Analyze multitrack using pre-extracted levels (PERF-008)\n   142→ * Skips FFmpeg processing for faster iteration during auto-balance\n   143→ *\n   144→ * @param {Array<Object>} cachedLevels - Pre-extracted track levels from extractTrackLevels\n   145→ * @param {Object} options - Analysis options\n   146→ * @returns {Object} Analysis results with switching decisions\n   147→ */\n   148→function analyzeMultitrackFromLevels(cachedLevels, options = {}) {\n   149→  const opts = { ...DEFAULT_OPTIONS, ...options };\n   150→\n   151→  // Validate all tracks have same duration\n   152→  const durations = cachedLevels.map(t => t.duration);\n   153→  const maxDuration = Math.max(...durations);\n   154→  const minDuration = Math.min(...durations);\n   155→  if (maxDuration - minDuration > 1) {\n   156→    console.warn('[SPLICE Multitrack] Track durations differ significantly');\n   157→  }\n   158→\n   159→  // Step 2: Calculate noise floor and clean up levels\n   160→  const cleanedLevels = cleanupAudioLevels(cachedLevels, opts);\n   161→\n   162→  // Step 3: Apply speaker boosts\n   163→  const boostedLevels = applySpeakerBoosts(cleanedLevels, opts.speakerBoosts);\n   164→\n   165→  // Step 4: Apply Gaussian smoothing\n   166→  const smoothedLevels = opts.smoothingWindow > 0\n   167→    ? smoothAudioLevels(boostedLevels, opts.smoothingWindow)\n   168→    : boostedLevels;\n   169→\n   170→  // Step 5: Generate switching decisions\n   171→  const decisions = generateSwitchingDecisions(smoothedLevels, {\n   172→    videoTrackMapping: opts.videoTrackMapping,\n   173→    minShotDuration: opts.minShotDuration,\n   174→    switchingFrequency: opts.switchingFrequency,\n   175→    wideShotEnabled: opts.wideShotEnabled,\n   176→    wideShotTolerance: opts.wideShotTolerance,\n   177→    wideShotTracks: opts.wideShotTracks,\n   178→    frameRate: opts.frameRate\n   179→  });\n   180→\n   181→  // Step 6: Fine-tune wide shot percentage\n   182→  const tunedDecisions = opts.wideShotEnabled\n   183→    ? tuneWideShotPercentage(decisions, opts.wideShotPercentage)\n   184→    : decisions;\n   185→\n   186→  // Step 7: Insert cutaways if enabled\n   187→  const finalDecisions = opts.cutawayEnabled\n   188→    ? insertCutaways(tunedDecisions, {\n   189→        cutawayTracks: opts.cutawayTracks,\n   190→        minDuration: opts.cutawayMinDuration,\n   191→        maxDuration: opts.cutawayMaxDuration\n   192→      })\n   193→    : tunedDecisions;\n   194→\n   195→  // Step 8: Calculate statistics\n   196→  const stats = calculateMultitrackStats(finalDecisions, cachedLevels[0].duration);\n   197→\n   198→  return {\n   199→    decisions: finalDecisions,\n   200→    metadata: {\n   201→      trackCount: cachedLevels.length,\n   202→      speakerNames: opts.speakerNames || cachedLevels.map((_, i) => `Speaker ${i + 1}`),\n   203→      duration: maxDuration,\n   204→      frameRate: opts.frameRate,\n   205→      ...stats\n   206→    },\n   207→    levels: {\n   208→      raw: cachedLevels.map(t => ({ speaker: t.speakerName, sampleCount: t.levels.length })),\n   209→    }\n   210→  };\n   211→}\n   212→\n   213→// =============================================================================\n   214→// Audio Level Extraction\n   215→// =============================================================================\n   216→\n   217→/**\n   218→ * Extract RMS levels from audio file\n   219→ *\n   220→ * @param {string} audioPath - Path to audio file\n   221→ * @param {Object} options - Extraction options\n   222→ * @returns {Promise<Object>} Track levels with timing\n   223→ */\n   224→async function extractTrackLevels(audioPath, options = {}) {\n   225→  const { windowMs = 100, speakerName = 'Unknown' } = options;\n   226→\n   227→  // SECURITY: Validate input path to prevent path traversal\n   228→  const pathValidation = await validateAudioPath(audioPath);\n   229→  if (!pathValidation.valid) {\n   230→    throw new Error(`Invalid audio path: ${pathValidation.error}`);\n   231→  }\n   232→  const validatedPath = pathValidation.path;\n   233→\n   234→  console.log(`[SPLICE Multitrack] Extracting levels: ${speakerName}`);\n   235→\n   236→  // SECURITY: Use execFile with array arguments to prevent command injection\n   237→  const infoArgs = [\n   238→    '-v', 'error',\n   239→    '-select_streams', 'a:0',\n   240→    '-show_entries', 'stream=sample_rate,duration',\n   241→    '-of', 'json',\n   242→    validatedPath\n   243→  ];\n   244→  let duration = 0;\n   245→\n   246→  try {\n   247→    const { stdout } = await safeFFprobe(infoArgs);\n   248→    const info = JSON.parse(stdout);\n   249→    if (info.streams?.[0]) {\n   250→      // Note: sample_rate available in info.streams[0].sample_rate for future resampling support\n   251→      duration = parseFloat(info.streams[0].duration) || 0;\n   252→    }\n   253→  } catch (err) {\n   254→    console.warn(`[SPLICE Multitrack] Could not get audio info: ${err.message}`);\n   255→  }\n   256→\n   257→  // Get duration from format if not in stream\n   258→  if (!duration) {\n   259→    try {\n   260→      const durationArgs = [\n   261→        '-v', 'error',\n   262→        '-show_entries', 'format=duration',\n   263→        '-of', 'default=noprint_wrappers=1:nokey=1',\n   264→        validatedPath\n   265→      ];\n   266→      const { stdout } = await safeFFprobe(durationArgs);\n   267→      duration = parseFloat(stdout.trim()) || 0;\n   268→    } catch (err) {\n   269→      console.warn(`[SPLICE Multitrack] Could not get duration: ${err.message}`);\n   270→    }\n   271→  }\n   272→\n   273→  // SECURITY: Use safe temp path generation\n   274→  const tempFile = safeTempPath('splice_multitrack', '.raw');\n   275→\n   276→  try {\n   277→    // SECURITY: Use execFile with array arguments to prevent command injection\n   278→    const extractArgs = [\n   279→      '-y',\n   280→      '-i', validatedPath,\n   281→      '-ac', '1',\n   282→      '-ar', '16000',\n   283→      '-f', 's16le',\n   284→      '-acodec', 'pcm_s16le',\n   285→      tempFile\n   286→    ];\n   287→    await safeFFmpeg(extractArgs);\n   288→\n   289→    // PERFORMANCE: Use async file read to avoid blocking event loop\n   290→    const rawData = await fsPromises.readFile(tempFile);\n   291→    const samples = new Float32Array(rawData.length / 2);\n   292→    for (let i = 0; i < samples.length; i++) {\n   293→      samples[i] = rawData.readInt16LE(i * 2) / 32768.0;\n   294→    }\n   295→\n   296→    // Calculate RMS per window\n   297→    const windowSamples = Math.floor((windowMs / 1000) * 16000);\n   298→    const hopSamples = windowSamples; // Non-overlapping windows\n   299→    const numWindows = Math.floor(samples.length / hopSamples);\n   300→\n   301→    const levels = new Float32Array(numWindows);\n   302→    const times = new Float32Array(numWindows);\n   303→\n   304→    for (let i = 0; i < numWindows; i++) {\n   305→      const start = i * hopSamples;\n   306→      const end = Math.min(start + windowSamples, samples.length);\n   307→\n   308→      let sumSquares = 0;\n   309→      for (let j = start; j < end; j++) {\n   310→        sumSquares += samples[j] * samples[j];\n   311→      }\n   312→\n   313→      const rms = Math.sqrt(sumSquares / (end - start));\n   314→      const dbfs = rms > 0 ? 20 * Math.log10(rms) : -100;\n   315→\n   316→      levels[i] = Math.max(-100, dbfs);\n   317→      times[i] = (start / 16000);\n   318→    }\n   319→\n   320→    // PERFORMANCE: Use async file operations\n   321→    await fsPromises.unlink(tempFile);\n   322→\n   323→    return { levels, times, duration, speakerName, sampleRate: 16000 };\n   324→  } catch (err) {\n   325→    // PERFORMANCE: Use async file operations for cleanup\n   326→    try {\n   327→      await fsPromises.access(tempFile);\n   328→      await fsPromises.unlink(tempFile);\n   329→    } catch {\n   330→      // File doesn't exist\n   331→    }\n   332→    throw new Error(`Failed to extract track levels: ${err.message}`);\n   333→  }\n   334→}\n   335→\n   336→// =============================================================================\n   337→// Level Processing\n   338→// =============================================================================\n   339→\n   340→/**\n   341→ * Clean up audio levels by subtracting noise floor\n   342→ *\n   343→ * @param {Array} trackLevels - Array of track level objects\n   344→ * @param {Object} options - Cleanup options\n   345→ * @returns {Array} Cleaned level arrays\n   346→ */\n   347→function cleanupAudioLevels(trackLevels, _options = {}) {\n   348→  const numTracks = trackLevels.length;\n   349→  const numSamples = trackLevels[0].levels.length;\n   350→\n   351→  // Calculate average level (noise floor estimate)\n   352→  const averageLevels = new Float32Array(numSamples);\n   353→  for (let i = 0; i < numSamples; i++) {\n   354→    let sum = 0;\n   355→    for (let t = 0; t < numTracks; t++) {\n   356→      sum += trackLevels[t].levels[i];\n   357→    }\n   358→    averageLevels[i] = sum / numTracks;\n   359→  }\n   360→\n   361→  // Subtract average from each track (relative loudness)\n   362→  const cleaned = trackLevels.map(track => {\n   363→    const cleanedLevels = new Float32Array(numSamples);\n   364→    for (let i = 0; i < numSamples; i++) {\n   365→      // Normalize to -100 to 0 range\n   366→      cleanedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] - averageLevels[i]));\n   367→    }\n   368→    return {\n   369→      ...track,\n   370→      levels: cleanedLevels\n   371→    };\n   372→  });\n   373→\n   374→  return cleaned;\n   375→}\n   376→\n   377→/**\n   378→ * Apply per-speaker dB boosts\n   379→ *\n   380→ * @param {Array} trackLevels - Cleaned track levels\n   381→ * @param {Object} boosts - Speaker boost mapping { \"Speaker 1\": 5, ... }\n   382→ * @returns {Array} Boosted levels\n   383→ */\n   384→function applySpeakerBoosts(trackLevels, boosts = {}) {\n   385→  return trackLevels.map(track => {\n   386→    const boost = boosts[track.speakerName] || 0;\n   387→    if (boost === 0) return track;\n   388→\n   389→    const boostedLevels = new Float32Array(track.levels.length);\n   390→    for (let i = 0; i < track.levels.length; i++) {\n   391→      boostedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] + boost));\n   392→    }\n   393→\n   394→    return { ...track, levels: boostedLevels };\n   395→  });\n   396→}\n   397→\n   398→/**\n   399→ * Apply Gaussian smoothing to reduce noise\n   400→ *\n   401→ * @param {Array} trackLevels - Track levels\n   402→ * @param {number} windowSize - Gaussian kernel size\n   403→ * @returns {Array} Smoothed levels\n   404→ */\n   405→function smoothAudioLevels(trackLevels, windowSize = 5) {\n   406→  const kernel = generateGaussianKernel(windowSize);\n   407→\n   408→  return trackLevels.map(track => {\n   409→    const smoothed = applyConvolution(track.levels, kernel);\n   410→    return { ...track, levels: smoothed };\n   411→  });\n   412→}\n   413→\n   414→/**\n   415→ * Generate Gaussian kernel\n   416→ */\n   417→function generateGaussianKernel(size) {\n   418→  const sigma = size / 3;\n   419→  const kernel = new Float32Array(size);\n   420→  const half = Math.floor(size / 2);\n   421→  let sum = 0;\n   422→\n   423→  for (let i = 0; i < size; i++) {\n   424→    const x = i - half;\n   425→    kernel[i] = Math.exp(-(x * x) / (2 * sigma * sigma));\n   426→    sum += kernel[i];\n   427→  }\n   428→\n   429→  // Normalize\n   430→  for (let i = 0; i < size; i++) {\n   431→    kernel[i] /= sum;\n   432→  }\n   433→\n   434→  return kernel;\n   435→}\n   436→\n   437→/**\n   438→ * Apply 1D convolution\n   439→ */\n   440→function applyConvolution(data, kernel) {\n   441→  const result = new Float32Array(data.length);\n   442→  const half = Math.floor(kernel.length / 2);\n   443→\n   444→  for (let i = 0; i < data.length; i++) {\n   445→    let sum = 0;\n   446→    for (let j = 0; j < kernel.length; j++) {\n   447→      const idx = i + j - half;\n   448→      if (idx >= 0 && idx < data.length) {\n   449→        sum += data[idx] * kernel[j];\n   450→      }\n   451→    }\n   452→    result[i] = sum;\n   453→  }\n   454→\n   455→  return result;\n   456→}\n   457→\n   458→// =============================================================================\n   459→// Switching Decision Generation\n   460→// =============================================================================\n   461→\n   462→/**\n   463→ * Generate video switching decisions based on audio levels\n   464→ *\n   465→ * @param {Array} trackLevels - Processed track levels\n   466→ * @param {Object} options - Decision options\n   467→ * @returns {Array} Switching decisions\n   468→ */\n   469→function generateSwitchingDecisions(trackLevels, options = {}) {\n   470→  const {\n   471→    videoTrackMapping = {},\n   472→    minShotDuration = 2.0,\n   473→    switchingFrequency = 50,\n   474→    wideShotEnabled = true,\n   475→    wideShotTolerance = 6,\n   476→    wideShotTracks = []\n   477→    // frameRate reserved for future frame-aligned cutting\n   478→  } = options;\n   479→\n   480→  const numSamples = trackLevels[0].levels.length;\n   481→  const times = trackLevels[0].times;\n   482→  const decisions = [];\n   483→\n   484→  // Calculate min samples between cuts based on switching frequency\n   485→  const minSamplesPerCut = Math.ceil(minShotDuration / (times[1] - times[0]));\n   486→  const switchingCooldown = Math.ceil(minSamplesPerCut * (1 - switchingFrequency / 100));\n   487→\n   488→  let currentSpeaker = -1;\n   489→  // currentDecisionStart reserved for future decision grouping\n   490→  let lastSwitchSample = 0;\n   491→\n   492→  for (let i = 0; i < numSamples; i++) {\n   493→    // Find loudest speaker\n   494→    let maxLevel = -100;\n   495→    let loudestSpeaker = 0;\n   496→    let speakersAboveThreshold = [];\n   497→\n   498→    for (let t = 0; t < trackLevels.length; t++) {\n   499→      const level = trackLevels[t].levels[i];\n   500→      if (level > maxLevel) {\n   501→        maxLevel = level;\n   502→        loudestSpeaker = t;\n   503→      }\n   504→\n   505→      // Track speakers within tolerance of max (for wide shot detection)\n   506→      if (wideShotEnabled && level >= maxLevel - wideShotTolerance) {\n   507→        speakersAboveThreshold.push(t);\n   508→      }\n   509→    }\n   510→\n   511→    // Determine if this should be a wide shot\n   512→    const isWideShot = wideShotEnabled && speakersAboveThreshold.length >= 2;\n   513→    const effectiveSpeaker = isWideShot ? -1 : loudestSpeaker; // -1 = wide shot\n   514→\n   515→    // Check if we should switch\n   516→    const canSwitch = (i - lastSwitchSample) >= switchingCooldown;\n   517→    const shouldSwitch = canSwitch && effectiveSpeaker !== currentSpeaker;\n   518→\n   519→    if (shouldSwitch || i === 0) {\n   520→      // Close previous decision\n   521→      if (i > 0 && decisions.length > 0) {\n   522→        decisions[decisions.length - 1].endTime = times[i];\n   523→        decisions[decisions.length - 1].endSample = i;\n   524→      }\n   525→\n   526→      // Start new decision\n   527→      const speakerName = isWideShot\n   528→        ? 'Wide Shot'\n   529→        : trackLevels[loudestSpeaker].speakerName;\n   530→\n   531→      const videoTrack = isWideShot\n   532→        ? (wideShotTracks[0] ?? 0)\n   533→        : (videoTrackMapping[loudestSpeaker] ?? loudestSpeaker);\n   534→\n   535→      decisions.push({\n   536→        speakerIndex: effectiveSpeaker,\n   537→        speakerName,\n   538→        speakers: speakersAboveThreshold.map(s => trackLevels[s].speakerName),\n   539→        videoTrack,\n   540→        startTime: times[i],\n   541→        startSample: i,\n   542→        endTime: times[numSamples - 1],\n   543→        endSample: numSamples - 1,\n   544→        isWideShot,\n   545→        reason: isWideShot ? 'Multiple speakers' : 'Loudest speaker'\n   546→      });\n   547→\n   548→      currentSpeaker = effectiveSpeaker;\n   549→      lastSwitchSample = i;\n   550→    }\n   551→  }\n   552→\n   553→  // Finalize last decision\n   554→  if (decisions.length > 0) {\n   555→    decisions[decisions.length - 1].endTime = times[numSamples - 1];\n   556→    decisions[decisions.length - 1].endSample = numSamples - 1;\n   557→  }\n   558→\n   559→  // Add duration to each decision\n   560→  return decisions.map(d => ({\n   561→    ...d,\n   562→    duration: parseFloat((d.endTime - d.startTime).toFixed(3))\n   563→  }));\n   564→}\n   565→\n   566→// =============================================================================\n   567→// Wide Shot Tuning\n   568→// =============================================================================\n   569→\n   570→/**\n   571→ * Fine-tune wide shot percentage using tolerance adjustment\n   572→ *\n   573→ * @param {Array} decisions - Initial decisions\n   574→ * @param {number} targetPercentage - Target wide shot percentage\n   575→ * @returns {Array} Tuned decisions\n   576→ */\n   577→function tuneWideShotPercentage(decisions, targetPercentage) {\n   578→  const totalDuration = decisions.reduce((sum, d) => sum + d.duration, 0);\n   579→  const currentWideTime = decisions\n   580→    .filter(d => d.isWideShot)\n   581→    .reduce((sum, d) => sum + d.duration, 0);\n   582→  const currentPercentage = (currentWideTime / totalDuration) * 100;\n   583→\n   584→  // If within 5% of target, don't adjust\n   585→  if (Math.abs(currentPercentage - targetPercentage) <= 5) {\n   586→    return decisions;\n   587→  }\n   588→\n   589→  console.log(`[SPLICE Multitrack] Wide shot: ${currentPercentage.toFixed(1)}% (target: ${targetPercentage}%)`);\n   590→\n   591→  // Simple approach: mark some single-speaker decisions as wide shots\n   592→  // or vice versa based on whether we need more or fewer wide shots\n   593→  // (Full implementation would re-run with adjusted tolerance)\n   594→\n   595→  return decisions; // Return as-is for now\n   596→}\n   597→\n   598→// =============================================================================\n   599→// Cutaway Insertion\n   600→// =============================================================================\n   601→\n   602→/**\n   603→ * Insert cutaways into long clips\n   604→ *\n   605→ * @param {Array} decisions - Switching decisions\n   606→ * @param {Object} options - Cutaway options\n   607→ * @returns {Array} Decisions with cutaways inserted\n   608→ */\n   609→function insertCutaways(decisions, options = {}) {\n   610→  const {\n   611→    cutawayTracks = [],\n   612→    minDuration = 1.5,\n   613→    maxDuration = 4.0,\n   614→    maxClipDuration = 30 // Insert cutaway if clip longer than this\n   615→  } = options;\n   616→\n   617→  if (cutawayTracks.length === 0) return decisions;\n   618→\n   619→  const result = [];\n   620→\n   621→  for (const decision of decisions) {\n   622→    if (decision.duration <= maxClipDuration) {\n   623→      result.push(decision);\n   624→      continue;\n   625→    }\n   626→\n   627→    // Split long clip with cutaways\n   628→    const numCutaways = Math.floor(decision.duration / maxClipDuration);\n   629→    const interval = decision.duration / (numCutaways + 1);\n   630→\n   631→    let currentStart = decision.startTime;\n   632→\n   633→    for (let i = 0; i < numCutaways; i++) {\n   634→      // Add main clip segment\n   635→      const cutawayStart = currentStart + interval - minDuration / 2;\n   636→      const cutawayDuration = minDuration + Math.random() * (maxDuration - minDuration);\n   637→      const cutawayEnd = cutawayStart + cutawayDuration;\n   638→\n   639→      result.push({\n   640→        ...decision,\n   641→        startTime: currentStart,\n   642→        endTime: cutawayStart,\n   643→        duration: cutawayStart - currentStart\n   644→      });\n   645→\n   646→      // Add cutaway\n   647→      const cutawayTrack = cutawayTracks[i % cutawayTracks.length];\n   648→      result.push({\n   649→        speakerIndex: -2, // Special index for cutaway\n   650→        speakerName: 'Cutaway',\n   651→        speakers: [],\n   652→        videoTrack: cutawayTrack,\n   653→        startTime: cutawayStart,\n   654→        endTime: cutawayEnd,\n   655→        duration: cutawayDuration,\n   656→        isWideShot: false,\n   657→        isCutaway: true,\n   658→        reason: 'Cutaway insert'\n   659→      });\n   660→\n   661→      currentStart = cutawayEnd;\n   662→    }\n   663→\n   664→    // Add final segment\n   665→    result.push({\n   666→      ...decision,\n   667→      startTime: currentStart,\n   668→      endTime: decision.endTime,\n   669→      duration: decision.endTime - currentStart\n   670→    });\n   671→  }\n   672→\n   673→  return result;\n   674→}\n   675→\n   676→// =============================================================================\n   677→// Statistics\n   678→// =============================================================================\n   679→\n   680→/**\n   681→ * Calculate multitrack analysis statistics\n   682→ *\n   683→ * @param {Array} decisions - Final decisions\n   684→ * @param {number} totalDuration - Total audio duration\n   685→ * @returns {Object} Statistics\n   686→ */\n   687→function calculateMultitrackStats(decisions, totalDuration) {\n   688→  const speakerTimes = {};\n   689→  let wideShotTime = 0;\n   690→  let cutawayTime = 0;\n   691→\n   692→  for (const d of decisions) {\n   693→    if (d.isCutaway) {\n   694→      cutawayTime += d.duration;\n   695→    } else if (d.isWideShot) {\n   696→      wideShotTime += d.duration;\n   697→    } else {\n   698→      speakerTimes[d.speakerName] = (speakerTimes[d.speakerName] || 0) + d.duration;\n   699→    }\n   700→  }\n   701→\n   702→  const speakerPercentages = {};\n   703→  for (const [speaker, time] of Object.entries(speakerTimes)) {\n   704→    speakerPercentages[speaker] = parseFloat(((time / totalDuration) * 100).toFixed(1));\n   705→  }\n   706→\n   707→  return {\n   708→    decisionCount: decisions.length,\n   709→    wideShotPercentage: parseFloat(((wideShotTime / totalDuration) * 100).toFixed(1)),\n   710→    cutawayPercentage: parseFloat(((cutawayTime / totalDuration) * 100).toFixed(1)),\n   711→    speakerPercentages,\n   712→    averageShotDuration: parseFloat((totalDuration / decisions.length).toFixed(2))\n   713→  };\n   714→}\n   715→\n   716→// =============================================================================\n   717→// Auto-Balance Optimizer\n   718→// =============================================================================\n   719→\n   720→/**\n   721→ * Auto-balance speaker screentime\n   722→ * Adjusts speaker boosts to achieve equal distribution\n   723→ * PERF-008: Caches audio levels to avoid re-extraction on each iteration\n   724→ *\n   725→ * @param {Array<string>} audioPaths - Audio file paths\n   726→ * @param {Object} options - Base analysis options\n   727→ * @returns {Promise<Object>} Optimized analysis with best parameters\n   728→ */\n   729→async function autoBalanceMultitrack(audioPaths, options = {}) {\n   730→  const opts = { ...DEFAULT_OPTIONS, ...options };\n   731→  const numSpeakers = audioPaths.length;\n   732→  const targetShare = 100 / numSpeakers;\n   733→\n   734→  console.log(`[SPLICE Multitrack] Auto-balancing for ${numSpeakers} speakers (target: ${targetShare.toFixed(1)}% each)`);\n   735→\n   736→  // PERF-008: Extract levels once and cache them (avoids re-running FFmpeg)\n   737→  const cachedLevels = await Promise.all(\n   738→    audioPaths.map((audioPath, i) =>\n   739→      extractTrackLevels(audioPath, {\n   740→        windowMs: opts.analysisWindowMs,\n   741→        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`\n   742→      })\n   743→    )\n   744→  );\n   745→\n   746→  // Run initial analysis using cached levels\n   747→  let bestResult = analyzeMultitrackFromLevels(cachedLevels, opts);\n   748→  let bestError = calculateDistributionError(bestResult.metadata.speakerPercentages, targetShare);\n   749→\n   750→  // Iterative optimization\n   751→  const maxIterations = 10;\n   752→  const boostStep = 3; // dB step\n   753→\n   754→  for (let iter = 0; iter < maxIterations; iter++) {\n   755→    // Find most over/under-represented speakers\n   756→    const percentages = bestResult.metadata.speakerPercentages;\n   757→    let maxOver = { speaker: null, diff: 0 };\n   758→    let maxUnder = { speaker: null, diff: 0 };\n   759→\n   760→    for (const [speaker, pct] of Object.entries(percentages)) {\n   761→      const diff = pct - targetShare;\n   762→      if (diff > maxOver.diff) maxOver = { speaker, diff };\n   763→      if (diff < maxUnder.diff) maxUnder = { speaker, diff };\n   764→    }\n   765→\n   766→    if (Math.abs(maxOver.diff) < 1 && Math.abs(maxUnder.diff) < 1) {\n   767→      console.log(`[SPLICE Multitrack] Converged at iteration ${iter + 1}`);\n   768→      break;\n   769→    }\n   770→\n   771→    // Adjust boosts\n   772→    const newBoosts = { ...opts.speakerBoosts };\n   773→    if (maxOver.speaker) {\n   774→      newBoosts[maxOver.speaker] = (newBoosts[maxOver.speaker] || 0) - boostStep;\n   775→    }\n   776→    if (maxUnder.speaker) {\n   777→      newBoosts[maxUnder.speaker] = (newBoosts[maxUnder.speaker] || 0) + boostStep;\n   778→    }\n   779→\n   780→    // PERF-008: Use cached levels instead of re-extracting via analyzeMultitrack\n   781→    const testResult = analyzeMultitrackFromLevels(cachedLevels, { ...opts, speakerBoosts: newBoosts });\n   782→    const testError = calculateDistributionError(testResult.metadata.speakerPercentages, targetShare);\n   783→\n   784→    if (testError < bestError) {\n   785→      bestResult = testResult;\n   786→      bestError = testError;\n   787→      opts.speakerBoosts = newBoosts;\n   788→      console.log(`[SPLICE Multitrack] Iteration ${iter + 1}: error ${testError.toFixed(2)}%`);\n   789→    }\n   790→  }\n   791→\n   792→  return {\n   793→    ...bestResult,\n   794→    optimized: true,\n   795→    speakerBoosts: opts.speakerBoosts,\n   796→    finalError: bestError\n   797→  };\n   798→}\n   799→\n   800→/**\n   801→ * Calculate distribution error (max absolute deviation from target)\n   802→ */\n   803→function calculateDistributionError(percentages, targetShare) {\n   804→  let maxError = 0;\n   805→  for (const pct of Object.values(percentages)) {\n   806→    maxError = Math.max(maxError, Math.abs(pct - targetShare));\n   807→  }\n   808→  return maxError;\n   809→}\n   810→\n   811→/**\n   812→ * Advanced multitrack balancing with genetic algorithm optimization\n   813→ * Provides finer control over switching behavior with constraints\n   814→ *\n   815→ * @param {Array<string>} audioPaths - Paths to audio files\n   816→ * @param {Object} options - Advanced balancing options\n   817→ * @returns {Promise<Object>} Optimized analysis result\n   818→ */\n   819→async function advancedBalanceMultitrack(audioPaths, options = {}) {\n   820→  const {\n   821→    maxConsecutiveSeconds = 30,   // Max time for single speaker\n   822→    momentumFactor = 0.7,          // Bias toward current speaker (0-1)\n   823→    populationSize = 20,           // GA population size\n   824→    generations = 10,              // GA generations\n   825→    mutationRate = 0.1,            // GA mutation rate\n   826→    targetDistribution = null,     // Custom target percentages per speaker\n   827→    ...baseOptions\n   828→  } = options;\n   829→\n   830→  const opts = { ...DEFAULT_OPTIONS, ...baseOptions };\n   831→  const numSpeakers = audioPaths.length;\n   832→  const targetShare = 100 / numSpeakers;\n   833→\n   834→  console.log(`[SPLICE Multitrack] Advanced balancing for ${numSpeakers} speakers`);\n   835→  console.log(`  - Max consecutive: ${maxConsecutiveSeconds}s`);\n   836→  console.log(`  - Momentum factor: ${momentumFactor}`);\n   837→  console.log(`  - GA population: ${populationSize}, generations: ${generations}`);\n   838→\n   839→  // Extract levels once and cache\n   840→  const cachedLevels = await Promise.all(\n   841→    audioPaths.map((audioPath, i) =>\n   842→      extractTrackLevels(audioPath, {\n   843→        windowMs: opts.analysisWindowMs,\n   844→        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`\n   845→      })\n   846→    )\n   847→  );\n   848→\n   849→  // Get speaker names for target distribution\n   850→  const speakerNames = cachedLevels.map(l => l.speakerName);\n   851→\n   852→  // Build target distribution\n   853→  const targetDist = targetDistribution || {};\n   854→  for (const name of speakerNames) {\n   855→    if (!targetDist[name]) {\n   856→      targetDist[name] = targetShare;\n   857→    }\n   858→  }\n   859→\n   860→  // Generate initial population of boost configurations\n   861→  let population = generatePopulation(speakerNames, populationSize);\n   862→\n   863→  // Evaluate fitness for each individual\n   864→  const evaluateFitness = (boosts) => {\n   865→    const result = analyzeMultitrackFromLevels(cachedLevels, {\n   866→      ...opts,\n   867→      speakerBoosts: boosts\n   868→    });\n   869→\n   870→    // Calculate fitness score (lower is better)\n   871→    let fitness = 0;\n   872→\n   873→    // Distribution error\n   874→    const pcts = result.metadata.speakerPercentages;\n   875→    for (const [speaker, target] of Object.entries(targetDist)) {\n   876→      fitness += Math.pow((pcts[speaker] || 0) - target, 2);\n   877→    }\n   878→\n   879→    // Consecutive speaker penalty\n   880→    const decisions = result.decisions || [];\n   881→    let currentSpeaker = null;\n   882→    let currentDuration = 0;\n   883→    let consecutivePenalty = 0;\n   884→\n   885→    for (const decision of decisions) {\n   886→      if (decision.speaker === currentSpeaker) {\n   887→        currentDuration += decision.duration;\n   888→        if (currentDuration > maxConsecutiveSeconds) {\n   889→          consecutivePenalty += (currentDuration - maxConsecutiveSeconds);\n   890→        }\n   891→      } else {\n   892→        currentSpeaker = decision.speaker;\n   893→        currentDuration = decision.duration;\n   894→      }\n   895→    }\n   896→    fitness += consecutivePenalty * 10;\n   897→\n   898→    return { fitness, result };\n   899→  };\n   900→\n   901→  // Genetic algorithm loop\n   902→  let bestIndividual = population[0];\n   903→  let bestFitness = Infinity;\n   904→  let bestResult = null;\n   905→\n   906→  for (let gen = 0; gen < generations; gen++) {\n   907→    // Evaluate all individuals\n   908→    const evaluated = population.map(boosts => {\n   909→      const { fitness, result } = evaluateFitness(boosts);\n   910→      if (fitness < bestFitness) {\n   911→        bestFitness = fitness;\n   912→        bestIndividual = boosts;\n   913→        bestResult = result;\n   914→      }\n   915→      return { boosts, fitness };\n   916→    });\n   917→\n   918→    // Sort by fitness\n   919→    evaluated.sort((a, b) => a.fitness - b.fitness);\n   920→\n   921→    // Selection - keep top 50%\n   922→    const survivors = evaluated.slice(0, Math.floor(populationSize / 2));\n   923→\n   924→    // Crossover and mutation\n   925→    const newPopulation = survivors.map(s => s.boosts);\n   926→\n   927→    while (newPopulation.length < populationSize) {\n   928→      // Select two parents\n   929→      const parent1 = survivors[Math.floor(Math.random() * survivors.length)].boosts;\n   930→      const parent2 = survivors[Math.floor(Math.random() * survivors.length)].boosts;\n   931→\n   932→      // Crossover\n   933→      const child = {};\n   934→      for (const speaker of speakerNames) {\n   935→        child[speaker] = Math.random() < 0.5 ? parent1[speaker] : parent2[speaker];\n   936→\n   937→        // Mutation\n   938→        if (Math.random() < mutationRate) {\n   939→          child[speaker] += (Math.random() - 0.5) * 6; // +/- 3dB\n   940→        }\n   941→      }\n   942→      newPopulation.push(child);\n   943→    }\n   944→\n   945→    population = newPopulation;\n   946→\n   947→    if (gen % 2 === 0 || gen === generations - 1) {\n   948→      console.log(`[SPLICE Multitrack] Generation ${gen + 1}: best fitness ${bestFitness.toFixed(2)}`);\n   949→    }\n   950→  }\n   951→\n   952→  // Apply momentum factor to final decisions\n   953→  if (bestResult && momentumFactor > 0) {\n   954→    bestResult = applyMomentum(bestResult, momentumFactor, maxConsecutiveSeconds);\n   955→  }\n   956→\n   957→  return {\n   958→    ...bestResult,\n   959→    optimized: true,\n   960→    advanced: true,\n   961→    speakerBoosts: bestIndividual,\n   962→    finalError: bestFitness,\n   963→    constraints: {\n   964→      maxConsecutiveSeconds,\n   965→      momentumFactor,\n   966→      generations,\n   967→      populationSize\n   968→    }\n   969→  };\n   970→}\n   971→\n   972→/**\n   973→ * Generate initial population of boost configurations\n   974→ */\n   975→function generatePopulation(speakerNames, size) {\n   976→  const population = [];\n   977→\n   978→  // Always include baseline (no boosts)\n   979→  const baseline = {};\n   980→  for (const name of speakerNames) {\n   981→    baseline[name] = 0;\n   982→  }\n   983→  population.push(baseline);\n   984→\n   985→  // Generate random variations\n   986→  for (let i = 1; i < size; i++) {\n   987→    const individual = {};\n   988→    for (const name of speakerNames) {\n   989→      individual[name] = (Math.random() - 0.5) * 12; // -6 to +6 dB\n   990→    }\n   991→    population.push(individual);\n   992→  }\n   993→\n   994→  return population;\n   995→}\n   996→\n   997→/**\n   998→ * Apply momentum factor to reduce rapid switching\n   999→ */\n  1000→function applyMomentum(result, momentumFactor, maxConsecutive) {\n  1001→  const decisions = result.decisions || [];\n  1002→  if (decisions.length < 2) return result;\n  1003→\n  1004→  const adjusted = [decisions[0]];\n  1005→  let currentSpeaker = decisions[0].speaker;\n  1006→  let currentDuration = decisions[0].duration;\n  1007→\n  1008→  for (let i = 1; i < decisions.length; i++) {\n  1009→    const decision = { ...decisions[i] };\n  1010→\n  1011→    // Check if switch is preferred by momentum\n  1012→    const switchProbability = 1 - momentumFactor;\n  1013→    const forcedSwitch = currentDuration >= maxConsecutive;\n  1014→\n  1015→    if (decision.speaker === currentSpeaker) {\n  1016→      // Same speaker - accumulate duration\n  1017→      currentDuration += decision.duration;\n  1018→      adjusted.push(decision);\n  1019→    } else if (forcedSwitch || Math.random() < switchProbability) {\n  1020→      // Switch speakers (forced or allowed by momentum)\n  1021→      currentSpeaker = decision.speaker;\n  1022→      currentDuration = decision.duration;\n  1023→      adjusted.push(decision);\n  1024→    } else {\n  1025→      // Momentum prevents switch - keep current speaker\n  1026→      const merged = {\n  1027→        ...adjusted[adjusted.length - 1],\n  1028→        end: decision.end,\n  1029→        duration: adjusted[adjusted.length - 1].duration + decision.duration\n  1030→      };\n  1031→      adjusted[adjusted.length - 1] = merged;\n  1032→      currentDuration += decision.duration;\n  1033→    }\n  1034→  }\n  1035→\n  1036→  // Recalculate speaker percentages\n  1037→  const totalDuration = adjusted.reduce((sum, d) => sum + d.duration, 0);\n  1038→  const speakerTimes = {};\n  1039→  for (const d of adjusted) {\n  1040→    speakerTimes[d.speaker] = (speakerTimes[d.speaker] || 0) + d.duration;\n  1041→  }\n  1042→  const speakerPercentages = {};\n  1043→  for (const [speaker, time] of Object.entries(speakerTimes)) {\n  1044→    speakerPercentages[speaker] = parseFloat(((time / totalDuration) * 100).toFixed(2));\n  1045→  }\n  1046→\n  1047→  return {\n  1048→    ...result,\n  1049→    decisions: adjusted,\n  1050→    metadata: {\n  1051→      ...result.metadata,\n  1052→      speakerPercentages,\n  1053→      decisionCount: adjusted.length,\n  1054→      momentumApplied: true\n  1055→    }\n  1056→  };\n  1057→}\n  1058→\n  1059→// =============================================================================\n  1060→// Exports\n  1061→// =============================================================================\n  1062→\n  1063→module.exports = {\n  1064→  // Core analysis\n  1065→  analyzeMultitrack,\n  1066→  analyzeMultitrackFromLevels,  // PERF-008: cached levels analysis\n  1067→  extractTrackLevels,\n  1068→\n  1069→  // Level processing\n  1070→  cleanupAudioLevels,\n  1071→  applySpeakerBoosts,\n  1072→  smoothAudioLevels,\n  1073→\n  1074→  // Decision generation\n  1075→  generateSwitchingDecisions,\n  1076→  tuneWideShotPercentage,\n  1077→  insertCutaways,\n  1078→\n  1079→  // Auto-balance\n  1080→  autoBalanceMultitrack,\n  1081→\n  1082→  // Advanced balancing (Feature 7)\n  1083→  advancedBalanceMultitrack,\n  1084→  generatePopulation,\n  1085→  applyMomentum,\n  1086→\n  1087→  // Statistics\n  1088→  calculateMultitrackStats,\n  1089→\n  1090→  // Config\n  1091→  DEFAULT_OPTIONS\n  1092→};\n  1093→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"0d276ab3-c61e-46c4-a334-30c7528a9cd1","timestamp":"2025-12-26T07:40:06.588Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js","content":"/**\n * Multitrack/Multicam Analysis Service\n *\n * Analyzes multiple audio tracks to determine optimal video angle selection.\n * Based on Fireside's proven multicam editing approach.\n *\n * Features:\n * - Per-speaker RMS audio level analysis\n * - Noise floor subtraction for cleaner detection\n * - Automatic video angle selection based on loudest speaker\n * - Wide shot detection (multiple speakers talking)\n * - Cutaway insertion for long clips\n * - Speaker screentime balancing\n * - Gaussian smoothing to reduce spurious cuts\n */\n\nconst fsPromises = require('fs').promises;\nconst { validateAudioPath, safeFFprobe, safeFFmpeg, safeTempPath } = require('./securityUtils');\n\n// =============================================================================\n// Configuration\n// =============================================================================\n\nconst DEFAULT_OPTIONS = {\n  // Analysis parameters\n  analysisWindowMs: 100,       // RMS window size in milliseconds\n  smoothingWindow: 5,          // Gaussian blur kernel size (frames)\n\n  // Switching parameters\n  switchingFrequency: 50,      // How often to allow cuts (0-100)\n  minShotDuration: 2.0,        // Minimum seconds before next cut\n  speakerBoosts: {},           // Per-speaker dB adjustments\n\n  // Wide shot parameters\n  wideShotEnabled: true,\n  wideShotPercentage: 20,      // Target % of time in wide shots\n  wideShotTolerance: 6,        // dB tolerance for multiple speaker detection\n\n  // Cutaway parameters\n  cutawayEnabled: false,\n  cutawayMinDuration: 1.5,\n  cutawayMaxDuration: 4.0,\n  cutawayTracks: [],           // Video track indices for cutaways\n\n  // Output\n  frameRate: 30\n};\n\n// =============================================================================\n// Core Analysis\n// =============================================================================\n\n/**\n * Analyze multiple audio tracks for multicam editing\n *\n * @param {Array<string>} audioPaths - Paths to audio files (one per speaker)\n * @param {Object} options - Analysis options\n * @returns {Promise<Object>} Analysis results with switching decisions\n */\nasync function analyzeMultitrack(audioPaths, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n\n  console.log(`[SPLICE Multitrack] Analyzing ${audioPaths.length} audio track(s)`);\n\n  // Step 1: Extract RMS levels for each track\n  const trackLevels = await Promise.all(\n    audioPaths.map((audioPath, i) =>\n      extractTrackLevels(audioPath, {\n        windowMs: opts.analysisWindowMs,\n        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`\n      })\n    )\n  );\n\n  // Validate all tracks have same duration\n  const durations = trackLevels.map(t => t.duration);\n  const maxDuration = Math.max(...durations);\n  const minDuration = Math.min(...durations);\n  if (maxDuration - minDuration > 1) {\n    console.warn('[SPLICE Multitrack] Track durations differ significantly');\n  }\n\n  // Step 2: Calculate noise floor and clean up levels\n  const cleanedLevels = cleanupAudioLevels(trackLevels, opts);\n\n  // Step 3: Apply speaker boosts\n  const boostedLevels = applySpeakerBoosts(cleanedLevels, opts.speakerBoosts);\n\n  // Step 4: Apply Gaussian smoothing\n  const smoothedLevels = opts.smoothingWindow > 0\n    ? smoothAudioLevels(boostedLevels, opts.smoothingWindow)\n    : boostedLevels;\n\n  // Step 5: Generate switching decisions\n  const decisions = generateSwitchingDecisions(smoothedLevels, {\n    videoTrackMapping: opts.videoTrackMapping,\n    minShotDuration: opts.minShotDuration,\n    switchingFrequency: opts.switchingFrequency,\n    wideShotEnabled: opts.wideShotEnabled,\n    wideShotTolerance: opts.wideShotTolerance,\n    wideShotTracks: opts.wideShotTracks,\n    frameRate: opts.frameRate\n  });\n\n  // Step 6: Fine-tune wide shot percentage\n  const tunedDecisions = opts.wideShotEnabled\n    ? tuneWideShotPercentage(decisions, opts.wideShotPercentage)\n    : decisions;\n\n  // Step 7: Insert cutaways if enabled\n  const finalDecisions = opts.cutawayEnabled\n    ? insertCutaways(tunedDecisions, {\n        cutawayTracks: opts.cutawayTracks,\n        minDuration: opts.cutawayMinDuration,\n        maxDuration: opts.cutawayMaxDuration\n      })\n    : tunedDecisions;\n\n  // Step 8: Calculate statistics\n  const stats = calculateMultitrackStats(finalDecisions, trackLevels[0].duration);\n\n  console.log(`[SPLICE Multitrack] Generated ${finalDecisions.length} switching decision(s)`);\n\n  return {\n    decisions: finalDecisions,\n    metadata: {\n      trackCount: audioPaths.length,\n      speakerNames: opts.speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n      duration: maxDuration,\n      frameRate: opts.frameRate,\n      ...stats\n    },\n    levels: {\n      raw: trackLevels.map(t => ({ speaker: t.speakerName, sampleCount: t.levels.length })),\n      // Don't include full level data in response (too large)\n    }\n  };\n}\n\n/**\n * Analyze multitrack using pre-extracted levels (PERF-008)\n * Skips FFmpeg processing for faster iteration during auto-balance\n *\n * @param {Array<Object>} cachedLevels - Pre-extracted track levels from extractTrackLevels\n * @param {Object} options - Analysis options\n * @returns {Object} Analysis results with switching decisions\n */\nfunction analyzeMultitrackFromLevels(cachedLevels, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n\n  // Validate all tracks have same duration\n  const durations = cachedLevels.map(t => t.duration);\n  const maxDuration = Math.max(...durations);\n  const minDuration = Math.min(...durations);\n  if (maxDuration - minDuration > 1) {\n    console.warn('[SPLICE Multitrack] Track durations differ significantly');\n  }\n\n  // Step 2: Calculate noise floor and clean up levels\n  const cleanedLevels = cleanupAudioLevels(cachedLevels, opts);\n\n  // Step 3: Apply speaker boosts\n  const boostedLevels = applySpeakerBoosts(cleanedLevels, opts.speakerBoosts);\n\n  // Step 4: Apply Gaussian smoothing\n  const smoothedLevels = opts.smoothingWindow > 0\n    ? smoothAudioLevels(boostedLevels, opts.smoothingWindow)\n    : boostedLevels;\n\n  // Step 5: Generate switching decisions\n  const decisions = generateSwitchingDecisions(smoothedLevels, {\n    videoTrackMapping: opts.videoTrackMapping,\n    minShotDuration: opts.minShotDuration,\n    switchingFrequency: opts.switchingFrequency,\n    wideShotEnabled: opts.wideShotEnabled,\n    wideShotTolerance: opts.wideShotTolerance,\n    wideShotTracks: opts.wideShotTracks,\n    frameRate: opts.frameRate\n  });\n\n  // Step 6: Fine-tune wide shot percentage\n  const tunedDecisions = opts.wideShotEnabled\n    ? tuneWideShotPercentage(decisions, opts.wideShotPercentage)\n    : decisions;\n\n  // Step 7: Insert cutaways if enabled\n  const finalDecisions = opts.cutawayEnabled\n    ? insertCutaways(tunedDecisions, {\n        cutawayTracks: opts.cutawayTracks,\n        minDuration: opts.cutawayMinDuration,\n        maxDuration: opts.cutawayMaxDuration\n      })\n    : tunedDecisions;\n\n  // Step 8: Calculate statistics\n  const stats = calculateMultitrackStats(finalDecisions, cachedLevels[0].duration);\n\n  return {\n    decisions: finalDecisions,\n    metadata: {\n      trackCount: cachedLevels.length,\n      speakerNames: opts.speakerNames || cachedLevels.map((_, i) => `Speaker ${i + 1}`),\n      duration: maxDuration,\n      frameRate: opts.frameRate,\n      ...stats\n    },\n    levels: {\n      raw: cachedLevels.map(t => ({ speaker: t.speakerName, sampleCount: t.levels.length })),\n    }\n  };\n}\n\n// =============================================================================\n// Audio Level Extraction\n// =============================================================================\n\n/**\n * Extract RMS levels from audio file\n *\n * @param {string} audioPath - Path to audio file\n * @param {Object} options - Extraction options\n * @returns {Promise<Object>} Track levels with timing\n */\nasync function extractTrackLevels(audioPath, options = {}) {\n  const { windowMs = 100, speakerName = 'Unknown' } = options;\n\n  // SECURITY: Validate input path to prevent path traversal\n  const pathValidation = await validateAudioPath(audioPath);\n  if (!pathValidation.valid) {\n    throw new Error(`Invalid audio path: ${pathValidation.error}`);\n  }\n  const validatedPath = pathValidation.path;\n\n  console.log(`[SPLICE Multitrack] Extracting levels: ${speakerName}`);\n\n  // SECURITY: Use execFile with array arguments to prevent command injection\n  const infoArgs = [\n    '-v', 'error',\n    '-select_streams', 'a:0',\n    '-show_entries', 'stream=sample_rate,duration',\n    '-of', 'json',\n    validatedPath\n  ];\n  let duration = 0;\n\n  try {\n    const { stdout } = await safeFFprobe(infoArgs);\n    const info = JSON.parse(stdout);\n    if (info.streams?.[0]) {\n      // Note: sample_rate available in info.streams[0].sample_rate for future resampling support\n      duration = parseFloat(info.streams[0].duration) || 0;\n    }\n  } catch (err) {\n    console.warn(`[SPLICE Multitrack] Could not get audio info: ${err.message}`);\n  }\n\n  // Get duration from format if not in stream\n  if (!duration) {\n    try {\n      const durationArgs = [\n        '-v', 'error',\n        '-show_entries', 'format=duration',\n        '-of', 'default=noprint_wrappers=1:nokey=1',\n        validatedPath\n      ];\n      const { stdout } = await safeFFprobe(durationArgs);\n      duration = parseFloat(stdout.trim()) || 0;\n    } catch (err) {\n      console.warn(`[SPLICE Multitrack] Could not get duration: ${err.message}`);\n    }\n  }\n\n  // SECURITY: Use safe temp path generation\n  const tempFile = safeTempPath('splice_multitrack', '.raw');\n\n  try {\n    // SECURITY: Use execFile with array arguments to prevent command injection\n    const extractArgs = [\n      '-y',\n      '-i', validatedPath,\n      '-ac', '1',\n      '-ar', '16000',\n      '-f', 's16le',\n      '-acodec', 'pcm_s16le',\n      tempFile\n    ];\n    await safeFFmpeg(extractArgs);\n\n    // PERFORMANCE: Use async file read to avoid blocking event loop\n    const rawData = await fsPromises.readFile(tempFile);\n    const samples = new Float32Array(rawData.length / 2);\n    for (let i = 0; i < samples.length; i++) {\n      samples[i] = rawData.readInt16LE(i * 2) / 32768.0;\n    }\n\n    // Calculate RMS per window\n    const windowSamples = Math.floor((windowMs / 1000) * 16000);\n    const hopSamples = windowSamples; // Non-overlapping windows\n    const numWindows = Math.floor(samples.length / hopSamples);\n\n    const levels = new Float32Array(numWindows);\n    const times = new Float32Array(numWindows);\n\n    for (let i = 0; i < numWindows; i++) {\n      const start = i * hopSamples;\n      const end = Math.min(start + windowSamples, samples.length);\n\n      let sumSquares = 0;\n      for (let j = start; j < end; j++) {\n        sumSquares += samples[j] * samples[j];\n      }\n\n      const rms = Math.sqrt(sumSquares / (end - start));\n      const dbfs = rms > 0 ? 20 * Math.log10(rms) : -100;\n\n      levels[i] = Math.max(-100, dbfs);\n      times[i] = (start / 16000);\n    }\n\n    // PERFORMANCE: Use async file operations\n    await fsPromises.unlink(tempFile);\n\n    return { levels, times, duration, speakerName, sampleRate: 16000 };\n  } catch (err) {\n    // PERFORMANCE: Use async file operations for cleanup\n    try {\n      await fsPromises.access(tempFile);\n      await fsPromises.unlink(tempFile);\n    } catch {\n      // File doesn't exist\n    }\n    throw new Error(`Failed to extract track levels: ${err.message}`);\n  }\n}\n\n// =============================================================================\n// Level Processing\n// =============================================================================\n\n/**\n * Clean up audio levels by subtracting noise floor\n *\n * @param {Array} trackLevels - Array of track level objects\n * @param {Object} options - Cleanup options\n * @returns {Array} Cleaned level arrays\n */\nfunction cleanupAudioLevels(trackLevels, _options = {}) {\n  const numTracks = trackLevels.length;\n  const numSamples = trackLevels[0].levels.length;\n\n  // Calculate average level (noise floor estimate)\n  const averageLevels = new Float32Array(numSamples);\n  for (let i = 0; i < numSamples; i++) {\n    let sum = 0;\n    for (let t = 0; t < numTracks; t++) {\n      sum += trackLevels[t].levels[i];\n    }\n    averageLevels[i] = sum / numTracks;\n  }\n\n  // Subtract average from each track (relative loudness)\n  const cleaned = trackLevels.map(track => {\n    const cleanedLevels = new Float32Array(numSamples);\n    for (let i = 0; i < numSamples; i++) {\n      // Normalize to -100 to 0 range\n      cleanedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] - averageLevels[i]));\n    }\n    return {\n      ...track,\n      levels: cleanedLevels\n    };\n  });\n\n  return cleaned;\n}\n\n/**\n * Apply per-speaker dB boosts\n *\n * @param {Array} trackLevels - Cleaned track levels\n * @param {Object} boosts - Speaker boost mapping { \"Speaker 1\": 5, ... }\n * @returns {Array} Boosted levels\n */\nfunction applySpeakerBoosts(trackLevels, boosts = {}) {\n  return trackLevels.map(track => {\n    const boost = boosts[track.speakerName] || 0;\n    if (boost === 0) return track;\n\n    const boostedLevels = new Float32Array(track.levels.length);\n    for (let i = 0; i < track.levels.length; i++) {\n      boostedLevels[i] = Math.max(-100, Math.min(0, track.levels[i] + boost));\n    }\n\n    return { ...track, levels: boostedLevels };\n  });\n}\n\n/**\n * Apply Gaussian smoothing to reduce noise\n *\n * @param {Array} trackLevels - Track levels\n * @param {number} windowSize - Gaussian kernel size\n * @returns {Array} Smoothed levels\n */\nfunction smoothAudioLevels(trackLevels, windowSize = 5) {\n  const kernel = generateGaussianKernel(windowSize);\n\n  return trackLevels.map(track => {\n    const smoothed = applyConvolution(track.levels, kernel);\n    return { ...track, levels: smoothed };\n  });\n}\n\n/**\n * Generate Gaussian kernel\n */\nfunction generateGaussianKernel(size) {\n  const sigma = size / 3;\n  const kernel = new Float32Array(size);\n  const half = Math.floor(size / 2);\n  let sum = 0;\n\n  for (let i = 0; i < size; i++) {\n    const x = i - half;\n    kernel[i] = Math.exp(-(x * x) / (2 * sigma * sigma));\n    sum += kernel[i];\n  }\n\n  // Normalize\n  for (let i = 0; i < size; i++) {\n    kernel[i] /= sum;\n  }\n\n  return kernel;\n}\n\n/**\n * Apply 1D convolution\n */\nfunction applyConvolution(data, kernel) {\n  const result = new Float32Array(data.length);\n  const half = Math.floor(kernel.length / 2);\n\n  for (let i = 0; i < data.length; i++) {\n    let sum = 0;\n    for (let j = 0; j < kernel.length; j++) {\n      const idx = i + j - half;\n      if (idx >= 0 && idx < data.length) {\n        sum += data[idx] * kernel[j];\n      }\n    }\n    result[i] = sum;\n  }\n\n  return result;\n}\n\n// =============================================================================\n// Switching Decision Generation\n// =============================================================================\n\n/**\n * Generate video switching decisions based on audio levels\n *\n * @param {Array} trackLevels - Processed track levels\n * @param {Object} options - Decision options\n * @returns {Array} Switching decisions\n */\nfunction generateSwitchingDecisions(trackLevels, options = {}) {\n  const {\n    videoTrackMapping = {},\n    minShotDuration = 2.0,\n    switchingFrequency = 50,\n    wideShotEnabled = true,\n    wideShotTolerance = 6,\n    wideShotTracks = []\n    // frameRate reserved for future frame-aligned cutting\n  } = options;\n\n  const numSamples = trackLevels[0].levels.length;\n  const times = trackLevels[0].times;\n  const decisions = [];\n\n  // Calculate min samples between cuts based on switching frequency\n  const minSamplesPerCut = Math.ceil(minShotDuration / (times[1] - times[0]));\n  const switchingCooldown = Math.ceil(minSamplesPerCut * (1 - switchingFrequency / 100));\n\n  let currentSpeaker = -1;\n  // currentDecisionStart reserved for future decision grouping\n  let lastSwitchSample = 0;\n\n  for (let i = 0; i < numSamples; i++) {\n    // Find loudest speaker\n    let maxLevel = -100;\n    let loudestSpeaker = 0;\n    let speakersAboveThreshold = [];\n\n    for (let t = 0; t < trackLevels.length; t++) {\n      const level = trackLevels[t].levels[i];\n      if (level > maxLevel) {\n        maxLevel = level;\n        loudestSpeaker = t;\n      }\n\n      // Track speakers within tolerance of max (for wide shot detection)\n      if (wideShotEnabled && level >= maxLevel - wideShotTolerance) {\n        speakersAboveThreshold.push(t);\n      }\n    }\n\n    // Determine if this should be a wide shot\n    const isWideShot = wideShotEnabled && speakersAboveThreshold.length >= 2;\n    const effectiveSpeaker = isWideShot ? -1 : loudestSpeaker; // -1 = wide shot\n\n    // Check if we should switch\n    const canSwitch = (i - lastSwitchSample) >= switchingCooldown;\n    const shouldSwitch = canSwitch && effectiveSpeaker !== currentSpeaker;\n\n    if (shouldSwitch || i === 0) {\n      // Close previous decision\n      if (i > 0 && decisions.length > 0) {\n        decisions[decisions.length - 1].endTime = times[i];\n        decisions[decisions.length - 1].endSample = i;\n      }\n\n      // Start new decision\n      const speakerName = isWideShot\n        ? 'Wide Shot'\n        : trackLevels[loudestSpeaker].speakerName;\n\n      const videoTrack = isWideShot\n        ? (wideShotTracks[0] ?? 0)\n        : (videoTrackMapping[loudestSpeaker] ?? loudestSpeaker);\n\n      decisions.push({\n        speakerIndex: effectiveSpeaker,\n        speakerName,\n        speakers: speakersAboveThreshold.map(s => trackLevels[s].speakerName),\n        videoTrack,\n        startTime: times[i],\n        startSample: i,\n        endTime: times[numSamples - 1],\n        endSample: numSamples - 1,\n        isWideShot,\n        reason: isWideShot ? 'Multiple speakers' : 'Loudest speaker'\n      });\n\n      currentSpeaker = effectiveSpeaker;\n      lastSwitchSample = i;\n    }\n  }\n\n  // Finalize last decision\n  if (decisions.length > 0) {\n    decisions[decisions.length - 1].endTime = times[numSamples - 1];\n    decisions[decisions.length - 1].endSample = numSamples - 1;\n  }\n\n  // Add duration to each decision\n  return decisions.map(d => ({\n    ...d,\n    duration: parseFloat((d.endTime - d.startTime).toFixed(3))\n  }));\n}\n\n// =============================================================================\n// Wide Shot Tuning\n// =============================================================================\n\n/**\n * Fine-tune wide shot percentage using tolerance adjustment\n *\n * @param {Array} decisions - Initial decisions\n * @param {number} targetPercentage - Target wide shot percentage\n * @returns {Array} Tuned decisions\n */\nfunction tuneWideShotPercentage(decisions, targetPercentage) {\n  const totalDuration = decisions.reduce((sum, d) => sum + d.duration, 0);\n  const currentWideTime = decisions\n    .filter(d => d.isWideShot)\n    .reduce((sum, d) => sum + d.duration, 0);\n  const currentPercentage = (currentWideTime / totalDuration) * 100;\n\n  // If within 5% of target, don't adjust\n  if (Math.abs(currentPercentage - targetPercentage) <= 5) {\n    return decisions;\n  }\n\n  console.log(`[SPLICE Multitrack] Wide shot: ${currentPercentage.toFixed(1)}% (target: ${targetPercentage}%)`);\n\n  // Simple approach: mark some single-speaker decisions as wide shots\n  // or vice versa based on whether we need more or fewer wide shots\n  // (Full implementation would re-run with adjusted tolerance)\n\n  return decisions; // Return as-is for now\n}\n\n// =============================================================================\n// Cutaway Insertion\n// =============================================================================\n\n/**\n * Insert cutaways into long clips\n *\n * @param {Array} decisions - Switching decisions\n * @param {Object} options - Cutaway options\n * @returns {Array} Decisions with cutaways inserted\n */\nfunction insertCutaways(decisions, options = {}) {\n  const {\n    cutawayTracks = [],\n    minDuration = 1.5,\n    maxDuration = 4.0,\n    maxClipDuration = 30 // Insert cutaway if clip longer than this\n  } = options;\n\n  if (cutawayTracks.length === 0) return decisions;\n\n  const result = [];\n\n  for (const decision of decisions) {\n    if (decision.duration <= maxClipDuration) {\n      result.push(decision);\n      continue;\n    }\n\n    // Split long clip with cutaways\n    const numCutaways = Math.floor(decision.duration / maxClipDuration);\n    const interval = decision.duration / (numCutaways + 1);\n\n    let currentStart = decision.startTime;\n\n    for (let i = 0; i < numCutaways; i++) {\n      // Add main clip segment\n      const cutawayStart = currentStart + interval - minDuration / 2;\n      const cutawayDuration = minDuration + Math.random() * (maxDuration - minDuration);\n      const cutawayEnd = cutawayStart + cutawayDuration;\n\n      result.push({\n        ...decision,\n        startTime: currentStart,\n        endTime: cutawayStart,\n        duration: cutawayStart - currentStart\n      });\n\n      // Add cutaway\n      const cutawayTrack = cutawayTracks[i % cutawayTracks.length];\n      result.push({\n        speakerIndex: -2, // Special index for cutaway\n        speakerName: 'Cutaway',\n        speakers: [],\n        videoTrack: cutawayTrack,\n        startTime: cutawayStart,\n        endTime: cutawayEnd,\n        duration: cutawayDuration,\n        isWideShot: false,\n        isCutaway: true,\n        reason: 'Cutaway insert'\n      });\n\n      currentStart = cutawayEnd;\n    }\n\n    // Add final segment\n    result.push({\n      ...decision,\n      startTime: currentStart,\n      endTime: decision.endTime,\n      duration: decision.endTime - currentStart\n    });\n  }\n\n  return result;\n}\n\n// =============================================================================\n// Statistics\n// =============================================================================\n\n/**\n * Calculate multitrack analysis statistics\n *\n * @param {Array} decisions - Final decisions\n * @param {number} totalDuration - Total audio duration\n * @returns {Object} Statistics\n */\nfunction calculateMultitrackStats(decisions, totalDuration) {\n  const speakerTimes = {};\n  let wideShotTime = 0;\n  let cutawayTime = 0;\n\n  for (const d of decisions) {\n    if (d.isCutaway) {\n      cutawayTime += d.duration;\n    } else if (d.isWideShot) {\n      wideShotTime += d.duration;\n    } else {\n      speakerTimes[d.speakerName] = (speakerTimes[d.speakerName] || 0) + d.duration;\n    }\n  }\n\n  const speakerPercentages = {};\n  for (const [speaker, time] of Object.entries(speakerTimes)) {\n    speakerPercentages[speaker] = parseFloat(((time / totalDuration) * 100).toFixed(1));\n  }\n\n  return {\n    decisionCount: decisions.length,\n    wideShotPercentage: parseFloat(((wideShotTime / totalDuration) * 100).toFixed(1)),\n    cutawayPercentage: parseFloat(((cutawayTime / totalDuration) * 100).toFixed(1)),\n    speakerPercentages,\n    averageShotDuration: parseFloat((totalDuration / decisions.length).toFixed(2))\n  };\n}\n\n// =============================================================================\n// Auto-Balance Optimizer\n// =============================================================================\n\n/**\n * Auto-balance speaker screentime\n * Adjusts speaker boosts to achieve equal distribution\n * PERF-008: Caches audio levels to avoid re-extraction on each iteration\n *\n * @param {Array<string>} audioPaths - Audio file paths\n * @param {Object} options - Base analysis options\n * @returns {Promise<Object>} Optimized analysis with best parameters\n */\nasync function autoBalanceMultitrack(audioPaths, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n  const numSpeakers = audioPaths.length;\n  const targetShare = 100 / numSpeakers;\n\n  console.log(`[SPLICE Multitrack] Auto-balancing for ${numSpeakers} speakers (target: ${targetShare.toFixed(1)}% each)`);\n\n  // PERF-008: Extract levels once and cache them (avoids re-running FFmpeg)\n  const cachedLevels = await Promise.all(\n    audioPaths.map((audioPath, i) =>\n      extractTrackLevels(audioPath, {\n        windowMs: opts.analysisWindowMs,\n        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`\n      })\n    )\n  );\n\n  // Run initial analysis using cached levels\n  let bestResult = analyzeMultitrackFromLevels(cachedLevels, opts);\n  let bestError = calculateDistributionError(bestResult.metadata.speakerPercentages, targetShare);\n\n  // Iterative optimization\n  const maxIterations = 10;\n  const boostStep = 3; // dB step\n\n  for (let iter = 0; iter < maxIterations; iter++) {\n    // Find most over/under-represented speakers\n    const percentages = bestResult.metadata.speakerPercentages;\n    let maxOver = { speaker: null, diff: 0 };\n    let maxUnder = { speaker: null, diff: 0 };\n\n    for (const [speaker, pct] of Object.entries(percentages)) {\n      const diff = pct - targetShare;\n      if (diff > maxOver.diff) maxOver = { speaker, diff };\n      if (diff < maxUnder.diff) maxUnder = { speaker, diff };\n    }\n\n    if (Math.abs(maxOver.diff) < 1 && Math.abs(maxUnder.diff) < 1) {\n      console.log(`[SPLICE Multitrack] Converged at iteration ${iter + 1}`);\n      break;\n    }\n\n    // Adjust boosts\n    const newBoosts = { ...opts.speakerBoosts };\n    if (maxOver.speaker) {\n      newBoosts[maxOver.speaker] = (newBoosts[maxOver.speaker] || 0) - boostStep;\n    }\n    if (maxUnder.speaker) {\n      newBoosts[maxUnder.speaker] = (newBoosts[maxUnder.speaker] || 0) + boostStep;\n    }\n\n    // PERF-008: Use cached levels instead of re-extracting via analyzeMultitrack\n    const testResult = analyzeMultitrackFromLevels(cachedLevels, { ...opts, speakerBoosts: newBoosts });\n    const testError = calculateDistributionError(testResult.metadata.speakerPercentages, targetShare);\n\n    if (testError < bestError) {\n      bestResult = testResult;\n      bestError = testError;\n      opts.speakerBoosts = newBoosts;\n      console.log(`[SPLICE Multitrack] Iteration ${iter + 1}: error ${testError.toFixed(2)}%`);\n    }\n  }\n\n  return {\n    ...bestResult,\n    optimized: true,\n    speakerBoosts: opts.speakerBoosts,\n    finalError: bestError\n  };\n}\n\n/**\n * Calculate distribution error (max absolute deviation from target)\n */\nfunction calculateDistributionError(percentages, targetShare) {\n  let maxError = 0;\n  for (const pct of Object.values(percentages)) {\n    maxError = Math.max(maxError, Math.abs(pct - targetShare));\n  }\n  return maxError;\n}\n\n/**\n * Advanced multitrack balancing with genetic algorithm optimization\n * Provides finer control over switching behavior with constraints\n *\n * @param {Array<string>} audioPaths - Paths to audio files\n * @param {Object} options - Advanced balancing options\n * @returns {Promise<Object>} Optimized analysis result\n */\nasync function advancedBalanceMultitrack(audioPaths, options = {}) {\n  const {\n    maxConsecutiveSeconds = 30,   // Max time for single speaker\n    momentumFactor = 0.7,          // Bias toward current speaker (0-1)\n    populationSize = 20,           // GA population size\n    generations = 10,              // GA generations\n    mutationRate = 0.1,            // GA mutation rate\n    targetDistribution = null,     // Custom target percentages per speaker\n    ...baseOptions\n  } = options;\n\n  const opts = { ...DEFAULT_OPTIONS, ...baseOptions };\n  const numSpeakers = audioPaths.length;\n  const targetShare = 100 / numSpeakers;\n\n  console.log(`[SPLICE Multitrack] Advanced balancing for ${numSpeakers} speakers`);\n  console.log(`  - Max consecutive: ${maxConsecutiveSeconds}s`);\n  console.log(`  - Momentum factor: ${momentumFactor}`);\n  console.log(`  - GA population: ${populationSize}, generations: ${generations}`);\n\n  // Extract levels once and cache\n  const cachedLevels = await Promise.all(\n    audioPaths.map((audioPath, i) =>\n      extractTrackLevels(audioPath, {\n        windowMs: opts.analysisWindowMs,\n        speakerName: opts.speakerNames?.[i] || `Speaker ${i + 1}`\n      })\n    )\n  );\n\n  // Get speaker names for target distribution\n  const speakerNames = cachedLevels.map(l => l.speakerName);\n\n  // Build target distribution\n  const targetDist = targetDistribution || {};\n  for (const name of speakerNames) {\n    if (!targetDist[name]) {\n      targetDist[name] = targetShare;\n    }\n  }\n\n  // Generate initial population of boost configurations\n  let population = generatePopulation(speakerNames, populationSize);\n\n  // Evaluate fitness for each individual\n  const evaluateFitness = (boosts) => {\n    const result = analyzeMultitrackFromLevels(cachedLevels, {\n      ...opts,\n      speakerBoosts: boosts\n    });\n\n    // Calculate fitness score (lower is better)\n    let fitness = 0;\n\n    // Distribution error\n    const pcts = result.metadata.speakerPercentages;\n    for (const [speaker, target] of Object.entries(targetDist)) {\n      fitness += Math.pow((pcts[speaker] || 0) - target, 2);\n    }\n\n    // Consecutive speaker penalty\n    const decisions = result.decisions || [];\n    let currentSpeaker = null;\n    let currentDuration = 0;\n    let consecutivePenalty = 0;\n\n    for (const decision of decisions) {\n      if (decision.speaker === currentSpeaker) {\n        currentDuration += decision.duration;\n        if (currentDuration > maxConsecutiveSeconds) {\n          consecutivePenalty += (currentDuration - maxConsecutiveSeconds);\n        }\n      } else {\n        currentSpeaker = decision.speaker;\n        currentDuration = decision.duration;\n      }\n    }\n    fitness += consecutivePenalty * 10;\n\n    return { fitness, result };\n  };\n\n  // Genetic algorithm loop\n  let bestIndividual = population[0];\n  let bestFitness = Infinity;\n  let bestResult = null;\n\n  for (let gen = 0; gen < generations; gen++) {\n    // Evaluate all individuals\n    const evaluated = population.map(boosts => {\n      const { fitness, result } = evaluateFitness(boosts);\n      if (fitness < bestFitness) {\n        bestFitness = fitness;\n        bestIndividual = boosts;\n        bestResult = result;\n      }\n      return { boosts, fitness };\n    });\n\n    // Sort by fitness\n    evaluated.sort((a, b) => a.fitness - b.fitness);\n\n    // Selection - keep top 50%\n    const survivors = evaluated.slice(0, Math.floor(populationSize / 2));\n\n    // Crossover and mutation\n    const newPopulation = survivors.map(s => s.boosts);\n\n    while (newPopulation.length < populationSize) {\n      // Select two parents\n      const parent1 = survivors[Math.floor(Math.random() * survivors.length)].boosts;\n      const parent2 = survivors[Math.floor(Math.random() * survivors.length)].boosts;\n\n      // Crossover\n      const child = {};\n      for (const speaker of speakerNames) {\n        child[speaker] = Math.random() < 0.5 ? parent1[speaker] : parent2[speaker];\n\n        // Mutation\n        if (Math.random() < mutationRate) {\n          child[speaker] += (Math.random() - 0.5) * 6; // +/- 3dB\n        }\n      }\n      newPopulation.push(child);\n    }\n\n    population = newPopulation;\n\n    if (gen % 2 === 0 || gen === generations - 1) {\n      console.log(`[SPLICE Multitrack] Generation ${gen + 1}: best fitness ${bestFitness.toFixed(2)}`);\n    }\n  }\n\n  // Apply momentum factor to final decisions\n  if (bestResult && momentumFactor > 0) {\n    bestResult = applyMomentum(bestResult, momentumFactor, maxConsecutiveSeconds);\n  }\n\n  return {\n    ...bestResult,\n    optimized: true,\n    advanced: true,\n    speakerBoosts: bestIndividual,\n    finalError: bestFitness,\n    constraints: {\n      maxConsecutiveSeconds,\n      momentumFactor,\n      generations,\n      populationSize\n    }\n  };\n}\n\n/**\n * Generate initial population of boost configurations\n */\nfunction generatePopulation(speakerNames, size) {\n  const population = [];\n\n  // Always include baseline (no boosts)\n  const baseline = {};\n  for (const name of speakerNames) {\n    baseline[name] = 0;\n  }\n  population.push(baseline);\n\n  // Generate random variations\n  for (let i = 1; i < size; i++) {\n    const individual = {};\n    for (const name of speakerNames) {\n      individual[name] = (Math.random() - 0.5) * 12; // -6 to +6 dB\n    }\n    population.push(individual);\n  }\n\n  return population;\n}\n\n/**\n * Apply momentum factor to reduce rapid switching\n */\nfunction applyMomentum(result, momentumFactor, maxConsecutive) {\n  const decisions = result.decisions || [];\n  if (decisions.length < 2) return result;\n\n  const adjusted = [decisions[0]];\n  let currentSpeaker = decisions[0].speaker;\n  let currentDuration = decisions[0].duration;\n\n  for (let i = 1; i < decisions.length; i++) {\n    const decision = { ...decisions[i] };\n\n    // Check if switch is preferred by momentum\n    const switchProbability = 1 - momentumFactor;\n    const forcedSwitch = currentDuration >= maxConsecutive;\n\n    if (decision.speaker === currentSpeaker) {\n      // Same speaker - accumulate duration\n      currentDuration += decision.duration;\n      adjusted.push(decision);\n    } else if (forcedSwitch || Math.random() < switchProbability) {\n      // Switch speakers (forced or allowed by momentum)\n      currentSpeaker = decision.speaker;\n      currentDuration = decision.duration;\n      adjusted.push(decision);\n    } else {\n      // Momentum prevents switch - keep current speaker\n      const merged = {\n        ...adjusted[adjusted.length - 1],\n        end: decision.end,\n        duration: adjusted[adjusted.length - 1].duration + decision.duration\n      };\n      adjusted[adjusted.length - 1] = merged;\n      currentDuration += decision.duration;\n    }\n  }\n\n  // Recalculate speaker percentages\n  const totalDuration = adjusted.reduce((sum, d) => sum + d.duration, 0);\n  const speakerTimes = {};\n  for (const d of adjusted) {\n    speakerTimes[d.speaker] = (speakerTimes[d.speaker] || 0) + d.duration;\n  }\n  const speakerPercentages = {};\n  for (const [speaker, time] of Object.entries(speakerTimes)) {\n    speakerPercentages[speaker] = parseFloat(((time / totalDuration) * 100).toFixed(2));\n  }\n\n  return {\n    ...result,\n    decisions: adjusted,\n    metadata: {\n      ...result.metadata,\n      speakerPercentages,\n      decisionCount: adjusted.length,\n      momentumApplied: true\n    }\n  };\n}\n\n// =============================================================================\n// Exports\n// =============================================================================\n\nmodule.exports = {\n  // Core analysis\n  analyzeMultitrack,\n  analyzeMultitrackFromLevels,  // PERF-008: cached levels analysis\n  extractTrackLevels,\n\n  // Level processing\n  cleanupAudioLevels,\n  applySpeakerBoosts,\n  smoothAudioLevels,\n\n  // Decision generation\n  generateSwitchingDecisions,\n  tuneWideShotPercentage,\n  insertCutaways,\n\n  // Auto-balance\n  autoBalanceMultitrack,\n\n  // Advanced balancing (Feature 7)\n  advancedBalanceMultitrack,\n  generatePopulation,\n  applyMomentum,\n\n  // Statistics\n  calculateMultitrackStats,\n\n  // Config\n  DEFAULT_OPTIONS\n};\n","numLines":1093,"startLine":1,"totalLines":1093}}}
{"parentUuid":"0d276ab3-c61e-46c4-a334-30c7528a9cd1","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01SruASx4AbdbnrdtVGaxPjD","type":"tool_result","content":"     1→/**\n     2→ * Usage Tracking Service\n     3→ *\n     4→ * Tracks user credit balances and usage for billing.\n     5→ * Uses PostgreSQL for persistence.\n     6→ */\n     7→\n     8→const { Pool } = require('pg');\n     9→\n    10→// PostgreSQL connection pool with proper limits\n    11→const pool = new Pool({\n    12→  connectionString: process.env.DATABASE_URL,\n    13→  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,\n    14→  // PERFORMANCE: Set connection pool limits\n    15→  max: 20,                    // Maximum number of clients in the pool\n    16→  idleTimeoutMillis: 30000,   // Close idle connections after 30 seconds\n    17→  connectionTimeoutMillis: 5000  // Timeout after 5 seconds when getting connection\n    18→});\n    19→\n    20→// Hours per tier (regular processing)\n    21→const TIER_HOURS = {\n    22→  starter: 15,\n    23→  pro: 50,\n    24→  team: 150,\n    25→  cancelled: 0\n    26→};\n    27→\n    28→// Isolation minutes per tier (sized to maintain 80% margins)\n    29→const TIER_ISOLATION_MINUTES = {\n    30→  starter: 0,     // No isolation access\n    31→  pro: 45,        // 45 minutes included\n    32→  team: 180,      // 3 hours included\n    33→  cancelled: 0\n    34→};\n    35→\n    36→// Convert to hours for database storage\n    37→const TIER_ISOLATION_HOURS = {\n    38→  starter: 0,\n    39→  pro: TIER_ISOLATION_MINUTES.pro / 60,      // 0.75 hours\n    40→  team: TIER_ISOLATION_MINUTES.team / 60,    // 3 hours\n    41→  cancelled: 0\n    42→};\n    43→\n    44→// Overage rate for isolation beyond included minutes\n    45→const ISOLATION_OVERAGE_RATE = 0.10; // $0.10 per minute (85% margin)\n    46→\n    47→/**\n    48→ * Initialize database tables\n    49→ */\n    50→async function initDatabase() {\n    51→  const client = await pool.connect();\n    52→  try {\n    53→    await client.query(`\n    54→      CREATE TABLE IF NOT EXISTS users (\n    55→        id SERIAL PRIMARY KEY,\n    56→        stripe_customer_id VARCHAR(255) UNIQUE NOT NULL,\n    57→        email VARCHAR(255),\n    58→        tier VARCHAR(50) DEFAULT 'starter',\n    59→        hours_remaining DECIMAL(10,4) DEFAULT 15,\n    60→        hours_total DECIMAL(10,4) DEFAULT 15,\n    61→        isolation_hours_remaining DECIMAL(10,4) DEFAULT 0,\n    62→        isolation_hours_total DECIMAL(10,4) DEFAULT 0,\n    63→        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    64→        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    65→      )\n    66→    `);\n    67→\n    68→    // Add isolation columns if they don't exist (for existing databases)\n    69→    await client.query(`\n    70→      DO $$\n    71→      BEGIN\n    72→        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_remaining') THEN\n    73→          ALTER TABLE users ADD COLUMN isolation_hours_remaining DECIMAL(10,4) DEFAULT 0;\n    74→        END IF;\n    75→        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_total') THEN\n    76→          ALTER TABLE users ADD COLUMN isolation_hours_total DECIMAL(10,4) DEFAULT 0;\n    77→        END IF;\n    78→      END $$;\n    79→    `);\n    80→\n    81→    await client.query(`\n    82→      CREATE TABLE IF NOT EXISTS usage_log (\n    83→        id SERIAL PRIMARY KEY,\n    84→        user_id INTEGER REFERENCES users(id),\n    85→        stripe_customer_id VARCHAR(255),\n    86→        audio_duration_seconds DECIMAL(10,2),\n    87→        hours_used DECIMAL(10,6),\n    88→        endpoint VARCHAR(100),\n    89→        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    90→      )\n    91→    `);\n    92→\n    93→    await client.query(`\n    94→      CREATE INDEX IF NOT EXISTS idx_users_stripe_customer_id ON users(stripe_customer_id)\n    95→    `);\n    96→\n    97→    await client.query(`\n    98→      CREATE INDEX IF NOT EXISTS idx_usage_log_user_id ON usage_log(user_id)\n    99→    `);\n   100→\n   101→    // PERFORMANCE: Add composite index for efficient customer usage history queries\n   102→    await client.query(`\n   103→      CREATE INDEX IF NOT EXISTS idx_usage_log_customer_created\n   104→      ON usage_log(stripe_customer_id, created_at DESC)\n   105→    `);\n   106→\n   107→    // Webhook events table for idempotency\n   108→    await client.query(`\n   109→      CREATE TABLE IF NOT EXISTS webhook_events (\n   110→        id SERIAL PRIMARY KEY,\n   111→        event_id VARCHAR(255) UNIQUE NOT NULL,\n   112→        event_type VARCHAR(100),\n   113→        processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n   114→      )\n   115→    `);\n   116→\n   117→    await client.query(`\n   118→      CREATE INDEX IF NOT EXISTS idx_webhook_events_event_id ON webhook_events(event_id)\n   119→    `);\n   120→\n   121→    console.log('[UsageTracking] Database initialized');\n   122→  } finally {\n   123→    client.release();\n   124→  }\n   125→}\n   126→\n   127→/**\n   128→ * Get or create a user by Stripe customer ID\n   129→ * PERF-005: Uses INSERT...ON CONFLICT upsert for single-query operation\n   130→ */\n   131→async function getOrCreateUser(stripeCustomerId, email = null) {\n   132→  // Use upsert with ON CONFLICT DO UPDATE to ensure RETURNING works for both cases\n   133→  // The update sets stripe_customer_id = EXCLUDED.stripe_customer_id (no-op) to trigger RETURNING\n   134→  const result = await pool.query(\n   135→    `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n   136→     VALUES ($1, $2, 'starter', $3, $3, $4, $4)\n   137→     ON CONFLICT (stripe_customer_id) DO UPDATE\n   138→     SET stripe_customer_id = EXCLUDED.stripe_customer_id\n   139→     RETURNING *`,\n   140→    [stripeCustomerId, email, TIER_HOURS.starter, TIER_ISOLATION_HOURS.starter]\n   141→  );\n   142→\n   143→  return result.rows[0];\n   144→}\n   145→\n   146→/**\n   147→ * Get user's current credit balance\n   148→ */\n   149→async function getBalance(stripeCustomerId) {\n   150→  const user = await getOrCreateUser(stripeCustomerId);\n   151→  const hoursTotal = parseFloat(user.hours_total);\n   152→  const hoursRemaining = parseFloat(user.hours_remaining);\n   153→  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n   154→  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n   155→\n   156→  // Tier display names\n   157→  const tierNames = {\n   158→    starter: 'Starter',\n   159→    pro: 'Pro',\n   160→    team: 'Team',\n   161→    cancelled: 'Cancelled'\n   162→  };\n   163→\n   164→  return {\n   165→    hoursRemaining,\n   166→    hoursTotal,\n   167→    isolationHoursRemaining,\n   168→    isolationHoursTotal,\n   169→    tier: user.tier,\n   170→    tierName: tierNames[user.tier] || user.tier,\n   171→    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n   172→    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n   173→    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n   174→    isolationOverageRate: ISOLATION_OVERAGE_RATE\n   175→  };\n   176→}\n   177→\n   178→/**\n   179→ * Check if user has enough credits for estimated duration\n   180→ */\n   181→async function hasCredits(stripeCustomerId, estimatedSeconds = 0) {\n   182→  const balance = await getBalance(stripeCustomerId);\n   183→  const estimatedHours = estimatedSeconds / 3600;\n   184→  return balance.hoursRemaining >= estimatedHours;\n   185→}\n   186→\n   187→/**\n   188→ * Reserve credits atomically using SELECT ... FOR UPDATE\n   189→ * This prevents race conditions where multiple requests pass credit check\n   190→ * before any deduction occurs.\n   191→ *\n   192→ * @param {string} stripeCustomerId - Customer ID\n   193→ * @param {number} estimatedSeconds - Estimated processing time in seconds\n   194→ * @param {string} endpoint - Endpoint name for tracking\n   195→ * @returns {Promise<{success: boolean, reservationId?: string, error?: string}>}\n   196→ */\n   197→async function reserveCredits(stripeCustomerId, estimatedSeconds = 0, endpoint = 'unknown') {\n   198→  const estimatedHours = estimatedSeconds / 3600;\n   199→  const client = await pool.connect();\n   200→\n   201→  try {\n   202→    await client.query('BEGIN');\n   203→\n   204→    // Lock the row for this customer - prevents concurrent modifications\n   205→    const result = await client.query(\n   206→      `SELECT id, hours_remaining FROM users\n   207→       WHERE stripe_customer_id = $1\n   208→       FOR UPDATE`,\n   209→      [stripeCustomerId]\n   210→    );\n   211→\n   212→    if (result.rows.length === 0) {\n   213→      // Create user if doesn't exist (within transaction)\n   214→      const newUser = await client.query(\n   215→        `INSERT INTO users (stripe_customer_id, tier, hours_remaining, hours_total)\n   216→         VALUES ($1, 'starter', $2, $2)\n   217→         RETURNING id, hours_remaining`,\n   218→        [stripeCustomerId, TIER_HOURS.starter]\n   219→      );\n   220→      result.rows = newUser.rows;\n   221→    }\n   222→\n   223→    const user = result.rows[0];\n   224→    const hoursRemaining = parseFloat(user.hours_remaining);\n   225→\n   226→    // Check if enough credits\n   227→    if (hoursRemaining < estimatedHours) {\n   228→      await client.query('ROLLBACK');\n   229→      return {\n   230→        success: false,\n   231→        error: 'Insufficient credits',\n   232→        hoursRemaining,\n   233→        hoursRequired: estimatedHours\n   234→      };\n   235→    }\n   236→\n   237→    // Generate unique reservation ID\n   238→    const reservationId = `res_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n   239→\n   240→    // Reserve credits by deducting the estimated amount\n   241→    await client.query(\n   242→      `UPDATE users\n   243→       SET hours_remaining = hours_remaining - $1,\n   244→           updated_at = CURRENT_TIMESTAMP\n   245→       WHERE stripe_customer_id = $2`,\n   246→      [estimatedHours, stripeCustomerId]\n   247→    );\n   248→\n   249→    // Log the reservation\n   250→    await client.query(\n   251→      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   252→       VALUES ($1, $2, $3, $4, $5)`,\n   253→      [user.id, stripeCustomerId, estimatedSeconds, estimatedHours, `${endpoint}:reserved`]\n   254→    );\n   255→\n   256→    await client.query('COMMIT');\n   257→\n   258→    console.log(`[UsageTracking] Reserved ${estimatedHours.toFixed(4)}hrs for ${stripeCustomerId} (${reservationId})`);\n   259→\n   260→    return {\n   261→      success: true,\n   262→      reservationId,\n   263→      hoursReserved: estimatedHours,\n   264→      hoursRemaining: hoursRemaining - estimatedHours\n   265→    };\n   266→  } catch (err) {\n   267→    await client.query('ROLLBACK');\n   268→    console.error('[UsageTracking] Reservation error:', err);\n   269→    throw err;\n   270→  } finally {\n   271→    client.release();\n   272→  }\n   273→}\n   274→\n   275→/**\n   276→ * Confirm a reservation (finalize usage) or adjust for actual duration\n   277→ * Optimized: uses RETURNING to avoid extra SELECT after UPDATE\n   278→ *\n   279→ * @param {string} stripeCustomerId - Customer ID\n   280→ * @param {number} actualSeconds - Actual processing time in seconds\n   281→ * @param {number} reservedSeconds - Originally reserved seconds\n   282→ * @param {string} endpoint - Endpoint name\n   283→ * @returns {Promise<object>} Updated balance\n   284→ */\n   285→async function confirmReservation(stripeCustomerId, actualSeconds, reservedSeconds, endpoint = 'unknown') {\n   286→  const actualHours = actualSeconds / 3600;\n   287→  const reservedHours = reservedSeconds / 3600;\n   288→  const difference = reservedHours - actualHours;\n   289→\n   290→  const client = await pool.connect();\n   291→  try {\n   292→    await client.query('BEGIN');\n   293→\n   294→    // Lock the row and get current state\n   295→    const result = await client.query(\n   296→      `SELECT id, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier\n   297→       FROM users\n   298→       WHERE stripe_customer_id = $1\n   299→       FOR UPDATE`,\n   300→      [stripeCustomerId]\n   301→    );\n   302→\n   303→    if (result.rows.length === 0) {\n   304→      await client.query('ROLLBACK');\n   305→      throw new Error('User not found');\n   306→    }\n   307→\n   308→    let user = result.rows[0];\n   309→\n   310→    // If actual usage was less than reserved, refund the difference\n   311→    if (difference > 0) {\n   312→      const updateResult = await client.query(\n   313→        `UPDATE users\n   314→         SET hours_remaining = hours_remaining + $1,\n   315→             updated_at = CURRENT_TIMESTAMP\n   316→         WHERE stripe_customer_id = $2\n   317→         RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n   318→        [difference, stripeCustomerId]\n   319→      );\n   320→      user = { ...user, ...updateResult.rows[0] };\n   321→\n   322→      // Log the adjustment\n   323→      await client.query(\n   324→        `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   325→         VALUES ($1, $2, $3, $4, $5)`,\n   326→        [user.id, stripeCustomerId, -(reservedSeconds - actualSeconds), -difference, `${endpoint}:adjusted`]\n   327→      );\n   328→    } else if (difference < 0) {\n   329→      // Actual usage was more than reserved - deduct the extra\n   330→      const updateResult = await client.query(\n   331→        `UPDATE users\n   332→         SET hours_remaining = GREATEST(0, hours_remaining - $1),\n   333→             updated_at = CURRENT_TIMESTAMP\n   334→         WHERE stripe_customer_id = $2\n   335→         RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n   336→        [-difference, stripeCustomerId]\n   337→      );\n   338→      user = { ...user, ...updateResult.rows[0] };\n   339→\n   340→      // Log the additional usage\n   341→      await client.query(\n   342→        `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   343→         VALUES ($1, $2, $3, $4, $5)`,\n   344→        [user.id, stripeCustomerId, actualSeconds - reservedSeconds, -difference, `${endpoint}:additional`]\n   345→      );\n   346→    }\n   347→\n   348→    await client.query('COMMIT');\n   349→\n   350→    console.log(`[UsageTracking] Confirmed ${actualHours.toFixed(4)}hrs (reserved: ${reservedHours.toFixed(4)}) for ${stripeCustomerId}`);\n   351→\n   352→    // Build balance from data we already have (no extra SELECT needed)\n   353→    const hoursTotal = parseFloat(user.hours_total);\n   354→    const hoursRemaining = parseFloat(user.hours_remaining);\n   355→    const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n   356→    const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n   357→\n   358→    const tierNames = {\n   359→      starter: 'Starter',\n   360→      pro: 'Pro',\n   361→      team: 'Team',\n   362→      cancelled: 'Cancelled'\n   363→    };\n   364→\n   365→    return {\n   366→      hoursRemaining,\n   367→      hoursTotal,\n   368→      isolationHoursRemaining,\n   369→      isolationHoursTotal,\n   370→      tier: user.tier,\n   371→      tierName: tierNames[user.tier] || user.tier,\n   372→      percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n   373→      isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n   374→      hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n   375→      isolationOverageRate: ISOLATION_OVERAGE_RATE\n   376→    };\n   377→  } catch (err) {\n   378→    await client.query('ROLLBACK');\n   379→    throw err;\n   380→  } finally {\n   381→    client.release();\n   382→  }\n   383→}\n   384→\n   385→/**\n   386→ * Release a reservation (refund credits on processing failure)\n   387→ * Optimized: uses RETURNING to avoid extra SELECT after UPDATE\n   388→ *\n   389→ * @param {string} stripeCustomerId - Customer ID\n   390→ * @param {number} reservedSeconds - Originally reserved seconds\n   391→ * @param {string} endpoint - Endpoint name\n   392→ * @param {string} reason - Reason for release\n   393→ * @returns {Promise<object>} Updated balance\n   394→ */\n   395→async function releaseReservation(stripeCustomerId, reservedSeconds, endpoint = 'unknown', reason = 'failed') {\n   396→  const reservedHours = reservedSeconds / 3600;\n   397→\n   398→  const client = await pool.connect();\n   399→  try {\n   400→    await client.query('BEGIN');\n   401→\n   402→    // Lock the row and get id for logging\n   403→    const result = await client.query(\n   404→      `SELECT id FROM users\n   405→       WHERE stripe_customer_id = $1\n   406→       FOR UPDATE`,\n   407→      [stripeCustomerId]\n   408→    );\n   409→\n   410→    if (result.rows.length === 0) {\n   411→      await client.query('ROLLBACK');\n   412→      throw new Error('User not found');\n   413→    }\n   414→\n   415→    const userId = result.rows[0].id;\n   416→\n   417→    // Refund the reserved amount with RETURNING\n   418→    const updateResult = await client.query(\n   419→      `UPDATE users\n   420→       SET hours_remaining = hours_remaining + $1,\n   421→           updated_at = CURRENT_TIMESTAMP\n   422→       WHERE stripe_customer_id = $2\n   423→       RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n   424→      [reservedHours, stripeCustomerId]\n   425→    );\n   426→\n   427→    const user = updateResult.rows[0];\n   428→\n   429→    // Log the refund\n   430→    await client.query(\n   431→      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   432→       VALUES ($1, $2, $3, $4, $5)`,\n   433→      [userId, stripeCustomerId, -reservedSeconds, -reservedHours, `${endpoint}:released:${reason}`]\n   434→    );\n   435→\n   436→    await client.query('COMMIT');\n   437→\n   438→    console.log(`[UsageTracking] Released ${reservedHours.toFixed(4)}hrs for ${stripeCustomerId} (${reason})`);\n   439→\n   440→    // Build balance from RETURNING data (no extra SELECT needed)\n   441→    const hoursTotal = parseFloat(user.hours_total);\n   442→    const hoursRemaining = parseFloat(user.hours_remaining);\n   443→    const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n   444→    const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n   445→\n   446→    const tierNames = {\n   447→      starter: 'Starter',\n   448→      pro: 'Pro',\n   449→      team: 'Team',\n   450→      cancelled: 'Cancelled'\n   451→    };\n   452→\n   453→    return {\n   454→      hoursRemaining,\n   455→      hoursTotal,\n   456→      isolationHoursRemaining,\n   457→      isolationHoursTotal,\n   458→      tier: user.tier,\n   459→      tierName: tierNames[user.tier] || user.tier,\n   460→      percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n   461→      isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n   462→      hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n   463→      isolationOverageRate: ISOLATION_OVERAGE_RATE\n   464→    };\n   465→  } catch (err) {\n   466→    await client.query('ROLLBACK');\n   467→    throw err;\n   468→  } finally {\n   469→    client.release();\n   470→  }\n   471→}\n   472→\n   473→/**\n   474→ * Check if user can use vocal isolation\n   475→ * Returns: { allowed, hasIncludedHours, overageRequired, overageCost }\n   476→ */\n   477→async function checkIsolationAccess(stripeCustomerId, estimatedMinutes = 0) {\n   478→  const balance = await getBalance(stripeCustomerId);\n   479→\n   480→  // Starter tier has no isolation access\n   481→  if (!balance.hasIsolationAccess) {\n   482→    return {\n   483→      allowed: false,\n   484→      reason: 'upgrade_required',\n   485→      message: 'Vocal isolation requires Pro or Team tier',\n   486→      hasIncludedHours: false,\n   487→      overageRequired: false,\n   488→      overageCost: 0\n   489→    };\n   490→  }\n   491→\n   492→  const estimatedHours = estimatedMinutes / 60;\n   493→  const remainingIncluded = balance.isolationHoursRemaining;\n   494→\n   495→  // Check if within included hours\n   496→  if (remainingIncluded >= estimatedHours) {\n   497→    return {\n   498→      allowed: true,\n   499→      reason: 'included',\n   500→      message: `Using ${estimatedMinutes.toFixed(1)} min of included isolation time`,\n   501→      hasIncludedHours: true,\n   502→      overageRequired: false,\n   503→      overageCost: 0,\n   504→      isolationHoursRemaining: remainingIncluded - estimatedHours\n   505→    };\n   506→  }\n   507→\n   508→  // Calculate overage\n   509→  const overageMinutes = estimatedMinutes - (remainingIncluded * 60);\n   510→  const overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n   511→\n   512→  return {\n   513→    allowed: true,\n   514→    reason: 'overage',\n   515→    message: `${remainingIncluded > 0 ? `Using ${(remainingIncluded * 60).toFixed(1)} included min + ` : ''}${overageMinutes.toFixed(1)} min overage ($${overageCost.toFixed(2)})`,\n   516→    hasIncludedHours: remainingIncluded > 0,\n   517→    overageRequired: true,\n   518→    overageMinutes,\n   519→    overageCost,\n   520→    isolationHoursRemaining: 0\n   521→  };\n   522→}\n   523→\n   524→/**\n   525→ * Deduct isolation usage from user's balance\n   526→ * Returns overage cost if beyond included hours\n   527→ */\n   528→async function deductIsolationUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'isolate-vocals') {\n   529→  const durationMinutes = audioDurationSeconds / 60;\n   530→  const durationHours = audioDurationSeconds / 3600;\n   531→\n   532→  const client = await pool.connect();\n   533→  try {\n   534→    // Get user\n   535→    const userResult = await client.query(\n   536→      'SELECT * FROM users WHERE stripe_customer_id = $1',\n   537→      [stripeCustomerId]\n   538→    );\n   539→\n   540→    if (userResult.rows.length === 0) {\n   541→      throw new Error('User not found');\n   542→    }\n   543→\n   544→    const user = userResult.rows[0];\n   545→    const remainingIncluded = parseFloat(user.isolation_hours_remaining || 0);\n   546→\n   547→    let overageCost = 0;\n   548→    let hoursFromIncluded = 0;\n   549→    let overageMinutes = 0;\n   550→\n   551→    if (remainingIncluded >= durationHours) {\n   552→      // Fully covered by included hours\n   553→      hoursFromIncluded = durationHours;\n   554→    } else {\n   555→      // Partial or full overage\n   556→      hoursFromIncluded = remainingIncluded;\n   557→      overageMinutes = durationMinutes - (remainingIncluded * 60);\n   558→      overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n   559→    }\n   560→\n   561→    // Update isolation balance\n   562→    await client.query(\n   563→      `UPDATE users\n   564→       SET isolation_hours_remaining = GREATEST(0, isolation_hours_remaining - $1),\n   565→           updated_at = CURRENT_TIMESTAMP\n   566→       WHERE stripe_customer_id = $2`,\n   567→      [durationHours, stripeCustomerId]\n   568→    );\n   569→\n   570→    // Log usage (mark as isolation)\n   571→    await client.query(\n   572→      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   573→       VALUES ($1, $2, $3, $4, $5)`,\n   574→      [user.id, stripeCustomerId, audioDurationSeconds, durationHours, endpoint]\n   575→    );\n   576→\n   577→    // Return updated balance and overage info\n   578→    const newBalance = await getBalance(stripeCustomerId);\n   579→    return {\n   580→      ...newBalance,\n   581→      isolationUsed: {\n   582→        durationMinutes,\n   583→        hoursFromIncluded,\n   584→        overageMinutes,\n   585→        overageCost\n   586→      }\n   587→    };\n   588→  } finally {\n   589→    client.release();\n   590→  }\n   591→}\n   592→\n   593→/**\n   594→ * Deduct usage from user's balance\n   595→ * Optimized: uses RETURNING to avoid extra SELECT after UPDATE\n   596→ */\n   597→async function deductUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'unknown') {\n   598→  const hoursUsed = audioDurationSeconds / 3600;\n   599→\n   600→  const client = await pool.connect();\n   601→  try {\n   602→    // Combined UPDATE with RETURNING - single query for update + get new values\n   603→    const updateResult = await client.query(\n   604→      `UPDATE users\n   605→       SET hours_remaining = GREATEST(0, hours_remaining - $1),\n   606→           updated_at = CURRENT_TIMESTAMP\n   607→       WHERE stripe_customer_id = $2\n   608→       RETURNING id, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n   609→      [hoursUsed, stripeCustomerId]\n   610→    );\n   611→\n   612→    if (updateResult.rows.length === 0) {\n   613→      throw new Error('User not found');\n   614→    }\n   615→\n   616→    const user = updateResult.rows[0];\n   617→\n   618→    // Log usage\n   619→    await client.query(\n   620→      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n   621→       VALUES ($1, $2, $3, $4, $5)`,\n   622→      [user.id, stripeCustomerId, audioDurationSeconds, hoursUsed, endpoint]\n   623→    );\n   624→\n   625→    // Build balance from RETURNING data (no extra SELECT needed)\n   626→    const hoursTotal = parseFloat(user.hours_total);\n   627→    const hoursRemaining = parseFloat(user.hours_remaining);\n   628→    const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n   629→    const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n   630→\n   631→    const tierNames = {\n   632→      starter: 'Starter',\n   633→      pro: 'Pro',\n   634→      team: 'Team',\n   635→      cancelled: 'Cancelled'\n   636→    };\n   637→\n   638→    return {\n   639→      hoursRemaining,\n   640→      hoursTotal,\n   641→      isolationHoursRemaining,\n   642→      isolationHoursTotal,\n   643→      tier: user.tier,\n   644→      tierName: tierNames[user.tier] || user.tier,\n   645→      percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n   646→      isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n   647→      hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n   648→      isolationOverageRate: ISOLATION_OVERAGE_RATE\n   649→    };\n   650→  } finally {\n   651→    client.release();\n   652→  }\n   653→}\n   654→\n   655→/**\n   656→ * Reset user's hours for new billing period\n   657→ * Optimized: uses RETURNING to avoid extra SELECT after UPDATE\n   658→ */\n   659→async function resetHours(stripeCustomerId, tier) {\n   660→  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n   661→  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n   662→\n   663→  const result = await pool.query(\n   664→    `UPDATE users\n   665→     SET hours_remaining = $1, hours_total = $1,\n   666→         isolation_hours_remaining = $2, isolation_hours_total = $2,\n   667→         tier = $3, updated_at = CURRENT_TIMESTAMP\n   668→     WHERE stripe_customer_id = $4\n   669→     RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n   670→    [hours, isolationHours, tier, stripeCustomerId]\n   671→  );\n   672→\n   673→  if (result.rows.length === 0) {\n   674→    // User doesn't exist, create and return default balance\n   675→    return await getBalance(stripeCustomerId);\n   676→  }\n   677→\n   678→  const user = result.rows[0];\n   679→  const hoursTotal = parseFloat(user.hours_total);\n   680→  const hoursRemaining = parseFloat(user.hours_remaining);\n   681→  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n   682→  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n   683→\n   684→  const tierNames = {\n   685→    starter: 'Starter',\n   686→    pro: 'Pro',\n   687→    team: 'Team',\n   688→    cancelled: 'Cancelled'\n   689→  };\n   690→\n   691→  return {\n   692→    hoursRemaining,\n   693→    hoursTotal,\n   694→    isolationHoursRemaining,\n   695→    isolationHoursTotal,\n   696→    tier: user.tier,\n   697→    tierName: tierNames[user.tier] || user.tier,\n   698→    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n   699→    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n   700→    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n   701→    isolationOverageRate: ISOLATION_OVERAGE_RATE\n   702→  };\n   703→}\n   704→\n   705→/**\n   706→ * Update user's tier (from Stripe webhook)\n   707→ * Optimized: uses RETURNING and INSERT...ON CONFLICT for upsert\n   708→ */\n   709→async function updateTier(stripeCustomerId, tier, email = null) {\n   710→  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n   711→  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n   712→\n   713→  // Use upsert (INSERT...ON CONFLICT) with RETURNING to handle create/update in one query\n   714→  const result = await pool.query(\n   715→    `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n   716→     VALUES ($1, $2, $3, $4, $4, $5, $5)\n   717→     ON CONFLICT (stripe_customer_id) DO UPDATE\n   718→     SET tier = $3, hours_remaining = $4, hours_total = $4,\n   719→         isolation_hours_remaining = $5, isolation_hours_total = $5,\n   720→         updated_at = CURRENT_TIMESTAMP\n   721→     RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n   722→    [stripeCustomerId, email, tier, hours, isolationHours]\n   723→  );\n   724→\n   725→  const user = result.rows[0];\n   726→  const hoursTotal = parseFloat(user.hours_total);\n   727→  const hoursRemaining = parseFloat(user.hours_remaining);\n   728→  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n   729→  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n   730→\n   731→  const tierNames = {\n   732→    starter: 'Starter',\n   733→    pro: 'Pro',\n   734→    team: 'Team',\n   735→    cancelled: 'Cancelled'\n   736→  };\n   737→\n   738→  return {\n   739→    hoursRemaining,\n   740→    hoursTotal,\n   741→    isolationHoursRemaining,\n   742→    isolationHoursTotal,\n   743→    tier: user.tier,\n   744→    tierName: tierNames[user.tier] || user.tier,\n   745→    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n   746→    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n   747→    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n   748→    isolationOverageRate: ISOLATION_OVERAGE_RATE\n   749→  };\n   750→}\n   751→\n   752→/**\n   753→ * Get usage history for a user\n   754→ */\n   755→async function getUsageHistory(stripeCustomerId, limit = 50) {\n   756→  const result = await pool.query(\n   757→    `SELECT * FROM usage_log\n   758→     WHERE stripe_customer_id = $1\n   759→     ORDER BY created_at DESC\n   760→     LIMIT $2`,\n   761→    [stripeCustomerId, limit]\n   762→  );\n   763→  return result.rows;\n   764→}\n   765→\n   766→/**\n   767→ * Check if webhook event has already been processed (idempotency)\n   768→ */\n   769→async function isEventProcessed(eventId) {\n   770→  const result = await pool.query(\n   771→    'SELECT id FROM webhook_events WHERE event_id = $1',\n   772→    [eventId]\n   773→  );\n   774→  return result.rows.length > 0;\n   775→}\n   776→\n   777→/**\n   778→ * Record a processed webhook event\n   779→ */\n   780→async function recordWebhookEvent(eventId, eventType) {\n   781→  try {\n   782→    await pool.query(\n   783→      'INSERT INTO webhook_events (event_id, event_type) VALUES ($1, $2) ON CONFLICT (event_id) DO NOTHING',\n   784→      [eventId, eventType]\n   785→    );\n   786→  } catch (err) {\n   787→    // Ignore duplicate key errors (race condition safety)\n   788→    if (err.code !== '23505') throw err;\n   789→  }\n   790→}\n   791→\n   792→module.exports = {\n   793→  initDatabase,\n   794→  getOrCreateUser,\n   795→  getBalance,\n   796→  hasCredits,\n   797→  reserveCredits,\n   798→  confirmReservation,\n   799→  releaseReservation,\n   800→  checkIsolationAccess,\n   801→  deductUsage,\n   802→  deductIsolationUsage,\n   803→  resetHours,\n   804→  updateTier,\n   805→  getUsageHistory,\n   806→  isEventProcessed,\n   807→  recordWebhookEvent,\n   808→  TIER_HOURS,\n   809→  TIER_ISOLATION_MINUTES,\n   810→  TIER_ISOLATION_HOURS,\n   811→  ISOLATION_OVERAGE_RATE\n   812→};\n   813→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"53f45da0-8b45-4826-9188-49ca7620332e","timestamp":"2025-12-26T07:40:06.614Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js","content":"/**\n * Usage Tracking Service\n *\n * Tracks user credit balances and usage for billing.\n * Uses PostgreSQL for persistence.\n */\n\nconst { Pool } = require('pg');\n\n// PostgreSQL connection pool with proper limits\nconst pool = new Pool({\n  connectionString: process.env.DATABASE_URL,\n  ssl: process.env.NODE_ENV === 'production' ? { rejectUnauthorized: false } : false,\n  // PERFORMANCE: Set connection pool limits\n  max: 20,                    // Maximum number of clients in the pool\n  idleTimeoutMillis: 30000,   // Close idle connections after 30 seconds\n  connectionTimeoutMillis: 5000  // Timeout after 5 seconds when getting connection\n});\n\n// Hours per tier (regular processing)\nconst TIER_HOURS = {\n  starter: 15,\n  pro: 50,\n  team: 150,\n  cancelled: 0\n};\n\n// Isolation minutes per tier (sized to maintain 80% margins)\nconst TIER_ISOLATION_MINUTES = {\n  starter: 0,     // No isolation access\n  pro: 45,        // 45 minutes included\n  team: 180,      // 3 hours included\n  cancelled: 0\n};\n\n// Convert to hours for database storage\nconst TIER_ISOLATION_HOURS = {\n  starter: 0,\n  pro: TIER_ISOLATION_MINUTES.pro / 60,      // 0.75 hours\n  team: TIER_ISOLATION_MINUTES.team / 60,    // 3 hours\n  cancelled: 0\n};\n\n// Overage rate for isolation beyond included minutes\nconst ISOLATION_OVERAGE_RATE = 0.10; // $0.10 per minute (85% margin)\n\n/**\n * Initialize database tables\n */\nasync function initDatabase() {\n  const client = await pool.connect();\n  try {\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS users (\n        id SERIAL PRIMARY KEY,\n        stripe_customer_id VARCHAR(255) UNIQUE NOT NULL,\n        email VARCHAR(255),\n        tier VARCHAR(50) DEFAULT 'starter',\n        hours_remaining DECIMAL(10,4) DEFAULT 15,\n        hours_total DECIMAL(10,4) DEFAULT 15,\n        isolation_hours_remaining DECIMAL(10,4) DEFAULT 0,\n        isolation_hours_total DECIMAL(10,4) DEFAULT 0,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n\n    // Add isolation columns if they don't exist (for existing databases)\n    await client.query(`\n      DO $$\n      BEGIN\n        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_remaining') THEN\n          ALTER TABLE users ADD COLUMN isolation_hours_remaining DECIMAL(10,4) DEFAULT 0;\n        END IF;\n        IF NOT EXISTS (SELECT 1 FROM information_schema.columns WHERE table_name = 'users' AND column_name = 'isolation_hours_total') THEN\n          ALTER TABLE users ADD COLUMN isolation_hours_total DECIMAL(10,4) DEFAULT 0;\n        END IF;\n      END $$;\n    `);\n\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS usage_log (\n        id SERIAL PRIMARY KEY,\n        user_id INTEGER REFERENCES users(id),\n        stripe_customer_id VARCHAR(255),\n        audio_duration_seconds DECIMAL(10,2),\n        hours_used DECIMAL(10,6),\n        endpoint VARCHAR(100),\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_users_stripe_customer_id ON users(stripe_customer_id)\n    `);\n\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_usage_log_user_id ON usage_log(user_id)\n    `);\n\n    // PERFORMANCE: Add composite index for efficient customer usage history queries\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_usage_log_customer_created\n      ON usage_log(stripe_customer_id, created_at DESC)\n    `);\n\n    // Webhook events table for idempotency\n    await client.query(`\n      CREATE TABLE IF NOT EXISTS webhook_events (\n        id SERIAL PRIMARY KEY,\n        event_id VARCHAR(255) UNIQUE NOT NULL,\n        event_type VARCHAR(100),\n        processed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n      )\n    `);\n\n    await client.query(`\n      CREATE INDEX IF NOT EXISTS idx_webhook_events_event_id ON webhook_events(event_id)\n    `);\n\n    console.log('[UsageTracking] Database initialized');\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Get or create a user by Stripe customer ID\n * PERF-005: Uses INSERT...ON CONFLICT upsert for single-query operation\n */\nasync function getOrCreateUser(stripeCustomerId, email = null) {\n  // Use upsert with ON CONFLICT DO UPDATE to ensure RETURNING works for both cases\n  // The update sets stripe_customer_id = EXCLUDED.stripe_customer_id (no-op) to trigger RETURNING\n  const result = await pool.query(\n    `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n     VALUES ($1, $2, 'starter', $3, $3, $4, $4)\n     ON CONFLICT (stripe_customer_id) DO UPDATE\n     SET stripe_customer_id = EXCLUDED.stripe_customer_id\n     RETURNING *`,\n    [stripeCustomerId, email, TIER_HOURS.starter, TIER_ISOLATION_HOURS.starter]\n  );\n\n  return result.rows[0];\n}\n\n/**\n * Get user's current credit balance\n */\nasync function getBalance(stripeCustomerId) {\n  const user = await getOrCreateUser(stripeCustomerId);\n  const hoursTotal = parseFloat(user.hours_total);\n  const hoursRemaining = parseFloat(user.hours_remaining);\n  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n\n  // Tier display names\n  const tierNames = {\n    starter: 'Starter',\n    pro: 'Pro',\n    team: 'Team',\n    cancelled: 'Cancelled'\n  };\n\n  return {\n    hoursRemaining,\n    hoursTotal,\n    isolationHoursRemaining,\n    isolationHoursTotal,\n    tier: user.tier,\n    tierName: tierNames[user.tier] || user.tier,\n    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n    isolationOverageRate: ISOLATION_OVERAGE_RATE\n  };\n}\n\n/**\n * Check if user has enough credits for estimated duration\n */\nasync function hasCredits(stripeCustomerId, estimatedSeconds = 0) {\n  const balance = await getBalance(stripeCustomerId);\n  const estimatedHours = estimatedSeconds / 3600;\n  return balance.hoursRemaining >= estimatedHours;\n}\n\n/**\n * Reserve credits atomically using SELECT ... FOR UPDATE\n * This prevents race conditions where multiple requests pass credit check\n * before any deduction occurs.\n *\n * @param {string} stripeCustomerId - Customer ID\n * @param {number} estimatedSeconds - Estimated processing time in seconds\n * @param {string} endpoint - Endpoint name for tracking\n * @returns {Promise<{success: boolean, reservationId?: string, error?: string}>}\n */\nasync function reserveCredits(stripeCustomerId, estimatedSeconds = 0, endpoint = 'unknown') {\n  const estimatedHours = estimatedSeconds / 3600;\n  const client = await pool.connect();\n\n  try {\n    await client.query('BEGIN');\n\n    // Lock the row for this customer - prevents concurrent modifications\n    const result = await client.query(\n      `SELECT id, hours_remaining FROM users\n       WHERE stripe_customer_id = $1\n       FOR UPDATE`,\n      [stripeCustomerId]\n    );\n\n    if (result.rows.length === 0) {\n      // Create user if doesn't exist (within transaction)\n      const newUser = await client.query(\n        `INSERT INTO users (stripe_customer_id, tier, hours_remaining, hours_total)\n         VALUES ($1, 'starter', $2, $2)\n         RETURNING id, hours_remaining`,\n        [stripeCustomerId, TIER_HOURS.starter]\n      );\n      result.rows = newUser.rows;\n    }\n\n    const user = result.rows[0];\n    const hoursRemaining = parseFloat(user.hours_remaining);\n\n    // Check if enough credits\n    if (hoursRemaining < estimatedHours) {\n      await client.query('ROLLBACK');\n      return {\n        success: false,\n        error: 'Insufficient credits',\n        hoursRemaining,\n        hoursRequired: estimatedHours\n      };\n    }\n\n    // Generate unique reservation ID\n    const reservationId = `res_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n\n    // Reserve credits by deducting the estimated amount\n    await client.query(\n      `UPDATE users\n       SET hours_remaining = hours_remaining - $1,\n           updated_at = CURRENT_TIMESTAMP\n       WHERE stripe_customer_id = $2`,\n      [estimatedHours, stripeCustomerId]\n    );\n\n    // Log the reservation\n    await client.query(\n      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n       VALUES ($1, $2, $3, $4, $5)`,\n      [user.id, stripeCustomerId, estimatedSeconds, estimatedHours, `${endpoint}:reserved`]\n    );\n\n    await client.query('COMMIT');\n\n    console.log(`[UsageTracking] Reserved ${estimatedHours.toFixed(4)}hrs for ${stripeCustomerId} (${reservationId})`);\n\n    return {\n      success: true,\n      reservationId,\n      hoursReserved: estimatedHours,\n      hoursRemaining: hoursRemaining - estimatedHours\n    };\n  } catch (err) {\n    await client.query('ROLLBACK');\n    console.error('[UsageTracking] Reservation error:', err);\n    throw err;\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Confirm a reservation (finalize usage) or adjust for actual duration\n * Optimized: uses RETURNING to avoid extra SELECT after UPDATE\n *\n * @param {string} stripeCustomerId - Customer ID\n * @param {number} actualSeconds - Actual processing time in seconds\n * @param {number} reservedSeconds - Originally reserved seconds\n * @param {string} endpoint - Endpoint name\n * @returns {Promise<object>} Updated balance\n */\nasync function confirmReservation(stripeCustomerId, actualSeconds, reservedSeconds, endpoint = 'unknown') {\n  const actualHours = actualSeconds / 3600;\n  const reservedHours = reservedSeconds / 3600;\n  const difference = reservedHours - actualHours;\n\n  const client = await pool.connect();\n  try {\n    await client.query('BEGIN');\n\n    // Lock the row and get current state\n    const result = await client.query(\n      `SELECT id, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier\n       FROM users\n       WHERE stripe_customer_id = $1\n       FOR UPDATE`,\n      [stripeCustomerId]\n    );\n\n    if (result.rows.length === 0) {\n      await client.query('ROLLBACK');\n      throw new Error('User not found');\n    }\n\n    let user = result.rows[0];\n\n    // If actual usage was less than reserved, refund the difference\n    if (difference > 0) {\n      const updateResult = await client.query(\n        `UPDATE users\n         SET hours_remaining = hours_remaining + $1,\n             updated_at = CURRENT_TIMESTAMP\n         WHERE stripe_customer_id = $2\n         RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n        [difference, stripeCustomerId]\n      );\n      user = { ...user, ...updateResult.rows[0] };\n\n      // Log the adjustment\n      await client.query(\n        `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n         VALUES ($1, $2, $3, $4, $5)`,\n        [user.id, stripeCustomerId, -(reservedSeconds - actualSeconds), -difference, `${endpoint}:adjusted`]\n      );\n    } else if (difference < 0) {\n      // Actual usage was more than reserved - deduct the extra\n      const updateResult = await client.query(\n        `UPDATE users\n         SET hours_remaining = GREATEST(0, hours_remaining - $1),\n             updated_at = CURRENT_TIMESTAMP\n         WHERE stripe_customer_id = $2\n         RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n        [-difference, stripeCustomerId]\n      );\n      user = { ...user, ...updateResult.rows[0] };\n\n      // Log the additional usage\n      await client.query(\n        `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n         VALUES ($1, $2, $3, $4, $5)`,\n        [user.id, stripeCustomerId, actualSeconds - reservedSeconds, -difference, `${endpoint}:additional`]\n      );\n    }\n\n    await client.query('COMMIT');\n\n    console.log(`[UsageTracking] Confirmed ${actualHours.toFixed(4)}hrs (reserved: ${reservedHours.toFixed(4)}) for ${stripeCustomerId}`);\n\n    // Build balance from data we already have (no extra SELECT needed)\n    const hoursTotal = parseFloat(user.hours_total);\n    const hoursRemaining = parseFloat(user.hours_remaining);\n    const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n    const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n\n    const tierNames = {\n      starter: 'Starter',\n      pro: 'Pro',\n      team: 'Team',\n      cancelled: 'Cancelled'\n    };\n\n    return {\n      hoursRemaining,\n      hoursTotal,\n      isolationHoursRemaining,\n      isolationHoursTotal,\n      tier: user.tier,\n      tierName: tierNames[user.tier] || user.tier,\n      percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n      isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n      hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n      isolationOverageRate: ISOLATION_OVERAGE_RATE\n    };\n  } catch (err) {\n    await client.query('ROLLBACK');\n    throw err;\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Release a reservation (refund credits on processing failure)\n * Optimized: uses RETURNING to avoid extra SELECT after UPDATE\n *\n * @param {string} stripeCustomerId - Customer ID\n * @param {number} reservedSeconds - Originally reserved seconds\n * @param {string} endpoint - Endpoint name\n * @param {string} reason - Reason for release\n * @returns {Promise<object>} Updated balance\n */\nasync function releaseReservation(stripeCustomerId, reservedSeconds, endpoint = 'unknown', reason = 'failed') {\n  const reservedHours = reservedSeconds / 3600;\n\n  const client = await pool.connect();\n  try {\n    await client.query('BEGIN');\n\n    // Lock the row and get id for logging\n    const result = await client.query(\n      `SELECT id FROM users\n       WHERE stripe_customer_id = $1\n       FOR UPDATE`,\n      [stripeCustomerId]\n    );\n\n    if (result.rows.length === 0) {\n      await client.query('ROLLBACK');\n      throw new Error('User not found');\n    }\n\n    const userId = result.rows[0].id;\n\n    // Refund the reserved amount with RETURNING\n    const updateResult = await client.query(\n      `UPDATE users\n       SET hours_remaining = hours_remaining + $1,\n           updated_at = CURRENT_TIMESTAMP\n       WHERE stripe_customer_id = $2\n       RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n      [reservedHours, stripeCustomerId]\n    );\n\n    const user = updateResult.rows[0];\n\n    // Log the refund\n    await client.query(\n      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n       VALUES ($1, $2, $3, $4, $5)`,\n      [userId, stripeCustomerId, -reservedSeconds, -reservedHours, `${endpoint}:released:${reason}`]\n    );\n\n    await client.query('COMMIT');\n\n    console.log(`[UsageTracking] Released ${reservedHours.toFixed(4)}hrs for ${stripeCustomerId} (${reason})`);\n\n    // Build balance from RETURNING data (no extra SELECT needed)\n    const hoursTotal = parseFloat(user.hours_total);\n    const hoursRemaining = parseFloat(user.hours_remaining);\n    const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n    const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n\n    const tierNames = {\n      starter: 'Starter',\n      pro: 'Pro',\n      team: 'Team',\n      cancelled: 'Cancelled'\n    };\n\n    return {\n      hoursRemaining,\n      hoursTotal,\n      isolationHoursRemaining,\n      isolationHoursTotal,\n      tier: user.tier,\n      tierName: tierNames[user.tier] || user.tier,\n      percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n      isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n      hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n      isolationOverageRate: ISOLATION_OVERAGE_RATE\n    };\n  } catch (err) {\n    await client.query('ROLLBACK');\n    throw err;\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Check if user can use vocal isolation\n * Returns: { allowed, hasIncludedHours, overageRequired, overageCost }\n */\nasync function checkIsolationAccess(stripeCustomerId, estimatedMinutes = 0) {\n  const balance = await getBalance(stripeCustomerId);\n\n  // Starter tier has no isolation access\n  if (!balance.hasIsolationAccess) {\n    return {\n      allowed: false,\n      reason: 'upgrade_required',\n      message: 'Vocal isolation requires Pro or Team tier',\n      hasIncludedHours: false,\n      overageRequired: false,\n      overageCost: 0\n    };\n  }\n\n  const estimatedHours = estimatedMinutes / 60;\n  const remainingIncluded = balance.isolationHoursRemaining;\n\n  // Check if within included hours\n  if (remainingIncluded >= estimatedHours) {\n    return {\n      allowed: true,\n      reason: 'included',\n      message: `Using ${estimatedMinutes.toFixed(1)} min of included isolation time`,\n      hasIncludedHours: true,\n      overageRequired: false,\n      overageCost: 0,\n      isolationHoursRemaining: remainingIncluded - estimatedHours\n    };\n  }\n\n  // Calculate overage\n  const overageMinutes = estimatedMinutes - (remainingIncluded * 60);\n  const overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n\n  return {\n    allowed: true,\n    reason: 'overage',\n    message: `${remainingIncluded > 0 ? `Using ${(remainingIncluded * 60).toFixed(1)} included min + ` : ''}${overageMinutes.toFixed(1)} min overage ($${overageCost.toFixed(2)})`,\n    hasIncludedHours: remainingIncluded > 0,\n    overageRequired: true,\n    overageMinutes,\n    overageCost,\n    isolationHoursRemaining: 0\n  };\n}\n\n/**\n * Deduct isolation usage from user's balance\n * Returns overage cost if beyond included hours\n */\nasync function deductIsolationUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'isolate-vocals') {\n  const durationMinutes = audioDurationSeconds / 60;\n  const durationHours = audioDurationSeconds / 3600;\n\n  const client = await pool.connect();\n  try {\n    // Get user\n    const userResult = await client.query(\n      'SELECT * FROM users WHERE stripe_customer_id = $1',\n      [stripeCustomerId]\n    );\n\n    if (userResult.rows.length === 0) {\n      throw new Error('User not found');\n    }\n\n    const user = userResult.rows[0];\n    const remainingIncluded = parseFloat(user.isolation_hours_remaining || 0);\n\n    let overageCost = 0;\n    let hoursFromIncluded = 0;\n    let overageMinutes = 0;\n\n    if (remainingIncluded >= durationHours) {\n      // Fully covered by included hours\n      hoursFromIncluded = durationHours;\n    } else {\n      // Partial or full overage\n      hoursFromIncluded = remainingIncluded;\n      overageMinutes = durationMinutes - (remainingIncluded * 60);\n      overageCost = overageMinutes * ISOLATION_OVERAGE_RATE;\n    }\n\n    // Update isolation balance\n    await client.query(\n      `UPDATE users\n       SET isolation_hours_remaining = GREATEST(0, isolation_hours_remaining - $1),\n           updated_at = CURRENT_TIMESTAMP\n       WHERE stripe_customer_id = $2`,\n      [durationHours, stripeCustomerId]\n    );\n\n    // Log usage (mark as isolation)\n    await client.query(\n      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n       VALUES ($1, $2, $3, $4, $5)`,\n      [user.id, stripeCustomerId, audioDurationSeconds, durationHours, endpoint]\n    );\n\n    // Return updated balance and overage info\n    const newBalance = await getBalance(stripeCustomerId);\n    return {\n      ...newBalance,\n      isolationUsed: {\n        durationMinutes,\n        hoursFromIncluded,\n        overageMinutes,\n        overageCost\n      }\n    };\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Deduct usage from user's balance\n * Optimized: uses RETURNING to avoid extra SELECT after UPDATE\n */\nasync function deductUsage(stripeCustomerId, audioDurationSeconds, endpoint = 'unknown') {\n  const hoursUsed = audioDurationSeconds / 3600;\n\n  const client = await pool.connect();\n  try {\n    // Combined UPDATE with RETURNING - single query for update + get new values\n    const updateResult = await client.query(\n      `UPDATE users\n       SET hours_remaining = GREATEST(0, hours_remaining - $1),\n           updated_at = CURRENT_TIMESTAMP\n       WHERE stripe_customer_id = $2\n       RETURNING id, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n      [hoursUsed, stripeCustomerId]\n    );\n\n    if (updateResult.rows.length === 0) {\n      throw new Error('User not found');\n    }\n\n    const user = updateResult.rows[0];\n\n    // Log usage\n    await client.query(\n      `INSERT INTO usage_log (user_id, stripe_customer_id, audio_duration_seconds, hours_used, endpoint)\n       VALUES ($1, $2, $3, $4, $5)`,\n      [user.id, stripeCustomerId, audioDurationSeconds, hoursUsed, endpoint]\n    );\n\n    // Build balance from RETURNING data (no extra SELECT needed)\n    const hoursTotal = parseFloat(user.hours_total);\n    const hoursRemaining = parseFloat(user.hours_remaining);\n    const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n    const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n\n    const tierNames = {\n      starter: 'Starter',\n      pro: 'Pro',\n      team: 'Team',\n      cancelled: 'Cancelled'\n    };\n\n    return {\n      hoursRemaining,\n      hoursTotal,\n      isolationHoursRemaining,\n      isolationHoursTotal,\n      tier: user.tier,\n      tierName: tierNames[user.tier] || user.tier,\n      percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n      isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n      hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n      isolationOverageRate: ISOLATION_OVERAGE_RATE\n    };\n  } finally {\n    client.release();\n  }\n}\n\n/**\n * Reset user's hours for new billing period\n * Optimized: uses RETURNING to avoid extra SELECT after UPDATE\n */\nasync function resetHours(stripeCustomerId, tier) {\n  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n\n  const result = await pool.query(\n    `UPDATE users\n     SET hours_remaining = $1, hours_total = $1,\n         isolation_hours_remaining = $2, isolation_hours_total = $2,\n         tier = $3, updated_at = CURRENT_TIMESTAMP\n     WHERE stripe_customer_id = $4\n     RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n    [hours, isolationHours, tier, stripeCustomerId]\n  );\n\n  if (result.rows.length === 0) {\n    // User doesn't exist, create and return default balance\n    return await getBalance(stripeCustomerId);\n  }\n\n  const user = result.rows[0];\n  const hoursTotal = parseFloat(user.hours_total);\n  const hoursRemaining = parseFloat(user.hours_remaining);\n  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n\n  const tierNames = {\n    starter: 'Starter',\n    pro: 'Pro',\n    team: 'Team',\n    cancelled: 'Cancelled'\n  };\n\n  return {\n    hoursRemaining,\n    hoursTotal,\n    isolationHoursRemaining,\n    isolationHoursTotal,\n    tier: user.tier,\n    tierName: tierNames[user.tier] || user.tier,\n    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n    isolationOverageRate: ISOLATION_OVERAGE_RATE\n  };\n}\n\n/**\n * Update user's tier (from Stripe webhook)\n * Optimized: uses RETURNING and INSERT...ON CONFLICT for upsert\n */\nasync function updateTier(stripeCustomerId, tier, email = null) {\n  const hours = tier in TIER_HOURS ? TIER_HOURS[tier] : TIER_HOURS.starter;\n  const isolationHours = tier in TIER_ISOLATION_HOURS ? TIER_ISOLATION_HOURS[tier] : 0;\n\n  // Use upsert (INSERT...ON CONFLICT) with RETURNING to handle create/update in one query\n  const result = await pool.query(\n    `INSERT INTO users (stripe_customer_id, email, tier, hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total)\n     VALUES ($1, $2, $3, $4, $4, $5, $5)\n     ON CONFLICT (stripe_customer_id) DO UPDATE\n     SET tier = $3, hours_remaining = $4, hours_total = $4,\n         isolation_hours_remaining = $5, isolation_hours_total = $5,\n         updated_at = CURRENT_TIMESTAMP\n     RETURNING hours_remaining, hours_total, isolation_hours_remaining, isolation_hours_total, tier`,\n    [stripeCustomerId, email, tier, hours, isolationHours]\n  );\n\n  const user = result.rows[0];\n  const hoursTotal = parseFloat(user.hours_total);\n  const hoursRemaining = parseFloat(user.hours_remaining);\n  const isolationHoursTotal = parseFloat(user.isolation_hours_total || 0);\n  const isolationHoursRemaining = parseFloat(user.isolation_hours_remaining || 0);\n\n  const tierNames = {\n    starter: 'Starter',\n    pro: 'Pro',\n    team: 'Team',\n    cancelled: 'Cancelled'\n  };\n\n  return {\n    hoursRemaining,\n    hoursTotal,\n    isolationHoursRemaining,\n    isolationHoursTotal,\n    tier: user.tier,\n    tierName: tierNames[user.tier] || user.tier,\n    percentUsed: hoursTotal > 0 ? ((hoursTotal - hoursRemaining) / hoursTotal * 100).toFixed(1) : '0.0',\n    isolationPercentUsed: isolationHoursTotal > 0 ? ((isolationHoursTotal - isolationHoursRemaining) / isolationHoursTotal * 100).toFixed(1) : '0.0',\n    hasIsolationAccess: TIER_ISOLATION_HOURS[user.tier] > 0,\n    isolationOverageRate: ISOLATION_OVERAGE_RATE\n  };\n}\n\n/**\n * Get usage history for a user\n */\nasync function getUsageHistory(stripeCustomerId, limit = 50) {\n  const result = await pool.query(\n    `SELECT * FROM usage_log\n     WHERE stripe_customer_id = $1\n     ORDER BY created_at DESC\n     LIMIT $2`,\n    [stripeCustomerId, limit]\n  );\n  return result.rows;\n}\n\n/**\n * Check if webhook event has already been processed (idempotency)\n */\nasync function isEventProcessed(eventId) {\n  const result = await pool.query(\n    'SELECT id FROM webhook_events WHERE event_id = $1',\n    [eventId]\n  );\n  return result.rows.length > 0;\n}\n\n/**\n * Record a processed webhook event\n */\nasync function recordWebhookEvent(eventId, eventType) {\n  try {\n    await pool.query(\n      'INSERT INTO webhook_events (event_id, event_type) VALUES ($1, $2) ON CONFLICT (event_id) DO NOTHING',\n      [eventId, eventType]\n    );\n  } catch (err) {\n    // Ignore duplicate key errors (race condition safety)\n    if (err.code !== '23505') throw err;\n  }\n}\n\nmodule.exports = {\n  initDatabase,\n  getOrCreateUser,\n  getBalance,\n  hasCredits,\n  reserveCredits,\n  confirmReservation,\n  releaseReservation,\n  checkIsolationAccess,\n  deductUsage,\n  deductIsolationUsage,\n  resetHours,\n  updateTier,\n  getUsageHistory,\n  isEventProcessed,\n  recordWebhookEvent,\n  TIER_HOURS,\n  TIER_ISOLATION_MINUTES,\n  TIER_ISOLATION_HOURS,\n  ISOLATION_OVERAGE_RATE\n};\n","numLines":813,"startLine":1,"totalLines":813}}}
{"parentUuid":"53f45da0-8b45-4826-9188-49ca7620332e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_019KCjWHxmGegLoXEBmRXHH1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01PocWsgHywWMfaYkZHpDtHw","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/profanityDetection.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":17844,"cache_creation_input_tokens":19034,"cache_read_input_tokens":25024,"cache_creation":{"ephemeral_5m_input_tokens":19034,"ephemeral_1h_input_tokens":0},"output_tokens":22,"service_tier":"standard"}},"requestId":"req_011CWUvHfeobGytusQyu64nh","type":"assistant","uuid":"f194a75d-097a-4cc2-9bb4-da544a273909","timestamp":"2025-12-26T07:40:12.559Z"}
{"parentUuid":"f194a75d-097a-4cc2-9bb4-da544a273909","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_019KCjWHxmGegLoXEBmRXHH1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Pq49qWiwWkfb54QkHzD65k","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/repetitionDetection.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":17844,"cache_creation_input_tokens":19034,"cache_read_input_tokens":25024,"cache_creation":{"ephemeral_5m_input_tokens":19034,"ephemeral_1h_input_tokens":0},"output_tokens":22,"service_tier":"standard"}},"requestId":"req_011CWUvHfeobGytusQyu64nh","type":"assistant","uuid":"6ef23f14-d876-4c0c-adaa-f22067d3f1dd","timestamp":"2025-12-26T07:40:13.309Z"}
{"parentUuid":"6ef23f14-d876-4c0c-adaa-f22067d3f1dd","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_019KCjWHxmGegLoXEBmRXHH1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_012voLeb6J3qaWZGKouutDEP","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":17844,"cache_creation_input_tokens":19034,"cache_read_input_tokens":25024,"cache_creation":{"ephemeral_5m_input_tokens":19034,"ephemeral_1h_input_tokens":0},"output_tokens":22,"service_tier":"standard"}},"requestId":"req_011CWUvHfeobGytusQyu64nh","type":"assistant","uuid":"3696f8ed-1ae3-42d1-80a3-39a5d41bf0b8","timestamp":"2025-12-26T07:40:14.063Z"}
{"parentUuid":"3696f8ed-1ae3-42d1-80a3-39a5d41bf0b8","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_019KCjWHxmGegLoXEBmRXHH1","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Vp6dYJznR83tGsL3TNkxVC","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/cutListGenerator.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":17844,"cache_creation_input_tokens":19034,"cache_read_input_tokens":25024,"cache_creation":{"ephemeral_5m_input_tokens":19034,"ephemeral_1h_input_tokens":0},"output_tokens":247,"service_tier":"standard"}},"requestId":"req_011CWUvHfeobGytusQyu64nh","type":"assistant","uuid":"cb2f3404-952e-4dd7-a14e-7a74a196507f","timestamp":"2025-12-26T07:40:14.842Z"}
{"parentUuid":"cb2f3404-952e-4dd7-a14e-7a74a196507f","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01PocWsgHywWMfaYkZHpDtHw","type":"tool_result","content":"     1→/**\n     2→ * Profanity Detection Service\n     3→ *\n     4→ * Detects profanity in transcripts and returns segments for muting/bleeping.\n     5→ * Based on Fireside's proven approach with enhancements.\n     6→ *\n     7→ * Features:\n     8→ * - Word-level profanity detection from transcript\n     9→ * - Custom blocklist (add words)\n    10→ * - Custom allowlist (exclude words)\n    11→ * - Contiguous segment grouping\n    12→ * - Frame-rate aligned boundaries\n    13→ * - Multiple language support\n    14→ */\n    15→\n    16→const path = require('path');\n    17→const fs = require('fs');\n    18→const crypto = require('crypto');\n    19→\n    20→// =============================================================================\n    21→// Default Profanity Lists\n    22→// =============================================================================\n    23→\n    24→/**\n    25→ * Default English profanity list\n    26→ * This is a comprehensive list that can be extended via custom blocklist\n    27→ */\n    28→const DEFAULT_PROFANITY_EN = [\n    29→  // Common expletives\n    30→  'fuck', 'fucking', 'fucked', 'fucker', 'fuckers', 'fucks', 'motherfucker', 'motherfucking',\n    31→  'shit', 'shitty', 'shitting', 'bullshit', 'horseshit', 'dipshit', 'shits',\n    32→  'damn', 'damned', 'goddamn', 'goddamned',\n    33→  'ass', 'asshole', 'assholes', 'asses', 'dumbass', 'jackass', 'badass',\n    34→  'bitch', 'bitches', 'bitchy', 'bitching', 'son of a bitch',\n    35→  'bastard', 'bastards',\n    36→  'crap', 'crappy',\n    37→  'hell', 'hellish',\n    38→  'piss', 'pissed', 'pissing',\n    39→\n    40→  // Slurs and offensive terms (heavily censored in most media)\n    41→  'cunt', 'cunts',\n    42→  'cock', 'cocks', 'cocksucker',\n    43→  'dick', 'dicks', 'dickhead',\n    44→  'pussy', 'pussies',\n    45→  'whore', 'whores',\n    46→  'slut', 'sluts',\n    47→  'fag', 'faggot', 'faggots',\n    48→  'retard', 'retarded', 'retards',\n    49→\n    50→  // Religious profanity\n    51→  'jesus christ', 'christ', 'god damn', 'oh my god',\n    52→\n    53→  // Racial slurs (abbreviated - these should always be censored)\n    54→  // Note: Full list would include more but keeping brief for code\n    55→\n    56→  // Sexual terms\n    57→  'dildo', 'dildos',\n    58→  'blowjob', 'handjob',\n    59→  'jerk off', 'jerking off',\n    60→  'wanker', 'wanking',\n    61→\n    62→  // Drug references (optional, uncomment if needed)\n    63→  // 'meth', 'cocaine', 'heroin',\n    64→\n    65→  // Common misspellings/slang variants\n    66→  'fck', 'fuk', 'phuck', 'fxck',\n    67→  'sht', 'sh1t', 's**t',\n    68→  'btch', 'b1tch',\n    69→  'azz', 'a$$',\n    70→  'cnt',\n    71→  'dck',\n    72→\n    73→  // Common euphemisms that sound like profanity\n    74→  'frickin', 'freakin', 'friggin', 'effing',\n    75→];\n    76→\n    77→/**\n    78→ * Words that contain profanity substrings but are NOT profane\n    79→ * Used to prevent false positives in partial matching\n    80→ */\n    81→const FALSE_POSITIVE_ALLOWLIST = [\n    82→  // Contains \"hell\"\n    83→  'hello', 'shell', 'shellfish', 'shelling', 'michelle', 'mitchell', 'rochelle',\n    84→  'seashell', 'eggshell', 'nutshell', 'bombshell', 'dumbbell', 'doorbell',\n    85→  'othello', 'cello', 'jello', 'bellow', 'fellow', 'mellow', 'yellow',\n    86→\n    87→  // Contains \"ass\"\n    88→  'class', 'pass', 'mass', 'grass', 'glass', 'brass', 'bass', 'sass',\n    89→  'classic', 'passage', 'passenger', 'passive', 'passion', 'passionate',\n    90→  'assassin', 'ambassador', 'embassy', 'harass', 'harassment', 'embarrass',\n    91→  'compass', 'bypass', 'surpass', 'trespass', 'amass', 'morass',\n    92→  'cassette', 'casserole', 'cassandra', 'lasso', 'molasses',\n    93→  'massachusetts', 'tallahassee', 'hassle', 'tassel', 'vessel',\n    94→\n    95→  // Contains \"damn\"\n    96→  'amsterdam', 'goddamn', // goddamn is profanity, but amsterdam isn't\n    97→\n    98→  // Contains \"crap\"\n    99→  'scrap', 'scrape', 'scrappy',\n   100→\n   101→  // Contains \"cock\"\n   102→  'peacock', 'hancock', 'cockpit', 'cocktail', 'cockatoo', 'cockatiel',\n   103→  'shuttlecock', 'stopcock', 'weathercock', 'woodcock',\n   104→\n   105→  // Contains \"dick\"\n   106→  'dickens', 'benedict', 'addicted', 'predicted', 'verdict',\n   107→  'contradiction', 'jurisdiction', 'index',\n   108→\n   109→  // Contains \"piss\"\n   110→  'mississippi',\n   111→\n   112→  // Contains \"tit\"\n   113→  'title', 'titanic', 'constitution', 'institution', 'competition',\n   114→  'repetition', 'partition', 'petition', 'nutrition', 'intuition',\n   115→  'appetite', 'quantity', 'entity', 'identity', 'activity',\n   116→\n   117→  // Contains \"cum\"\n   118→  'document', 'documentary', 'accumulate', 'circumference', 'cucumber',\n   119→  'circumstance', 'incumbent', 'encumber', 'succumb', 'vacuum',\n   120→\n   121→  // Contains \"fag\"\n   122→  'faggot', // this IS profanity, keeping it out\n   123→\n   124→  // Contains \"ho\"\n   125→  'house', 'home', 'honest', 'honor', 'hope', 'hour', 'holiday',\n   126→  'whole', 'whose', 'show', 'shoe', 'shop', 'short', 'shot',\n   127→\n   128→  // Contains \"nig\" - be careful here\n   129→  'night', 'knight', 'nightmare', 'nightclub', 'ignite', 'dignity',\n   130→  'significant', 'indignant', 'benign', 'malignant',\n   131→];\n   132→\n   133→/**\n   134→ * Default Spanish profanity list\n   135→ */\n   136→const DEFAULT_PROFANITY_ES = [\n   137→  'mierda', 'puta', 'puto', 'joder', 'coño', 'cojones', 'cabron', 'cabrón',\n   138→  'pendejo', 'pendeja', 'chingar', 'chingada', 'verga', 'culo', 'carajo',\n   139→  'marica', 'maricón', 'hostia', 'gilipollas', 'capullo', 'hijo de puta'\n   140→];\n   141→\n   142→/**\n   143→ * Default French profanity list\n   144→ */\n   145→const DEFAULT_PROFANITY_FR = [\n   146→  'merde', 'putain', 'bordel', 'salaud', 'salope', 'connard', 'connasse',\n   147→  'enculé', 'nique', 'bite', 'couille', 'foutre', 'baiser', 'chier', 'con'\n   148→];\n   149→\n   150→/**\n   151→ * Default German profanity list\n   152→ */\n   153→const DEFAULT_PROFANITY_DE = [\n   154→  'scheiße', 'scheisse', 'arschloch', 'hurensohn', 'wichser', 'fotze',\n   155→  'fick', 'ficken', 'verdammt', 'schlampe', 'hure', 'mistkerl'\n   156→];\n   157→\n   158→// Language mapping\n   159→const PROFANITY_LISTS = {\n   160→  'en': DEFAULT_PROFANITY_EN,\n   161→  'es': DEFAULT_PROFANITY_ES,\n   162→  'fr': DEFAULT_PROFANITY_FR,\n   163→  'de': DEFAULT_PROFANITY_DE\n   164→};\n   165→\n   166→// =============================================================================\n   167→// Core Detection\n   168→// =============================================================================\n   169→\n   170→/**\n   171→ * Detect profanity in transcript words\n   172→ *\n   173→ * @param {Object} transcript - Transcript with words array\n   174→ * @param {Object} options - Detection options\n   175→ * @returns {Object} Detection results with marked words and segments\n   176→ */\n   177→function detectProfanity(transcript, options = {}) {\n   178→  const {\n   179→    language = 'en',\n   180→    customBlocklist = [],\n   181→    customAllowlist = [],\n   182→    caseSensitive = false,\n   183→    includePartialMatches = true,\n   184→    frameRate = 30  // For frame alignment\n   185→  } = options;\n   186→\n   187→  // Get base profanity list for language\n   188→  const baseProfanity = PROFANITY_LISTS[language] || PROFANITY_LISTS['en'];\n   189→\n   190→  // Build final profanity set\n   191→  const profanitySet = new Set([\n   192→    ...baseProfanity,\n   193→    ...customBlocklist.map(w => caseSensitive ? w : w.toLowerCase())\n   194→  ]);\n   195→\n   196→  // Remove allowlist items\n   197→  const allowlistSet = new Set(customAllowlist.map(w => caseSensitive ? w : w.toLowerCase()));\n   198→  for (const allowed of allowlistSet) {\n   199→    profanitySet.delete(allowed);\n   200→  }\n   201→\n   202→  // Also add common variants (e.g., \"fucking\" -> \"fuckin'\")\n   203→  const variants = new Set();\n   204→  for (const word of profanitySet) {\n   205→    if (word.endsWith('ing')) {\n   206→      variants.add(word.slice(0, -1)); // fuckin\n   207→      variants.add(word.slice(0, -3) + \"in'\"); // fuckin'\n   208→    }\n   209→    if (word.endsWith('ed')) {\n   210→      variants.add(word.slice(0, -2) + \"'d\"); // fuck'd\n   211→    }\n   212→  }\n   213→  for (const v of variants) {\n   214→    profanitySet.add(v);\n   215→  }\n   216→\n   217→  // Check each word in transcript\n   218→  const words = transcript.words || [];\n   219→  const markedWords = [];\n   220→\n   221→  // Create false positive set once for performance\n   222→  const falsePositiveSet = new Set(FALSE_POSITIVE_ALLOWLIST);\n   223→\n   224→  for (let i = 0; i < words.length; i++) {\n   225→    const word = words[i];\n   226→    const wordText = word.word || word.text || '';\n   227→    const normalizedWord = caseSensitive ? wordText : wordText.toLowerCase();\n   228→\n   229→    // Remove punctuation for matching\n   230→    const cleanWord = normalizedWord.replace(/[^\\w\\s'-]/g, '').trim();\n   231→\n   232→    // Check for exact match\n   233→    let isProfanity = profanitySet.has(cleanWord);\n   234→\n   235→    // Check for partial matches (word contains profanity)\n   236→    // But skip words that are known false positives\n   237→    if (!isProfanity && includePartialMatches) {\n   238→      // Skip if word is in the false positive allowlist\n   239→      if (!falsePositiveSet.has(cleanWord)) {\n   240→        for (const profane of profanitySet) {\n   241→          if (cleanWord.includes(profane) && profane.length >= 4) {\n   242→            isProfanity = true;\n   243→            break;\n   244→          }\n   245→        }\n   246→      }\n   247→    }\n   248→\n   249→    markedWords.push({\n   250→      text: wordText,\n   251→      start: word.start,\n   252→      end: word.end,\n   253→      index: i,\n   254→      isProfanity,\n   255→      // Can be toggled by user in UI\n   256→      shouldCensor: isProfanity\n   257→    });\n   258→  }\n   259→\n   260→  // Group contiguous profanity into segments\n   261→  const segments = groupProfanitySegments(markedWords, { frameRate });\n   262→\n   263→  // Calculate statistics\n   264→  const profanityCount = markedWords.filter(w => w.isProfanity).length;\n   265→  const totalDuration = segments.reduce((sum, s) => sum + s.duration, 0);\n   266→\n   267→  console.log(`[SPLICE Profanity] Detected ${profanityCount} profane word(s) in ${segments.length} segment(s)`);\n   268→\n   269→  return {\n   270→    words: markedWords,\n   271→    segments,\n   272→    metadata: {\n   273→      totalWords: words.length,\n   274→      profanityCount,\n   275→      segmentCount: segments.length,\n   276→      totalCensorDuration: parseFloat(totalDuration.toFixed(3)),\n   277→      language,\n   278→      customBlocklistCount: customBlocklist.length,\n   279→      customAllowlistCount: customAllowlist.length\n   280→    }\n   281→  };\n   282→}\n   283→\n   284→/**\n   285→ * Group contiguous profanity words into segments\n   286→ *\n   287→ * @param {Array} markedWords - Array of words with isProfanity flag\n   288→ * @param {Object} options - Grouping options\n   289→ * @returns {Array} Array of profanity segments\n   290→ */\n   291→function groupProfanitySegments(markedWords, options = {}) {\n   292→  const { frameRate = 30, maxGap = 0.3 } = options;\n   293→\n   294→  const segments = [];\n   295→  let currentSegment = null;\n   296→\n   297→  for (const word of markedWords) {\n   298→    if (!word.isProfanity) {\n   299→      // End current segment if exists\n   300→      if (currentSegment) {\n   301→        segments.push(finalizeSeg(currentSegment, frameRate));\n   302→        currentSegment = null;\n   303→      }\n   304→      continue;\n   305→    }\n   306→\n   307→    if (!currentSegment) {\n   308→      // Start new segment\n   309→      currentSegment = {\n   310→        start: word.start,\n   311→        end: word.end,\n   312→        words: [word.text],\n   313→        wordIndices: [word.index]\n   314→      };\n   315→    } else if (word.start - currentSegment.end <= maxGap) {\n   316→      // Extend current segment (words are close together)\n   317→      currentSegment.end = word.end;\n   318→      currentSegment.words.push(word.text);\n   319→      currentSegment.wordIndices.push(word.index);\n   320→    } else {\n   321→      // Gap too large - finalize current and start new\n   322→      segments.push(finalizeSeg(currentSegment, frameRate));\n   323→      currentSegment = {\n   324→        start: word.start,\n   325→        end: word.end,\n   326→        words: [word.text],\n   327→        wordIndices: [word.index]\n   328→      };\n   329→    }\n   330→  }\n   331→\n   332→  // Finalize last segment\n   333→  if (currentSegment) {\n   334→    segments.push(finalizeSeg(currentSegment, frameRate));\n   335→  }\n   336→\n   337→  return segments;\n   338→}\n   339→\n   340→/**\n   341→ * Finalize a segment with frame-aligned boundaries\n   342→ */\n   343→function finalizeSeg(segment, frameRate) {\n   344→  // Add small padding around profanity (50ms before, 100ms after)\n   345→  const paddedStart = Math.max(0, segment.start - 0.05);\n   346→  const paddedEnd = segment.end + 0.1;\n   347→\n   348→  // Align to frame boundaries\n   349→  const alignedStart = alignToFrame(paddedStart, frameRate, 'floor');\n   350→  const alignedEnd = alignToFrame(paddedEnd, frameRate, 'ceil');\n   351→\n   352→  return {\n   353→    start: parseFloat(alignedStart.toFixed(4)),\n   354→    end: parseFloat(alignedEnd.toFixed(4)),\n   355→    duration: parseFloat((alignedEnd - alignedStart).toFixed(4)),\n   356→    words: segment.words,\n   357→    wordCount: segment.words.length,\n   358→    text: segment.words.join(' ')\n   359→  };\n   360→}\n   361→\n   362→/**\n   363→ * Align time to frame boundary\n   364→ */\n   365→function alignToFrame(time, frameRate, mode = 'round') {\n   366→  const frameDuration = 1 / frameRate;\n   367→  switch (mode) {\n   368→    case 'floor':\n   369→      return Math.floor(time / frameDuration) * frameDuration;\n   370→    case 'ceil':\n   371→      return Math.ceil(time / frameDuration) * frameDuration;\n   372→    default:\n   373→      return Math.round(time / frameDuration) * frameDuration;\n   374→  }\n   375→}\n   376→\n   377→// =============================================================================\n   378→// Bleep Sound Management\n   379→// =============================================================================\n   380→\n   381→/**\n   382→ * Available bleep sounds\n   383→ */\n   384→const BLEEP_SOUNDS = {\n   385→  'standard': {\n   386→    name: 'Standard Censor Beep',\n   387→    frequency: 1000,\n   388→    description: 'Classic 1kHz censor beep'\n   389→  },\n   390→  'tv': {\n   391→    name: 'TV Bleep',\n   392→    frequency: 1000,\n   393→    description: 'Television-style censor beep'\n   394→  },\n   395→  'radio': {\n   396→    name: 'Radio Bleep',\n   397→    frequency: 880,\n   398→    description: 'Radio broadcast censor tone'\n   399→  },\n   400→  'duck': {\n   401→    name: 'Audio Duck',\n   402→    frequency: 0,\n   403→    description: 'Volume reduction (no beep)'\n   404→  },\n   405→  'mute': {\n   406→    name: 'Complete Mute',\n   407→    frequency: 0,\n   408→    description: 'Complete silence'\n   409→  }\n   410→};\n   411→\n   412→/**\n   413→ * Get list of available bleep sounds\n   414→ */\n   415→function getAvailableBleepSounds() {\n   416→  return Object.entries(BLEEP_SOUNDS).map(([id, info]) => ({\n   417→    id,\n   418→    ...info\n   419→  }));\n   420→}\n   421→\n   422→// =============================================================================\n   423→// Bleep Audio Generation\n   424→// =============================================================================\n   425→\n   426→/**\n   427→ * Generate a bleep/censor tone audio file\n   428→ *\n   429→ * @param {Object} options - Generation options\n   430→ * @param {number} options.duration - Duration in seconds\n   431→ * @param {string} options.bleepType - Type of bleep ('standard', 'tv', 'radio', 'duck', 'mute')\n   432→ * @param {number} options.sampleRate - Sample rate (default: 44100)\n   433→ * @param {number} options.volume - Volume level 0-1 (default: 0.5)\n   434→ * @param {string} options.outputPath - Optional output path (generates temp file if not provided)\n   435→ * @returns {Promise<Object>} Result with path to generated file\n   436→ */\n   437→async function generateBleepAudio(options = {}) {\n   438→  const {\n   439→    duration = 1.0,\n   440→    bleepType = 'standard',\n   441→    sampleRate = 44100,\n   442→    volume = 0.5,\n   443→    outputPath = null\n   444→  } = options;\n   445→\n   446→  const bleepConfig = BLEEP_SOUNDS[bleepType] || BLEEP_SOUNDS.standard;\n   447→  const frequency = bleepConfig.frequency;\n   448→\n   449→  console.log(`[SPLICE Bleep] Generating ${bleepType} bleep: ${duration}s @ ${frequency}Hz`);\n   450→\n   451→  // Calculate number of samples\n   452→  const numSamples = Math.ceil(duration * sampleRate);\n   453→  const numChannels = 1; // Mono\n   454→  const bitsPerSample = 16;\n   455→  const bytesPerSample = bitsPerSample / 8;\n   456→\n   457→  // Generate audio samples\n   458→  const samples = new Int16Array(numSamples);\n   459→\n   460→  if (frequency > 0) {\n   461→    // Generate sine wave at specified frequency\n   462→    const angularFrequency = 2 * Math.PI * frequency / sampleRate;\n   463→    const amplitude = Math.floor(32767 * volume);\n   464→\n   465→    // Apply fade in/out to avoid clicks (10ms each)\n   466→    const fadeLength = Math.min(Math.floor(0.01 * sampleRate), numSamples / 4);\n   467→\n   468→    for (let i = 0; i < numSamples; i++) {\n   469→      // Base sine wave\n   470→      let sample = Math.sin(angularFrequency * i) * amplitude;\n   471→\n   472→      // Apply fade in\n   473→      if (i < fadeLength) {\n   474→        sample *= i / fadeLength;\n   475→      }\n   476→\n   477→      // Apply fade out\n   478→      if (i > numSamples - fadeLength) {\n   479→        sample *= (numSamples - i) / fadeLength;\n   480→      }\n   481→\n   482→      samples[i] = Math.round(sample);\n   483→    }\n   484→  } else {\n   485→    // Mute/duck: generate silence\n   486→    samples.fill(0);\n   487→  }\n   488→\n   489→  // Create WAV file\n   490→  const dataSize = numSamples * numChannels * bytesPerSample;\n   491→  const fileSize = 44 + dataSize; // 44-byte WAV header + data\n   492→\n   493→  const buffer = Buffer.alloc(fileSize);\n   494→\n   495→  // WAV header\n   496→  buffer.write('RIFF', 0);\n   497→  buffer.writeUInt32LE(fileSize - 8, 4);\n   498→  buffer.write('WAVE', 8);\n   499→  buffer.write('fmt ', 12);\n   500→  buffer.writeUInt32LE(16, 16); // Subchunk1Size (16 for PCM)\n   501→  buffer.writeUInt16LE(1, 20); // AudioFormat (1 = PCM)\n   502→  buffer.writeUInt16LE(numChannels, 22);\n   503→  buffer.writeUInt32LE(sampleRate, 24);\n   504→  buffer.writeUInt32LE(sampleRate * numChannels * bytesPerSample, 28); // ByteRate\n   505→  buffer.writeUInt16LE(numChannels * bytesPerSample, 32); // BlockAlign\n   506→  buffer.writeUInt16LE(bitsPerSample, 34);\n   507→  buffer.write('data', 36);\n   508→  buffer.writeUInt32LE(dataSize, 40);\n   509→\n   510→  // Write samples\n   511→  for (let i = 0; i < numSamples; i++) {\n   512→    buffer.writeInt16LE(samples[i], 44 + i * bytesPerSample);\n   513→  }\n   514→\n   515→  // Write to file\n   516→  const filePath = outputPath || path.join('/tmp', `splice_bleep_${crypto.randomBytes(4).toString('hex')}.wav`);\n   517→\n   518→  await fs.promises.writeFile(filePath, buffer);\n   519→\n   520→  console.log(`[SPLICE Bleep] Generated: ${filePath}`);\n   521→\n   522→  return {\n   523→    success: true,\n   524→    path: filePath,\n   525→    duration,\n   526→    frequency,\n   527→    bleepType,\n   528→    sampleRate,\n   529→    fileSize\n   530→  };\n   531→}\n   532→\n   533→/**\n   534→ * Generate bleep audio for multiple segments\n   535→ *\n   536→ * @param {Array} segments - Array of profanity segments with start/end\n   537→ * @param {Object} options - Generation options\n   538→ * @returns {Promise<Array>} Array of generated bleep info for each segment\n   539→ */\n   540→async function generateBleepsForSegments(segments, options = {}) {\n   541→  const { bleepType = 'standard', volume = 0.5, outputDir = '/tmp' } = options;\n   542→\n   543→  // Generate all bleeps in parallel (I/O bound, safe to parallelize)\n   544→  const results = await Promise.all(\n   545→    segments.map(async (segment, i) => {\n   546→      const duration = segment.end - segment.start;\n   547→      const outputPath = path.join(outputDir, `splice_bleep_segment_${i}_${crypto.randomBytes(4).toString('hex')}.wav`);\n   548→\n   549→      try {\n   550→        const result = await generateBleepAudio({\n   551→          duration,\n   552→          bleepType,\n   553→          volume,\n   554→          outputPath\n   555→        });\n   556→\n   557→        return {\n   558→          segmentIndex: i,\n   559→          segmentStart: segment.start,\n   560→          segmentEnd: segment.end,\n   561→          ...result\n   562→        };\n   563→      } catch (err) {\n   564→        console.error(`[SPLICE Bleep] Failed to generate bleep for segment ${i}:`, err);\n   565→        return {\n   566→          segmentIndex: i,\n   567→          success: false,\n   568→          error: err.message\n   569→        };\n   570→      }\n   571→    })\n   572→  );\n   573→\n   574→  return results;\n   575→}\n   576→\n   577→/**\n   578→ * Clean up generated bleep files\n   579→ *\n   580→ * @param {Array} bleepPaths - Array of bleep file paths to delete\n   581→ */\n   582→async function cleanupBleepFiles(bleepPaths) {\n   583→  for (const filePath of bleepPaths) {\n   584→    try {\n   585→      await fs.promises.unlink(filePath);\n   586→    } catch (_err) {\n   587→      // Ignore errors (file may already be deleted)\n   588→    }\n   589→  }\n   590→}\n   591→\n   592→// =============================================================================\n   593→// Profanity List Management\n   594→// =============================================================================\n   595→\n   596→/**\n   597→ * Get the default profanity list for a language\n   598→ *\n   599→ * @param {string} language - Language code (en, es, fr, de)\n   600→ * @returns {Array<string>} Profanity word list\n   601→ */\n   602→function getProfanityList(language = 'en') {\n   603→  return PROFANITY_LISTS[language] || PROFANITY_LISTS['en'];\n   604→}\n   605→\n   606→/**\n   607→ * Get all supported languages\n   608→ */\n   609→function getSupportedLanguages() {\n   610→  return [\n   611→    { code: 'en', name: 'English' },\n   612→    { code: 'es', name: 'Spanish' },\n   613→    { code: 'fr', name: 'French' },\n   614→    { code: 'de', name: 'German' }\n   615→  ];\n   616→}\n   617→\n   618→/**\n   619→ * Validate and clean a custom word list\n   620→ *\n   621→ * @param {string|Array} input - Comma-separated string or array\n   622→ * @returns {Array<string>} Cleaned word list\n   623→ */\n   624→function parseWordList(input) {\n   625→  if (!input) return [];\n   626→\n   627→  if (Array.isArray(input)) {\n   628→    return input.map(w => w.trim().toLowerCase()).filter(w => w.length > 0);\n   629→  }\n   630→\n   631→  if (typeof input === 'string') {\n   632→    return input\n   633→      .split(',')\n   634→      .map(w => w.trim().toLowerCase())\n   635→      .filter(w => w.length > 0);\n   636→  }\n   637→\n   638→  return [];\n   639→}\n   640→\n   641→// =============================================================================\n   642→// Segment Operations\n   643→// =============================================================================\n   644→\n   645→/**\n   646→ * Merge profanity segments that are close together\n   647→ *\n   648→ * @param {Array} segments - Array of profanity segments\n   649→ * @param {number} maxGap - Maximum gap to merge (seconds)\n   650→ * @returns {Array} Merged segments\n   651→ */\n   652→function mergeAdjacentSegments(segments, maxGap = 0.5) {\n   653→  if (segments.length <= 1) return segments;\n   654→\n   655→  const sorted = [...segments].sort((a, b) => a.start - b.start);\n   656→  const merged = [];\n   657→  let current = { ...sorted[0] };\n   658→\n   659→  for (let i = 1; i < sorted.length; i++) {\n   660→    const next = sorted[i];\n   661→\n   662→    if (next.start - current.end <= maxGap) {\n   663→      // Merge\n   664→      current.end = next.end;\n   665→      current.duration = current.end - current.start;\n   666→      current.words = [...current.words, ...next.words];\n   667→      current.wordCount = current.words.length;\n   668→      current.text = current.words.join(' ');\n   669→    } else {\n   670→      merged.push(current);\n   671→      current = { ...next };\n   672→    }\n   673→  }\n   674→\n   675→  merged.push(current);\n   676→  return merged;\n   677→}\n   678→\n   679→/**\n   680→ * Filter segments by user selection (for UI toggle)\n   681→ *\n   682→ * @param {Array} segments - All profanity segments\n   683→ * @param {Array<number>} selectedIndices - Indices of segments to keep\n   684→ * @returns {Array} Filtered segments\n   685→ */\n   686→function filterSelectedSegments(segments, selectedIndices) {\n   687→  const indexSet = new Set(selectedIndices);\n   688→  return segments.filter((_, i) => indexSet.has(i));\n   689→}\n   690→\n   691→/**\n   692→ * Convert segments to timeline format for cut list generator\n   693→ *\n   694→ * @param {Array} segments - Profanity segments\n   695→ * @returns {Array<{start: number, end: number, type: string}>} Timeline-ready segments\n   696→ */\n   697→function segmentsToTimeline(segments) {\n   698→  return segments.map(seg => ({\n   699→    start: seg.start,\n   700→    end: seg.end,\n   701→    duration: seg.duration,\n   702→    type: 'profanity',\n   703→    label: `Profanity: ${seg.text.substring(0, 30)}${seg.text.length > 30 ? '...' : ''}`\n   704→  }));\n   705→}\n   706→\n   707→// =============================================================================\n   708→// Exports\n   709→// =============================================================================\n   710→\n   711→module.exports = {\n   712→  // Core detection\n   713→  detectProfanity,\n   714→  groupProfanitySegments,\n   715→\n   716→  // Profanity lists\n   717→  getProfanityList,\n   718→  getSupportedLanguages,\n   719→  parseWordList,\n   720→  PROFANITY_LISTS,\n   721→\n   722→  // Bleep sounds\n   723→  getAvailableBleepSounds,\n   724→  BLEEP_SOUNDS,\n   725→\n   726→  // Bleep audio generation\n   727→  generateBleepAudio,\n   728→  generateBleepsForSegments,\n   729→  cleanupBleepFiles,\n   730→\n   731→  // Segment operations\n   732→  mergeAdjacentSegments,\n   733→  filterSelectedSegments,\n   734→  segmentsToTimeline,\n   735→\n   736→  // Utilities\n   737→  alignToFrame\n   738→};\n   739→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"a34df3f9-0cc8-4565-b68e-9239056d3898","timestamp":"2025-12-26T07:40:14.886Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/profanityDetection.js","content":"/**\n * Profanity Detection Service\n *\n * Detects profanity in transcripts and returns segments for muting/bleeping.\n * Based on Fireside's proven approach with enhancements.\n *\n * Features:\n * - Word-level profanity detection from transcript\n * - Custom blocklist (add words)\n * - Custom allowlist (exclude words)\n * - Contiguous segment grouping\n * - Frame-rate aligned boundaries\n * - Multiple language support\n */\n\nconst path = require('path');\nconst fs = require('fs');\nconst crypto = require('crypto');\n\n// =============================================================================\n// Default Profanity Lists\n// =============================================================================\n\n/**\n * Default English profanity list\n * This is a comprehensive list that can be extended via custom blocklist\n */\nconst DEFAULT_PROFANITY_EN = [\n  // Common expletives\n  'fuck', 'fucking', 'fucked', 'fucker', 'fuckers', 'fucks', 'motherfucker', 'motherfucking',\n  'shit', 'shitty', 'shitting', 'bullshit', 'horseshit', 'dipshit', 'shits',\n  'damn', 'damned', 'goddamn', 'goddamned',\n  'ass', 'asshole', 'assholes', 'asses', 'dumbass', 'jackass', 'badass',\n  'bitch', 'bitches', 'bitchy', 'bitching', 'son of a bitch',\n  'bastard', 'bastards',\n  'crap', 'crappy',\n  'hell', 'hellish',\n  'piss', 'pissed', 'pissing',\n\n  // Slurs and offensive terms (heavily censored in most media)\n  'cunt', 'cunts',\n  'cock', 'cocks', 'cocksucker',\n  'dick', 'dicks', 'dickhead',\n  'pussy', 'pussies',\n  'whore', 'whores',\n  'slut', 'sluts',\n  'fag', 'faggot', 'faggots',\n  'retard', 'retarded', 'retards',\n\n  // Religious profanity\n  'jesus christ', 'christ', 'god damn', 'oh my god',\n\n  // Racial slurs (abbreviated - these should always be censored)\n  // Note: Full list would include more but keeping brief for code\n\n  // Sexual terms\n  'dildo', 'dildos',\n  'blowjob', 'handjob',\n  'jerk off', 'jerking off',\n  'wanker', 'wanking',\n\n  // Drug references (optional, uncomment if needed)\n  // 'meth', 'cocaine', 'heroin',\n\n  // Common misspellings/slang variants\n  'fck', 'fuk', 'phuck', 'fxck',\n  'sht', 'sh1t', 's**t',\n  'btch', 'b1tch',\n  'azz', 'a$$',\n  'cnt',\n  'dck',\n\n  // Common euphemisms that sound like profanity\n  'frickin', 'freakin', 'friggin', 'effing',\n];\n\n/**\n * Words that contain profanity substrings but are NOT profane\n * Used to prevent false positives in partial matching\n */\nconst FALSE_POSITIVE_ALLOWLIST = [\n  // Contains \"hell\"\n  'hello', 'shell', 'shellfish', 'shelling', 'michelle', 'mitchell', 'rochelle',\n  'seashell', 'eggshell', 'nutshell', 'bombshell', 'dumbbell', 'doorbell',\n  'othello', 'cello', 'jello', 'bellow', 'fellow', 'mellow', 'yellow',\n\n  // Contains \"ass\"\n  'class', 'pass', 'mass', 'grass', 'glass', 'brass', 'bass', 'sass',\n  'classic', 'passage', 'passenger', 'passive', 'passion', 'passionate',\n  'assassin', 'ambassador', 'embassy', 'harass', 'harassment', 'embarrass',\n  'compass', 'bypass', 'surpass', 'trespass', 'amass', 'morass',\n  'cassette', 'casserole', 'cassandra', 'lasso', 'molasses',\n  'massachusetts', 'tallahassee', 'hassle', 'tassel', 'vessel',\n\n  // Contains \"damn\"\n  'amsterdam', 'goddamn', // goddamn is profanity, but amsterdam isn't\n\n  // Contains \"crap\"\n  'scrap', 'scrape', 'scrappy',\n\n  // Contains \"cock\"\n  'peacock', 'hancock', 'cockpit', 'cocktail', 'cockatoo', 'cockatiel',\n  'shuttlecock', 'stopcock', 'weathercock', 'woodcock',\n\n  // Contains \"dick\"\n  'dickens', 'benedict', 'addicted', 'predicted', 'verdict',\n  'contradiction', 'jurisdiction', 'index',\n\n  // Contains \"piss\"\n  'mississippi',\n\n  // Contains \"tit\"\n  'title', 'titanic', 'constitution', 'institution', 'competition',\n  'repetition', 'partition', 'petition', 'nutrition', 'intuition',\n  'appetite', 'quantity', 'entity', 'identity', 'activity',\n\n  // Contains \"cum\"\n  'document', 'documentary', 'accumulate', 'circumference', 'cucumber',\n  'circumstance', 'incumbent', 'encumber', 'succumb', 'vacuum',\n\n  // Contains \"fag\"\n  'faggot', // this IS profanity, keeping it out\n\n  // Contains \"ho\"\n  'house', 'home', 'honest', 'honor', 'hope', 'hour', 'holiday',\n  'whole', 'whose', 'show', 'shoe', 'shop', 'short', 'shot',\n\n  // Contains \"nig\" - be careful here\n  'night', 'knight', 'nightmare', 'nightclub', 'ignite', 'dignity',\n  'significant', 'indignant', 'benign', 'malignant',\n];\n\n/**\n * Default Spanish profanity list\n */\nconst DEFAULT_PROFANITY_ES = [\n  'mierda', 'puta', 'puto', 'joder', 'coño', 'cojones', 'cabron', 'cabrón',\n  'pendejo', 'pendeja', 'chingar', 'chingada', 'verga', 'culo', 'carajo',\n  'marica', 'maricón', 'hostia', 'gilipollas', 'capullo', 'hijo de puta'\n];\n\n/**\n * Default French profanity list\n */\nconst DEFAULT_PROFANITY_FR = [\n  'merde', 'putain', 'bordel', 'salaud', 'salope', 'connard', 'connasse',\n  'enculé', 'nique', 'bite', 'couille', 'foutre', 'baiser', 'chier', 'con'\n];\n\n/**\n * Default German profanity list\n */\nconst DEFAULT_PROFANITY_DE = [\n  'scheiße', 'scheisse', 'arschloch', 'hurensohn', 'wichser', 'fotze',\n  'fick', 'ficken', 'verdammt', 'schlampe', 'hure', 'mistkerl'\n];\n\n// Language mapping\nconst PROFANITY_LISTS = {\n  'en': DEFAULT_PROFANITY_EN,\n  'es': DEFAULT_PROFANITY_ES,\n  'fr': DEFAULT_PROFANITY_FR,\n  'de': DEFAULT_PROFANITY_DE\n};\n\n// =============================================================================\n// Core Detection\n// =============================================================================\n\n/**\n * Detect profanity in transcript words\n *\n * @param {Object} transcript - Transcript with words array\n * @param {Object} options - Detection options\n * @returns {Object} Detection results with marked words and segments\n */\nfunction detectProfanity(transcript, options = {}) {\n  const {\n    language = 'en',\n    customBlocklist = [],\n    customAllowlist = [],\n    caseSensitive = false,\n    includePartialMatches = true,\n    frameRate = 30  // For frame alignment\n  } = options;\n\n  // Get base profanity list for language\n  const baseProfanity = PROFANITY_LISTS[language] || PROFANITY_LISTS['en'];\n\n  // Build final profanity set\n  const profanitySet = new Set([\n    ...baseProfanity,\n    ...customBlocklist.map(w => caseSensitive ? w : w.toLowerCase())\n  ]);\n\n  // Remove allowlist items\n  const allowlistSet = new Set(customAllowlist.map(w => caseSensitive ? w : w.toLowerCase()));\n  for (const allowed of allowlistSet) {\n    profanitySet.delete(allowed);\n  }\n\n  // Also add common variants (e.g., \"fucking\" -> \"fuckin'\")\n  const variants = new Set();\n  for (const word of profanitySet) {\n    if (word.endsWith('ing')) {\n      variants.add(word.slice(0, -1)); // fuckin\n      variants.add(word.slice(0, -3) + \"in'\"); // fuckin'\n    }\n    if (word.endsWith('ed')) {\n      variants.add(word.slice(0, -2) + \"'d\"); // fuck'd\n    }\n  }\n  for (const v of variants) {\n    profanitySet.add(v);\n  }\n\n  // Check each word in transcript\n  const words = transcript.words || [];\n  const markedWords = [];\n\n  // Create false positive set once for performance\n  const falsePositiveSet = new Set(FALSE_POSITIVE_ALLOWLIST);\n\n  for (let i = 0; i < words.length; i++) {\n    const word = words[i];\n    const wordText = word.word || word.text || '';\n    const normalizedWord = caseSensitive ? wordText : wordText.toLowerCase();\n\n    // Remove punctuation for matching\n    const cleanWord = normalizedWord.replace(/[^\\w\\s'-]/g, '').trim();\n\n    // Check for exact match\n    let isProfanity = profanitySet.has(cleanWord);\n\n    // Check for partial matches (word contains profanity)\n    // But skip words that are known false positives\n    if (!isProfanity && includePartialMatches) {\n      // Skip if word is in the false positive allowlist\n      if (!falsePositiveSet.has(cleanWord)) {\n        for (const profane of profanitySet) {\n          if (cleanWord.includes(profane) && profane.length >= 4) {\n            isProfanity = true;\n            break;\n          }\n        }\n      }\n    }\n\n    markedWords.push({\n      text: wordText,\n      start: word.start,\n      end: word.end,\n      index: i,\n      isProfanity,\n      // Can be toggled by user in UI\n      shouldCensor: isProfanity\n    });\n  }\n\n  // Group contiguous profanity into segments\n  const segments = groupProfanitySegments(markedWords, { frameRate });\n\n  // Calculate statistics\n  const profanityCount = markedWords.filter(w => w.isProfanity).length;\n  const totalDuration = segments.reduce((sum, s) => sum + s.duration, 0);\n\n  console.log(`[SPLICE Profanity] Detected ${profanityCount} profane word(s) in ${segments.length} segment(s)`);\n\n  return {\n    words: markedWords,\n    segments,\n    metadata: {\n      totalWords: words.length,\n      profanityCount,\n      segmentCount: segments.length,\n      totalCensorDuration: parseFloat(totalDuration.toFixed(3)),\n      language,\n      customBlocklistCount: customBlocklist.length,\n      customAllowlistCount: customAllowlist.length\n    }\n  };\n}\n\n/**\n * Group contiguous profanity words into segments\n *\n * @param {Array} markedWords - Array of words with isProfanity flag\n * @param {Object} options - Grouping options\n * @returns {Array} Array of profanity segments\n */\nfunction groupProfanitySegments(markedWords, options = {}) {\n  const { frameRate = 30, maxGap = 0.3 } = options;\n\n  const segments = [];\n  let currentSegment = null;\n\n  for (const word of markedWords) {\n    if (!word.isProfanity) {\n      // End current segment if exists\n      if (currentSegment) {\n        segments.push(finalizeSeg(currentSegment, frameRate));\n        currentSegment = null;\n      }\n      continue;\n    }\n\n    if (!currentSegment) {\n      // Start new segment\n      currentSegment = {\n        start: word.start,\n        end: word.end,\n        words: [word.text],\n        wordIndices: [word.index]\n      };\n    } else if (word.start - currentSegment.end <= maxGap) {\n      // Extend current segment (words are close together)\n      currentSegment.end = word.end;\n      currentSegment.words.push(word.text);\n      currentSegment.wordIndices.push(word.index);\n    } else {\n      // Gap too large - finalize current and start new\n      segments.push(finalizeSeg(currentSegment, frameRate));\n      currentSegment = {\n        start: word.start,\n        end: word.end,\n        words: [word.text],\n        wordIndices: [word.index]\n      };\n    }\n  }\n\n  // Finalize last segment\n  if (currentSegment) {\n    segments.push(finalizeSeg(currentSegment, frameRate));\n  }\n\n  return segments;\n}\n\n/**\n * Finalize a segment with frame-aligned boundaries\n */\nfunction finalizeSeg(segment, frameRate) {\n  // Add small padding around profanity (50ms before, 100ms after)\n  const paddedStart = Math.max(0, segment.start - 0.05);\n  const paddedEnd = segment.end + 0.1;\n\n  // Align to frame boundaries\n  const alignedStart = alignToFrame(paddedStart, frameRate, 'floor');\n  const alignedEnd = alignToFrame(paddedEnd, frameRate, 'ceil');\n\n  return {\n    start: parseFloat(alignedStart.toFixed(4)),\n    end: parseFloat(alignedEnd.toFixed(4)),\n    duration: parseFloat((alignedEnd - alignedStart).toFixed(4)),\n    words: segment.words,\n    wordCount: segment.words.length,\n    text: segment.words.join(' ')\n  };\n}\n\n/**\n * Align time to frame boundary\n */\nfunction alignToFrame(time, frameRate, mode = 'round') {\n  const frameDuration = 1 / frameRate;\n  switch (mode) {\n    case 'floor':\n      return Math.floor(time / frameDuration) * frameDuration;\n    case 'ceil':\n      return Math.ceil(time / frameDuration) * frameDuration;\n    default:\n      return Math.round(time / frameDuration) * frameDuration;\n  }\n}\n\n// =============================================================================\n// Bleep Sound Management\n// =============================================================================\n\n/**\n * Available bleep sounds\n */\nconst BLEEP_SOUNDS = {\n  'standard': {\n    name: 'Standard Censor Beep',\n    frequency: 1000,\n    description: 'Classic 1kHz censor beep'\n  },\n  'tv': {\n    name: 'TV Bleep',\n    frequency: 1000,\n    description: 'Television-style censor beep'\n  },\n  'radio': {\n    name: 'Radio Bleep',\n    frequency: 880,\n    description: 'Radio broadcast censor tone'\n  },\n  'duck': {\n    name: 'Audio Duck',\n    frequency: 0,\n    description: 'Volume reduction (no beep)'\n  },\n  'mute': {\n    name: 'Complete Mute',\n    frequency: 0,\n    description: 'Complete silence'\n  }\n};\n\n/**\n * Get list of available bleep sounds\n */\nfunction getAvailableBleepSounds() {\n  return Object.entries(BLEEP_SOUNDS).map(([id, info]) => ({\n    id,\n    ...info\n  }));\n}\n\n// =============================================================================\n// Bleep Audio Generation\n// =============================================================================\n\n/**\n * Generate a bleep/censor tone audio file\n *\n * @param {Object} options - Generation options\n * @param {number} options.duration - Duration in seconds\n * @param {string} options.bleepType - Type of bleep ('standard', 'tv', 'radio', 'duck', 'mute')\n * @param {number} options.sampleRate - Sample rate (default: 44100)\n * @param {number} options.volume - Volume level 0-1 (default: 0.5)\n * @param {string} options.outputPath - Optional output path (generates temp file if not provided)\n * @returns {Promise<Object>} Result with path to generated file\n */\nasync function generateBleepAudio(options = {}) {\n  const {\n    duration = 1.0,\n    bleepType = 'standard',\n    sampleRate = 44100,\n    volume = 0.5,\n    outputPath = null\n  } = options;\n\n  const bleepConfig = BLEEP_SOUNDS[bleepType] || BLEEP_SOUNDS.standard;\n  const frequency = bleepConfig.frequency;\n\n  console.log(`[SPLICE Bleep] Generating ${bleepType} bleep: ${duration}s @ ${frequency}Hz`);\n\n  // Calculate number of samples\n  const numSamples = Math.ceil(duration * sampleRate);\n  const numChannels = 1; // Mono\n  const bitsPerSample = 16;\n  const bytesPerSample = bitsPerSample / 8;\n\n  // Generate audio samples\n  const samples = new Int16Array(numSamples);\n\n  if (frequency > 0) {\n    // Generate sine wave at specified frequency\n    const angularFrequency = 2 * Math.PI * frequency / sampleRate;\n    const amplitude = Math.floor(32767 * volume);\n\n    // Apply fade in/out to avoid clicks (10ms each)\n    const fadeLength = Math.min(Math.floor(0.01 * sampleRate), numSamples / 4);\n\n    for (let i = 0; i < numSamples; i++) {\n      // Base sine wave\n      let sample = Math.sin(angularFrequency * i) * amplitude;\n\n      // Apply fade in\n      if (i < fadeLength) {\n        sample *= i / fadeLength;\n      }\n\n      // Apply fade out\n      if (i > numSamples - fadeLength) {\n        sample *= (numSamples - i) / fadeLength;\n      }\n\n      samples[i] = Math.round(sample);\n    }\n  } else {\n    // Mute/duck: generate silence\n    samples.fill(0);\n  }\n\n  // Create WAV file\n  const dataSize = numSamples * numChannels * bytesPerSample;\n  const fileSize = 44 + dataSize; // 44-byte WAV header + data\n\n  const buffer = Buffer.alloc(fileSize);\n\n  // WAV header\n  buffer.write('RIFF', 0);\n  buffer.writeUInt32LE(fileSize - 8, 4);\n  buffer.write('WAVE', 8);\n  buffer.write('fmt ', 12);\n  buffer.writeUInt32LE(16, 16); // Subchunk1Size (16 for PCM)\n  buffer.writeUInt16LE(1, 20); // AudioFormat (1 = PCM)\n  buffer.writeUInt16LE(numChannels, 22);\n  buffer.writeUInt32LE(sampleRate, 24);\n  buffer.writeUInt32LE(sampleRate * numChannels * bytesPerSample, 28); // ByteRate\n  buffer.writeUInt16LE(numChannels * bytesPerSample, 32); // BlockAlign\n  buffer.writeUInt16LE(bitsPerSample, 34);\n  buffer.write('data', 36);\n  buffer.writeUInt32LE(dataSize, 40);\n\n  // Write samples\n  for (let i = 0; i < numSamples; i++) {\n    buffer.writeInt16LE(samples[i], 44 + i * bytesPerSample);\n  }\n\n  // Write to file\n  const filePath = outputPath || path.join('/tmp', `splice_bleep_${crypto.randomBytes(4).toString('hex')}.wav`);\n\n  await fs.promises.writeFile(filePath, buffer);\n\n  console.log(`[SPLICE Bleep] Generated: ${filePath}`);\n\n  return {\n    success: true,\n    path: filePath,\n    duration,\n    frequency,\n    bleepType,\n    sampleRate,\n    fileSize\n  };\n}\n\n/**\n * Generate bleep audio for multiple segments\n *\n * @param {Array} segments - Array of profanity segments with start/end\n * @param {Object} options - Generation options\n * @returns {Promise<Array>} Array of generated bleep info for each segment\n */\nasync function generateBleepsForSegments(segments, options = {}) {\n  const { bleepType = 'standard', volume = 0.5, outputDir = '/tmp' } = options;\n\n  // Generate all bleeps in parallel (I/O bound, safe to parallelize)\n  const results = await Promise.all(\n    segments.map(async (segment, i) => {\n      const duration = segment.end - segment.start;\n      const outputPath = path.join(outputDir, `splice_bleep_segment_${i}_${crypto.randomBytes(4).toString('hex')}.wav`);\n\n      try {\n        const result = await generateBleepAudio({\n          duration,\n          bleepType,\n          volume,\n          outputPath\n        });\n\n        return {\n          segmentIndex: i,\n          segmentStart: segment.start,\n          segmentEnd: segment.end,\n          ...result\n        };\n      } catch (err) {\n        console.error(`[SPLICE Bleep] Failed to generate bleep for segment ${i}:`, err);\n        return {\n          segmentIndex: i,\n          success: false,\n          error: err.message\n        };\n      }\n    })\n  );\n\n  return results;\n}\n\n/**\n * Clean up generated bleep files\n *\n * @param {Array} bleepPaths - Array of bleep file paths to delete\n */\nasync function cleanupBleepFiles(bleepPaths) {\n  for (const filePath of bleepPaths) {\n    try {\n      await fs.promises.unlink(filePath);\n    } catch (_err) {\n      // Ignore errors (file may already be deleted)\n    }\n  }\n}\n\n// =============================================================================\n// Profanity List Management\n// =============================================================================\n\n/**\n * Get the default profanity list for a language\n *\n * @param {string} language - Language code (en, es, fr, de)\n * @returns {Array<string>} Profanity word list\n */\nfunction getProfanityList(language = 'en') {\n  return PROFANITY_LISTS[language] || PROFANITY_LISTS['en'];\n}\n\n/**\n * Get all supported languages\n */\nfunction getSupportedLanguages() {\n  return [\n    { code: 'en', name: 'English' },\n    { code: 'es', name: 'Spanish' },\n    { code: 'fr', name: 'French' },\n    { code: 'de', name: 'German' }\n  ];\n}\n\n/**\n * Validate and clean a custom word list\n *\n * @param {string|Array} input - Comma-separated string or array\n * @returns {Array<string>} Cleaned word list\n */\nfunction parseWordList(input) {\n  if (!input) return [];\n\n  if (Array.isArray(input)) {\n    return input.map(w => w.trim().toLowerCase()).filter(w => w.length > 0);\n  }\n\n  if (typeof input === 'string') {\n    return input\n      .split(',')\n      .map(w => w.trim().toLowerCase())\n      .filter(w => w.length > 0);\n  }\n\n  return [];\n}\n\n// =============================================================================\n// Segment Operations\n// =============================================================================\n\n/**\n * Merge profanity segments that are close together\n *\n * @param {Array} segments - Array of profanity segments\n * @param {number} maxGap - Maximum gap to merge (seconds)\n * @returns {Array} Merged segments\n */\nfunction mergeAdjacentSegments(segments, maxGap = 0.5) {\n  if (segments.length <= 1) return segments;\n\n  const sorted = [...segments].sort((a, b) => a.start - b.start);\n  const merged = [];\n  let current = { ...sorted[0] };\n\n  for (let i = 1; i < sorted.length; i++) {\n    const next = sorted[i];\n\n    if (next.start - current.end <= maxGap) {\n      // Merge\n      current.end = next.end;\n      current.duration = current.end - current.start;\n      current.words = [...current.words, ...next.words];\n      current.wordCount = current.words.length;\n      current.text = current.words.join(' ');\n    } else {\n      merged.push(current);\n      current = { ...next };\n    }\n  }\n\n  merged.push(current);\n  return merged;\n}\n\n/**\n * Filter segments by user selection (for UI toggle)\n *\n * @param {Array} segments - All profanity segments\n * @param {Array<number>} selectedIndices - Indices of segments to keep\n * @returns {Array} Filtered segments\n */\nfunction filterSelectedSegments(segments, selectedIndices) {\n  const indexSet = new Set(selectedIndices);\n  return segments.filter((_, i) => indexSet.has(i));\n}\n\n/**\n * Convert segments to timeline format for cut list generator\n *\n * @param {Array} segments - Profanity segments\n * @returns {Array<{start: number, end: number, type: string}>} Timeline-ready segments\n */\nfunction segmentsToTimeline(segments) {\n  return segments.map(seg => ({\n    start: seg.start,\n    end: seg.end,\n    duration: seg.duration,\n    type: 'profanity',\n    label: `Profanity: ${seg.text.substring(0, 30)}${seg.text.length > 30 ? '...' : ''}`\n  }));\n}\n\n// =============================================================================\n// Exports\n// =============================================================================\n\nmodule.exports = {\n  // Core detection\n  detectProfanity,\n  groupProfanitySegments,\n\n  // Profanity lists\n  getProfanityList,\n  getSupportedLanguages,\n  parseWordList,\n  PROFANITY_LISTS,\n\n  // Bleep sounds\n  getAvailableBleepSounds,\n  BLEEP_SOUNDS,\n\n  // Bleep audio generation\n  generateBleepAudio,\n  generateBleepsForSegments,\n  cleanupBleepFiles,\n\n  // Segment operations\n  mergeAdjacentSegments,\n  filterSelectedSegments,\n  segmentsToTimeline,\n\n  // Utilities\n  alignToFrame\n};\n","numLines":739,"startLine":1,"totalLines":739}}}
{"parentUuid":"a34df3f9-0cc8-4565-b68e-9239056d3898","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Pq49qWiwWkfb54QkHzD65k","type":"tool_result","content":"     1→/**\n     2→ * Repetition/Stutter Detection Service\n     3→ *\n     4→ * Detects repeated words, phrases, and stutters in transcripts.\n     5→ * Based on Fireside's proven approach with enhancements.\n     6→ *\n     7→ * Features:\n     8→ * - Frequency-based pattern detection\n     9→ * - Phrase similarity matching\n    10→ * - OpenAI refinement for boundary detection\n    11→ * - Configurable tolerance and phrase size\n    12→ * - Word-level result with grouping\n    13→ *\n    14→ * Two Modes:\n    15→ * - Basic: Fast local detection (no API calls)\n    16→ * - Advanced: Uses OpenAI for precise boundary detection\n    17→ */\n    18→\n    19→const OpenAI = require('openai');\n    20→\n    21→// =============================================================================\n    22→// Configuration\n    23→// =============================================================================\n    24→\n    25→const DEFAULT_OPTIONS = {\n    26→  mode: 'basic',            // 'basic' or 'advanced'\n    27→  phraseSize: 5,            // Words per comparison window\n    28→  tolerance: 0.7,           // Similarity threshold (0-1)\n    29→  searchRadius: 100,        // Words to search ahead\n    30→  minOccurrences: 2,        // Minimum occurrences to count as repetition\n    31→  extendMode: 'next-punctuation', // 'next-punctuation' or 'longest-occurrence'\n    32→  useOpenAI: false,         // Use OpenAI for boundary refinement\n    33→  maxGapWords: 20           // Max words between occurrences to group\n    34→};\n    35→\n    36→// Note: Punctuation pattern available for future boundary detection enhancements\n    37→// const PUNCTUATION = /[.!?,;:]/;\n    38→\n    39→// =============================================================================\n    40→// Core Detection - Basic Mode\n    41→// =============================================================================\n    42→\n    43→/**\n    44→ * Detect repetitions in transcript using basic frequency analysis\n    45→ *\n    46→ * @param {Object} transcript - Transcript with words array\n    47→ * @param {Object} options - Detection options\n    48→ * @returns {Object} Detection results with repetitions\n    49→ */\n    50→function detectRepetitionsBasic(transcript, options = {}) {\n    51→  const opts = { ...DEFAULT_OPTIONS, ...options };\n    52→  const words = transcript.words || [];\n    53→\n    54→  if (words.length < opts.phraseSize * 2) {\n    55→    return { repetitions: [], metadata: { reason: 'Not enough words for analysis' } };\n    56→  }\n    57→\n    58→  console.log(`[SPLICE Repetition] Basic mode: ${words.length} words, phraseSize=${opts.phraseSize}, tolerance=${opts.tolerance}`);\n    59→\n    60→  // Normalize words for comparison\n    61→  const normalizedWords = words.map(w => normalizeWord(w.word || w.text || ''));\n    62→\n    63→  // Find all phrase occurrences\n    64→  const phraseGroups = findPhraseOccurrences(normalizedWords, {\n    65→    phraseSize: opts.phraseSize,\n    66→    searchRadius: opts.searchRadius,\n    67→    tolerance: opts.tolerance,\n    68→    minOccurrences: opts.minOccurrences\n    69→  });\n    70→\n    71→  // Convert to time-based repetitions\n    72→  const repetitions = phraseGroups.map((group, index) => ({\n    73→    id: index,\n    74→    occurrences: group.occurrences.map(occ => ({\n    75→      startWord: occ.startIndex,\n    76→      endWord: occ.endIndex,\n    77→      start: words[occ.startIndex].start,\n    78→      end: words[occ.endIndex].end,\n    79→      text: words.slice(occ.startIndex, occ.endIndex + 1).map(w => w.word || w.text).join(' ')\n    80→    })),\n    81→    bestOccurrence: 0, // First occurrence is usually the \"keeper\"\n    82→    similarity: group.similarity,\n    83→    wordCount: opts.phraseSize\n    84→  }));\n    85→\n    86→  // Filter overlapping repetitions\n    87→  const filtered = filterOverlappingRepetitions(repetitions);\n    88→\n    89→  console.log(`[SPLICE Repetition] Found ${filtered.length} repetition group(s)`);\n    90→\n    91→  return {\n    92→    repetitions: filtered,\n    93→    metadata: {\n    94→      mode: 'basic',\n    95→      totalWords: words.length,\n    96→      phraseSize: opts.phraseSize,\n    97→      tolerance: opts.tolerance,\n    98→      groupsFound: filtered.length,\n    99→      totalOccurrences: filtered.reduce((sum, r) => sum + r.occurrences.length, 0)\n   100→    }\n   101→  };\n   102→}\n   103→\n   104→/**\n   105→ * Find phrase occurrences using sliding window comparison\n   106→ *\n   107→ * @param {Array<string>} normalizedWords - Normalized word strings\n   108→ * @param {Object} options - Search options\n   109→ * @returns {Array} Groups of similar phrases\n   110→ */\n   111→function findPhraseOccurrences(normalizedWords, options) {\n   112→  const { phraseSize, searchRadius, tolerance, minOccurrences } = options;\n   113→  const groups = [];\n   114→  const usedIndices = new Set();\n   115→\n   116→  for (let i = 0; i <= normalizedWords.length - phraseSize; i++) {\n   117→    if (usedIndices.has(i)) continue;\n   118→\n   119→    const phrase = normalizedWords.slice(i, i + phraseSize);\n   120→    const occurrences = [{\n   121→      startIndex: i,\n   122→      endIndex: i + phraseSize - 1\n   123→    }];\n   124→\n   125→    // Search ahead for similar phrases\n   126→    const searchEnd = Math.min(i + searchRadius, normalizedWords.length - phraseSize);\n   127→\n   128→    for (let j = i + phraseSize; j <= searchEnd; j++) {\n   129→      if (usedIndices.has(j)) continue;\n   130→\n   131→      const comparePhrase = normalizedWords.slice(j, j + phraseSize);\n   132→      const similarity = calculateSimilarity(phrase, comparePhrase);\n   133→\n   134→      if (similarity >= tolerance) {\n   135→        occurrences.push({\n   136→          startIndex: j,\n   137→          endIndex: j + phraseSize - 1\n   138→        });\n   139→        // Mark these indices as used\n   140→        for (let k = j; k < j + phraseSize; k++) {\n   141→          usedIndices.add(k);\n   142→        }\n   143→      }\n   144→    }\n   145→\n   146→    if (occurrences.length >= minOccurrences) {\n   147→      // Mark original phrase indices as used\n   148→      for (let k = i; k < i + phraseSize; k++) {\n   149→        usedIndices.add(k);\n   150→      }\n   151→\n   152→      groups.push({\n   153→        occurrences,\n   154→        similarity: 1.0, // First match is reference\n   155→        phraseText: phrase.join(' ')\n   156→      });\n   157→    }\n   158→  }\n   159→\n   160→  return groups;\n   161→}\n   162→\n   163→// =============================================================================\n   164→// Core Detection - Advanced Mode (with OpenAI)\n   165→// =============================================================================\n   166→\n   167→/**\n   168→ * Detect repetitions with OpenAI boundary refinement\n   169→ *\n   170→ * @param {Object} transcript - Transcript with words array\n   171→ * @param {Object} options - Detection options\n   172→ * @returns {Promise<Object>} Detection results with refined boundaries\n   173→ */\n   174→async function detectRepetitionsAdvanced(transcript, options = {}) {\n   175→  const opts = { ...DEFAULT_OPTIONS, ...options, mode: 'advanced' };\n   176→\n   177→  // First, do basic detection\n   178→  const basicResult = detectRepetitionsBasic(transcript, opts);\n   179→\n   180→  if (basicResult.repetitions.length === 0) {\n   181→    return basicResult;\n   182→  }\n   183→\n   184→  // If OpenAI refinement is disabled, return basic results\n   185→  if (!opts.useOpenAI) {\n   186→    return {\n   187→      ...basicResult,\n   188→      metadata: { ...basicResult.metadata, mode: 'advanced-local' }\n   189→    };\n   190→  }\n   191→\n   192→  // Refine boundaries using OpenAI\n   193→  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n   194→  const refinedRepetitions = [];\n   195→\n   196→  for (const rep of basicResult.repetitions) {\n   197→    try {\n   198→      const refined = await refineRepetitionBoundaries(openai, transcript.words, rep);\n   199→      refinedRepetitions.push(refined);\n   200→    } catch (err) {\n   201→      console.warn('[SPLICE Repetition] OpenAI refinement failed, using basic:', err.message);\n   202→      refinedRepetitions.push(rep);\n   203→    }\n   204→  }\n   205→\n   206→  return {\n   207→    repetitions: refinedRepetitions,\n   208→    metadata: {\n   209→      ...basicResult.metadata,\n   210→      mode: 'advanced-openai',\n   211→      openaiRefined: true\n   212→    }\n   213→  };\n   214→}\n   215→\n   216→/**\n   217→ * Refine repetition boundaries using OpenAI\n   218→ *\n   219→ * @param {OpenAI} openai - OpenAI client\n   220→ * @param {Array} words - All transcript words\n   221→ * @param {Object} repetition - Basic repetition detection\n   222→ * @returns {Promise<Object>} Refined repetition with better boundaries\n   223→ */\n   224→async function refineRepetitionBoundaries(openai, words, repetition) {\n   225→  // Get context around the repetition\n   226→  const allOccurrences = repetition.occurrences;\n   227→  const minStart = Math.max(0, allOccurrences[0].startWord - 5);\n   228→  const maxEnd = Math.min(words.length - 1, allOccurrences[allOccurrences.length - 1].endWord + 5);\n   229→\n   230→  // Build context string with markers\n   231→  let contextText = '';\n   232→  for (let i = minStart; i <= maxEnd; i++) {\n   233→    const word = words[i].word || words[i].text;\n   234→    // Mark occurrence boundaries\n   235→    const isStart = allOccurrences.some(o => o.startWord === i);\n   236→    const isEnd = allOccurrences.some(o => o.endWord === i);\n   237→\n   238→    if (isStart) contextText += '[';\n   239→    contextText += word + ' ';\n   240→    if (isEnd) contextText += '] ';\n   241→  }\n   242→\n   243→  const prompt = `Analyze this text for repeated content. The brackets mark detected repetitions:\n   244→\n   245→\"${contextText.trim()}\"\n   246→\n   247→For each bracketed section, determine:\n   248→1. Is this truly a repetition/restart/stutter?\n   249→2. What are the exact word boundaries that should be removed (keeping only the best take)?\n   250→\n   251→Respond in JSON format:\n   252→{\n   253→  \"isRepetition\": true/false,\n   254→  \"keepOccurrence\": 0-based index of the occurrence to keep (usually the last/most complete),\n   255→  \"removeRanges\": [{\"start\": wordIndex, \"end\": wordIndex}]\n   256→}`;\n   257→\n   258→  const response = await openai.chat.completions.create({\n   259→    model: 'gpt-4o-mini',\n   260→    messages: [\n   261→      { role: 'system', content: 'You are an expert video editor identifying repeated content in speech transcripts. Be precise about boundaries.' },\n   262→      { role: 'user', content: prompt }\n   263→    ],\n   264→    temperature: 0,\n   265→    max_tokens: 500\n   266→  });\n   267→\n   268→  try {\n   269→    const result = JSON.parse(response.choices[0].message.content);\n   270→\n   271→    if (!result.isRepetition) {\n   272→      // False positive - return empty\n   273→      return { ...repetition, occurrences: [], metadata: { refined: true, isRepetition: false } };\n   274→    }\n   275→\n   276→    // Update best occurrence\n   277→    return {\n   278→      ...repetition,\n   279→      bestOccurrence: result.keepOccurrence || 0,\n   280→      metadata: { refined: true, openaiResult: result }\n   281→    };\n   282→  } catch (_parseErr) {\n   283→    // Return original if parsing fails\n   284→    return repetition;\n   285→  }\n   286→}\n   287→\n   288→// =============================================================================\n   289→// Stutter Detection (Single-Word Repetitions)\n   290→// =============================================================================\n   291→\n   292→/**\n   293→ * Detect single-word stutters (e.g., \"I I I think\")\n   294→ *\n   295→ * @param {Object} transcript - Transcript with words array\n   296→ * @param {Object} options - Detection options\n   297→ * @returns {Object} Stutter detection results\n   298→ */\n   299→function detectStutters(transcript, options = {}) {\n   300→  const {\n   301→    minRepeats = 2,      // Minimum times word must repeat\n   302→    maxGapMs = 500,      // Max gap between words (milliseconds)\n   303→    ignoreFillers = true, // Ignore common filler words\n   304→    minWordLength = 1    // Minimum word length (1 allows \"I\", \"a\", etc.)\n   305→  } = options;\n   306→\n   307→  const words = transcript.words || [];\n   308→  const stutters = [];\n   309→\n   310→  // Common filler words to optionally ignore\n   311→  const fillerWords = new Set(['um', 'uh', 'ah', 'er', 'like', 'so', 'well', 'you know']);\n   312→\n   313→  let i = 0;\n   314→  while (i < words.length) {\n   315→    const word = words[i];\n   316→    const normalized = normalizeWord(word.word || word.text || '');\n   317→\n   318→    // Skip fillers if configured\n   319→    if (ignoreFillers && fillerWords.has(normalized)) {\n   320→      i++;\n   321→      continue;\n   322→    }\n   323→\n   324→    // Skip very short words (configurable, default allows single chars like \"I\")\n   325→    if (normalized.length < minWordLength) {\n   326→      i++;\n   327→      continue;\n   328→    }\n   329→\n   330→    // Look for consecutive repeats\n   331→    let repeatCount = 1;\n   332→    let j = i + 1;\n   333→\n   334→    while (j < words.length) {\n   335→      const nextWord = words[j];\n   336→      const nextNormalized = normalizeWord(nextWord.word || nextWord.text || '');\n   337→\n   338→      // Check if same word\n   339→      if (nextNormalized !== normalized) break;\n   340→\n   341→      // Check gap (convert to ms)\n   342→      const gap = (nextWord.start - words[j - 1].end) * 1000;\n   343→      if (gap > maxGapMs) break;\n   344→\n   345→      repeatCount++;\n   346→      j++;\n   347→    }\n   348→\n   349→    if (repeatCount >= minRepeats) {\n   350→      stutters.push({\n   351→        word: normalized,\n   352→        occurrences: repeatCount,\n   353→        startWord: i,\n   354→        endWord: j - 1,\n   355→        start: words[i].start,\n   356→        end: words[j - 1].end,\n   357→        duration: words[j - 1].end - words[i].start,\n   358→        // Keep first occurrence, remove rest\n   359→        keepStart: words[i].start,\n   360→        keepEnd: words[i].end,\n   361→        removeStart: words[i + 1].start,\n   362→        removeEnd: words[j - 1].end\n   363→      });\n   364→    }\n   365→\n   366→    i = j; // Skip past all repeats\n   367→  }\n   368→\n   369→  console.log(`[SPLICE Repetition] Found ${stutters.length} stutter(s)`);\n   370→\n   371→  return {\n   372→    stutters,\n   373→    metadata: {\n   374→      type: 'stutters',\n   375→      totalWords: words.length,\n   376→      stutterCount: stutters.length,\n   377→      totalRepeatedWords: stutters.reduce((sum, s) => sum + s.occurrences, 0)\n   378→    }\n   379→  };\n   380→}\n   381→\n   382→// =============================================================================\n   383→// Utility Functions\n   384→// =============================================================================\n   385→\n   386→/**\n   387→ * Normalize a word for comparison\n   388→ */\n   389→function normalizeWord(word) {\n   390→  return word\n   391→    .toLowerCase()\n   392→    .replace(/[^\\w\\s'-]/g, '') // Remove punctuation except apostrophe/hyphen\n   393→    .trim();\n   394→}\n   395→\n   396→/**\n   397→ * Calculate similarity between two word arrays\n   398→ * Uses Jaccard-like similarity with position weighting\n   399→ *\n   400→ * @param {Array<string>} arr1 - First word array\n   401→ * @param {Array<string>} arr2 - Second word array\n   402→ * @returns {number} Similarity score (0-1)\n   403→ */\n   404→function calculateSimilarity(arr1, arr2) {\n   405→  if (arr1.length !== arr2.length) return 0;\n   406→  if (arr1.length === 0) return 0;\n   407→\n   408→  let matches = 0;\n   409→  for (let i = 0; i < arr1.length; i++) {\n   410→    if (arr1[i] === arr2[i]) {\n   411→      matches++;\n   412→    } else {\n   413→      // Partial credit for similar words (edit distance)\n   414→      const maxLen = Math.max(arr1[i].length, arr2[i].length);\n   415→      if (maxLen > 0) {\n   416→        // Early exit: if edit distance is > 30% of max length, similarity < 0.7\n   417→        const maxAllowedDist = Math.floor(maxLen * 0.3);\n   418→        const editDist = levenshteinDistance(arr1[i], arr2[i], maxAllowedDist);\n   419→        if (editDist <= maxAllowedDist) {\n   420→          const wordSim = 1 - (editDist / maxLen);\n   421→          matches += wordSim;\n   422→        }\n   423→        // If editDist > maxAllowedDist, wordSim < 0.7, so no credit added\n   424→      }\n   425→    }\n   426→  }\n   427→\n   428→  return matches / arr1.length;\n   429→}\n   430→\n   431→// Pre-allocated buffers for Levenshtein (avoids allocation per call)\n   432→// Max word length we support (longer words use inline allocation)\n   433→const LEVENSHTEIN_MAX_LEN = 100;\n   434→let _prevRow = new Uint16Array(LEVENSHTEIN_MAX_LEN + 1);\n   435→let _currRow = new Uint16Array(LEVENSHTEIN_MAX_LEN + 1);\n   436→\n   437→/**\n   438→ * Levenshtein edit distance between two strings\n   439→ * Optimized single-row DP with O(n) space and early exit\n   440→ */\n   441→function levenshteinDistance(s1, s2, maxThreshold = Infinity) {\n   442→  // Fast path: exact match\n   443→  if (s1 === s2) return 0;\n   444→\n   445→  const m = s1.length;\n   446→  const n = s2.length;\n   447→\n   448→  // Fast path: empty string\n   449→  if (m === 0) return n;\n   450→  if (n === 0) return m;\n   451→\n   452→  // Early exit if length difference exceeds threshold\n   453→  if (Math.abs(m - n) > maxThreshold) return maxThreshold + 1;\n   454→\n   455→  // Use pre-allocated buffers if possible, otherwise allocate\n   456→  let prev, curr;\n   457→  if (n < LEVENSHTEIN_MAX_LEN) {\n   458→    prev = _prevRow;\n   459→    curr = _currRow;\n   460→  } else {\n   461→    prev = new Uint16Array(n + 1);\n   462→    curr = new Uint16Array(n + 1);\n   463→  }\n   464→\n   465→  // Initialize first row\n   466→  for (let j = 0; j <= n; j++) prev[j] = j;\n   467→\n   468→  for (let i = 1; i <= m; i++) {\n   469→    curr[0] = i;\n   470→    const s1Char = s1.charCodeAt(i - 1);\n   471→    let minInRow = i; // Track minimum value in current row for early exit\n   472→\n   473→    for (let j = 1; j <= n; j++) {\n   474→      const cost = s1Char === s2.charCodeAt(j - 1) ? 0 : 1;\n   475→\n   476→      // curr[j] = min(insertion, deletion, substitution)\n   477→      const ins = curr[j - 1] + 1;\n   478→      const del = prev[j] + 1;\n   479→      const sub = prev[j - 1] + cost;\n   480→\n   481→      curr[j] = ins < del ? (ins < sub ? ins : sub) : (del < sub ? del : sub);\n   482→\n   483→      if (curr[j] < minInRow) minInRow = curr[j];\n   484→    }\n   485→\n   486→    // Early exit: if minimum in row exceeds threshold, result will exceed threshold\n   487→    if (minInRow > maxThreshold) return maxThreshold + 1;\n   488→\n   489→    // Swap rows\n   490→    const tmp = prev;\n   491→    prev = curr;\n   492→    curr = tmp;\n   493→  }\n   494→\n   495→  return prev[n];\n   496→}\n   497→\n   498→/**\n   499→ * Filter overlapping repetition groups\n   500→ */\n   501→function filterOverlappingRepetitions(repetitions) {\n   502→  if (repetitions.length <= 1) return repetitions;\n   503→\n   504→  // Sort by first occurrence start\n   505→  const sorted = [...repetitions].sort((a, b) =>\n   506→    a.occurrences[0].start - b.occurrences[0].start\n   507→  );\n   508→\n   509→  const result = [];\n   510→  let lastEnd = -1;\n   511→\n   512→  for (const rep of sorted) {\n   513→    const firstOcc = rep.occurrences[0];\n   514→    // Check if overlaps with previous\n   515→    if (firstOcc.start >= lastEnd) {\n   516→      result.push(rep);\n   517→      // Update lastEnd to include all occurrences\n   518→      lastEnd = Math.max(...rep.occurrences.map(o => o.end));\n   519→    }\n   520→  }\n   521→\n   522→  return result;\n   523→}\n   524→\n   525→/**\n   526→ * Convert repetitions to removal segments\n   527→ * Keeps the \"best\" occurrence, returns segments to remove\n   528→ *\n   529→ * @param {Array} repetitions - Detected repetitions\n   530→ * @returns {Array} Segments to remove from timeline\n   531→ */\n   532→function repetitionsToRemovalSegments(repetitions) {\n   533→  const segments = [];\n   534→\n   535→  for (const rep of repetitions) {\n   536→    const keepIndex = rep.bestOccurrence || 0;\n   537→\n   538→    rep.occurrences.forEach((occ, i) => {\n   539→      if (i !== keepIndex) {\n   540→        segments.push({\n   541→          start: occ.start,\n   542→          end: occ.end,\n   543→          type: 'repetition',\n   544→          text: occ.text,\n   545→          reason: `Duplicate of occurrence ${keepIndex + 1}`\n   546→        });\n   547→      }\n   548→    });\n   549→  }\n   550→\n   551→  // Sort by start time and merge adjacent\n   552→  return mergeAdjacentSegments(segments);\n   553→}\n   554→\n   555→/**\n   556→ * Convert stutters to removal segments\n   557→ *\n   558→ * @param {Array} stutters - Detected stutters\n   559→ * @returns {Array} Segments to remove\n   560→ */\n   561→function stuttersToRemovalSegments(stutters) {\n   562→  return stutters.map(s => ({\n   563→    start: s.removeStart,\n   564→    end: s.removeEnd,\n   565→    type: 'stutter',\n   566→    text: `${s.word} (x${s.occurrences})`,\n   567→    keepRange: { start: s.keepStart, end: s.keepEnd }\n   568→  }));\n   569→}\n   570→\n   571→/**\n   572→ * Merge adjacent removal segments\n   573→ */\n   574→function mergeAdjacentSegments(segments, maxGap = 0.1) {\n   575→  if (segments.length <= 1) return segments;\n   576→\n   577→  const sorted = [...segments].sort((a, b) => a.start - b.start);\n   578→  const merged = [];\n   579→  let current = { ...sorted[0] };\n   580→\n   581→  for (let i = 1; i < sorted.length; i++) {\n   582→    const next = sorted[i];\n   583→\n   584→    if (next.start - current.end <= maxGap) {\n   585→      // Merge\n   586→      current.end = next.end;\n   587→      current.text = `${current.text}, ${next.text}`;\n   588→    } else {\n   589→      merged.push(current);\n   590→      current = { ...next };\n   591→    }\n   592→  }\n   593→\n   594→  merged.push(current);\n   595→  return merged;\n   596→}\n   597→\n   598→// =============================================================================\n   599→// Combined Detection\n   600→// =============================================================================\n   601→\n   602→/**\n   603→ * Run all repetition detection (phrases + stutters)\n   604→ *\n   605→ * @param {Object} transcript - Transcript with words array\n   606→ * @param {Object} options - Detection options\n   607→ * @returns {Promise<Object>} Combined detection results\n   608→ */\n   609→async function detectAllRepetitions(transcript, options = {}) {\n   610→  const opts = { ...DEFAULT_OPTIONS, ...options };\n   611→\n   612→  // Detect phrase repetitions\n   613→  const phraseResult = opts.useOpenAI\n   614→    ? await detectRepetitionsAdvanced(transcript, opts)\n   615→    : detectRepetitionsBasic(transcript, opts);\n   616→\n   617→  // Detect stutters\n   618→  const stutterResult = detectStutters(transcript, opts);\n   619→\n   620→  // Combine removal segments\n   621→  const phraseSegments = repetitionsToRemovalSegments(phraseResult.repetitions);\n   622→  const stutterSegments = stuttersToRemovalSegments(stutterResult.stutters);\n   623→  const allSegments = mergeAdjacentSegments([...phraseSegments, ...stutterSegments]);\n   624→\n   625→  // Calculate total removable time\n   626→  const totalRemovableTime = allSegments.reduce((sum, s) => sum + (s.end - s.start), 0);\n   627→\n   628→  return {\n   629→    repetitions: phraseResult.repetitions,\n   630→    stutters: stutterResult.stutters,\n   631→    removalSegments: allSegments,\n   632→    metadata: {\n   633→      ...phraseResult.metadata,\n   634→      stutterCount: stutterResult.stutters.length,\n   635→      totalRemovalSegments: allSegments.length,\n   636→      totalRemovableTime: parseFloat(totalRemovableTime.toFixed(3))\n   637→    }\n   638→  };\n   639→}\n   640→\n   641→// =============================================================================\n   642→// Exports\n   643→// =============================================================================\n   644→\n   645→module.exports = {\n   646→  // Core detection\n   647→  detectRepetitionsBasic,\n   648→  detectRepetitionsAdvanced,\n   649→  detectStutters,\n   650→  detectAllRepetitions,\n   651→\n   652→  // Conversion utilities\n   653→  repetitionsToRemovalSegments,\n   654→  stuttersToRemovalSegments,\n   655→\n   656→  // Helper utilities\n   657→  normalizeWord,\n   658→  calculateSimilarity,\n   659→  levenshteinDistance,\n   660→  filterOverlappingRepetitions,\n   661→  mergeAdjacentSegments,\n   662→\n   663→  // Config\n   664→  DEFAULT_OPTIONS\n   665→};\n   666→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"b6bf733e-1a43-4faf-844b-bcf215e79890","timestamp":"2025-12-26T07:40:14.886Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/repetitionDetection.js","content":"/**\n * Repetition/Stutter Detection Service\n *\n * Detects repeated words, phrases, and stutters in transcripts.\n * Based on Fireside's proven approach with enhancements.\n *\n * Features:\n * - Frequency-based pattern detection\n * - Phrase similarity matching\n * - OpenAI refinement for boundary detection\n * - Configurable tolerance and phrase size\n * - Word-level result with grouping\n *\n * Two Modes:\n * - Basic: Fast local detection (no API calls)\n * - Advanced: Uses OpenAI for precise boundary detection\n */\n\nconst OpenAI = require('openai');\n\n// =============================================================================\n// Configuration\n// =============================================================================\n\nconst DEFAULT_OPTIONS = {\n  mode: 'basic',            // 'basic' or 'advanced'\n  phraseSize: 5,            // Words per comparison window\n  tolerance: 0.7,           // Similarity threshold (0-1)\n  searchRadius: 100,        // Words to search ahead\n  minOccurrences: 2,        // Minimum occurrences to count as repetition\n  extendMode: 'next-punctuation', // 'next-punctuation' or 'longest-occurrence'\n  useOpenAI: false,         // Use OpenAI for boundary refinement\n  maxGapWords: 20           // Max words between occurrences to group\n};\n\n// Note: Punctuation pattern available for future boundary detection enhancements\n// const PUNCTUATION = /[.!?,;:]/;\n\n// =============================================================================\n// Core Detection - Basic Mode\n// =============================================================================\n\n/**\n * Detect repetitions in transcript using basic frequency analysis\n *\n * @param {Object} transcript - Transcript with words array\n * @param {Object} options - Detection options\n * @returns {Object} Detection results with repetitions\n */\nfunction detectRepetitionsBasic(transcript, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n  const words = transcript.words || [];\n\n  if (words.length < opts.phraseSize * 2) {\n    return { repetitions: [], metadata: { reason: 'Not enough words for analysis' } };\n  }\n\n  console.log(`[SPLICE Repetition] Basic mode: ${words.length} words, phraseSize=${opts.phraseSize}, tolerance=${opts.tolerance}`);\n\n  // Normalize words for comparison\n  const normalizedWords = words.map(w => normalizeWord(w.word || w.text || ''));\n\n  // Find all phrase occurrences\n  const phraseGroups = findPhraseOccurrences(normalizedWords, {\n    phraseSize: opts.phraseSize,\n    searchRadius: opts.searchRadius,\n    tolerance: opts.tolerance,\n    minOccurrences: opts.minOccurrences\n  });\n\n  // Convert to time-based repetitions\n  const repetitions = phraseGroups.map((group, index) => ({\n    id: index,\n    occurrences: group.occurrences.map(occ => ({\n      startWord: occ.startIndex,\n      endWord: occ.endIndex,\n      start: words[occ.startIndex].start,\n      end: words[occ.endIndex].end,\n      text: words.slice(occ.startIndex, occ.endIndex + 1).map(w => w.word || w.text).join(' ')\n    })),\n    bestOccurrence: 0, // First occurrence is usually the \"keeper\"\n    similarity: group.similarity,\n    wordCount: opts.phraseSize\n  }));\n\n  // Filter overlapping repetitions\n  const filtered = filterOverlappingRepetitions(repetitions);\n\n  console.log(`[SPLICE Repetition] Found ${filtered.length} repetition group(s)`);\n\n  return {\n    repetitions: filtered,\n    metadata: {\n      mode: 'basic',\n      totalWords: words.length,\n      phraseSize: opts.phraseSize,\n      tolerance: opts.tolerance,\n      groupsFound: filtered.length,\n      totalOccurrences: filtered.reduce((sum, r) => sum + r.occurrences.length, 0)\n    }\n  };\n}\n\n/**\n * Find phrase occurrences using sliding window comparison\n *\n * @param {Array<string>} normalizedWords - Normalized word strings\n * @param {Object} options - Search options\n * @returns {Array} Groups of similar phrases\n */\nfunction findPhraseOccurrences(normalizedWords, options) {\n  const { phraseSize, searchRadius, tolerance, minOccurrences } = options;\n  const groups = [];\n  const usedIndices = new Set();\n\n  for (let i = 0; i <= normalizedWords.length - phraseSize; i++) {\n    if (usedIndices.has(i)) continue;\n\n    const phrase = normalizedWords.slice(i, i + phraseSize);\n    const occurrences = [{\n      startIndex: i,\n      endIndex: i + phraseSize - 1\n    }];\n\n    // Search ahead for similar phrases\n    const searchEnd = Math.min(i + searchRadius, normalizedWords.length - phraseSize);\n\n    for (let j = i + phraseSize; j <= searchEnd; j++) {\n      if (usedIndices.has(j)) continue;\n\n      const comparePhrase = normalizedWords.slice(j, j + phraseSize);\n      const similarity = calculateSimilarity(phrase, comparePhrase);\n\n      if (similarity >= tolerance) {\n        occurrences.push({\n          startIndex: j,\n          endIndex: j + phraseSize - 1\n        });\n        // Mark these indices as used\n        for (let k = j; k < j + phraseSize; k++) {\n          usedIndices.add(k);\n        }\n      }\n    }\n\n    if (occurrences.length >= minOccurrences) {\n      // Mark original phrase indices as used\n      for (let k = i; k < i + phraseSize; k++) {\n        usedIndices.add(k);\n      }\n\n      groups.push({\n        occurrences,\n        similarity: 1.0, // First match is reference\n        phraseText: phrase.join(' ')\n      });\n    }\n  }\n\n  return groups;\n}\n\n// =============================================================================\n// Core Detection - Advanced Mode (with OpenAI)\n// =============================================================================\n\n/**\n * Detect repetitions with OpenAI boundary refinement\n *\n * @param {Object} transcript - Transcript with words array\n * @param {Object} options - Detection options\n * @returns {Promise<Object>} Detection results with refined boundaries\n */\nasync function detectRepetitionsAdvanced(transcript, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options, mode: 'advanced' };\n\n  // First, do basic detection\n  const basicResult = detectRepetitionsBasic(transcript, opts);\n\n  if (basicResult.repetitions.length === 0) {\n    return basicResult;\n  }\n\n  // If OpenAI refinement is disabled, return basic results\n  if (!opts.useOpenAI) {\n    return {\n      ...basicResult,\n      metadata: { ...basicResult.metadata, mode: 'advanced-local' }\n    };\n  }\n\n  // Refine boundaries using OpenAI\n  const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });\n  const refinedRepetitions = [];\n\n  for (const rep of basicResult.repetitions) {\n    try {\n      const refined = await refineRepetitionBoundaries(openai, transcript.words, rep);\n      refinedRepetitions.push(refined);\n    } catch (err) {\n      console.warn('[SPLICE Repetition] OpenAI refinement failed, using basic:', err.message);\n      refinedRepetitions.push(rep);\n    }\n  }\n\n  return {\n    repetitions: refinedRepetitions,\n    metadata: {\n      ...basicResult.metadata,\n      mode: 'advanced-openai',\n      openaiRefined: true\n    }\n  };\n}\n\n/**\n * Refine repetition boundaries using OpenAI\n *\n * @param {OpenAI} openai - OpenAI client\n * @param {Array} words - All transcript words\n * @param {Object} repetition - Basic repetition detection\n * @returns {Promise<Object>} Refined repetition with better boundaries\n */\nasync function refineRepetitionBoundaries(openai, words, repetition) {\n  // Get context around the repetition\n  const allOccurrences = repetition.occurrences;\n  const minStart = Math.max(0, allOccurrences[0].startWord - 5);\n  const maxEnd = Math.min(words.length - 1, allOccurrences[allOccurrences.length - 1].endWord + 5);\n\n  // Build context string with markers\n  let contextText = '';\n  for (let i = minStart; i <= maxEnd; i++) {\n    const word = words[i].word || words[i].text;\n    // Mark occurrence boundaries\n    const isStart = allOccurrences.some(o => o.startWord === i);\n    const isEnd = allOccurrences.some(o => o.endWord === i);\n\n    if (isStart) contextText += '[';\n    contextText += word + ' ';\n    if (isEnd) contextText += '] ';\n  }\n\n  const prompt = `Analyze this text for repeated content. The brackets mark detected repetitions:\n\n\"${contextText.trim()}\"\n\nFor each bracketed section, determine:\n1. Is this truly a repetition/restart/stutter?\n2. What are the exact word boundaries that should be removed (keeping only the best take)?\n\nRespond in JSON format:\n{\n  \"isRepetition\": true/false,\n  \"keepOccurrence\": 0-based index of the occurrence to keep (usually the last/most complete),\n  \"removeRanges\": [{\"start\": wordIndex, \"end\": wordIndex}]\n}`;\n\n  const response = await openai.chat.completions.create({\n    model: 'gpt-4o-mini',\n    messages: [\n      { role: 'system', content: 'You are an expert video editor identifying repeated content in speech transcripts. Be precise about boundaries.' },\n      { role: 'user', content: prompt }\n    ],\n    temperature: 0,\n    max_tokens: 500\n  });\n\n  try {\n    const result = JSON.parse(response.choices[0].message.content);\n\n    if (!result.isRepetition) {\n      // False positive - return empty\n      return { ...repetition, occurrences: [], metadata: { refined: true, isRepetition: false } };\n    }\n\n    // Update best occurrence\n    return {\n      ...repetition,\n      bestOccurrence: result.keepOccurrence || 0,\n      metadata: { refined: true, openaiResult: result }\n    };\n  } catch (_parseErr) {\n    // Return original if parsing fails\n    return repetition;\n  }\n}\n\n// =============================================================================\n// Stutter Detection (Single-Word Repetitions)\n// =============================================================================\n\n/**\n * Detect single-word stutters (e.g., \"I I I think\")\n *\n * @param {Object} transcript - Transcript with words array\n * @param {Object} options - Detection options\n * @returns {Object} Stutter detection results\n */\nfunction detectStutters(transcript, options = {}) {\n  const {\n    minRepeats = 2,      // Minimum times word must repeat\n    maxGapMs = 500,      // Max gap between words (milliseconds)\n    ignoreFillers = true, // Ignore common filler words\n    minWordLength = 1    // Minimum word length (1 allows \"I\", \"a\", etc.)\n  } = options;\n\n  const words = transcript.words || [];\n  const stutters = [];\n\n  // Common filler words to optionally ignore\n  const fillerWords = new Set(['um', 'uh', 'ah', 'er', 'like', 'so', 'well', 'you know']);\n\n  let i = 0;\n  while (i < words.length) {\n    const word = words[i];\n    const normalized = normalizeWord(word.word || word.text || '');\n\n    // Skip fillers if configured\n    if (ignoreFillers && fillerWords.has(normalized)) {\n      i++;\n      continue;\n    }\n\n    // Skip very short words (configurable, default allows single chars like \"I\")\n    if (normalized.length < minWordLength) {\n      i++;\n      continue;\n    }\n\n    // Look for consecutive repeats\n    let repeatCount = 1;\n    let j = i + 1;\n\n    while (j < words.length) {\n      const nextWord = words[j];\n      const nextNormalized = normalizeWord(nextWord.word || nextWord.text || '');\n\n      // Check if same word\n      if (nextNormalized !== normalized) break;\n\n      // Check gap (convert to ms)\n      const gap = (nextWord.start - words[j - 1].end) * 1000;\n      if (gap > maxGapMs) break;\n\n      repeatCount++;\n      j++;\n    }\n\n    if (repeatCount >= minRepeats) {\n      stutters.push({\n        word: normalized,\n        occurrences: repeatCount,\n        startWord: i,\n        endWord: j - 1,\n        start: words[i].start,\n        end: words[j - 1].end,\n        duration: words[j - 1].end - words[i].start,\n        // Keep first occurrence, remove rest\n        keepStart: words[i].start,\n        keepEnd: words[i].end,\n        removeStart: words[i + 1].start,\n        removeEnd: words[j - 1].end\n      });\n    }\n\n    i = j; // Skip past all repeats\n  }\n\n  console.log(`[SPLICE Repetition] Found ${stutters.length} stutter(s)`);\n\n  return {\n    stutters,\n    metadata: {\n      type: 'stutters',\n      totalWords: words.length,\n      stutterCount: stutters.length,\n      totalRepeatedWords: stutters.reduce((sum, s) => sum + s.occurrences, 0)\n    }\n  };\n}\n\n// =============================================================================\n// Utility Functions\n// =============================================================================\n\n/**\n * Normalize a word for comparison\n */\nfunction normalizeWord(word) {\n  return word\n    .toLowerCase()\n    .replace(/[^\\w\\s'-]/g, '') // Remove punctuation except apostrophe/hyphen\n    .trim();\n}\n\n/**\n * Calculate similarity between two word arrays\n * Uses Jaccard-like similarity with position weighting\n *\n * @param {Array<string>} arr1 - First word array\n * @param {Array<string>} arr2 - Second word array\n * @returns {number} Similarity score (0-1)\n */\nfunction calculateSimilarity(arr1, arr2) {\n  if (arr1.length !== arr2.length) return 0;\n  if (arr1.length === 0) return 0;\n\n  let matches = 0;\n  for (let i = 0; i < arr1.length; i++) {\n    if (arr1[i] === arr2[i]) {\n      matches++;\n    } else {\n      // Partial credit for similar words (edit distance)\n      const maxLen = Math.max(arr1[i].length, arr2[i].length);\n      if (maxLen > 0) {\n        // Early exit: if edit distance is > 30% of max length, similarity < 0.7\n        const maxAllowedDist = Math.floor(maxLen * 0.3);\n        const editDist = levenshteinDistance(arr1[i], arr2[i], maxAllowedDist);\n        if (editDist <= maxAllowedDist) {\n          const wordSim = 1 - (editDist / maxLen);\n          matches += wordSim;\n        }\n        // If editDist > maxAllowedDist, wordSim < 0.7, so no credit added\n      }\n    }\n  }\n\n  return matches / arr1.length;\n}\n\n// Pre-allocated buffers for Levenshtein (avoids allocation per call)\n// Max word length we support (longer words use inline allocation)\nconst LEVENSHTEIN_MAX_LEN = 100;\nlet _prevRow = new Uint16Array(LEVENSHTEIN_MAX_LEN + 1);\nlet _currRow = new Uint16Array(LEVENSHTEIN_MAX_LEN + 1);\n\n/**\n * Levenshtein edit distance between two strings\n * Optimized single-row DP with O(n) space and early exit\n */\nfunction levenshteinDistance(s1, s2, maxThreshold = Infinity) {\n  // Fast path: exact match\n  if (s1 === s2) return 0;\n\n  const m = s1.length;\n  const n = s2.length;\n\n  // Fast path: empty string\n  if (m === 0) return n;\n  if (n === 0) return m;\n\n  // Early exit if length difference exceeds threshold\n  if (Math.abs(m - n) > maxThreshold) return maxThreshold + 1;\n\n  // Use pre-allocated buffers if possible, otherwise allocate\n  let prev, curr;\n  if (n < LEVENSHTEIN_MAX_LEN) {\n    prev = _prevRow;\n    curr = _currRow;\n  } else {\n    prev = new Uint16Array(n + 1);\n    curr = new Uint16Array(n + 1);\n  }\n\n  // Initialize first row\n  for (let j = 0; j <= n; j++) prev[j] = j;\n\n  for (let i = 1; i <= m; i++) {\n    curr[0] = i;\n    const s1Char = s1.charCodeAt(i - 1);\n    let minInRow = i; // Track minimum value in current row for early exit\n\n    for (let j = 1; j <= n; j++) {\n      const cost = s1Char === s2.charCodeAt(j - 1) ? 0 : 1;\n\n      // curr[j] = min(insertion, deletion, substitution)\n      const ins = curr[j - 1] + 1;\n      const del = prev[j] + 1;\n      const sub = prev[j - 1] + cost;\n\n      curr[j] = ins < del ? (ins < sub ? ins : sub) : (del < sub ? del : sub);\n\n      if (curr[j] < minInRow) minInRow = curr[j];\n    }\n\n    // Early exit: if minimum in row exceeds threshold, result will exceed threshold\n    if (minInRow > maxThreshold) return maxThreshold + 1;\n\n    // Swap rows\n    const tmp = prev;\n    prev = curr;\n    curr = tmp;\n  }\n\n  return prev[n];\n}\n\n/**\n * Filter overlapping repetition groups\n */\nfunction filterOverlappingRepetitions(repetitions) {\n  if (repetitions.length <= 1) return repetitions;\n\n  // Sort by first occurrence start\n  const sorted = [...repetitions].sort((a, b) =>\n    a.occurrences[0].start - b.occurrences[0].start\n  );\n\n  const result = [];\n  let lastEnd = -1;\n\n  for (const rep of sorted) {\n    const firstOcc = rep.occurrences[0];\n    // Check if overlaps with previous\n    if (firstOcc.start >= lastEnd) {\n      result.push(rep);\n      // Update lastEnd to include all occurrences\n      lastEnd = Math.max(...rep.occurrences.map(o => o.end));\n    }\n  }\n\n  return result;\n}\n\n/**\n * Convert repetitions to removal segments\n * Keeps the \"best\" occurrence, returns segments to remove\n *\n * @param {Array} repetitions - Detected repetitions\n * @returns {Array} Segments to remove from timeline\n */\nfunction repetitionsToRemovalSegments(repetitions) {\n  const segments = [];\n\n  for (const rep of repetitions) {\n    const keepIndex = rep.bestOccurrence || 0;\n\n    rep.occurrences.forEach((occ, i) => {\n      if (i !== keepIndex) {\n        segments.push({\n          start: occ.start,\n          end: occ.end,\n          type: 'repetition',\n          text: occ.text,\n          reason: `Duplicate of occurrence ${keepIndex + 1}`\n        });\n      }\n    });\n  }\n\n  // Sort by start time and merge adjacent\n  return mergeAdjacentSegments(segments);\n}\n\n/**\n * Convert stutters to removal segments\n *\n * @param {Array} stutters - Detected stutters\n * @returns {Array} Segments to remove\n */\nfunction stuttersToRemovalSegments(stutters) {\n  return stutters.map(s => ({\n    start: s.removeStart,\n    end: s.removeEnd,\n    type: 'stutter',\n    text: `${s.word} (x${s.occurrences})`,\n    keepRange: { start: s.keepStart, end: s.keepEnd }\n  }));\n}\n\n/**\n * Merge adjacent removal segments\n */\nfunction mergeAdjacentSegments(segments, maxGap = 0.1) {\n  if (segments.length <= 1) return segments;\n\n  const sorted = [...segments].sort((a, b) => a.start - b.start);\n  const merged = [];\n  let current = { ...sorted[0] };\n\n  for (let i = 1; i < sorted.length; i++) {\n    const next = sorted[i];\n\n    if (next.start - current.end <= maxGap) {\n      // Merge\n      current.end = next.end;\n      current.text = `${current.text}, ${next.text}`;\n    } else {\n      merged.push(current);\n      current = { ...next };\n    }\n  }\n\n  merged.push(current);\n  return merged;\n}\n\n// =============================================================================\n// Combined Detection\n// =============================================================================\n\n/**\n * Run all repetition detection (phrases + stutters)\n *\n * @param {Object} transcript - Transcript with words array\n * @param {Object} options - Detection options\n * @returns {Promise<Object>} Combined detection results\n */\nasync function detectAllRepetitions(transcript, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n\n  // Detect phrase repetitions\n  const phraseResult = opts.useOpenAI\n    ? await detectRepetitionsAdvanced(transcript, opts)\n    : detectRepetitionsBasic(transcript, opts);\n\n  // Detect stutters\n  const stutterResult = detectStutters(transcript, opts);\n\n  // Combine removal segments\n  const phraseSegments = repetitionsToRemovalSegments(phraseResult.repetitions);\n  const stutterSegments = stuttersToRemovalSegments(stutterResult.stutters);\n  const allSegments = mergeAdjacentSegments([...phraseSegments, ...stutterSegments]);\n\n  // Calculate total removable time\n  const totalRemovableTime = allSegments.reduce((sum, s) => sum + (s.end - s.start), 0);\n\n  return {\n    repetitions: phraseResult.repetitions,\n    stutters: stutterResult.stutters,\n    removalSegments: allSegments,\n    metadata: {\n      ...phraseResult.metadata,\n      stutterCount: stutterResult.stutters.length,\n      totalRemovalSegments: allSegments.length,\n      totalRemovableTime: parseFloat(totalRemovableTime.toFixed(3))\n    }\n  };\n}\n\n// =============================================================================\n// Exports\n// =============================================================================\n\nmodule.exports = {\n  // Core detection\n  detectRepetitionsBasic,\n  detectRepetitionsAdvanced,\n  detectStutters,\n  detectAllRepetitions,\n\n  // Conversion utilities\n  repetitionsToRemovalSegments,\n  stuttersToRemovalSegments,\n\n  // Helper utilities\n  normalizeWord,\n  calculateSimilarity,\n  levenshteinDistance,\n  filterOverlappingRepetitions,\n  mergeAdjacentSegments,\n\n  // Config\n  DEFAULT_OPTIONS\n};\n","numLines":666,"startLine":1,"totalLines":666}}}
{"parentUuid":"b6bf733e-1a43-4faf-844b-bcf215e79890","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Vp6dYJznR83tGsL3TNkxVC","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Cut List Generator (v3.5)\n     3→ *\n     4→ * Generates JSON cut lists for direct DOM sequence building.\n     5→ * Replaces XML processing for v3.5 architecture.\n     6→ *\n     7→ * Cut list format:\n     8→ * {\n     9→ *   version: '3.5',\n    10→ *   source: { name, path, duration },\n    11→ *   segments: [{ type, sourceName, sourcePath, inPoint, outPoint }],\n    12→ *   metadata: { silencesRemoved, takesDetected, generatedAt }\n    13→ * }\n    14→ *\n    15→ * Frame Alignment:\n    16→ * When frameRate is provided, all cut points are aligned to frame boundaries.\n    17→ * This prevents sub-frame edits that can cause playback issues.\n    18→ */\n    19→\n    20→/**\n    21→ * Align a time value to the nearest frame boundary\n    22→ * @param {number} time - Time in seconds\n    23→ * @param {number} frameRate - Frames per second (e.g., 23.976, 24, 29.97, 30, 60)\n    24→ * @returns {number} Time aligned to frame boundary\n    25→ */\n    26→function alignToFrame(time, frameRate) {\n    27→  if (!frameRate || frameRate <= 0) return time;\n    28→  const frameDuration = 1 / frameRate;\n    29→  return Math.round(time / frameDuration) * frameDuration;\n    30→}\n    31→\n    32→/**\n    33→ * Align to frame boundary, rounding down (for in-points)\n    34→ */\n    35→function alignToFrameFloor(time, frameRate) {\n    36→  if (!frameRate || frameRate <= 0) return time;\n    37→  const frameDuration = 1 / frameRate;\n    38→  return Math.floor(time / frameDuration) * frameDuration;\n    39→}\n    40→\n    41→/**\n    42→ * Align to frame boundary, rounding up (for out-points)\n    43→ */\n    44→function alignToFrameCeil(time, frameRate) {\n    45→  if (!frameRate || frameRate <= 0) return time;\n    46→  const frameDuration = 1 / frameRate;\n    47→  return Math.ceil(time / frameDuration) * frameDuration;\n    48→}\n    49→\n    50→/**\n    51→ * Generate a cut list from detected silences\n    52→ * Creates segments that represent speech (non-silence) portions\n    53→ *\n    54→ * @param {Object} options - Generation options\n    55→ * @param {string} options.sourceName - Name of the source clip\n    56→ * @param {string} options.sourcePath - Full path to the source file\n    57→ * @param {number} options.duration - Total duration of source in seconds\n    58→ * @param {Array} options.silences - Array of silence segments [{start, end, duration}]\n    59→ * @param {Array} [options.takes] - Optional array of detected takes\n    60→ * @param {Object} [options.settings] - Optional generation settings\n    61→ * @returns {Object} The generated cut list\n    62→ */\n    63→function generateCutList(options) {\n    64→  const {\n    65→    sourceName,\n    66→    sourcePath,\n    67→    duration,\n    68→    silences = [],\n    69→    takes = [],\n    70→    settings = {}\n    71→  } = options;\n    72→\n    73→  // Settings with defaults\n    74→  const {\n    75→    minSegmentDuration = 0.1,  // Minimum segment length to include\n    76→    padding = 0,               // Padding around speech (extend into silence)\n    77→    markBestTakes = true,      // Color-code best takes differently\n    78→    frameRate = 0,             // Frame rate for alignment (0 = no alignment)\n    79→    jCutOffset = 0,            // J-cut: audio starts before video (negative value)\n    80→    lCutOffset = 0,            // L-cut: audio extends after video (positive value)\n    81→    labelTakes = true,         // Add take numbering and labels\n    82→    colorCode = true           // Apply color coding to segments\n    83→  } = settings;\n    84→\n    85→  // Take numbering counter\n    86→  let takeNumber = 1;\n    87→\n    88→  // Sort silences by start time\n    89→  const sortedSilences = [...silences].sort((a, b) => a.start - b.start);\n    90→\n    91→  // Build segments from gaps between silences (speech segments)\n    92→  const segments = [];\n    93→  let lastEnd = 0;\n    94→  let silencesRemoved = 0;\n    95→\n    96→  for (const silence of sortedSilences) {\n    97→    // Calculate segment boundaries with padding and frame alignment\n    98→    let segmentStart = Math.max(0, lastEnd - padding);\n    99→    let segmentEnd = Math.min(duration, silence.start + padding);\n   100→\n   101→    // Align to frame boundaries if frameRate is specified\n   102→    if (frameRate > 0) {\n   103→      segmentStart = alignToFrameFloor(segmentStart, frameRate);\n   104→      segmentEnd = alignToFrameCeil(segmentEnd, frameRate);\n   105→    }\n   106→\n   107→    // Only add if segment meets minimum duration\n   108→    if (segmentEnd - segmentStart >= minSegmentDuration) {\n   109→      const segment = {\n   110→        type: 'speech',\n   111→        sourceName,\n   112→        sourcePath,\n   113→        inPoint: parseFloat(segmentStart.toFixed(6)),\n   114→        outPoint: parseFloat(segmentEnd.toFixed(6))\n   115→      };\n   116→\n   117→      // Add J-cut/L-cut audio offsets if specified\n   118→      if (jCutOffset !== 0 || lCutOffset !== 0) {\n   119→        // Audio in/out points differ from video\n   120→        segment.audioInOffset = jCutOffset;  // Negative = audio starts earlier (J-cut)\n   121→        segment.audioOutOffset = lCutOffset; // Positive = audio ends later (L-cut)\n   122→\n   123→        // Calculate actual audio in/out points\n   124→        segment.audioInPoint = parseFloat(Math.max(0, segmentStart + jCutOffset).toFixed(6));\n   125→        segment.audioOutPoint = parseFloat(Math.min(duration, segmentEnd + lCutOffset).toFixed(6));\n   126→      }\n   127→\n   128→      // Check if this segment contains a take\n   129→      const containingTake = findContainingTake(segmentStart, segmentEnd, takes);\n   130→      if (containingTake) {\n   131→        segment.take = {\n   132→          text: containingTake.text?.substring(0, 100),\n   133→          isBest: containingTake.isBest || false\n   134→        };\n   135→        if (containingTake.isBest && markBestTakes) {\n   136→          segment.type = 'best_take';\n   137→        }\n   138→\n   139→        // Add take labeling (Phase 1 feature)\n   140→        if (labelTakes) {\n   141→          const shortLabel = generateShortLabel(containingTake.text);\n   142→          segment.takeNumber = takeNumber;\n   143→          segment.takeLabel = `Take ${takeNumber}${shortLabel ? `: ${shortLabel}` : ''}`;\n   144→          takeNumber++;\n   145→        }\n   146→      }\n   147→\n   148→      // Add color code hint for builder\n   149→      if (colorCode) {\n   150→        segment.colorHint = getColorHintForType(segment.type);\n   151→      }\n   152→\n   153→      segments.push(segment);\n   154→    }\n   155→\n   156→    silencesRemoved++;\n   157→    lastEnd = silence.end;\n   158→  }\n   159→\n   160→  // Add final segment after last silence\n   161→  if (lastEnd < duration) {\n   162→    let segmentStart = Math.max(0, lastEnd - padding);\n   163→    let segmentEnd = duration;\n   164→\n   165→    // Align to frame boundaries if frameRate is specified\n   166→    if (frameRate > 0) {\n   167→      segmentStart = alignToFrameFloor(segmentStart, frameRate);\n   168→      segmentEnd = alignToFrameCeil(segmentEnd, frameRate);\n   169→    }\n   170→\n   171→    if (segmentEnd - segmentStart >= minSegmentDuration) {\n   172→      const segment = {\n   173→        type: 'speech',\n   174→        sourceName,\n   175→        sourcePath,\n   176→        inPoint: parseFloat(segmentStart.toFixed(6)),\n   177→        outPoint: parseFloat(segmentEnd.toFixed(6))\n   178→      };\n   179→\n   180→      // Add J-cut/L-cut audio offsets if specified\n   181→      if (jCutOffset !== 0 || lCutOffset !== 0) {\n   182→        segment.audioInOffset = jCutOffset;\n   183→        segment.audioOutOffset = lCutOffset;\n   184→        segment.audioInPoint = parseFloat(Math.max(0, segmentStart + jCutOffset).toFixed(6));\n   185→        segment.audioOutPoint = parseFloat(Math.min(duration, segmentEnd + lCutOffset).toFixed(6));\n   186→      }\n   187→\n   188→      // Check if this segment contains a take\n   189→      const containingTake = findContainingTake(segmentStart, segmentEnd, takes);\n   190→      if (containingTake) {\n   191→        segment.take = {\n   192→          text: containingTake.text?.substring(0, 100),\n   193→          isBest: containingTake.isBest || false\n   194→        };\n   195→        if (containingTake.isBest && markBestTakes) {\n   196→          segment.type = 'best_take';\n   197→        }\n   198→\n   199→        // Add take labeling (Phase 1 feature)\n   200→        if (labelTakes) {\n   201→          const shortLabel = generateShortLabel(containingTake.text);\n   202→          segment.takeNumber = takeNumber;\n   203→          segment.takeLabel = `Take ${takeNumber}${shortLabel ? `: ${shortLabel}` : ''}`;\n   204→          takeNumber++;\n   205→        }\n   206→      }\n   207→\n   208→      // Add color code hint for builder\n   209→      if (colorCode) {\n   210→        segment.colorHint = getColorHintForType(segment.type);\n   211→      }\n   212→\n   213→      segments.push(segment);\n   214→    }\n   215→  }\n   216→\n   217→  // Calculate stats\n   218→  const totalKeptDuration = segments.reduce(\n   219→    (sum, seg) => sum + (seg.outPoint - seg.inPoint),\n   220→    0\n   221→  );\n   222→  const totalRemovedDuration = duration - totalKeptDuration;\n   223→\n   224→  const cutList = {\n   225→    version: '3.5',\n   226→    source: {\n   227→      name: sourceName,\n   228→      path: sourcePath,\n   229→      duration\n   230→    },\n   231→    segments,\n   232→    metadata: {\n   233→      silencesRemoved,\n   234→      takesDetected: takes.length,\n   235→      takesLabeled: labelTakes && takeNumber > 1,\n   236→      colorCoded: colorCode,\n   237→      totalKeptDuration: parseFloat(totalKeptDuration.toFixed(3)),\n   238→      totalRemovedDuration: parseFloat(totalRemovedDuration.toFixed(3)),\n   239→      segmentCount: segments.length,\n   240→      frameRate: frameRate > 0 ? frameRate : null,\n   241→      frameAligned: frameRate > 0,\n   242→      jCutOffset: jCutOffset !== 0 ? jCutOffset : null,\n   243→      lCutOffset: lCutOffset !== 0 ? lCutOffset : null,\n   244→      hasAudioOffsets: jCutOffset !== 0 || lCutOffset !== 0,\n   245→      generatedAt: new Date().toISOString()\n   246→    }\n   247→  };\n   248→\n   249→  console.log(`[SPLICE] Generated cut list: ${segments.length} segments, ${silencesRemoved} silences removed`);\n   250→\n   251→  return cutList;\n   252→}\n   253→\n   254→/**\n   255→ * Find a take that overlaps with a given time range\n   256→ * @param {number} start - Start time in seconds\n   257→ * @param {number} end - End time in seconds\n   258→ * @param {Array} takes - Array of takes\n   259→ * @returns {Object|null} The overlapping take or null\n   260→ */\n   261→function findContainingTake(start, end, takes) {\n   262→  if (!takes || takes.length === 0) return null;\n   263→\n   264→  for (const take of takes) {\n   265→    const takeStart = take.startTime || take.start;\n   266→    const takeEnd = take.endTime || take.end;\n   267→\n   268→    // Check for overlap\n   269→    if (start < takeEnd && end > takeStart) {\n   270→      return take;\n   271→    }\n   272→  }\n   273→\n   274→  return null;\n   275→}\n   276→\n   277→/**\n   278→ * Generate a short label from take text (first 3-5 words)\n   279→ * @param {string} text - Full take text\n   280→ * @returns {string} Short label (max 30 chars)\n   281→ */\n   282→function generateShortLabel(text) {\n   283→  if (!text || typeof text !== 'string') return '';\n   284→\n   285→  // Clean and split into words\n   286→  const words = text.trim().split(/\\s+/);\n   287→\n   288→  // Take first 5 words max\n   289→  const shortWords = words.slice(0, 5);\n   290→\n   291→  // Join and truncate to 30 chars\n   292→  let label = shortWords.join(' ');\n   293→  if (label.length > 30) {\n   294→    label = label.substring(0, 27) + '...';\n   295→  }\n   296→\n   297→  return label;\n   298→}\n   299→\n   300→/**\n   301→ * Get color hint for segment type\n   302→ * Color hints are used by the builder to apply appropriate colors\n   303→ *\n   304→ * Color mapping:\n   305→ * - speech: green (keeps attention, standard content)\n   306→ * - best_take: cerulean/light blue (highlight best content)\n   307→ * - take: lavender (regular take)\n   308→ * - silence: violet (rarely used, mostly removed)\n   309→ * - wide_shot: yellow (visual variety)\n   310→ * - speaker_a: mango/orange (speaker differentiation)\n   311→ * - speaker_b: caribbean/teal (speaker differentiation)\n   312→ *\n   313→ * @param {string} type - Segment type\n   314→ * @returns {string} Color hint name\n   315→ */\n   316→function getColorHintForType(type) {\n   317→  switch (type) {\n   318→    case 'best_take':\n   319→      return 'cerulean';  // Light blue - best content\n   320→    case 'take':\n   321→      return 'lavender';  // Light purple - regular take\n   322→    case 'speech':\n   323→      return 'green';     // Green - speech segments\n   324→    case 'silence':\n   325→      return 'violet';    // Purple - silence (if kept)\n   326→    case 'wide_shot':\n   327→      return 'yellow';    // Yellow - visual variety\n   328→    case 'speaker_a':\n   329→      return 'mango';     // Orange - first speaker\n   330→    case 'speaker_b':\n   331→      return 'caribbean'; // Teal - second speaker\n   332→    default:\n   333→      return 'none';\n   334→  }\n   335→}\n   336→\n   337→/**\n   338→ * Generate a cut list that keeps only specific takes\n   339→ * Useful for \"keep best takes only\" workflow\n   340→ *\n   341→ * @param {Object} options - Generation options\n   342→ * @param {string} options.sourceName - Name of the source clip\n   343→ * @param {string} options.sourcePath - Full path to the source file\n   344→ * @param {number} options.duration - Total duration of source in seconds\n   345→ * @param {Array} options.takes - Array of takes to keep\n   346→ * @param {Object} [options.settings] - Optional generation settings\n   347→ * @returns {Object} The generated cut list\n   348→ */\n   349→function generateTakesCutList(options) {\n   350→  const {\n   351→    sourceName,\n   352→    sourcePath,\n   353→    duration,\n   354→    takes = [],\n   355→    settings = {}\n   356→  } = options;\n   357→\n   358→  const {\n   359→    padding = 0.2,          // Small padding around takes\n   360→    onlyBestTakes = false   // If true, only include takes marked as best\n   361→  } = settings;\n   362→\n   363→  // Filter to best takes if requested\n   364→  const takesToKeep = onlyBestTakes\n   365→    ? takes.filter(t => t.isBest)\n   366→    : takes;\n   367→\n   368→  // Sort by start time\n   369→  const sortedTakes = [...takesToKeep].sort((a, b) => {\n   370→    const aStart = a.startTime || a.start;\n   371→    const bStart = b.startTime || b.start;\n   372→    return aStart - bStart;\n   373→  });\n   374→\n   375→  // Build segments from takes\n   376→  const segments = sortedTakes.map(take => {\n   377→    const takeStart = take.startTime || take.start;\n   378→    const takeEnd = take.endTime || take.end;\n   379→\n   380→    return {\n   381→      type: take.isBest ? 'best_take' : 'take',\n   382→      sourceName,\n   383→      sourcePath,\n   384→      inPoint: Math.max(0, takeStart - padding),\n   385→      outPoint: Math.min(duration, takeEnd + padding),\n   386→      take: {\n   387→        text: take.text?.substring(0, 100),\n   388→        isBest: take.isBest || false\n   389→      }\n   390→    };\n   391→  });\n   392→\n   393→  const totalKeptDuration = segments.reduce(\n   394→    (sum, seg) => sum + (seg.outPoint - seg.inPoint),\n   395→    0\n   396→  );\n   397→\n   398→  const cutList = {\n   399→    version: '3.5',\n   400→    source: {\n   401→      name: sourceName,\n   402→      path: sourcePath,\n   403→      duration\n   404→    },\n   405→    segments,\n   406→    metadata: {\n   407→      takesKept: segments.length,\n   408→      takesTotal: takes.length,\n   409→      totalKeptDuration: parseFloat(totalKeptDuration.toFixed(3)),\n   410→      totalRemovedDuration: parseFloat((duration - totalKeptDuration).toFixed(3)),\n   411→      generatedAt: new Date().toISOString()\n   412→    }\n   413→  };\n   414→\n   415→  console.log(`[SPLICE] Generated takes cut list: ${segments.length} takes kept`);\n   416→\n   417→  return cutList;\n   418→}\n   419→\n   420→/**\n   421→ * Merge overlapping segments in a cut list\n   422→ * Useful when silences and takes create overlapping regions\n   423→ *\n   424→ * @param {Object} cutList - The cut list to merge\n   425→ * @returns {Object} Cut list with merged segments\n   426→ */\n   427→function mergeOverlappingSegments(cutList) {\n   428→  if (!cutList.segments || cutList.segments.length < 2) {\n   429→    return cutList;\n   430→  }\n   431→\n   432→  // Sort by start time\n   433→  const sorted = [...cutList.segments].sort((a, b) => a.inPoint - b.inPoint);\n   434→\n   435→  const merged = [];\n   436→  let current = { ...sorted[0] };\n   437→\n   438→  for (let i = 1; i < sorted.length; i++) {\n   439→    const next = sorted[i];\n   440→\n   441→    // Check for overlap\n   442→    if (next.inPoint <= current.outPoint) {\n   443→      // Merge: extend current segment\n   444→      current.outPoint = Math.max(current.outPoint, next.outPoint);\n   445→      // Keep the more specific type (best_take > take > speech)\n   446→      if (next.type === 'best_take' || (next.type === 'take' && current.type === 'speech')) {\n   447→        current.type = next.type;\n   448→        current.take = next.take;\n   449→      }\n   450→    } else {\n   451→      // No overlap: save current and start new\n   452→      merged.push(current);\n   453→      current = { ...next };\n   454→    }\n   455→  }\n   456→\n   457→  // Don't forget the last segment\n   458→  merged.push(current);\n   459→\n   460→  // Update metadata\n   461→  const newCutList = {\n   462→    ...cutList,\n   463→    segments: merged,\n   464→    metadata: {\n   465→      ...cutList.metadata,\n   466→      segmentCount: merged.length,\n   467→      mergedFrom: cutList.segments.length\n   468→    }\n   469→  };\n   470→\n   471→  console.log(`[SPLICE] Merged ${cutList.segments.length} segments into ${merged.length}`);\n   472→\n   473→  return newCutList;\n   474→}\n   475→\n   476→/**\n   477→ * Validate a cut list structure\n   478→ * @param {Object} cutList - The cut list to validate\n   479→ * @returns {{valid: boolean, errors: string[]}}\n   480→ */\n   481→function validateCutList(cutList) {\n   482→  const errors = [];\n   483→\n   484→  if (!cutList) {\n   485→    errors.push('Cut list is null or undefined');\n   486→    return { valid: false, errors };\n   487→  }\n   488→\n   489→  if (!cutList.version) {\n   490→    errors.push('Missing version field');\n   491→  }\n   492→\n   493→  if (!cutList.source) {\n   494→    errors.push('Missing source field');\n   495→  } else {\n   496→    if (!cutList.source.name && !cutList.source.path) {\n   497→      errors.push('Source must have name or path');\n   498→    }\n   499→    if (typeof cutList.source.duration !== 'number' || cutList.source.duration <= 0) {\n   500→      errors.push('Source duration must be a positive number');\n   501→    }\n   502→  }\n   503→\n   504→  if (!Array.isArray(cutList.segments)) {\n   505→    errors.push('Segments must be an array');\n   506→  } else {\n   507→    cutList.segments.forEach((seg, i) => {\n   508→      if (typeof seg.inPoint !== 'number') {\n   509→        errors.push(`Segment ${i}: inPoint must be a number`);\n   510→      }\n   511→      if (typeof seg.outPoint !== 'number') {\n   512→        errors.push(`Segment ${i}: outPoint must be a number`);\n   513→      }\n   514→      if (seg.inPoint >= seg.outPoint) {\n   515→        errors.push(`Segment ${i}: inPoint must be less than outPoint`);\n   516→      }\n   517→      if (!seg.sourceName && !seg.sourcePath) {\n   518→        errors.push(`Segment ${i}: must have sourceName or sourcePath`);\n   519→      }\n   520→    });\n   521→  }\n   522→\n   523→  return {\n   524→    valid: errors.length === 0,\n   525→    errors\n   526→  };\n   527→}\n   528→\n   529→module.exports = {\n   530→  generateCutList,\n   531→  generateTakesCutList,\n   532→  mergeOverlappingSegments,\n   533→  validateCutList,\n   534→  // Frame alignment utilities\n   535→  alignToFrame,\n   536→  alignToFrameFloor,\n   537→  alignToFrameCeil\n   538→};\n   539→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"39aa84e0-f54a-4209-8491-6c76ac1a77bf","timestamp":"2025-12-26T07:40:14.887Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/cutListGenerator.js","content":"/**\n * SPLICE Cut List Generator (v3.5)\n *\n * Generates JSON cut lists for direct DOM sequence building.\n * Replaces XML processing for v3.5 architecture.\n *\n * Cut list format:\n * {\n *   version: '3.5',\n *   source: { name, path, duration },\n *   segments: [{ type, sourceName, sourcePath, inPoint, outPoint }],\n *   metadata: { silencesRemoved, takesDetected, generatedAt }\n * }\n *\n * Frame Alignment:\n * When frameRate is provided, all cut points are aligned to frame boundaries.\n * This prevents sub-frame edits that can cause playback issues.\n */\n\n/**\n * Align a time value to the nearest frame boundary\n * @param {number} time - Time in seconds\n * @param {number} frameRate - Frames per second (e.g., 23.976, 24, 29.97, 30, 60)\n * @returns {number} Time aligned to frame boundary\n */\nfunction alignToFrame(time, frameRate) {\n  if (!frameRate || frameRate <= 0) return time;\n  const frameDuration = 1 / frameRate;\n  return Math.round(time / frameDuration) * frameDuration;\n}\n\n/**\n * Align to frame boundary, rounding down (for in-points)\n */\nfunction alignToFrameFloor(time, frameRate) {\n  if (!frameRate || frameRate <= 0) return time;\n  const frameDuration = 1 / frameRate;\n  return Math.floor(time / frameDuration) * frameDuration;\n}\n\n/**\n * Align to frame boundary, rounding up (for out-points)\n */\nfunction alignToFrameCeil(time, frameRate) {\n  if (!frameRate || frameRate <= 0) return time;\n  const frameDuration = 1 / frameRate;\n  return Math.ceil(time / frameDuration) * frameDuration;\n}\n\n/**\n * Generate a cut list from detected silences\n * Creates segments that represent speech (non-silence) portions\n *\n * @param {Object} options - Generation options\n * @param {string} options.sourceName - Name of the source clip\n * @param {string} options.sourcePath - Full path to the source file\n * @param {number} options.duration - Total duration of source in seconds\n * @param {Array} options.silences - Array of silence segments [{start, end, duration}]\n * @param {Array} [options.takes] - Optional array of detected takes\n * @param {Object} [options.settings] - Optional generation settings\n * @returns {Object} The generated cut list\n */\nfunction generateCutList(options) {\n  const {\n    sourceName,\n    sourcePath,\n    duration,\n    silences = [],\n    takes = [],\n    settings = {}\n  } = options;\n\n  // Settings with defaults\n  const {\n    minSegmentDuration = 0.1,  // Minimum segment length to include\n    padding = 0,               // Padding around speech (extend into silence)\n    markBestTakes = true,      // Color-code best takes differently\n    frameRate = 0,             // Frame rate for alignment (0 = no alignment)\n    jCutOffset = 0,            // J-cut: audio starts before video (negative value)\n    lCutOffset = 0,            // L-cut: audio extends after video (positive value)\n    labelTakes = true,         // Add take numbering and labels\n    colorCode = true           // Apply color coding to segments\n  } = settings;\n\n  // Take numbering counter\n  let takeNumber = 1;\n\n  // Sort silences by start time\n  const sortedSilences = [...silences].sort((a, b) => a.start - b.start);\n\n  // Build segments from gaps between silences (speech segments)\n  const segments = [];\n  let lastEnd = 0;\n  let silencesRemoved = 0;\n\n  for (const silence of sortedSilences) {\n    // Calculate segment boundaries with padding and frame alignment\n    let segmentStart = Math.max(0, lastEnd - padding);\n    let segmentEnd = Math.min(duration, silence.start + padding);\n\n    // Align to frame boundaries if frameRate is specified\n    if (frameRate > 0) {\n      segmentStart = alignToFrameFloor(segmentStart, frameRate);\n      segmentEnd = alignToFrameCeil(segmentEnd, frameRate);\n    }\n\n    // Only add if segment meets minimum duration\n    if (segmentEnd - segmentStart >= minSegmentDuration) {\n      const segment = {\n        type: 'speech',\n        sourceName,\n        sourcePath,\n        inPoint: parseFloat(segmentStart.toFixed(6)),\n        outPoint: parseFloat(segmentEnd.toFixed(6))\n      };\n\n      // Add J-cut/L-cut audio offsets if specified\n      if (jCutOffset !== 0 || lCutOffset !== 0) {\n        // Audio in/out points differ from video\n        segment.audioInOffset = jCutOffset;  // Negative = audio starts earlier (J-cut)\n        segment.audioOutOffset = lCutOffset; // Positive = audio ends later (L-cut)\n\n        // Calculate actual audio in/out points\n        segment.audioInPoint = parseFloat(Math.max(0, segmentStart + jCutOffset).toFixed(6));\n        segment.audioOutPoint = parseFloat(Math.min(duration, segmentEnd + lCutOffset).toFixed(6));\n      }\n\n      // Check if this segment contains a take\n      const containingTake = findContainingTake(segmentStart, segmentEnd, takes);\n      if (containingTake) {\n        segment.take = {\n          text: containingTake.text?.substring(0, 100),\n          isBest: containingTake.isBest || false\n        };\n        if (containingTake.isBest && markBestTakes) {\n          segment.type = 'best_take';\n        }\n\n        // Add take labeling (Phase 1 feature)\n        if (labelTakes) {\n          const shortLabel = generateShortLabel(containingTake.text);\n          segment.takeNumber = takeNumber;\n          segment.takeLabel = `Take ${takeNumber}${shortLabel ? `: ${shortLabel}` : ''}`;\n          takeNumber++;\n        }\n      }\n\n      // Add color code hint for builder\n      if (colorCode) {\n        segment.colorHint = getColorHintForType(segment.type);\n      }\n\n      segments.push(segment);\n    }\n\n    silencesRemoved++;\n    lastEnd = silence.end;\n  }\n\n  // Add final segment after last silence\n  if (lastEnd < duration) {\n    let segmentStart = Math.max(0, lastEnd - padding);\n    let segmentEnd = duration;\n\n    // Align to frame boundaries if frameRate is specified\n    if (frameRate > 0) {\n      segmentStart = alignToFrameFloor(segmentStart, frameRate);\n      segmentEnd = alignToFrameCeil(segmentEnd, frameRate);\n    }\n\n    if (segmentEnd - segmentStart >= minSegmentDuration) {\n      const segment = {\n        type: 'speech',\n        sourceName,\n        sourcePath,\n        inPoint: parseFloat(segmentStart.toFixed(6)),\n        outPoint: parseFloat(segmentEnd.toFixed(6))\n      };\n\n      // Add J-cut/L-cut audio offsets if specified\n      if (jCutOffset !== 0 || lCutOffset !== 0) {\n        segment.audioInOffset = jCutOffset;\n        segment.audioOutOffset = lCutOffset;\n        segment.audioInPoint = parseFloat(Math.max(0, segmentStart + jCutOffset).toFixed(6));\n        segment.audioOutPoint = parseFloat(Math.min(duration, segmentEnd + lCutOffset).toFixed(6));\n      }\n\n      // Check if this segment contains a take\n      const containingTake = findContainingTake(segmentStart, segmentEnd, takes);\n      if (containingTake) {\n        segment.take = {\n          text: containingTake.text?.substring(0, 100),\n          isBest: containingTake.isBest || false\n        };\n        if (containingTake.isBest && markBestTakes) {\n          segment.type = 'best_take';\n        }\n\n        // Add take labeling (Phase 1 feature)\n        if (labelTakes) {\n          const shortLabel = generateShortLabel(containingTake.text);\n          segment.takeNumber = takeNumber;\n          segment.takeLabel = `Take ${takeNumber}${shortLabel ? `: ${shortLabel}` : ''}`;\n          takeNumber++;\n        }\n      }\n\n      // Add color code hint for builder\n      if (colorCode) {\n        segment.colorHint = getColorHintForType(segment.type);\n      }\n\n      segments.push(segment);\n    }\n  }\n\n  // Calculate stats\n  const totalKeptDuration = segments.reduce(\n    (sum, seg) => sum + (seg.outPoint - seg.inPoint),\n    0\n  );\n  const totalRemovedDuration = duration - totalKeptDuration;\n\n  const cutList = {\n    version: '3.5',\n    source: {\n      name: sourceName,\n      path: sourcePath,\n      duration\n    },\n    segments,\n    metadata: {\n      silencesRemoved,\n      takesDetected: takes.length,\n      takesLabeled: labelTakes && takeNumber > 1,\n      colorCoded: colorCode,\n      totalKeptDuration: parseFloat(totalKeptDuration.toFixed(3)),\n      totalRemovedDuration: parseFloat(totalRemovedDuration.toFixed(3)),\n      segmentCount: segments.length,\n      frameRate: frameRate > 0 ? frameRate : null,\n      frameAligned: frameRate > 0,\n      jCutOffset: jCutOffset !== 0 ? jCutOffset : null,\n      lCutOffset: lCutOffset !== 0 ? lCutOffset : null,\n      hasAudioOffsets: jCutOffset !== 0 || lCutOffset !== 0,\n      generatedAt: new Date().toISOString()\n    }\n  };\n\n  console.log(`[SPLICE] Generated cut list: ${segments.length} segments, ${silencesRemoved} silences removed`);\n\n  return cutList;\n}\n\n/**\n * Find a take that overlaps with a given time range\n * @param {number} start - Start time in seconds\n * @param {number} end - End time in seconds\n * @param {Array} takes - Array of takes\n * @returns {Object|null} The overlapping take or null\n */\nfunction findContainingTake(start, end, takes) {\n  if (!takes || takes.length === 0) return null;\n\n  for (const take of takes) {\n    const takeStart = take.startTime || take.start;\n    const takeEnd = take.endTime || take.end;\n\n    // Check for overlap\n    if (start < takeEnd && end > takeStart) {\n      return take;\n    }\n  }\n\n  return null;\n}\n\n/**\n * Generate a short label from take text (first 3-5 words)\n * @param {string} text - Full take text\n * @returns {string} Short label (max 30 chars)\n */\nfunction generateShortLabel(text) {\n  if (!text || typeof text !== 'string') return '';\n\n  // Clean and split into words\n  const words = text.trim().split(/\\s+/);\n\n  // Take first 5 words max\n  const shortWords = words.slice(0, 5);\n\n  // Join and truncate to 30 chars\n  let label = shortWords.join(' ');\n  if (label.length > 30) {\n    label = label.substring(0, 27) + '...';\n  }\n\n  return label;\n}\n\n/**\n * Get color hint for segment type\n * Color hints are used by the builder to apply appropriate colors\n *\n * Color mapping:\n * - speech: green (keeps attention, standard content)\n * - best_take: cerulean/light blue (highlight best content)\n * - take: lavender (regular take)\n * - silence: violet (rarely used, mostly removed)\n * - wide_shot: yellow (visual variety)\n * - speaker_a: mango/orange (speaker differentiation)\n * - speaker_b: caribbean/teal (speaker differentiation)\n *\n * @param {string} type - Segment type\n * @returns {string} Color hint name\n */\nfunction getColorHintForType(type) {\n  switch (type) {\n    case 'best_take':\n      return 'cerulean';  // Light blue - best content\n    case 'take':\n      return 'lavender';  // Light purple - regular take\n    case 'speech':\n      return 'green';     // Green - speech segments\n    case 'silence':\n      return 'violet';    // Purple - silence (if kept)\n    case 'wide_shot':\n      return 'yellow';    // Yellow - visual variety\n    case 'speaker_a':\n      return 'mango';     // Orange - first speaker\n    case 'speaker_b':\n      return 'caribbean'; // Teal - second speaker\n    default:\n      return 'none';\n  }\n}\n\n/**\n * Generate a cut list that keeps only specific takes\n * Useful for \"keep best takes only\" workflow\n *\n * @param {Object} options - Generation options\n * @param {string} options.sourceName - Name of the source clip\n * @param {string} options.sourcePath - Full path to the source file\n * @param {number} options.duration - Total duration of source in seconds\n * @param {Array} options.takes - Array of takes to keep\n * @param {Object} [options.settings] - Optional generation settings\n * @returns {Object} The generated cut list\n */\nfunction generateTakesCutList(options) {\n  const {\n    sourceName,\n    sourcePath,\n    duration,\n    takes = [],\n    settings = {}\n  } = options;\n\n  const {\n    padding = 0.2,          // Small padding around takes\n    onlyBestTakes = false   // If true, only include takes marked as best\n  } = settings;\n\n  // Filter to best takes if requested\n  const takesToKeep = onlyBestTakes\n    ? takes.filter(t => t.isBest)\n    : takes;\n\n  // Sort by start time\n  const sortedTakes = [...takesToKeep].sort((a, b) => {\n    const aStart = a.startTime || a.start;\n    const bStart = b.startTime || b.start;\n    return aStart - bStart;\n  });\n\n  // Build segments from takes\n  const segments = sortedTakes.map(take => {\n    const takeStart = take.startTime || take.start;\n    const takeEnd = take.endTime || take.end;\n\n    return {\n      type: take.isBest ? 'best_take' : 'take',\n      sourceName,\n      sourcePath,\n      inPoint: Math.max(0, takeStart - padding),\n      outPoint: Math.min(duration, takeEnd + padding),\n      take: {\n        text: take.text?.substring(0, 100),\n        isBest: take.isBest || false\n      }\n    };\n  });\n\n  const totalKeptDuration = segments.reduce(\n    (sum, seg) => sum + (seg.outPoint - seg.inPoint),\n    0\n  );\n\n  const cutList = {\n    version: '3.5',\n    source: {\n      name: sourceName,\n      path: sourcePath,\n      duration\n    },\n    segments,\n    metadata: {\n      takesKept: segments.length,\n      takesTotal: takes.length,\n      totalKeptDuration: parseFloat(totalKeptDuration.toFixed(3)),\n      totalRemovedDuration: parseFloat((duration - totalKeptDuration).toFixed(3)),\n      generatedAt: new Date().toISOString()\n    }\n  };\n\n  console.log(`[SPLICE] Generated takes cut list: ${segments.length} takes kept`);\n\n  return cutList;\n}\n\n/**\n * Merge overlapping segments in a cut list\n * Useful when silences and takes create overlapping regions\n *\n * @param {Object} cutList - The cut list to merge\n * @returns {Object} Cut list with merged segments\n */\nfunction mergeOverlappingSegments(cutList) {\n  if (!cutList.segments || cutList.segments.length < 2) {\n    return cutList;\n  }\n\n  // Sort by start time\n  const sorted = [...cutList.segments].sort((a, b) => a.inPoint - b.inPoint);\n\n  const merged = [];\n  let current = { ...sorted[0] };\n\n  for (let i = 1; i < sorted.length; i++) {\n    const next = sorted[i];\n\n    // Check for overlap\n    if (next.inPoint <= current.outPoint) {\n      // Merge: extend current segment\n      current.outPoint = Math.max(current.outPoint, next.outPoint);\n      // Keep the more specific type (best_take > take > speech)\n      if (next.type === 'best_take' || (next.type === 'take' && current.type === 'speech')) {\n        current.type = next.type;\n        current.take = next.take;\n      }\n    } else {\n      // No overlap: save current and start new\n      merged.push(current);\n      current = { ...next };\n    }\n  }\n\n  // Don't forget the last segment\n  merged.push(current);\n\n  // Update metadata\n  const newCutList = {\n    ...cutList,\n    segments: merged,\n    metadata: {\n      ...cutList.metadata,\n      segmentCount: merged.length,\n      mergedFrom: cutList.segments.length\n    }\n  };\n\n  console.log(`[SPLICE] Merged ${cutList.segments.length} segments into ${merged.length}`);\n\n  return newCutList;\n}\n\n/**\n * Validate a cut list structure\n * @param {Object} cutList - The cut list to validate\n * @returns {{valid: boolean, errors: string[]}}\n */\nfunction validateCutList(cutList) {\n  const errors = [];\n\n  if (!cutList) {\n    errors.push('Cut list is null or undefined');\n    return { valid: false, errors };\n  }\n\n  if (!cutList.version) {\n    errors.push('Missing version field');\n  }\n\n  if (!cutList.source) {\n    errors.push('Missing source field');\n  } else {\n    if (!cutList.source.name && !cutList.source.path) {\n      errors.push('Source must have name or path');\n    }\n    if (typeof cutList.source.duration !== 'number' || cutList.source.duration <= 0) {\n      errors.push('Source duration must be a positive number');\n    }\n  }\n\n  if (!Array.isArray(cutList.segments)) {\n    errors.push('Segments must be an array');\n  } else {\n    cutList.segments.forEach((seg, i) => {\n      if (typeof seg.inPoint !== 'number') {\n        errors.push(`Segment ${i}: inPoint must be a number`);\n      }\n      if (typeof seg.outPoint !== 'number') {\n        errors.push(`Segment ${i}: outPoint must be a number`);\n      }\n      if (seg.inPoint >= seg.outPoint) {\n        errors.push(`Segment ${i}: inPoint must be less than outPoint`);\n      }\n      if (!seg.sourceName && !seg.sourcePath) {\n        errors.push(`Segment ${i}: must have sourceName or sourcePath`);\n      }\n    });\n  }\n\n  return {\n    valid: errors.length === 0,\n    errors\n  };\n}\n\nmodule.exports = {\n  generateCutList,\n  generateTakesCutList,\n  mergeOverlappingSegments,\n  validateCutList,\n  // Frame alignment utilities\n  alignToFrame,\n  alignToFrameFloor,\n  alignToFrameCeil\n};\n","numLines":539,"startLine":1,"totalLines":539}}}
{"parentUuid":"39aa84e0-f54a-4209-8491-6c76ac1a77bf","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_012voLeb6J3qaWZGKouutDEP","type":"tool_result","content":"     1→/**\n     2→ * RMS-Based Silence Detection Service\n     3→ *\n     4→ * Advanced silence detection using Root Mean Square (RMS) audio analysis.\n     5→ * Based on Fireside's proven approach with enhancements.\n     6→ *\n     7→ * Features:\n     8→ * - RMS calculation from raw audio data\n     9→ * - dBFS (decibels relative to full scale) conversion\n    10→ * - Auto-threshold detection from audio histogram\n    11→ * - Gaussian blur smoothing for noise reduction\n    12→ * - Local maxima detection for speech/silence peak identification\n    13→ * - Configurable padding, seek step, min duration\n    14→ * - Silence merging for adjacent regions\n    15→ */\n    16→\n    17→const fsPromises = require('fs').promises;\n    18→const { validateAudioPath, safeFFprobe, safeFFmpeg, safeTempPath } = require('./securityUtils');\n    19→\n    20→// =============================================================================\n    21→// Configuration\n    22→// =============================================================================\n    23→\n    24→const DEFAULT_OPTIONS = {\n    25→  threshold: -30,           // dBFS threshold (-60 to -20)\n    26→  minSilenceLength: 0.5,    // Minimum silence duration in seconds\n    27→  seekStep: 0.05,           // Analysis window step in seconds (50ms)\n    28→  paddingStart: 0.1,        // Buffer before silence cut (seconds)\n    29→  paddingEnd: 0.05,         // Buffer after silence cut (seconds)\n    30→  autoThreshold: false,     // Auto-detect optimal threshold\n    31→  mergeDistance: 0.2,       // Merge silences closer than this (seconds)\n    32→  sampleRate: 48000,        // Default sample rate\n    33→  chunkDuration: 30,        // Chunk duration in seconds for long files\n    34→  chunkingEnabled: true     // Enable chunked processing for memory efficiency\n    35→};\n    36→\n    37→// =============================================================================\n    38→// Core RMS Detection\n    39→// =============================================================================\n    40→\n    41→/**\n    42→ * Detect silences using RMS-based audio analysis\n    43→ *\n    44→ * @param {string} audioPath - Path to audio file (WAV, MP3, etc.)\n    45→ * @param {Object} options - Detection options\n    46→ * @returns {Promise<Object>} Detection results with silences and metadata\n    47→ */\n    48→async function detectSilencesRMS(audioPath, options = {}) {\n    49→  const opts = { ...DEFAULT_OPTIONS, ...options };\n    50→\n    51→  // SECURITY: Validate input path to prevent path traversal\n    52→  const pathValidation = await validateAudioPath(audioPath);\n    53→  if (!pathValidation.valid) {\n    54→    throw new Error(`Invalid audio path: ${pathValidation.error}`);\n    55→  }\n    56→  const validatedPath = pathValidation.path;\n    57→\n    58→  console.log(`[SPLICE RMS] Analyzing: ${validatedPath}`);\n    59→  console.log(`[SPLICE RMS] Options: threshold=${opts.threshold}dB, minLength=${opts.minSilenceLength}s, seekStep=${opts.seekStep}s`);\n    60→\n    61→  // Check if we should use chunked processing (for files longer than 45 seconds)\n    62→  if (opts.chunkingEnabled !== false) {\n    63→    const duration = await getAudioDuration(validatedPath);\n    64→    if (duration > 45) {\n    65→      console.log(`[SPLICE RMS] Long file detected (${duration}s), using chunked processing`);\n    66→      return detectSilencesRMSChunked(validatedPath, options);\n    67→    }\n    68→  }\n    69→\n    70→  // Step 1: Extract raw audio data using FFmpeg\n    71→  const audioData = await extractAudioData(validatedPath);\n    72→\n    73→  // Step 2: Calculate RMS values for sliding windows\n    74→  const rmsValues = calculateRMSWindows(audioData.samples, {\n    75→    windowSize: Math.floor(opts.seekStep * audioData.sampleRate),\n    76→    hopSize: Math.floor(opts.seekStep * audioData.sampleRate / 2)\n    77→  });\n    78→\n    79→  // Step 3: Convert to dBFS\n    80→  const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n    81→\n    82→  // Step 4: Determine threshold (auto-detect or use provided)\n    83→  let threshold = opts.threshold;\n    84→  let thresholdInfo = { method: 'manual', value: threshold };\n    85→\n    86→  if (opts.autoThreshold) {\n    87→    const autoResult = autoDetectThreshold(dBFSValues);\n    88→    threshold = autoResult.threshold;\n    89→    thresholdInfo = autoResult;\n    90→    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB (${autoResult.method})`);\n    91→  }\n    92→\n    93→  // Step 5: Find silence regions\n    94→  const rawSilences = findSilenceRegions(dBFSValues, {\n    95→    threshold,\n    96→    seekStep: opts.seekStep,\n    97→    minSilenceLength: opts.minSilenceLength\n    98→  });\n    99→\n   100→  // Step 6: Merge adjacent silences\n   101→  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n   102→\n   103→  // Step 7: Apply padding\n   104→  const paddedSilences = applyPadding(mergedSilences, {\n   105→    paddingStart: opts.paddingStart,\n   106→    paddingEnd: opts.paddingEnd,\n   107→    audioDuration: audioData.duration\n   108→  });\n   109→\n   110→  // Calculate statistics\n   111→  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n   112→  const speechDuration = audioData.duration - totalSilenceDuration;\n   113→\n   114→  console.log(`[SPLICE RMS] Detected ${paddedSilences.length} silence(s), total ${totalSilenceDuration.toFixed(2)}s`);\n   115→\n   116→  return {\n   117→    silences: paddedSilences,\n   118→    metadata: {\n   119→      audioDuration: audioData.duration,\n   120→      sampleRate: audioData.sampleRate,\n   121→      channels: audioData.channels,\n   122→      silenceCount: paddedSilences.length,\n   123→      totalSilenceDuration,\n   124→      speechDuration,\n   125→      silencePercentage: ((totalSilenceDuration / audioData.duration) * 100).toFixed(1),\n   126→      threshold: thresholdInfo,\n   127→      rmsAnalysis: {\n   128→        windowCount: dBFSValues.length,\n   129→        ...computeDBFSStats(dBFSValues)\n   130→      }\n   131→    }\n   132→  };\n   133→}\n   134→\n   135→// =============================================================================\n   136→// Chunked Audio Processing\n   137→// =============================================================================\n   138→\n   139→/**\n   140→ * Detect silences using chunked RMS analysis for long files\n   141→ * Processes audio in chunks to reduce memory usage\n   142→ *\n   143→ * @param {string} audioPath - Path to audio file\n   144→ * @param {Object} options - Detection options including chunkDuration\n   145→ * @returns {Promise<Object>} Detection results with silences and metadata\n   146→ */\n   147→async function detectSilencesRMSChunked(audioPath, options = {}) {\n   148→  const opts = { ...DEFAULT_OPTIONS, ...options };\n   149→  const chunkDuration = opts.chunkDuration || 30;\n   150→\n   151→  // SECURITY: Validate input path to prevent path traversal\n   152→  // (may already be validated by caller, but double-check for safety)\n   153→  const pathValidation = await validateAudioPath(audioPath);\n   154→  if (!pathValidation.valid) {\n   155→    throw new Error(`Invalid audio path: ${pathValidation.error}`);\n   156→  }\n   157→  const validatedPath = pathValidation.path;\n   158→\n   159→  console.log(`[SPLICE RMS] Chunked analysis: ${validatedPath}`);\n   160→\n   161→  // Get audio duration first\n   162→  const duration = await getAudioDuration(validatedPath);\n   163→  const numChunks = Math.ceil(duration / chunkDuration);\n   164→\n   165→  console.log(`[SPLICE RMS] Duration: ${duration}s, processing in ${numChunks} chunk(s) of ${chunkDuration}s`);\n   166→\n   167→  // If short enough, use regular processing\n   168→  if (duration <= chunkDuration * 1.5) {\n   169→    return detectSilencesRMS(validatedPath, { ...opts, chunkingEnabled: false });\n   170→  }\n   171→\n   172→  let allDBFSValues = [];\n   173→  let sampleRateUsed = 16000;\n   174→\n   175→  // Process each chunk\n   176→  for (let i = 0; i < numChunks; i++) {\n   177→    const startTime = i * chunkDuration;\n   178→    const endTime = Math.min((i + 1) * chunkDuration, duration);\n   179→\n   180→    console.log(`[SPLICE RMS] Processing chunk ${i + 1}/${numChunks} (${startTime}s - ${endTime}s)`);\n   181→\n   182→    // Extract chunk audio data\n   183→    const chunkData = await extractAudioDataChunk(validatedPath, startTime, endTime - startTime);\n   184→    sampleRateUsed = chunkData.sampleRate;\n   185→\n   186→    // Calculate RMS values for this chunk\n   187→    const rmsValues = calculateRMSWindows(chunkData.samples, {\n   188→      windowSize: Math.floor(opts.seekStep * chunkData.sampleRate),\n   189→      hopSize: Math.floor(opts.seekStep * chunkData.sampleRate / 2)\n   190→    });\n   191→\n   192→    // Convert to dBFS and append efficiently (avoid concat which allocates new array)\n   193→    const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n   194→    for (let k = 0; k < dBFSValues.length; k++) {\n   195→      allDBFSValues.push(dBFSValues[k]);\n   196→    }\n   197→  }\n   198→\n   199→  // Determine threshold\n   200→  let threshold = opts.threshold;\n   201→  let thresholdInfo = { method: 'manual', value: threshold };\n   202→\n   203→  if (opts.autoThreshold) {\n   204→    const autoResult = autoDetectThreshold(allDBFSValues);\n   205→    threshold = autoResult.threshold;\n   206→    thresholdInfo = autoResult;\n   207→    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB`);\n   208→  }\n   209→\n   210→  // Find silence regions\n   211→  const rawSilences = findSilenceRegions(allDBFSValues, {\n   212→    threshold,\n   213→    seekStep: opts.seekStep,\n   214→    minSilenceLength: opts.minSilenceLength\n   215→  });\n   216→\n   217→  // Merge adjacent silences\n   218→  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n   219→\n   220→  // Apply padding\n   221→  const paddedSilences = applyPadding(mergedSilences, {\n   222→    paddingStart: opts.paddingStart,\n   223→    paddingEnd: opts.paddingEnd,\n   224→    audioDuration: duration\n   225→  });\n   226→\n   227→  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n   228→\n   229→  console.log(`[SPLICE RMS] Chunked detection found ${paddedSilences.length} silence(s)`);\n   230→\n   231→  return {\n   232→    silences: paddedSilences,\n   233→    metadata: {\n   234→      audioDuration: duration,\n   235→      sampleRate: sampleRateUsed,\n   236→      channels: 1,\n   237→      silenceCount: paddedSilences.length,\n   238→      totalSilenceDuration,\n   239→      speechDuration: duration - totalSilenceDuration,\n   240→      silencePercentage: ((totalSilenceDuration / duration) * 100).toFixed(1),\n   241→      threshold: thresholdInfo,\n   242→      chunked: true,\n   243→      chunkCount: numChunks,\n   244→      rmsAnalysis: {\n   245→        windowCount: allDBFSValues.length,\n   246→        ...computeDBFSStats(allDBFSValues)\n   247→      }\n   248→    }\n   249→  };\n   250→}\n   251→\n   252→/**\n   253→ * Get audio duration using FFprobe\n   254→ *\n   255→ * @param {string} audioPath - Path to audio file\n   256→ * @returns {Promise<number>} Duration in seconds\n   257→ */\n   258→async function getAudioDuration(audioPath) {\n   259→  try {\n   260→    // SECURITY: Use execFile with array arguments to prevent command injection\n   261→    const args = [\n   262→      '-v', 'error',\n   263→      '-show_entries', 'format=duration',\n   264→      '-of', 'default=noprint_wrappers=1:nokey=1',\n   265→      audioPath\n   266→    ];\n   267→    const { stdout } = await safeFFprobe(args);\n   268→    return parseFloat(stdout.trim()) || 0;\n   269→  } catch (err) {\n   270→    console.warn('[SPLICE RMS] Could not get duration:', err.message);\n   271→    return 0;\n   272→  }\n   273→}\n   274→\n   275→/**\n   276→ * Extract a chunk of audio data starting at a specific time\n   277→ *\n   278→ * @param {string} audioPath - Path to audio file\n   279→ * @param {number} startTime - Start time in seconds\n   280→ * @param {number} duration - Duration to extract in seconds\n   281→ * @returns {Promise<Object>} Audio chunk data\n   282→ */\n   283→async function extractAudioDataChunk(audioPath, startTime, duration) {\n   284→  // SECURITY: Use safe temp path generation\n   285→  const tempFile = safeTempPath('splice_rms_chunk', '.raw');\n   286→\n   287→  try {\n   288→    // SECURITY: Use execFile with array arguments to prevent command injection\n   289→    const args = [\n   290→      '-y',\n   291→      '-ss', String(startTime),\n   292→      '-t', String(duration),\n   293→      '-i', audioPath,\n   294→      '-ac', '1',\n   295→      '-ar', '16000',\n   296→      '-f', 's16le',\n   297→      '-acodec', 'pcm_s16le',\n   298→      tempFile\n   299→    ];\n   300→    await safeFFmpeg(args);\n   301→\n   302→    const rawData = await fsPromises.readFile(tempFile);\n   303→\n   304→    // Convert to float samples\n   305→    const samples = new Float32Array(rawData.length / 2);\n   306→    for (let i = 0; i < samples.length; i++) {\n   307→      const int16 = rawData.readInt16LE(i * 2);\n   308→      samples[i] = int16 / 32768.0;\n   309→    }\n   310→\n   311→    await fsPromises.unlink(tempFile);\n   312→\n   313→    return {\n   314→      samples,\n   315→      sampleRate: 16000,\n   316→      channels: 1,\n   317→      duration\n   318→    };\n   319→  } catch (err) {\n   320→    try {\n   321→      await fsPromises.access(tempFile);\n   322→      await fsPromises.unlink(tempFile);\n   323→    } catch {\n   324→      // File doesn't exist\n   325→    }\n   326→    throw new Error(`Failed to extract audio chunk: ${err.message}`);\n   327→  }\n   328→}\n   329→\n   330→// =============================================================================\n   331→// Audio Extraction\n   332→// =============================================================================\n   333→\n   334→/**\n   335→ * Extract raw audio samples from file using FFmpeg\n   336→ *\n   337→ * @param {string} audioPath - Path to audio file\n   338→ * @returns {Promise<Object>} Audio data with samples, sampleRate, channels, duration\n   339→ */\n   340→async function extractAudioData(audioPath) {\n   341→  // SECURITY: Use execFile with array arguments to prevent command injection\n   342→  const infoArgs = [\n   343→    '-v', 'error',\n   344→    '-select_streams', 'a:0',\n   345→    '-show_entries', 'stream=sample_rate,channels,duration',\n   346→    '-of', 'json',\n   347→    audioPath\n   348→  ];\n   349→\n   350→  let duration = 0;\n   351→\n   352→  try {\n   353→    const { stdout } = await safeFFprobe(infoArgs);\n   354→    const info = JSON.parse(stdout);\n   355→    if (info.streams && info.streams[0]) {\n   356→      // sampleRate and channels extracted but not currently used - reserved for future resampling\n   357→      duration = parseFloat(info.streams[0].duration) || 0;\n   358→    }\n   359→  } catch (err) {\n   360→    console.warn('[SPLICE RMS] Could not get audio info, using defaults:', err.message);\n   361→  }\n   362→\n   363→  // If duration not available from stream, get from format\n   364→  if (!duration) {\n   365→    try {\n   366→      const durationArgs = [\n   367→        '-v', 'error',\n   368→        '-show_entries', 'format=duration',\n   369→        '-of', 'default=noprint_wrappers=1:nokey=1',\n   370→        audioPath\n   371→      ];\n   372→      const { stdout } = await safeFFprobe(durationArgs);\n   373→      duration = parseFloat(stdout.trim()) || 0;\n   374→    } catch (err) {\n   375→      console.warn('[SPLICE RMS] Could not get duration:', err.message);\n   376→    }\n   377→  }\n   378→\n   379→  // SECURITY: Use safe temp path generation\n   380→  const tempFile = safeTempPath('splice_rms', '.raw');\n   381→\n   382→  try {\n   383→    // SECURITY: Use execFile with array arguments to prevent command injection\n   384→    const extractArgs = [\n   385→      '-y',\n   386→      '-i', audioPath,\n   387→      '-ac', '1',\n   388→      '-ar', '16000',\n   389→      '-f', 's16le',\n   390→      '-acodec', 'pcm_s16le',\n   391→      tempFile\n   392→    ];\n   393→    await safeFFmpeg(extractArgs);\n   394→\n   395→    // Read the raw PCM data (async to avoid blocking event loop)\n   396→    const rawData = await fsPromises.readFile(tempFile);\n   397→\n   398→    // Convert to float samples (-1 to 1)\n   399→    const samples = new Float32Array(rawData.length / 2);\n   400→    for (let i = 0; i < samples.length; i++) {\n   401→      // Read signed 16-bit little-endian\n   402→      const int16 = rawData.readInt16LE(i * 2);\n   403→      samples[i] = int16 / 32768.0;\n   404→    }\n   405→\n   406→    // Clean up temp file (async)\n   407→    await fsPromises.unlink(tempFile);\n   408→\n   409→    return {\n   410→      samples,\n   411→      sampleRate: 16000, // We resampled to 16kHz\n   412→      channels: 1,\n   413→      duration\n   414→    };\n   415→  } catch (err) {\n   416→    // Clean up temp file if exists\n   417→    try {\n   418→      await fsPromises.access(tempFile);\n   419→      await fsPromises.unlink(tempFile);\n   420→    } catch {\n   421→      // File doesn't exist or already deleted\n   422→    }\n   423→    throw new Error(`Failed to extract audio: ${err.message}`);\n   424→  }\n   425→}\n   426→\n   427→// =============================================================================\n   428→// RMS Calculation\n   429→// =============================================================================\n   430→\n   431→/**\n   432→ * Calculate RMS values for sliding windows across audio samples\n   433→ *\n   434→ * @param {Float32Array} samples - Audio samples (-1 to 1)\n   435→ * @param {Object} options - Window and hop size\n   436→ * @returns {Float32Array} RMS values for each window\n   437→ */\n   438→function calculateRMSWindows(samples, options = {}) {\n   439→  const { windowSize = 800, hopSize = 400 } = options; // Default ~50ms at 16kHz\n   440→\n   441→  const numWindows = Math.floor((samples.length - windowSize) / hopSize) + 1;\n   442→  const rmsValues = new Float32Array(numWindows);\n   443→\n   444→  for (let i = 0; i < numWindows; i++) {\n   445→    const start = i * hopSize;\n   446→    const end = Math.min(start + windowSize, samples.length);\n   447→\n   448→    // Calculate RMS for this window\n   449→    let sumSquares = 0;\n   450→    for (let j = start; j < end; j++) {\n   451→      sumSquares += samples[j] * samples[j];\n   452→    }\n   453→\n   454→    const rms = Math.sqrt(sumSquares / (end - start));\n   455→    rmsValues[i] = rms;\n   456→  }\n   457→\n   458→  return rmsValues;\n   459→}\n   460→\n   461→/**\n   462→ * Convert RMS value to dBFS (decibels relative to full scale)\n   463→ *\n   464→ * @param {number} rms - RMS value (0 to 1)\n   465→ * @returns {number} dBFS value (negative, -inf for silence)\n   466→ */\n   467→function rmsToDBFS(rms) {\n   468→  if (rms <= 0) return -100; // Effective silence floor\n   469→  const dbfs = 20 * Math.log10(rms);\n   470→  return Math.max(-100, dbfs); // Clamp to reasonable floor\n   471→}\n   472→\n   473→/**\n   474→ * Compute min, max, avg stats without using spread operator\n   475→ * Avoids call stack overflow for large arrays (100k+ elements)\n   476→ *\n   477→ * @param {Array<number>|Float32Array} values - dBFS values\n   478→ * @returns {Object} Stats object with minDB, maxDB, avgDB\n   479→ */\n   480→function computeDBFSStats(values) {\n   481→  if (!values || values.length === 0) {\n   482→    return { minDB: '-100.0', maxDB: '-100.0', avgDB: '-100.0' };\n   483→  }\n   484→\n   485→  let minDB = Infinity;\n   486→  let maxDB = -Infinity;\n   487→  let sum = 0;\n   488→\n   489→  for (let i = 0; i < values.length; i++) {\n   490→    const v = values[i];\n   491→    if (v < minDB) minDB = v;\n   492→    if (v > maxDB) maxDB = v;\n   493→    sum += v;\n   494→  }\n   495→\n   496→  return {\n   497→    minDB: minDB.toFixed(1),\n   498→    maxDB: maxDB.toFixed(1),\n   499→    avgDB: (sum / values.length).toFixed(1)\n   500→  };\n   501→}\n   502→\n   503→// =============================================================================\n   504→// Auto-Threshold Detection\n   505→// =============================================================================\n   506→\n   507→/**\n   508→ * Auto-detect optimal silence threshold from audio histogram\n   509→ * Uses Gaussian blur + local maxima detection\n   510→ *\n   511→ * @param {Float32Array} dBFSValues - Array of dBFS values\n   512→ * @returns {Object} Detected threshold and method info\n   513→ */\n   514→function autoDetectThreshold(dBFSValues) {\n   515→  // Build histogram of dBFS values\n   516→  const histBins = 100;\n   517→  const minDB = -80;\n   518→  const maxDB = 0;\n   519→  const binWidth = (maxDB - minDB) / histBins;\n   520→\n   521→  const histogram = new Array(histBins).fill(0);\n   522→\n   523→  for (const db of dBFSValues) {\n   524→    const binIndex = Math.min(histBins - 1, Math.max(0, Math.floor((db - minDB) / binWidth)));\n   525→    histogram[binIndex]++;\n   526→  }\n   527→\n   528→  // Apply Gaussian blur to smooth histogram\n   529→  const smoothed = gaussianBlur(histogram, 5);\n   530→\n   531→  // Find local maxima (peaks)\n   532→  const peaks = findLocalMaxima(smoothed, 3);\n   533→\n   534→  // Sort peaks by value (highest first)\n   535→  peaks.sort((a, b) => b.value - a.value);\n   536→\n   537→  if (peaks.length >= 2) {\n   538→    // Interpolate between the two highest peaks\n   539→    // Assume: higher peak = speech, lower peak = silence\n   540→    const peak1DB = minDB + peaks[0].index * binWidth;\n   541→    const peak2DB = minDB + peaks[1].index * binWidth;\n   542→\n   543→    const silencePeakDB = Math.min(peak1DB, peak2DB);\n   544→    const speechPeakDB = Math.max(peak1DB, peak2DB);\n   545→\n   546→    // Set threshold between peaks (closer to silence peak)\n   547→    const threshold = silencePeakDB + (speechPeakDB - silencePeakDB) * 0.3;\n   548→\n   549→    return {\n   550→      threshold: Math.max(-60, Math.min(-20, threshold)),\n   551→      method: 'dual-peak',\n   552→      silencePeak: silencePeakDB.toFixed(1),\n   553→      speechPeak: speechPeakDB.toFixed(1)\n   554→    };\n   555→  } else if (peaks.length === 1) {\n   556→    // Single peak - use percentile-based fallback\n   557→    const sorted = [...dBFSValues].sort((a, b) => a - b);\n   558→    const percentile25 = sorted[Math.floor(sorted.length * 0.25)];\n   559→\n   560→    return {\n   561→      threshold: Math.max(-60, Math.min(-20, percentile25)),\n   562→      method: 'percentile-25',\n   563→      value: percentile25.toFixed(1)\n   564→    };\n   565→  } else {\n   566→    // No peaks found - use conservative default\n   567→    return {\n   568→      threshold: -35,\n   569→      method: 'default-fallback'\n   570→    };\n   571→  }\n   572→}\n   573→\n   574→/**\n   575→ * Apply Gaussian blur to an array\n   576→ *\n   577→ * @param {Array<number>} arr - Input array\n   578→ * @param {number} sigma - Standard deviation\n   579→ * @returns {Array<number>} Smoothed array\n   580→ */\n   581→function gaussianBlur(arr, sigma = 5) {\n   582→  const kernelSize = Math.ceil(sigma * 3) * 2 + 1;\n   583→  const kernel = [];\n   584→  const halfSize = Math.floor(kernelSize / 2);\n   585→\n   586→  // Generate Gaussian kernel\n   587→  let sum = 0;\n   588→  for (let i = 0; i < kernelSize; i++) {\n   589→    const x = i - halfSize;\n   590→    const g = Math.exp(-(x * x) / (2 * sigma * sigma));\n   591→    kernel.push(g);\n   592→    sum += g;\n   593→  }\n   594→\n   595→  // Normalize kernel\n   596→  for (let i = 0; i < kernelSize; i++) {\n   597→    kernel[i] /= sum;\n   598→  }\n   599→\n   600→  // Apply convolution\n   601→  const result = new Array(arr.length).fill(0);\n   602→  for (let i = 0; i < arr.length; i++) {\n   603→    let value = 0;\n   604→    for (let j = 0; j < kernelSize; j++) {\n   605→      const idx = i + j - halfSize;\n   606→      if (idx >= 0 && idx < arr.length) {\n   607→        value += arr[idx] * kernel[j];\n   608→      }\n   609→    }\n   610→    result[i] = value;\n   611→  }\n   612→\n   613→  return result;\n   614→}\n   615→\n   616→/**\n   617→ * Find local maxima in an array\n   618→ *\n   619→ * @param {Array<number>} arr - Input array\n   620→ * @param {number} windowSize - Neighborhood size for local maximum detection\n   621→ * @returns {Array<{index: number, value: number}>} Array of peak positions and values\n   622→ */\n   623→function findLocalMaxima(arr, windowSize = 3) {\n   624→  const peaks = [];\n   625→  const halfWindow = Math.floor(windowSize / 2);\n   626→\n   627→  for (let i = halfWindow; i < arr.length - halfWindow; i++) {\n   628→    let isMax = true;\n   629→    for (let j = i - halfWindow; j <= i + halfWindow; j++) {\n   630→      if (j !== i && arr[j] >= arr[i]) {\n   631→        isMax = false;\n   632→        break;\n   633→      }\n   634→    }\n   635→    if (isMax && arr[i] > 0) {\n   636→      peaks.push({ index: i, value: arr[i] });\n   637→    }\n   638→  }\n   639→\n   640→  return peaks;\n   641→}\n   642→\n   643→// =============================================================================\n   644→// Silence Region Detection\n   645→// =============================================================================\n   646→\n   647→/**\n   648→ * Find silence regions based on dBFS threshold\n   649→ *\n   650→ * @param {Float32Array} dBFSValues - Array of dBFS values\n   651→ * @param {Object} options - Detection options\n   652→ * @returns {Array<{start: number, end: number, duration: number}>} Silence regions\n   653→ */\n   654→function findSilenceRegions(dBFSValues, options) {\n   655→  const { threshold, seekStep, minSilenceLength } = options;\n   656→\n   657→  const silences = [];\n   658→  let silenceStart = null;\n   659→\n   660→  for (let i = 0; i < dBFSValues.length; i++) {\n   661→    const time = i * seekStep;\n   662→    const isSilent = dBFSValues[i] < threshold;\n   663→\n   664→    if (isSilent && silenceStart === null) {\n   665→      // Start of silence\n   666→      silenceStart = time;\n   667→    } else if (!isSilent && silenceStart !== null) {\n   668→      // End of silence\n   669→      const duration = time - silenceStart;\n   670→      if (duration >= minSilenceLength) {\n   671→        silences.push({\n   672→          start: silenceStart,\n   673→          end: time,\n   674→          duration\n   675→        });\n   676→      }\n   677→      silenceStart = null;\n   678→    }\n   679→  }\n   680→\n   681→  // Handle silence at end of audio\n   682→  if (silenceStart !== null) {\n   683→    const endTime = dBFSValues.length * seekStep;\n   684→    const duration = endTime - silenceStart;\n   685→    if (duration >= minSilenceLength) {\n   686→      silences.push({\n   687→        start: silenceStart,\n   688→        end: endTime,\n   689→        duration\n   690→      });\n   691→    }\n   692→  }\n   693→\n   694→  return silences;\n   695→}\n   696→\n   697→// =============================================================================\n   698→// Post-Processing\n   699→// =============================================================================\n   700→\n   701→/**\n   702→ * Merge silences that are close together\n   703→ *\n   704→ * @param {Array} silences - Array of silence regions\n   705→ * @param {number} mergeDistance - Maximum gap to merge (seconds)\n   706→ * @returns {Array} Merged silence regions\n   707→ */\n   708→function mergeSilences(silences, mergeDistance) {\n   709→  if (silences.length <= 1) return silences;\n   710→\n   711→  const merged = [];\n   712→  let current = { ...silences[0] };\n   713→\n   714→  for (let i = 1; i < silences.length; i++) {\n   715→    const next = silences[i];\n   716→\n   717→    // If gap is small enough, merge\n   718→    if (next.start - current.end <= mergeDistance) {\n   719→      current.end = next.end;\n   720→      current.duration = current.end - current.start;\n   721→    } else {\n   722→      merged.push(current);\n   723→      current = { ...next };\n   724→    }\n   725→  }\n   726→\n   727→  merged.push(current);\n   728→  return merged;\n   729→}\n   730→\n   731→/**\n   732→ * Apply padding to silence boundaries\n   733→ *\n   734→ * @param {Array} silences - Array of silence regions\n   735→ * @param {Object} options - Padding options\n   736→ * @returns {Array} Padded silence regions\n   737→ */\n   738→function applyPadding(silences, options) {\n   739→  const { paddingStart, paddingEnd, audioDuration } = options;\n   740→\n   741→  return silences\n   742→    .map(silence => {\n   743→      const paddedStart = Math.max(0, silence.start + paddingStart);\n   744→      const paddedEnd = Math.min(audioDuration, silence.end - paddingEnd);\n   745→      const duration = paddedEnd - paddedStart;\n   746→\n   747→      // Skip if padding made the silence invalid\n   748→      if (duration <= 0) return null;\n   749→\n   750→      return {\n   751→        start: parseFloat(paddedStart.toFixed(3)),\n   752→        end: parseFloat(paddedEnd.toFixed(3)),\n   753→        duration: parseFloat(duration.toFixed(3))\n   754→      };\n   755→    })\n   756→    .filter(s => s !== null);\n   757→}\n   758→\n   759→// =============================================================================\n   760→// Sensitivity Mapping\n   761→// =============================================================================\n   762→\n   763→/**\n   764→ * Map UI sensitivity (0-100) to detection parameters\n   765→ * Based on Fireside's aggressiveness mapping\n   766→ *\n   767→ * @param {number} sensitivity - UI sensitivity value (0-100)\n   768→ * @returns {Object} Detection parameters\n   769→ */\n   770→function sensitivityToParams(sensitivity) {\n   771→  // Clamp to 0-100\n   772→  const s = Math.max(0, Math.min(100, sensitivity));\n   773→\n   774→  // Linear interpolation helper\n   775→  const lerp = (min, max, t) => min + (max - min) * (t / 100);\n   776→\n   777→  return {\n   778→    // Threshold: -50dB (conservative) to -20dB (aggressive)\n   779→    threshold: lerp(-50, -20, s),\n   780→\n   781→    // Min silence length: 2.0s (conservative) to 0.3s (aggressive)\n   782→    minSilenceLength: lerp(2.0, 0.3, s),\n   783→\n   784→    // Padding start: 0.2s (conservative) to 0.05s (aggressive)\n   785→    paddingStart: lerp(0.2, 0.05, s),\n   786→\n   787→    // Padding end: 0.15s (conservative) to 0.03s (aggressive)\n   788→    paddingEnd: lerp(0.15, 0.03, s),\n   789→\n   790→    // Seek step stays constant\n   791→    seekStep: 0.05\n   792→  };\n   793→}\n   794→\n   795→// =============================================================================\n   796→// Exports\n   797→// =============================================================================\n   798→\n   799→/**\n   800→ * Get waveform data for visualization\n   801→ * Returns downsampled RMS values suitable for canvas rendering\n   802→ *\n   803→ * @param {string} wavPath - Path to WAV file\n   804→ * @param {Object} options - Options\n   805→ * @returns {Promise<Object>} Waveform data with values and metadata\n   806→ */\n   807→async function getWaveformData(wavPath, options = {}) {\n   808→  const {\n   809→    targetPoints = 400,   // Number of points for visualization\n   810→    windowMs = 50         // RMS window size\n   811→  } = options;\n   812→\n   813→  const fs = require('fs');\n   814→  const path = require('path');\n   815→\n   816→  if (!fs.existsSync(wavPath)) {\n   817→    throw new Error(`File not found: ${wavPath}`);\n   818→  }\n   819→\n   820→  // Read audio file\n   821→  const buffer = fs.readFileSync(wavPath);\n   822→  const audioData = parseWavBuffer(buffer);\n   823→\n   824→  // Calculate RMS values\n   825→  const rmsValues = calculateRMSWindows(audioData.samples, {\n   826→    sampleRate: audioData.sampleRate,\n   827→    windowMs\n   828→  });\n   829→\n   830→  // Downsample to target points\n   831→  const step = Math.max(1, Math.floor(rmsValues.length / targetPoints));\n   832→  const downsampled = [];\n   833→\n   834→  for (let i = 0; i < rmsValues.length; i += step) {\n   835→    // Take max value in this chunk for peak display\n   836→    let maxVal = 0;\n   837→    for (let j = i; j < Math.min(i + step, rmsValues.length); j++) {\n   838→      if (rmsValues[j] > maxVal) maxVal = rmsValues[j];\n   839→    }\n   840→    downsampled.push(maxVal);\n   841→  }\n   842→\n   843→  // Normalize to 0-1 range\n   844→  const maxRMS = Math.max(...downsampled, 0.001);\n   845→  const normalized = downsampled.map(v => v / maxRMS);\n   846→\n   847→  const duration = audioData.samples.length / audioData.sampleRate;\n   848→\n   849→  return {\n   850→    waveform: normalized,\n   851→    pointCount: normalized.length,\n   852→    duration,\n   853→    sampleRate: audioData.sampleRate,\n   854→    maxRMS,\n   855→    fileName: path.basename(wavPath)\n   856→  };\n   857→}\n   858→\n   859→/**\n   860→ * Parse WAV buffer to extract audio samples\n   861→ * (Reusing internal function pattern from detectSilencesRMS)\n   862→ */\n   863→function parseWavBuffer(buffer) {\n   864→  // WAV header parsing\n   865→  const sampleRate = buffer.readUInt32LE(24);\n   866→  const bitsPerSample = buffer.readUInt16LE(34);\n   867→  const numChannels = buffer.readUInt16LE(22);\n   868→\n   869→  // Find data chunk\n   870→  let dataOffset = 44; // Default WAV header size\n   871→  for (let i = 12; i < Math.min(buffer.length - 8, 200); i++) {\n   872→    if (buffer.toString('ascii', i, i + 4) === 'data') {\n   873→      dataOffset = i + 8;\n   874→      break;\n   875→    }\n   876→  }\n   877→\n   878→  const samples = [];\n   879→  const bytesPerSample = bitsPerSample / 8;\n   880→\n   881→  for (let i = dataOffset; i < buffer.length - bytesPerSample; i += bytesPerSample * numChannels) {\n   882→    let sample = 0;\n   883→    if (bitsPerSample === 16) {\n   884→      sample = buffer.readInt16LE(i) / 32768;\n   885→    } else if (bitsPerSample === 24) {\n   886→      sample = (buffer.readInt16LE(i + 1) | (buffer.readInt8(i) << 8)) / 8388608;\n   887→    } else if (bitsPerSample === 32) {\n   888→      sample = buffer.readFloatLE(i);\n   889→    }\n   890→    samples.push(sample);\n   891→  }\n   892→\n   893→  return { samples, sampleRate, bitsPerSample, numChannels };\n   894→}\n   895→\n   896→module.exports = {\n   897→  detectSilencesRMS,\n   898→  detectSilencesRMSChunked,\n   899→  sensitivityToParams,\n   900→  autoDetectThreshold,\n   901→  getAudioDuration,\n   902→  getWaveformData,\n   903→  // Export internal functions for testing\n   904→  calculateRMSWindows,\n   905→  rmsToDBFS,\n   906→  gaussianBlur,\n   907→  findLocalMaxima,\n   908→  mergeSilences,\n   909→  applyPadding,\n   910→  extractAudioDataChunk,\n   911→  DEFAULT_OPTIONS\n   912→};\n   913→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"7a4f8299-e5b8-4092-bcee-55c9fdeadc98","timestamp":"2025-12-26T07:40:15.012Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js","content":"/**\n * RMS-Based Silence Detection Service\n *\n * Advanced silence detection using Root Mean Square (RMS) audio analysis.\n * Based on Fireside's proven approach with enhancements.\n *\n * Features:\n * - RMS calculation from raw audio data\n * - dBFS (decibels relative to full scale) conversion\n * - Auto-threshold detection from audio histogram\n * - Gaussian blur smoothing for noise reduction\n * - Local maxima detection for speech/silence peak identification\n * - Configurable padding, seek step, min duration\n * - Silence merging for adjacent regions\n */\n\nconst fsPromises = require('fs').promises;\nconst { validateAudioPath, safeFFprobe, safeFFmpeg, safeTempPath } = require('./securityUtils');\n\n// =============================================================================\n// Configuration\n// =============================================================================\n\nconst DEFAULT_OPTIONS = {\n  threshold: -30,           // dBFS threshold (-60 to -20)\n  minSilenceLength: 0.5,    // Minimum silence duration in seconds\n  seekStep: 0.05,           // Analysis window step in seconds (50ms)\n  paddingStart: 0.1,        // Buffer before silence cut (seconds)\n  paddingEnd: 0.05,         // Buffer after silence cut (seconds)\n  autoThreshold: false,     // Auto-detect optimal threshold\n  mergeDistance: 0.2,       // Merge silences closer than this (seconds)\n  sampleRate: 48000,        // Default sample rate\n  chunkDuration: 30,        // Chunk duration in seconds for long files\n  chunkingEnabled: true     // Enable chunked processing for memory efficiency\n};\n\n// =============================================================================\n// Core RMS Detection\n// =============================================================================\n\n/**\n * Detect silences using RMS-based audio analysis\n *\n * @param {string} audioPath - Path to audio file (WAV, MP3, etc.)\n * @param {Object} options - Detection options\n * @returns {Promise<Object>} Detection results with silences and metadata\n */\nasync function detectSilencesRMS(audioPath, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n\n  // SECURITY: Validate input path to prevent path traversal\n  const pathValidation = await validateAudioPath(audioPath);\n  if (!pathValidation.valid) {\n    throw new Error(`Invalid audio path: ${pathValidation.error}`);\n  }\n  const validatedPath = pathValidation.path;\n\n  console.log(`[SPLICE RMS] Analyzing: ${validatedPath}`);\n  console.log(`[SPLICE RMS] Options: threshold=${opts.threshold}dB, minLength=${opts.minSilenceLength}s, seekStep=${opts.seekStep}s`);\n\n  // Check if we should use chunked processing (for files longer than 45 seconds)\n  if (opts.chunkingEnabled !== false) {\n    const duration = await getAudioDuration(validatedPath);\n    if (duration > 45) {\n      console.log(`[SPLICE RMS] Long file detected (${duration}s), using chunked processing`);\n      return detectSilencesRMSChunked(validatedPath, options);\n    }\n  }\n\n  // Step 1: Extract raw audio data using FFmpeg\n  const audioData = await extractAudioData(validatedPath);\n\n  // Step 2: Calculate RMS values for sliding windows\n  const rmsValues = calculateRMSWindows(audioData.samples, {\n    windowSize: Math.floor(opts.seekStep * audioData.sampleRate),\n    hopSize: Math.floor(opts.seekStep * audioData.sampleRate / 2)\n  });\n\n  // Step 3: Convert to dBFS\n  const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n\n  // Step 4: Determine threshold (auto-detect or use provided)\n  let threshold = opts.threshold;\n  let thresholdInfo = { method: 'manual', value: threshold };\n\n  if (opts.autoThreshold) {\n    const autoResult = autoDetectThreshold(dBFSValues);\n    threshold = autoResult.threshold;\n    thresholdInfo = autoResult;\n    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB (${autoResult.method})`);\n  }\n\n  // Step 5: Find silence regions\n  const rawSilences = findSilenceRegions(dBFSValues, {\n    threshold,\n    seekStep: opts.seekStep,\n    minSilenceLength: opts.minSilenceLength\n  });\n\n  // Step 6: Merge adjacent silences\n  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n\n  // Step 7: Apply padding\n  const paddedSilences = applyPadding(mergedSilences, {\n    paddingStart: opts.paddingStart,\n    paddingEnd: opts.paddingEnd,\n    audioDuration: audioData.duration\n  });\n\n  // Calculate statistics\n  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n  const speechDuration = audioData.duration - totalSilenceDuration;\n\n  console.log(`[SPLICE RMS] Detected ${paddedSilences.length} silence(s), total ${totalSilenceDuration.toFixed(2)}s`);\n\n  return {\n    silences: paddedSilences,\n    metadata: {\n      audioDuration: audioData.duration,\n      sampleRate: audioData.sampleRate,\n      channels: audioData.channels,\n      silenceCount: paddedSilences.length,\n      totalSilenceDuration,\n      speechDuration,\n      silencePercentage: ((totalSilenceDuration / audioData.duration) * 100).toFixed(1),\n      threshold: thresholdInfo,\n      rmsAnalysis: {\n        windowCount: dBFSValues.length,\n        ...computeDBFSStats(dBFSValues)\n      }\n    }\n  };\n}\n\n// =============================================================================\n// Chunked Audio Processing\n// =============================================================================\n\n/**\n * Detect silences using chunked RMS analysis for long files\n * Processes audio in chunks to reduce memory usage\n *\n * @param {string} audioPath - Path to audio file\n * @param {Object} options - Detection options including chunkDuration\n * @returns {Promise<Object>} Detection results with silences and metadata\n */\nasync function detectSilencesRMSChunked(audioPath, options = {}) {\n  const opts = { ...DEFAULT_OPTIONS, ...options };\n  const chunkDuration = opts.chunkDuration || 30;\n\n  // SECURITY: Validate input path to prevent path traversal\n  // (may already be validated by caller, but double-check for safety)\n  const pathValidation = await validateAudioPath(audioPath);\n  if (!pathValidation.valid) {\n    throw new Error(`Invalid audio path: ${pathValidation.error}`);\n  }\n  const validatedPath = pathValidation.path;\n\n  console.log(`[SPLICE RMS] Chunked analysis: ${validatedPath}`);\n\n  // Get audio duration first\n  const duration = await getAudioDuration(validatedPath);\n  const numChunks = Math.ceil(duration / chunkDuration);\n\n  console.log(`[SPLICE RMS] Duration: ${duration}s, processing in ${numChunks} chunk(s) of ${chunkDuration}s`);\n\n  // If short enough, use regular processing\n  if (duration <= chunkDuration * 1.5) {\n    return detectSilencesRMS(validatedPath, { ...opts, chunkingEnabled: false });\n  }\n\n  let allDBFSValues = [];\n  let sampleRateUsed = 16000;\n\n  // Process each chunk\n  for (let i = 0; i < numChunks; i++) {\n    const startTime = i * chunkDuration;\n    const endTime = Math.min((i + 1) * chunkDuration, duration);\n\n    console.log(`[SPLICE RMS] Processing chunk ${i + 1}/${numChunks} (${startTime}s - ${endTime}s)`);\n\n    // Extract chunk audio data\n    const chunkData = await extractAudioDataChunk(validatedPath, startTime, endTime - startTime);\n    sampleRateUsed = chunkData.sampleRate;\n\n    // Calculate RMS values for this chunk\n    const rmsValues = calculateRMSWindows(chunkData.samples, {\n      windowSize: Math.floor(opts.seekStep * chunkData.sampleRate),\n      hopSize: Math.floor(opts.seekStep * chunkData.sampleRate / 2)\n    });\n\n    // Convert to dBFS and append efficiently (avoid concat which allocates new array)\n    const dBFSValues = rmsValues.map(rms => rmsToDBFS(rms));\n    for (let k = 0; k < dBFSValues.length; k++) {\n      allDBFSValues.push(dBFSValues[k]);\n    }\n  }\n\n  // Determine threshold\n  let threshold = opts.threshold;\n  let thresholdInfo = { method: 'manual', value: threshold };\n\n  if (opts.autoThreshold) {\n    const autoResult = autoDetectThreshold(allDBFSValues);\n    threshold = autoResult.threshold;\n    thresholdInfo = autoResult;\n    console.log(`[SPLICE RMS] Auto-detected threshold: ${threshold.toFixed(1)}dB`);\n  }\n\n  // Find silence regions\n  const rawSilences = findSilenceRegions(allDBFSValues, {\n    threshold,\n    seekStep: opts.seekStep,\n    minSilenceLength: opts.minSilenceLength\n  });\n\n  // Merge adjacent silences\n  const mergedSilences = mergeSilences(rawSilences, opts.mergeDistance);\n\n  // Apply padding\n  const paddedSilences = applyPadding(mergedSilences, {\n    paddingStart: opts.paddingStart,\n    paddingEnd: opts.paddingEnd,\n    audioDuration: duration\n  });\n\n  const totalSilenceDuration = paddedSilences.reduce((sum, s) => sum + s.duration, 0);\n\n  console.log(`[SPLICE RMS] Chunked detection found ${paddedSilences.length} silence(s)`);\n\n  return {\n    silences: paddedSilences,\n    metadata: {\n      audioDuration: duration,\n      sampleRate: sampleRateUsed,\n      channels: 1,\n      silenceCount: paddedSilences.length,\n      totalSilenceDuration,\n      speechDuration: duration - totalSilenceDuration,\n      silencePercentage: ((totalSilenceDuration / duration) * 100).toFixed(1),\n      threshold: thresholdInfo,\n      chunked: true,\n      chunkCount: numChunks,\n      rmsAnalysis: {\n        windowCount: allDBFSValues.length,\n        ...computeDBFSStats(allDBFSValues)\n      }\n    }\n  };\n}\n\n/**\n * Get audio duration using FFprobe\n *\n * @param {string} audioPath - Path to audio file\n * @returns {Promise<number>} Duration in seconds\n */\nasync function getAudioDuration(audioPath) {\n  try {\n    // SECURITY: Use execFile with array arguments to prevent command injection\n    const args = [\n      '-v', 'error',\n      '-show_entries', 'format=duration',\n      '-of', 'default=noprint_wrappers=1:nokey=1',\n      audioPath\n    ];\n    const { stdout } = await safeFFprobe(args);\n    return parseFloat(stdout.trim()) || 0;\n  } catch (err) {\n    console.warn('[SPLICE RMS] Could not get duration:', err.message);\n    return 0;\n  }\n}\n\n/**\n * Extract a chunk of audio data starting at a specific time\n *\n * @param {string} audioPath - Path to audio file\n * @param {number} startTime - Start time in seconds\n * @param {number} duration - Duration to extract in seconds\n * @returns {Promise<Object>} Audio chunk data\n */\nasync function extractAudioDataChunk(audioPath, startTime, duration) {\n  // SECURITY: Use safe temp path generation\n  const tempFile = safeTempPath('splice_rms_chunk', '.raw');\n\n  try {\n    // SECURITY: Use execFile with array arguments to prevent command injection\n    const args = [\n      '-y',\n      '-ss', String(startTime),\n      '-t', String(duration),\n      '-i', audioPath,\n      '-ac', '1',\n      '-ar', '16000',\n      '-f', 's16le',\n      '-acodec', 'pcm_s16le',\n      tempFile\n    ];\n    await safeFFmpeg(args);\n\n    const rawData = await fsPromises.readFile(tempFile);\n\n    // Convert to float samples\n    const samples = new Float32Array(rawData.length / 2);\n    for (let i = 0; i < samples.length; i++) {\n      const int16 = rawData.readInt16LE(i * 2);\n      samples[i] = int16 / 32768.0;\n    }\n\n    await fsPromises.unlink(tempFile);\n\n    return {\n      samples,\n      sampleRate: 16000,\n      channels: 1,\n      duration\n    };\n  } catch (err) {\n    try {\n      await fsPromises.access(tempFile);\n      await fsPromises.unlink(tempFile);\n    } catch {\n      // File doesn't exist\n    }\n    throw new Error(`Failed to extract audio chunk: ${err.message}`);\n  }\n}\n\n// =============================================================================\n// Audio Extraction\n// =============================================================================\n\n/**\n * Extract raw audio samples from file using FFmpeg\n *\n * @param {string} audioPath - Path to audio file\n * @returns {Promise<Object>} Audio data with samples, sampleRate, channels, duration\n */\nasync function extractAudioData(audioPath) {\n  // SECURITY: Use execFile with array arguments to prevent command injection\n  const infoArgs = [\n    '-v', 'error',\n    '-select_streams', 'a:0',\n    '-show_entries', 'stream=sample_rate,channels,duration',\n    '-of', 'json',\n    audioPath\n  ];\n\n  let duration = 0;\n\n  try {\n    const { stdout } = await safeFFprobe(infoArgs);\n    const info = JSON.parse(stdout);\n    if (info.streams && info.streams[0]) {\n      // sampleRate and channels extracted but not currently used - reserved for future resampling\n      duration = parseFloat(info.streams[0].duration) || 0;\n    }\n  } catch (err) {\n    console.warn('[SPLICE RMS] Could not get audio info, using defaults:', err.message);\n  }\n\n  // If duration not available from stream, get from format\n  if (!duration) {\n    try {\n      const durationArgs = [\n        '-v', 'error',\n        '-show_entries', 'format=duration',\n        '-of', 'default=noprint_wrappers=1:nokey=1',\n        audioPath\n      ];\n      const { stdout } = await safeFFprobe(durationArgs);\n      duration = parseFloat(stdout.trim()) || 0;\n    } catch (err) {\n      console.warn('[SPLICE RMS] Could not get duration:', err.message);\n    }\n  }\n\n  // SECURITY: Use safe temp path generation\n  const tempFile = safeTempPath('splice_rms', '.raw');\n\n  try {\n    // SECURITY: Use execFile with array arguments to prevent command injection\n    const extractArgs = [\n      '-y',\n      '-i', audioPath,\n      '-ac', '1',\n      '-ar', '16000',\n      '-f', 's16le',\n      '-acodec', 'pcm_s16le',\n      tempFile\n    ];\n    await safeFFmpeg(extractArgs);\n\n    // Read the raw PCM data (async to avoid blocking event loop)\n    const rawData = await fsPromises.readFile(tempFile);\n\n    // Convert to float samples (-1 to 1)\n    const samples = new Float32Array(rawData.length / 2);\n    for (let i = 0; i < samples.length; i++) {\n      // Read signed 16-bit little-endian\n      const int16 = rawData.readInt16LE(i * 2);\n      samples[i] = int16 / 32768.0;\n    }\n\n    // Clean up temp file (async)\n    await fsPromises.unlink(tempFile);\n\n    return {\n      samples,\n      sampleRate: 16000, // We resampled to 16kHz\n      channels: 1,\n      duration\n    };\n  } catch (err) {\n    // Clean up temp file if exists\n    try {\n      await fsPromises.access(tempFile);\n      await fsPromises.unlink(tempFile);\n    } catch {\n      // File doesn't exist or already deleted\n    }\n    throw new Error(`Failed to extract audio: ${err.message}`);\n  }\n}\n\n// =============================================================================\n// RMS Calculation\n// =============================================================================\n\n/**\n * Calculate RMS values for sliding windows across audio samples\n *\n * @param {Float32Array} samples - Audio samples (-1 to 1)\n * @param {Object} options - Window and hop size\n * @returns {Float32Array} RMS values for each window\n */\nfunction calculateRMSWindows(samples, options = {}) {\n  const { windowSize = 800, hopSize = 400 } = options; // Default ~50ms at 16kHz\n\n  const numWindows = Math.floor((samples.length - windowSize) / hopSize) + 1;\n  const rmsValues = new Float32Array(numWindows);\n\n  for (let i = 0; i < numWindows; i++) {\n    const start = i * hopSize;\n    const end = Math.min(start + windowSize, samples.length);\n\n    // Calculate RMS for this window\n    let sumSquares = 0;\n    for (let j = start; j < end; j++) {\n      sumSquares += samples[j] * samples[j];\n    }\n\n    const rms = Math.sqrt(sumSquares / (end - start));\n    rmsValues[i] = rms;\n  }\n\n  return rmsValues;\n}\n\n/**\n * Convert RMS value to dBFS (decibels relative to full scale)\n *\n * @param {number} rms - RMS value (0 to 1)\n * @returns {number} dBFS value (negative, -inf for silence)\n */\nfunction rmsToDBFS(rms) {\n  if (rms <= 0) return -100; // Effective silence floor\n  const dbfs = 20 * Math.log10(rms);\n  return Math.max(-100, dbfs); // Clamp to reasonable floor\n}\n\n/**\n * Compute min, max, avg stats without using spread operator\n * Avoids call stack overflow for large arrays (100k+ elements)\n *\n * @param {Array<number>|Float32Array} values - dBFS values\n * @returns {Object} Stats object with minDB, maxDB, avgDB\n */\nfunction computeDBFSStats(values) {\n  if (!values || values.length === 0) {\n    return { minDB: '-100.0', maxDB: '-100.0', avgDB: '-100.0' };\n  }\n\n  let minDB = Infinity;\n  let maxDB = -Infinity;\n  let sum = 0;\n\n  for (let i = 0; i < values.length; i++) {\n    const v = values[i];\n    if (v < minDB) minDB = v;\n    if (v > maxDB) maxDB = v;\n    sum += v;\n  }\n\n  return {\n    minDB: minDB.toFixed(1),\n    maxDB: maxDB.toFixed(1),\n    avgDB: (sum / values.length).toFixed(1)\n  };\n}\n\n// =============================================================================\n// Auto-Threshold Detection\n// =============================================================================\n\n/**\n * Auto-detect optimal silence threshold from audio histogram\n * Uses Gaussian blur + local maxima detection\n *\n * @param {Float32Array} dBFSValues - Array of dBFS values\n * @returns {Object} Detected threshold and method info\n */\nfunction autoDetectThreshold(dBFSValues) {\n  // Build histogram of dBFS values\n  const histBins = 100;\n  const minDB = -80;\n  const maxDB = 0;\n  const binWidth = (maxDB - minDB) / histBins;\n\n  const histogram = new Array(histBins).fill(0);\n\n  for (const db of dBFSValues) {\n    const binIndex = Math.min(histBins - 1, Math.max(0, Math.floor((db - minDB) / binWidth)));\n    histogram[binIndex]++;\n  }\n\n  // Apply Gaussian blur to smooth histogram\n  const smoothed = gaussianBlur(histogram, 5);\n\n  // Find local maxima (peaks)\n  const peaks = findLocalMaxima(smoothed, 3);\n\n  // Sort peaks by value (highest first)\n  peaks.sort((a, b) => b.value - a.value);\n\n  if (peaks.length >= 2) {\n    // Interpolate between the two highest peaks\n    // Assume: higher peak = speech, lower peak = silence\n    const peak1DB = minDB + peaks[0].index * binWidth;\n    const peak2DB = minDB + peaks[1].index * binWidth;\n\n    const silencePeakDB = Math.min(peak1DB, peak2DB);\n    const speechPeakDB = Math.max(peak1DB, peak2DB);\n\n    // Set threshold between peaks (closer to silence peak)\n    const threshold = silencePeakDB + (speechPeakDB - silencePeakDB) * 0.3;\n\n    return {\n      threshold: Math.max(-60, Math.min(-20, threshold)),\n      method: 'dual-peak',\n      silencePeak: silencePeakDB.toFixed(1),\n      speechPeak: speechPeakDB.toFixed(1)\n    };\n  } else if (peaks.length === 1) {\n    // Single peak - use percentile-based fallback\n    const sorted = [...dBFSValues].sort((a, b) => a - b);\n    const percentile25 = sorted[Math.floor(sorted.length * 0.25)];\n\n    return {\n      threshold: Math.max(-60, Math.min(-20, percentile25)),\n      method: 'percentile-25',\n      value: percentile25.toFixed(1)\n    };\n  } else {\n    // No peaks found - use conservative default\n    return {\n      threshold: -35,\n      method: 'default-fallback'\n    };\n  }\n}\n\n/**\n * Apply Gaussian blur to an array\n *\n * @param {Array<number>} arr - Input array\n * @param {number} sigma - Standard deviation\n * @returns {Array<number>} Smoothed array\n */\nfunction gaussianBlur(arr, sigma = 5) {\n  const kernelSize = Math.ceil(sigma * 3) * 2 + 1;\n  const kernel = [];\n  const halfSize = Math.floor(kernelSize / 2);\n\n  // Generate Gaussian kernel\n  let sum = 0;\n  for (let i = 0; i < kernelSize; i++) {\n    const x = i - halfSize;\n    const g = Math.exp(-(x * x) / (2 * sigma * sigma));\n    kernel.push(g);\n    sum += g;\n  }\n\n  // Normalize kernel\n  for (let i = 0; i < kernelSize; i++) {\n    kernel[i] /= sum;\n  }\n\n  // Apply convolution\n  const result = new Array(arr.length).fill(0);\n  for (let i = 0; i < arr.length; i++) {\n    let value = 0;\n    for (let j = 0; j < kernelSize; j++) {\n      const idx = i + j - halfSize;\n      if (idx >= 0 && idx < arr.length) {\n        value += arr[idx] * kernel[j];\n      }\n    }\n    result[i] = value;\n  }\n\n  return result;\n}\n\n/**\n * Find local maxima in an array\n *\n * @param {Array<number>} arr - Input array\n * @param {number} windowSize - Neighborhood size for local maximum detection\n * @returns {Array<{index: number, value: number}>} Array of peak positions and values\n */\nfunction findLocalMaxima(arr, windowSize = 3) {\n  const peaks = [];\n  const halfWindow = Math.floor(windowSize / 2);\n\n  for (let i = halfWindow; i < arr.length - halfWindow; i++) {\n    let isMax = true;\n    for (let j = i - halfWindow; j <= i + halfWindow; j++) {\n      if (j !== i && arr[j] >= arr[i]) {\n        isMax = false;\n        break;\n      }\n    }\n    if (isMax && arr[i] > 0) {\n      peaks.push({ index: i, value: arr[i] });\n    }\n  }\n\n  return peaks;\n}\n\n// =============================================================================\n// Silence Region Detection\n// =============================================================================\n\n/**\n * Find silence regions based on dBFS threshold\n *\n * @param {Float32Array} dBFSValues - Array of dBFS values\n * @param {Object} options - Detection options\n * @returns {Array<{start: number, end: number, duration: number}>} Silence regions\n */\nfunction findSilenceRegions(dBFSValues, options) {\n  const { threshold, seekStep, minSilenceLength } = options;\n\n  const silences = [];\n  let silenceStart = null;\n\n  for (let i = 0; i < dBFSValues.length; i++) {\n    const time = i * seekStep;\n    const isSilent = dBFSValues[i] < threshold;\n\n    if (isSilent && silenceStart === null) {\n      // Start of silence\n      silenceStart = time;\n    } else if (!isSilent && silenceStart !== null) {\n      // End of silence\n      const duration = time - silenceStart;\n      if (duration >= minSilenceLength) {\n        silences.push({\n          start: silenceStart,\n          end: time,\n          duration\n        });\n      }\n      silenceStart = null;\n    }\n  }\n\n  // Handle silence at end of audio\n  if (silenceStart !== null) {\n    const endTime = dBFSValues.length * seekStep;\n    const duration = endTime - silenceStart;\n    if (duration >= minSilenceLength) {\n      silences.push({\n        start: silenceStart,\n        end: endTime,\n        duration\n      });\n    }\n  }\n\n  return silences;\n}\n\n// =============================================================================\n// Post-Processing\n// =============================================================================\n\n/**\n * Merge silences that are close together\n *\n * @param {Array} silences - Array of silence regions\n * @param {number} mergeDistance - Maximum gap to merge (seconds)\n * @returns {Array} Merged silence regions\n */\nfunction mergeSilences(silences, mergeDistance) {\n  if (silences.length <= 1) return silences;\n\n  const merged = [];\n  let current = { ...silences[0] };\n\n  for (let i = 1; i < silences.length; i++) {\n    const next = silences[i];\n\n    // If gap is small enough, merge\n    if (next.start - current.end <= mergeDistance) {\n      current.end = next.end;\n      current.duration = current.end - current.start;\n    } else {\n      merged.push(current);\n      current = { ...next };\n    }\n  }\n\n  merged.push(current);\n  return merged;\n}\n\n/**\n * Apply padding to silence boundaries\n *\n * @param {Array} silences - Array of silence regions\n * @param {Object} options - Padding options\n * @returns {Array} Padded silence regions\n */\nfunction applyPadding(silences, options) {\n  const { paddingStart, paddingEnd, audioDuration } = options;\n\n  return silences\n    .map(silence => {\n      const paddedStart = Math.max(0, silence.start + paddingStart);\n      const paddedEnd = Math.min(audioDuration, silence.end - paddingEnd);\n      const duration = paddedEnd - paddedStart;\n\n      // Skip if padding made the silence invalid\n      if (duration <= 0) return null;\n\n      return {\n        start: parseFloat(paddedStart.toFixed(3)),\n        end: parseFloat(paddedEnd.toFixed(3)),\n        duration: parseFloat(duration.toFixed(3))\n      };\n    })\n    .filter(s => s !== null);\n}\n\n// =============================================================================\n// Sensitivity Mapping\n// =============================================================================\n\n/**\n * Map UI sensitivity (0-100) to detection parameters\n * Based on Fireside's aggressiveness mapping\n *\n * @param {number} sensitivity - UI sensitivity value (0-100)\n * @returns {Object} Detection parameters\n */\nfunction sensitivityToParams(sensitivity) {\n  // Clamp to 0-100\n  const s = Math.max(0, Math.min(100, sensitivity));\n\n  // Linear interpolation helper\n  const lerp = (min, max, t) => min + (max - min) * (t / 100);\n\n  return {\n    // Threshold: -50dB (conservative) to -20dB (aggressive)\n    threshold: lerp(-50, -20, s),\n\n    // Min silence length: 2.0s (conservative) to 0.3s (aggressive)\n    minSilenceLength: lerp(2.0, 0.3, s),\n\n    // Padding start: 0.2s (conservative) to 0.05s (aggressive)\n    paddingStart: lerp(0.2, 0.05, s),\n\n    // Padding end: 0.15s (conservative) to 0.03s (aggressive)\n    paddingEnd: lerp(0.15, 0.03, s),\n\n    // Seek step stays constant\n    seekStep: 0.05\n  };\n}\n\n// =============================================================================\n// Exports\n// =============================================================================\n\n/**\n * Get waveform data for visualization\n * Returns downsampled RMS values suitable for canvas rendering\n *\n * @param {string} wavPath - Path to WAV file\n * @param {Object} options - Options\n * @returns {Promise<Object>} Waveform data with values and metadata\n */\nasync function getWaveformData(wavPath, options = {}) {\n  const {\n    targetPoints = 400,   // Number of points for visualization\n    windowMs = 50         // RMS window size\n  } = options;\n\n  const fs = require('fs');\n  const path = require('path');\n\n  if (!fs.existsSync(wavPath)) {\n    throw new Error(`File not found: ${wavPath}`);\n  }\n\n  // Read audio file\n  const buffer = fs.readFileSync(wavPath);\n  const audioData = parseWavBuffer(buffer);\n\n  // Calculate RMS values\n  const rmsValues = calculateRMSWindows(audioData.samples, {\n    sampleRate: audioData.sampleRate,\n    windowMs\n  });\n\n  // Downsample to target points\n  const step = Math.max(1, Math.floor(rmsValues.length / targetPoints));\n  const downsampled = [];\n\n  for (let i = 0; i < rmsValues.length; i += step) {\n    // Take max value in this chunk for peak display\n    let maxVal = 0;\n    for (let j = i; j < Math.min(i + step, rmsValues.length); j++) {\n      if (rmsValues[j] > maxVal) maxVal = rmsValues[j];\n    }\n    downsampled.push(maxVal);\n  }\n\n  // Normalize to 0-1 range\n  const maxRMS = Math.max(...downsampled, 0.001);\n  const normalized = downsampled.map(v => v / maxRMS);\n\n  const duration = audioData.samples.length / audioData.sampleRate;\n\n  return {\n    waveform: normalized,\n    pointCount: normalized.length,\n    duration,\n    sampleRate: audioData.sampleRate,\n    maxRMS,\n    fileName: path.basename(wavPath)\n  };\n}\n\n/**\n * Parse WAV buffer to extract audio samples\n * (Reusing internal function pattern from detectSilencesRMS)\n */\nfunction parseWavBuffer(buffer) {\n  // WAV header parsing\n  const sampleRate = buffer.readUInt32LE(24);\n  const bitsPerSample = buffer.readUInt16LE(34);\n  const numChannels = buffer.readUInt16LE(22);\n\n  // Find data chunk\n  let dataOffset = 44; // Default WAV header size\n  for (let i = 12; i < Math.min(buffer.length - 8, 200); i++) {\n    if (buffer.toString('ascii', i, i + 4) === 'data') {\n      dataOffset = i + 8;\n      break;\n    }\n  }\n\n  const samples = [];\n  const bytesPerSample = bitsPerSample / 8;\n\n  for (let i = dataOffset; i < buffer.length - bytesPerSample; i += bytesPerSample * numChannels) {\n    let sample = 0;\n    if (bitsPerSample === 16) {\n      sample = buffer.readInt16LE(i) / 32768;\n    } else if (bitsPerSample === 24) {\n      sample = (buffer.readInt16LE(i + 1) | (buffer.readInt8(i) << 8)) / 8388608;\n    } else if (bitsPerSample === 32) {\n      sample = buffer.readFloatLE(i);\n    }\n    samples.push(sample);\n  }\n\n  return { samples, sampleRate, bitsPerSample, numChannels };\n}\n\nmodule.exports = {\n  detectSilencesRMS,\n  detectSilencesRMSChunked,\n  sensitivityToParams,\n  autoDetectThreshold,\n  getAudioDuration,\n  getWaveformData,\n  // Export internal functions for testing\n  calculateRMSWindows,\n  rmsToDBFS,\n  gaussianBlur,\n  findLocalMaxima,\n  mergeSilences,\n  applyPadding,\n  extractAudioDataChunk,\n  DEFAULT_OPTIONS\n};\n","numLines":913,"startLine":1,"totalLines":913}}}
{"parentUuid":"7a4f8299-e5b8-4092-bcee-55c9fdeadc98","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Jqh3n16CohqAvJ1NUDXTha","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_0146PoxZHoA5yaZnKQGDTSEU","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/vocalIsolation.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6889,"cache_creation_input_tokens":47823,"cache_read_input_tokens":44058,"cache_creation":{"ephemeral_5m_input_tokens":47823,"ephemeral_1h_input_tokens":0},"output_tokens":18,"service_tier":"standard"}},"requestId":"req_011CWUvJHPsgtk966Z4hku8o","type":"assistant","uuid":"27604b27-c51e-4c91-a8d0-26d7c33450a5","timestamp":"2025-12-26T07:40:21.685Z"}
{"parentUuid":"27604b27-c51e-4c91-a8d0-26d7c33450a5","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Jqh3n16CohqAvJ1NUDXTha","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_015HH1ZpbTwyYB4EU6hFg1yu","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/animatedCaptions.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6889,"cache_creation_input_tokens":47823,"cache_read_input_tokens":44058,"cache_creation":{"ephemeral_5m_input_tokens":47823,"ephemeral_1h_input_tokens":0},"output_tokens":18,"service_tier":"standard"}},"requestId":"req_011CWUvJHPsgtk966Z4hku8o","type":"assistant","uuid":"3cadbecb-186a-49d9-9d97-4b0524bf3ac5","timestamp":"2025-12-26T07:40:22.229Z"}
{"parentUuid":"3cadbecb-186a-49d9-9d97-4b0524bf3ac5","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Jqh3n16CohqAvJ1NUDXTha","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_019Sr2SxVZT9Zh37qQ2p9raj","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/textBasedEditing.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6889,"cache_creation_input_tokens":47823,"cache_read_input_tokens":44058,"cache_creation":{"ephemeral_5m_input_tokens":47823,"ephemeral_1h_input_tokens":0},"output_tokens":18,"service_tier":"standard"}},"requestId":"req_011CWUvJHPsgtk966Z4hku8o","type":"assistant","uuid":"790f3252-760e-4f9c-a051-e92ed43f2f95","timestamp":"2025-12-26T07:40:22.798Z"}
{"parentUuid":"790f3252-760e-4f9c-a051-e92ed43f2f95","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Jqh3n16CohqAvJ1NUDXTha","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018yjZGaCeX1UgVjmpgj3y73","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/server.js","offset":500,"limit":500}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":6889,"cache_creation_input_tokens":47823,"cache_read_input_tokens":44058,"cache_creation":{"ephemeral_5m_input_tokens":47823,"ephemeral_1h_input_tokens":0},"output_tokens":18,"service_tier":"standard"}},"requestId":"req_011CWUvJHPsgtk966Z4hku8o","type":"assistant","uuid":"bdfd8be5-5b09-42ad-9843-24b9412bcf7f","timestamp":"2025-12-26T07:40:23.328Z"}
{"parentUuid":"bdfd8be5-5b09-42ad-9843-24b9412bcf7f","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_0146PoxZHoA5yaZnKQGDTSEU","type":"tool_result","content":"     1→/**\n     2→ * Vocal Isolation Service\n     3→ *\n     4→ * Uses Replicate's Demucs model to separate vocals from audio.\n     5→ * Cost: ~$0.015/min\n     6→ *\n     7→ * Workflow:\n     8→ * 1. Upload audio to temporary URL (or use file path)\n     9→ * 2. Call Replicate Demucs API\n    10→ * 3. Download isolated vocals track\n    11→ * 4. Return path to clean vocals file\n    12→ */\n    13→\n    14→const fs = require('fs');\n    15→const path = require('path');\n    16→const https = require('https');\n    17→const http = require('http');\n    18→\n    19→// Replicate API configuration\n    20→const REPLICATE_API_TOKEN = process.env.REPLICATE_API_TOKEN;\n    21→const DEMUCS_MODEL = 'cjwbw/demucs:25a173108cff36ef9f80f854c162d01df9e6528be175794b81158fa03836d953';\n    22→\n    23→/**\n    24→ * Isolate vocals from an audio file using Demucs via Replicate\n    25→ *\n    26→ * @param {string} audioPath - Path to input audio file\n    27→ * @param {Object} options - Configuration options\n    28→ * @param {string} options.outputDir - Directory for output (default: same as input)\n    29→ * @param {string} options.stem - Which stem to extract: 'vocals', 'drums', 'bass', 'other' (default: 'vocals')\n    30→ * @returns {Promise<Object>} Result with path to isolated vocals\n    31→ */\n    32→async function isolateVocals(audioPath, options = {}) {\n    33→  const { outputDir = path.dirname(audioPath), stem = 'vocals' } = options;\n    34→\n    35→  if (!REPLICATE_API_TOKEN) {\n    36→    throw new Error('REPLICATE_API_TOKEN environment variable is required');\n    37→  }\n    38→\n    39→  if (!fs.existsSync(audioPath)) {\n    40→    throw new Error(`Audio file not found: ${audioPath}`);\n    41→  }\n    42→\n    43→  console.log(`[VocalIsolation] Starting isolation for: ${audioPath}`);\n    44→  const startTime = Date.now();\n    45→\n    46→  // Read audio file and convert to base64 data URI (async to avoid blocking)\n    47→  const audioBuffer = await fs.promises.readFile(audioPath);\n    48→  const base64Audio = audioBuffer.toString('base64');\n    49→  const mimeType = getMimeType(audioPath);\n    50→  const dataUri = `data:${mimeType};base64,${base64Audio}`;\n    51→\n    52→  // Create prediction on Replicate\n    53→  const prediction = await createPrediction(dataUri);\n    54→  console.log(`[VocalIsolation] Prediction created: ${prediction.id}`);\n    55→\n    56→  // Poll for completion\n    57→  const result = await pollPrediction(prediction.id);\n    58→\n    59→  if (result.status === 'failed') {\n    60→    throw new Error(`Vocal isolation failed: ${result.error || 'Unknown error'}`);\n    61→  }\n    62→\n    63→  // Download the requested stem\n    64→  const stemUrl = result.output[stem];\n    65→  if (!stemUrl) {\n    66→    throw new Error(`Stem '${stem}' not found in output. Available: ${Object.keys(result.output).join(', ')}`);\n    67→  }\n    68→\n    69→  // Generate output filename\n    70→  const inputBasename = path.basename(audioPath, path.extname(audioPath));\n    71→  const outputFilename = `${inputBasename}_${stem}.wav`;\n    72→  const outputPath = path.join(outputDir, outputFilename);\n    73→\n    74→  // Download the isolated track\n    75→  await downloadFile(stemUrl, outputPath);\n    76→\n    77→  const duration = ((Date.now() - startTime) / 1000).toFixed(1);\n    78→  console.log(`[VocalIsolation] Complete in ${duration}s: ${outputPath}`);\n    79→\n    80→  return {\n    81→    success: true,\n    82→    inputPath: audioPath,\n    83→    outputPath,\n    84→    stem,\n    85→    processingTime: parseFloat(duration),\n    86→    allStems: Object.keys(result.output)\n    87→  };\n    88→}\n    89→\n    90→/**\n    91→ * Create a prediction on Replicate\n    92→ */\n    93→async function createPrediction(audioDataUri) {\n    94→  const requestBody = JSON.stringify({\n    95→    version: DEMUCS_MODEL.split(':')[1],\n    96→    input: {\n    97→      audio: audioDataUri,\n    98→      output_format: 'wav'\n    99→    }\n   100→  });\n   101→\n   102→  return new Promise((resolve, reject) => {\n   103→    const req = https.request({\n   104→      hostname: 'api.replicate.com',\n   105→      path: '/v1/predictions',\n   106→      method: 'POST',\n   107→      headers: {\n   108→        'Authorization': `Bearer ${REPLICATE_API_TOKEN}`,\n   109→        'Content-Type': 'application/json',\n   110→        'Content-Length': Buffer.byteLength(requestBody)\n   111→      }\n   112→    }, (res) => {\n   113→      let data = '';\n   114→      res.on('data', chunk => data += chunk);\n   115→      res.on('end', () => {\n   116→        try {\n   117→          const parsed = JSON.parse(data);\n   118→          if (res.statusCode >= 400) {\n   119→            reject(new Error(`Replicate API error: ${parsed.detail || data}`));\n   120→          } else {\n   121→            resolve(parsed);\n   122→          }\n   123→        } catch {\n   124→          reject(new Error(`Failed to parse Replicate response: ${data}`));\n   125→        }\n   126→      });\n   127→    });\n   128→\n   129→    req.on('error', reject);\n   130→    req.write(requestBody);\n   131→    req.end();\n   132→  });\n   133→}\n   134→\n   135→/**\n   136→ * Poll prediction status until complete\n   137→ */\n   138→async function pollPrediction(predictionId, maxWaitMs = 300000) {\n   139→  const startTime = Date.now();\n   140→  const pollInterval = 2000; // 2 seconds\n   141→\n   142→  while (Date.now() - startTime < maxWaitMs) {\n   143→    const prediction = await getPrediction(predictionId);\n   144→\n   145→    if (prediction.status === 'succeeded') {\n   146→      return prediction;\n   147→    }\n   148→\n   149→    if (prediction.status === 'failed' || prediction.status === 'canceled') {\n   150→      return prediction;\n   151→    }\n   152→\n   153→    console.log(`[VocalIsolation] Status: ${prediction.status}...`);\n   154→    await sleep(pollInterval);\n   155→  }\n   156→\n   157→  throw new Error(`Prediction timed out after ${maxWaitMs / 1000}s`);\n   158→}\n   159→\n   160→/**\n   161→ * Get prediction status from Replicate\n   162→ */\n   163→async function getPrediction(predictionId) {\n   164→  return new Promise((resolve, reject) => {\n   165→    const req = https.request({\n   166→      hostname: 'api.replicate.com',\n   167→      path: `/v1/predictions/${predictionId}`,\n   168→      method: 'GET',\n   169→      headers: {\n   170→        'Authorization': `Bearer ${REPLICATE_API_TOKEN}`\n   171→      }\n   172→    }, (res) => {\n   173→      let data = '';\n   174→      res.on('data', chunk => data += chunk);\n   175→      res.on('end', () => {\n   176→        try {\n   177→          resolve(JSON.parse(data));\n   178→        } catch {\n   179→          reject(new Error(`Failed to parse prediction status: ${data}`));\n   180→        }\n   181→      });\n   182→    });\n   183→\n   184→    req.on('error', reject);\n   185→    req.end();\n   186→  });\n   187→}\n   188→\n   189→/**\n   190→ * Download a file from URL to local path\n   191→ */\n   192→async function downloadFile(url, outputPath) {\n   193→  return new Promise((resolve, reject) => {\n   194→    const file = fs.createWriteStream(outputPath);\n   195→    const protocol = url.startsWith('https') ? https : http;\n   196→\n   197→    protocol.get(url, (response) => {\n   198→      // Handle redirects\n   199→      if (response.statusCode === 301 || response.statusCode === 302) {\n   200→        file.close();\n   201→        fs.unlinkSync(outputPath);\n   202→        return downloadFile(response.headers.location, outputPath)\n   203→          .then(resolve)\n   204→          .catch(reject);\n   205→      }\n   206→\n   207→      if (response.statusCode !== 200) {\n   208→        file.close();\n   209→        fs.unlinkSync(outputPath);\n   210→        reject(new Error(`Failed to download: HTTP ${response.statusCode}`));\n   211→        return;\n   212→      }\n   213→\n   214→      response.pipe(file);\n   215→      file.on('finish', () => {\n   216→        file.close();\n   217→        resolve();\n   218→      });\n   219→    }).on('error', (err) => {\n   220→      file.close();\n   221→      fs.unlink(outputPath, () => {}); // Clean up partial file\n   222→      reject(err);\n   223→    });\n   224→  });\n   225→}\n   226→\n   227→/**\n   228→ * Get MIME type from file extension\n   229→ */\n   230→function getMimeType(filePath) {\n   231→  const ext = path.extname(filePath).toLowerCase();\n   232→  const mimeTypes = {\n   233→    '.wav': 'audio/wav',\n   234→    '.mp3': 'audio/mpeg',\n   235→    '.m4a': 'audio/mp4',\n   236→    '.flac': 'audio/flac',\n   237→    '.ogg': 'audio/ogg',\n   238→    '.aac': 'audio/aac'\n   239→  };\n   240→  return mimeTypes[ext] || 'audio/wav';\n   241→}\n   242→\n   243→/**\n   244→ * Sleep utility\n   245→ */\n   246→function sleep(ms) {\n   247→  return new Promise(resolve => global.setTimeout(resolve, ms));\n   248→}\n   249→\n   250→/**\n   251→ * Check if Replicate API token is configured\n   252→ */\n   253→function isReplicateConfigured() {\n   254→  return !!REPLICATE_API_TOKEN;\n   255→}\n   256→\n   257→module.exports = {\n   258→  isolateVocals,\n   259→  isReplicateConfigured\n   260→};\n   261→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"5a100b46-bb73-4b98-95d1-dadae54374db","timestamp":"2025-12-26T07:40:23.409Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/vocalIsolation.js","content":"/**\n * Vocal Isolation Service\n *\n * Uses Replicate's Demucs model to separate vocals from audio.\n * Cost: ~$0.015/min\n *\n * Workflow:\n * 1. Upload audio to temporary URL (or use file path)\n * 2. Call Replicate Demucs API\n * 3. Download isolated vocals track\n * 4. Return path to clean vocals file\n */\n\nconst fs = require('fs');\nconst path = require('path');\nconst https = require('https');\nconst http = require('http');\n\n// Replicate API configuration\nconst REPLICATE_API_TOKEN = process.env.REPLICATE_API_TOKEN;\nconst DEMUCS_MODEL = 'cjwbw/demucs:25a173108cff36ef9f80f854c162d01df9e6528be175794b81158fa03836d953';\n\n/**\n * Isolate vocals from an audio file using Demucs via Replicate\n *\n * @param {string} audioPath - Path to input audio file\n * @param {Object} options - Configuration options\n * @param {string} options.outputDir - Directory for output (default: same as input)\n * @param {string} options.stem - Which stem to extract: 'vocals', 'drums', 'bass', 'other' (default: 'vocals')\n * @returns {Promise<Object>} Result with path to isolated vocals\n */\nasync function isolateVocals(audioPath, options = {}) {\n  const { outputDir = path.dirname(audioPath), stem = 'vocals' } = options;\n\n  if (!REPLICATE_API_TOKEN) {\n    throw new Error('REPLICATE_API_TOKEN environment variable is required');\n  }\n\n  if (!fs.existsSync(audioPath)) {\n    throw new Error(`Audio file not found: ${audioPath}`);\n  }\n\n  console.log(`[VocalIsolation] Starting isolation for: ${audioPath}`);\n  const startTime = Date.now();\n\n  // Read audio file and convert to base64 data URI (async to avoid blocking)\n  const audioBuffer = await fs.promises.readFile(audioPath);\n  const base64Audio = audioBuffer.toString('base64');\n  const mimeType = getMimeType(audioPath);\n  const dataUri = `data:${mimeType};base64,${base64Audio}`;\n\n  // Create prediction on Replicate\n  const prediction = await createPrediction(dataUri);\n  console.log(`[VocalIsolation] Prediction created: ${prediction.id}`);\n\n  // Poll for completion\n  const result = await pollPrediction(prediction.id);\n\n  if (result.status === 'failed') {\n    throw new Error(`Vocal isolation failed: ${result.error || 'Unknown error'}`);\n  }\n\n  // Download the requested stem\n  const stemUrl = result.output[stem];\n  if (!stemUrl) {\n    throw new Error(`Stem '${stem}' not found in output. Available: ${Object.keys(result.output).join(', ')}`);\n  }\n\n  // Generate output filename\n  const inputBasename = path.basename(audioPath, path.extname(audioPath));\n  const outputFilename = `${inputBasename}_${stem}.wav`;\n  const outputPath = path.join(outputDir, outputFilename);\n\n  // Download the isolated track\n  await downloadFile(stemUrl, outputPath);\n\n  const duration = ((Date.now() - startTime) / 1000).toFixed(1);\n  console.log(`[VocalIsolation] Complete in ${duration}s: ${outputPath}`);\n\n  return {\n    success: true,\n    inputPath: audioPath,\n    outputPath,\n    stem,\n    processingTime: parseFloat(duration),\n    allStems: Object.keys(result.output)\n  };\n}\n\n/**\n * Create a prediction on Replicate\n */\nasync function createPrediction(audioDataUri) {\n  const requestBody = JSON.stringify({\n    version: DEMUCS_MODEL.split(':')[1],\n    input: {\n      audio: audioDataUri,\n      output_format: 'wav'\n    }\n  });\n\n  return new Promise((resolve, reject) => {\n    const req = https.request({\n      hostname: 'api.replicate.com',\n      path: '/v1/predictions',\n      method: 'POST',\n      headers: {\n        'Authorization': `Bearer ${REPLICATE_API_TOKEN}`,\n        'Content-Type': 'application/json',\n        'Content-Length': Buffer.byteLength(requestBody)\n      }\n    }, (res) => {\n      let data = '';\n      res.on('data', chunk => data += chunk);\n      res.on('end', () => {\n        try {\n          const parsed = JSON.parse(data);\n          if (res.statusCode >= 400) {\n            reject(new Error(`Replicate API error: ${parsed.detail || data}`));\n          } else {\n            resolve(parsed);\n          }\n        } catch {\n          reject(new Error(`Failed to parse Replicate response: ${data}`));\n        }\n      });\n    });\n\n    req.on('error', reject);\n    req.write(requestBody);\n    req.end();\n  });\n}\n\n/**\n * Poll prediction status until complete\n */\nasync function pollPrediction(predictionId, maxWaitMs = 300000) {\n  const startTime = Date.now();\n  const pollInterval = 2000; // 2 seconds\n\n  while (Date.now() - startTime < maxWaitMs) {\n    const prediction = await getPrediction(predictionId);\n\n    if (prediction.status === 'succeeded') {\n      return prediction;\n    }\n\n    if (prediction.status === 'failed' || prediction.status === 'canceled') {\n      return prediction;\n    }\n\n    console.log(`[VocalIsolation] Status: ${prediction.status}...`);\n    await sleep(pollInterval);\n  }\n\n  throw new Error(`Prediction timed out after ${maxWaitMs / 1000}s`);\n}\n\n/**\n * Get prediction status from Replicate\n */\nasync function getPrediction(predictionId) {\n  return new Promise((resolve, reject) => {\n    const req = https.request({\n      hostname: 'api.replicate.com',\n      path: `/v1/predictions/${predictionId}`,\n      method: 'GET',\n      headers: {\n        'Authorization': `Bearer ${REPLICATE_API_TOKEN}`\n      }\n    }, (res) => {\n      let data = '';\n      res.on('data', chunk => data += chunk);\n      res.on('end', () => {\n        try {\n          resolve(JSON.parse(data));\n        } catch {\n          reject(new Error(`Failed to parse prediction status: ${data}`));\n        }\n      });\n    });\n\n    req.on('error', reject);\n    req.end();\n  });\n}\n\n/**\n * Download a file from URL to local path\n */\nasync function downloadFile(url, outputPath) {\n  return new Promise((resolve, reject) => {\n    const file = fs.createWriteStream(outputPath);\n    const protocol = url.startsWith('https') ? https : http;\n\n    protocol.get(url, (response) => {\n      // Handle redirects\n      if (response.statusCode === 301 || response.statusCode === 302) {\n        file.close();\n        fs.unlinkSync(outputPath);\n        return downloadFile(response.headers.location, outputPath)\n          .then(resolve)\n          .catch(reject);\n      }\n\n      if (response.statusCode !== 200) {\n        file.close();\n        fs.unlinkSync(outputPath);\n        reject(new Error(`Failed to download: HTTP ${response.statusCode}`));\n        return;\n      }\n\n      response.pipe(file);\n      file.on('finish', () => {\n        file.close();\n        resolve();\n      });\n    }).on('error', (err) => {\n      file.close();\n      fs.unlink(outputPath, () => {}); // Clean up partial file\n      reject(err);\n    });\n  });\n}\n\n/**\n * Get MIME type from file extension\n */\nfunction getMimeType(filePath) {\n  const ext = path.extname(filePath).toLowerCase();\n  const mimeTypes = {\n    '.wav': 'audio/wav',\n    '.mp3': 'audio/mpeg',\n    '.m4a': 'audio/mp4',\n    '.flac': 'audio/flac',\n    '.ogg': 'audio/ogg',\n    '.aac': 'audio/aac'\n  };\n  return mimeTypes[ext] || 'audio/wav';\n}\n\n/**\n * Sleep utility\n */\nfunction sleep(ms) {\n  return new Promise(resolve => global.setTimeout(resolve, ms));\n}\n\n/**\n * Check if Replicate API token is configured\n */\nfunction isReplicateConfigured() {\n  return !!REPLICATE_API_TOKEN;\n}\n\nmodule.exports = {\n  isolateVocals,\n  isReplicateConfigured\n};\n","numLines":261,"startLine":1,"totalLines":261}}}
{"parentUuid":"5a100b46-bb73-4b98-95d1-dadae54374db","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_015HH1ZpbTwyYB4EU6hFg1yu","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Animated Captions Service\n     3→ *\n     4→ * Generates word-by-word animated captions with popular styles.\n     5→ * Supports keyword highlighting, emoji insertion, and multiple export formats.\n     6→ */\n     7→\n     8→const OpenAI = require('openai');\n     9→\n    10→// Lazy-load OpenAI client\n    11→let openai = null;\n    12→function getOpenAI() {\n    13→  if (!openai) {\n    14→    openai = new OpenAI();\n    15→  }\n    16→  return openai;\n    17→}\n    18→\n    19→// ============================================================================\n    20→// TEMPLATE DEFINITIONS\n    21→// ============================================================================\n    22→\n    23→const TEMPLATES = {\n    24→  mrbeast: {\n    25→    id: 'mrbeast',\n    26→    name: 'MrBeast',\n    27→    description: 'Bold, uppercase, colorful with shake animation',\n    28→    style: {\n    29→      fontFamily: 'Impact',\n    30→      fontSize: 72,\n    31→      fontWeight: 'bold',\n    32→      textTransform: 'uppercase',\n    33→      color: '#FFFFFF',\n    34→      strokeColor: '#000000',\n    35→      strokeWidth: 4,\n    36→      shadowColor: 'rgba(0,0,0,0.8)',\n    37→      shadowBlur: 8,\n    38→      position: 'bottom-center',\n    39→      padding: 20\n    40→    },\n    41→    animation: {\n    42→      type: 'shake-scale',\n    43→      duration: 0.15,\n    44→      scale: 1.2,\n    45→      shake: 3,\n    46→      easing: 'ease-out'\n    47→    },\n    48→    highlight: {\n    49→      enabled: true,\n    50→      color: '#FFFF00',\n    51→      keywords: ['amazing', 'insane', 'crazy', 'million', 'thousand', 'win', 'lose', 'money', 'challenge']\n    52→    }\n    53→  },\n    54→\n    55→  hormozi: {\n    56→    id: 'hormozi',\n    57→    name: 'Hormozi',\n    58→    description: 'Clean, yellow highlight bar, professional',\n    59→    style: {\n    60→      fontFamily: 'Arial Black',\n    61→      fontSize: 64,\n    62→      fontWeight: 'bold',\n    63→      textTransform: 'none',\n    64→      color: '#FFFFFF',\n    65→      strokeColor: '#000000',\n    66→      strokeWidth: 2,\n    67→      backgroundColor: 'rgba(0,0,0,0.7)',\n    68→      position: 'bottom-center',\n    69→      padding: 16\n    70→    },\n    71→    animation: {\n    72→      type: 'slide-in',\n    73→      duration: 0.2,\n    74→      direction: 'left',\n    75→      easing: 'ease-out'\n    76→    },\n    77→    highlight: {\n    78→      enabled: true,\n    79→      color: '#FFD700',\n    80→      backgroundColor: '#FFD700',\n    81→      textColor: '#000000',\n    82→      keywords: ['money', 'business', 'million', 'profit', 'revenue', 'growth', 'scale']\n    83→    }\n    84→  },\n    85→\n    86→  gaming: {\n    87→    id: 'gaming',\n    88→    name: 'Gaming',\n    89→    description: 'Neon glow, glitch effects, fast transitions',\n    90→    style: {\n    91→      fontFamily: 'Orbitron',\n    92→      fontSize: 56,\n    93→      fontWeight: 'bold',\n    94→      textTransform: 'uppercase',\n    95→      color: '#00FF00',\n    96→      strokeColor: '#000000',\n    97→      strokeWidth: 2,\n    98→      glowColor: '#00FF00',\n    99→      glowBlur: 15,\n   100→      position: 'bottom-center',\n   101→      padding: 12\n   102→    },\n   103→    animation: {\n   104→      type: 'glitch',\n   105→      duration: 0.1,\n   106→      glitchIntensity: 5,\n   107→      chromatic: true,\n   108→      easing: 'linear'\n   109→    },\n   110→    highlight: {\n   111→      enabled: true,\n   112→      color: '#FF00FF',\n   113→      glowColor: '#FF00FF',\n   114→      keywords: ['kill', 'win', 'gg', 'clutch', 'epic', 'legendary', 'headshot', 'victory']\n   115→    }\n   116→  },\n   117→\n   118→  corporate: {\n   119→    id: 'corporate',\n   120→    name: 'Corporate',\n   121→    description: 'Subtle, professional, clean fade animations',\n   122→    style: {\n   123→      fontFamily: 'Helvetica Neue',\n   124→      fontSize: 48,\n   125→      fontWeight: '500',\n   126→      textTransform: 'none',\n   127→      color: '#FFFFFF',\n   128→      strokeColor: 'transparent',\n   129→      strokeWidth: 0,\n   130→      backgroundColor: 'rgba(0,0,0,0.6)',\n   131→      borderRadius: 8,\n   132→      position: 'bottom-center',\n   133→      padding: 20\n   134→    },\n   135→    animation: {\n   136→      type: 'fade',\n   137→      duration: 0.3,\n   138→      easing: 'ease-in-out'\n   139→    },\n   140→    highlight: {\n   141→      enabled: false,\n   142→      color: '#3B82F6',\n   143→      keywords: []\n   144→    }\n   145→  },\n   146→\n   147→  karaoke: {\n   148→    id: 'karaoke',\n   149→    name: 'Karaoke',\n   150→    description: 'Word-by-word highlight bar, progressive fill',\n   151→    style: {\n   152→      fontFamily: 'Arial',\n   153→      fontSize: 60,\n   154→      fontWeight: 'bold',\n   155→      textTransform: 'none',\n   156→      color: '#FFFFFF',\n   157→      highlightColor: '#FFD700',\n   158→      strokeColor: '#000000',\n   159→      strokeWidth: 3,\n   160→      position: 'bottom-center',\n   161→      padding: 16\n   162→    },\n   163→    animation: {\n   164→      type: 'progressive-fill',\n   165→      duration: 'word-timed',\n   166→      fillDirection: 'left-to-right',\n   167→      easing: 'linear'\n   168→    },\n   169→    highlight: {\n   170→      enabled: true,\n   171→      mode: 'progressive',\n   172→      color: '#FFD700',\n   173→      keywords: []\n   174→    }\n   175→  }\n   176→};\n   177→\n   178→// ============================================================================\n   179→// EMOJI MAPPINGS\n   180→// ============================================================================\n   181→\n   182→const EMOJI_MAPPINGS = {\n   183→  // Emotions\n   184→  happy: '😊', excited: '🎉', sad: '😢', angry: '😠', love: '❤️', laugh: '😂',\n   185→  surprise: '😮', think: '🤔', cool: '😎', fire: '🔥', hot: '🔥',\n   186→\n   187→  // Actions\n   188→  win: '🏆', lose: '😞', money: '💰', work: '💼', sleep: '😴', eat: '🍽️',\n   189→  drink: '🍺', play: '🎮', music: '🎵', video: '🎬', phone: '📱',\n   190→\n   191→  // Objects\n   192→  car: '🚗', house: '🏠', computer: '💻', camera: '📷', book: '📚',\n   193→\n   194→  // Emphasis\n   195→  important: '⚠️', question: '❓', check: '✅', cross: '❌', star: '⭐',\n   196→  rocket: '🚀', target: '🎯', idea: '💡', time: '⏰', point: '👉'\n   197→};\n   198→\n   199→// ============================================================================\n   200→// MAIN FUNCTIONS\n   201→// ============================================================================\n   202→\n   203→/**\n   204→ * Generate animated captions from transcript\n   205→ * @param {Object} transcript - Transcript with words array (word, start, end)\n   206→ * @param {string} templateId - Template to apply\n   207→ * @param {Object} settings - Custom settings\n   208→ * @returns {Promise<Object>} Animated caption data\n   209→ */\n   210→async function generateAnimatedCaptions(transcript, templateId = 'mrbeast', settings = {}) {\n   211→  const {\n   212→    maxWordsPerLine = 4,\n   213→    highlightKeywords = true,\n   214→    insertEmojis = false,\n   215→    emojiFrequency = 'medium', // low, medium, high\n   216→    customKeywords = [],\n   217→    language = 'en'\n   218→  } = settings;\n   219→\n   220→  // Get template\n   221→  const template = TEMPLATES[templateId] || TEMPLATES.mrbeast;\n   222→\n   223→  // Extract words from transcript\n   224→  const words = extractWords(transcript);\n   225→\n   226→  if (!words || words.length === 0) {\n   227→    return {\n   228→      success: false,\n   229→      error: 'No words found in transcript',\n   230→      captions: []\n   231→    };\n   232→  }\n   233→\n   234→  console.log(`[SPLICE Captions] Generating ${templateId} captions for ${words.length} words`);\n   235→\n   236→  // Group words into caption lines\n   237→  const lines = groupWordsIntoLines(words, maxWordsPerLine);\n   238→\n   239→  // Detect keywords if enabled\n   240→  let keywords = [];\n   241→  if (highlightKeywords) {\n   242→    keywords = await detectKeywords(transcript, {\n   243→      customKeywords,\n   244→      templateKeywords: template.highlight.keywords || []\n   245→    });\n   246→  }\n   247→\n   248→  // Generate caption data for each line\n   249→  const captions = lines.map((line, index) => {\n   250→    const lineWords = line.words.map(word => {\n   251→      const isKeyword = keywords.some(kw =>\n   252→        word.word.toLowerCase().includes(kw.toLowerCase())\n   253→      );\n   254→\n   255→      return {\n   256→        word: word.word,\n   257→        start: word.start,\n   258→        end: word.end,\n   259→        isKeyword,\n   260→        emoji: insertEmojis ? getEmojiForWord(word.word, emojiFrequency) : null\n   261→      };\n   262→    });\n   263→\n   264→    return {\n   265→      id: `caption_${index}`,\n   266→      lineNumber: index + 1,\n   267→      text: line.text,\n   268→      words: lineWords,\n   269→      start: line.start,\n   270→      end: line.end,\n   271→      duration: line.end - line.start,\n   272→      template: templateId,\n   273→      style: { ...template.style },\n   274→      animation: { ...template.animation }\n   275→    };\n   276→  });\n   277→\n   278→  // Insert emojis if enabled\n   279→  let finalCaptions = captions;\n   280→  if (insertEmojis) {\n   281→    finalCaptions = addEmojisToCaption(captions, emojiFrequency);\n   282→  }\n   283→\n   284→  return {\n   285→    success: true,\n   286→    template: templateId,\n   287→    templateInfo: {\n   288→      name: template.name,\n   289→      description: template.description\n   290→    },\n   291→    totalCaptions: finalCaptions.length,\n   292→    totalWords: words.length,\n   293→    duration: words[words.length - 1]?.end || 0,\n   294→    captions: finalCaptions,\n   295→    keywords: keywords,\n   296→    metadata: {\n   297→      maxWordsPerLine,\n   298→      highlightKeywords,\n   299→      insertEmojis,\n   300→      emojiFrequency\n   301→    }\n   302→  };\n   303→}\n   304→\n   305→/**\n   306→ * Detect keywords using AI\n   307→ * @param {Object} transcript - Transcript data\n   308→ * @param {Object} options - Detection options\n   309→ * @returns {Promise<string[]>} Array of keywords\n   310→ */\n   311→async function detectKeywords(transcript, options = {}) {\n   312→  const {\n   313→    customKeywords = [],\n   314→    templateKeywords = [],\n   315→    maxKeywords = 20,\n   316→    useAI = true\n   317→  } = options;\n   318→\n   319→  // Start with template and custom keywords\n   320→  let keywords = [...new Set([...templateKeywords, ...customKeywords])];\n   321→\n   322→  if (!useAI) {\n   323→    return keywords.slice(0, maxKeywords);\n   324→  }\n   325→\n   326→  // Extract text\n   327→  const text = extractText(transcript);\n   328→  if (!text || text.length < 50) {\n   329→    return keywords.slice(0, maxKeywords);\n   330→  }\n   331→\n   332→  try {\n   333→    console.log('[SPLICE Captions] Detecting keywords with AI...');\n   334→\n   335→    const response = await getOpenAI().chat.completions.create({\n   336→      model: 'gpt-4o-mini',\n   337→      messages: [\n   338→        {\n   339→          role: 'system',\n   340→          content: 'You are a keyword extraction expert. Extract impactful words that should be highlighted in video captions for emphasis and engagement.'\n   341→        },\n   342→        {\n   343→          role: 'user',\n   344→          content: `Extract ${maxKeywords} keywords from this transcript that would be impactful to highlight in animated captions. Focus on: numbers, emotional words, action words, important nouns, and phrases that viewers would want emphasized.\n   345→\n   346→TRANSCRIPT:\n   347→${text.substring(0, 3000)}\n   348→\n   349→Return JSON: { \"keywords\": [\"word1\", \"word2\", ...] }`\n   350→        }\n   351→      ],\n   352→      temperature: 0.5,\n   353→      max_tokens: 200,\n   354→      response_format: { type: 'json_object' }\n   355→    });\n   356→\n   357→    const result = JSON.parse(response.choices[0].message.content);\n   358→    const aiKeywords = result.keywords || [];\n   359→\n   360→    // Merge with existing keywords\n   361→    keywords = [...new Set([...keywords, ...aiKeywords])];\n   362→\n   363→    console.log(`[SPLICE Captions] Detected ${aiKeywords.length} AI keywords`);\n   364→\n   365→  } catch (err) {\n   366→    console.error('[SPLICE Captions] Keyword detection error:', err.message);\n   367→    // Continue with template/custom keywords only\n   368→  }\n   369→\n   370→  return keywords.slice(0, maxKeywords);\n   371→}\n   372→\n   373→/**\n   374→ * Insert emojis into transcript based on context\n   375→ * @param {Object} transcript - Transcript data\n   376→ * @param {Object} options - Emoji options\n   377→ * @returns {Promise<Object>} Transcript with emoji suggestions\n   378→ */\n   379→async function insertEmojis(transcript, options = {}) {\n   380→  const {\n   381→    frequency = 'medium', // low (~1 per 30 words), medium (~1 per 15 words), high (~1 per 8 words)\n   382→    position = 'after', // before, after, replace\n   383→    useAI = true\n   384→  } = options;\n   385→\n   386→  const frequencyMap = { low: 30, medium: 15, high: 8 };\n   387→  const interval = frequencyMap[frequency] || 15;\n   388→\n   389→  const words = extractWords(transcript);\n   390→  if (!words || words.length === 0) {\n   391→    return { success: false, error: 'No words found', emojis: [] };\n   392→  }\n   393→\n   394→  const emojis = [];\n   395→\n   396→  if (useAI) {\n   397→    // Use AI to suggest contextual emojis\n   398→    try {\n   399→      const text = extractText(transcript);\n   400→      const response = await getOpenAI().chat.completions.create({\n   401→        model: 'gpt-4o-mini',\n   402→        messages: [\n   403→          {\n   404→            role: 'system',\n   405→            content: 'You suggest contextually appropriate emojis for video captions. Return relevant emojis that match the tone and content.'\n   406→          },\n   407→          {\n   408→            role: 'user',\n   409→            content: `Suggest emojis to insert into this transcript at approximately every ${interval} words. Return the word index and emoji.\n   410→\n   411→TRANSCRIPT (${words.length} words):\n   412→${text.substring(0, 2000)}\n   413→\n   414→Return JSON: { \"emojis\": [{ \"afterWordIndex\": 5, \"emoji\": \"🔥\" }, ...] }`\n   415→          }\n   416→        ],\n   417→        temperature: 0.7,\n   418→        max_tokens: 300,\n   419→        response_format: { type: 'json_object' }\n   420→      });\n   421→\n   422→      const result = JSON.parse(response.choices[0].message.content);\n   423→      emojis.push(...(result.emojis || []));\n   424→\n   425→    } catch (err) {\n   426→      console.error('[SPLICE Captions] Emoji insertion error:', err.message);\n   427→      // Fall back to rule-based\n   428→    }\n   429→  }\n   430→\n   431→  // Rule-based fallback/supplement\n   432→  if (emojis.length === 0) {\n   433→    words.forEach((word, index) => {\n   434→      if (index % interval === 0 && index > 0) {\n   435→        const emoji = getEmojiForWord(word.word, frequency);\n   436→        if (emoji) {\n   437→          emojis.push({ afterWordIndex: index, emoji, word: word.word });\n   438→        }\n   439→      }\n   440→    });\n   441→  }\n   442→\n   443→  return {\n   444→    success: true,\n   445→    totalEmojis: emojis.length,\n   446→    frequency,\n   447→    emojis\n   448→  };\n   449→}\n   450→\n   451→/**\n   452→ * Apply template to caption data\n   453→ * @param {Object} captionData - Existing caption data\n   454→ * @param {string} templateId - Template to apply\n   455→ * @returns {Object} Caption data with new template applied\n   456→ */\n   457→function applyTemplate(captionData, templateId) {\n   458→  const template = TEMPLATES[templateId];\n   459→  if (!template) {\n   460→    return {\n   461→      success: false,\n   462→      error: `Template '${templateId}' not found`,\n   463→      availableTemplates: Object.keys(TEMPLATES)\n   464→    };\n   465→  }\n   466→\n   467→  const updatedCaptions = captionData.captions.map(caption => ({\n   468→    ...caption,\n   469→    template: templateId,\n   470→    style: { ...template.style },\n   471→    animation: { ...template.animation }\n   472→  }));\n   473→\n   474→  return {\n   475→    success: true,\n   476→    template: templateId,\n   477→    templateInfo: {\n   478→      name: template.name,\n   479→      description: template.description\n   480→    },\n   481→    captions: updatedCaptions\n   482→  };\n   483→}\n   484→\n   485→/**\n   486→ * Generate MOGRT-compatible data for After Effects/Premiere\n   487→ * @param {Array} captions - Caption data array\n   488→ * @param {Object} settings - MOGRT settings\n   489→ * @returns {Object} MOGRT-compatible JSON\n   490→ */\n   491→function generateMOGRTData(captions, settings = {}) {\n   492→  const {\n   493→    frameRate = 30,\n   494→    width = 1920,\n   495→    height = 1080,\n   496→    outputFormat = 'json'\n   497→  } = settings;\n   498→\n   499→  const mogrtData = {\n   500→    version: '1.0',\n   501→    type: 'animated_captions',\n   502→    composition: {\n   503→      width,\n   504→      height,\n   505→      frameRate,\n   506→      duration: captions.length > 0 ? captions[captions.length - 1].end : 0\n   507→    },\n   508→    layers: captions.map((caption, index) => ({\n   509→      id: `text_layer_${index}`,\n   510→      type: 'text',\n   511→      name: `Caption ${index + 1}`,\n   512→      inPoint: Math.round(caption.start * frameRate),\n   513→      outPoint: Math.round(caption.end * frameRate),\n   514→      inPointSeconds: caption.start,\n   515→      outPointSeconds: caption.end,\n   516→      content: {\n   517→        text: caption.text,\n   518→        words: caption.words.map(w => ({\n   519→          text: w.word,\n   520→          inFrame: Math.round(w.start * frameRate),\n   521→          outFrame: Math.round(w.end * frameRate),\n   522→          isKeyword: w.isKeyword || false,\n   523→          emoji: w.emoji || null\n   524→        }))\n   525→      },\n   526→      style: caption.style,\n   527→      animation: {\n   528→        ...caption.animation,\n   529→        keyframes: generateKeyframes(caption, frameRate)\n   530→      }\n   531→    })),\n   532→    metadata: {\n   533→      totalLayers: captions.length,\n   534→      generatedAt: new Date().toISOString(),\n   535→      generator: 'SPLICE Animated Captions'\n   536→    }\n   537→  };\n   538→\n   539→  return {\n   540→    success: true,\n   541→    format: outputFormat,\n   542→    data: mogrtData\n   543→  };\n   544→}\n   545→\n   546→/**\n   547→ * Export captions burned into video (requires FFmpeg)\n   548→ * @param {string} videoPath - Source video path\n   549→ * @param {Array} captions - Caption data\n   550→ * @param {string} outputPath - Output video path\n   551→ * @returns {Promise<Object>} Export result\n   552→ */\n   553→async function exportBurnedInCaptions(videoPath, captions, outputPath) {\n   554→  // This would require FFmpeg integration\n   555→  // For now, return ASS subtitle format that can be burned in\n   556→\n   557→  const assContent = generateASSSubtitles(captions);\n   558→\n   559→  return {\n   560→    success: true,\n   561→    format: 'ass',\n   562→    subtitleContent: assContent,\n   563→    ffmpegCommand: `ffmpeg -i \"${videoPath}\" -vf \"ass=captions.ass\" -c:a copy \"${outputPath}\"`,\n   564→    instructions: 'Save subtitleContent to captions.ass and run the ffmpegCommand'\n   565→  };\n   566→}\n   567→\n   568→/**\n   569→ * Get all available templates\n   570→ * @returns {Object} Templates with metadata\n   571→ */\n   572→function getTemplates() {\n   573→  return {\n   574→    templates: Object.values(TEMPLATES).map(t => ({\n   575→      id: t.id,\n   576→      name: t.name,\n   577→      description: t.description,\n   578→      previewStyle: {\n   579→        fontFamily: t.style.fontFamily,\n   580→        color: t.style.color,\n   581→        animation: t.animation.type\n   582→      }\n   583→    })),\n   584→    count: Object.keys(TEMPLATES).length\n   585→  };\n   586→}\n   587→\n   588→/**\n   589→ * Get template by ID\n   590→ * @param {string} templateId - Template ID\n   591→ * @returns {Object|null} Template or null\n   592→ */\n   593→function getTemplateById(templateId) {\n   594→  return TEMPLATES[templateId] || null;\n   595→}\n   596→\n   597→// ============================================================================\n   598→// HELPER FUNCTIONS\n   599→// ============================================================================\n   600→\n   601→/**\n   602→ * Extract words array from transcript\n   603→ */\n   604→function extractWords(transcript) {\n   605→  if (!transcript) return [];\n   606→\n   607→  // Direct words array\n   608→  if (Array.isArray(transcript.words)) {\n   609→    return transcript.words.map(w => ({\n   610→      word: w.word || w.text || '',\n   611→      start: w.start || 0,\n   612→      end: w.end || 0\n   613→    }));\n   614→  }\n   615→\n   616→  // Words in segments\n   617→  if (Array.isArray(transcript.segments)) {\n   618→    const words = [];\n   619→    transcript.segments.forEach(seg => {\n   620→      if (Array.isArray(seg.words)) {\n   621→        seg.words.forEach(w => {\n   622→          words.push({\n   623→            word: w.word || w.text || '',\n   624→            start: w.start || 0,\n   625→            end: w.end || 0\n   626→          });\n   627→        });\n   628→      }\n   629→    });\n   630→    return words;\n   631→  }\n   632→\n   633→  return [];\n   634→}\n   635→\n   636→/**\n   637→ * Extract plain text from transcript\n   638→ */\n   639→function extractText(transcript) {\n   640→  if (typeof transcript === 'string') return transcript;\n   641→  if (transcript.text) return transcript.text;\n   642→\n   643→  const words = extractWords(transcript);\n   644→  return words.map(w => w.word).join(' ');\n   645→}\n   646→\n   647→/**\n   648→ * Group words into caption lines\n   649→ */\n   650→function groupWordsIntoLines(words, maxWordsPerLine = 4) {\n   651→  const lines = [];\n   652→  let currentLine = { words: [], text: '', start: 0, end: 0 };\n   653→\n   654→  words.forEach((word, index) => {\n   655→    if (currentLine.words.length === 0) {\n   656→      currentLine.start = word.start;\n   657→    }\n   658→\n   659→    currentLine.words.push(word);\n   660→    currentLine.end = word.end;\n   661→\n   662→    // Check if we should start a new line\n   663→    const shouldBreak =\n   664→      currentLine.words.length >= maxWordsPerLine ||\n   665→      (word.word.endsWith('.') || word.word.endsWith('!') || word.word.endsWith('?'));\n   666→\n   667→    if (shouldBreak || index === words.length - 1) {\n   668→      currentLine.text = currentLine.words.map(w => w.word).join(' ');\n   669→      lines.push({ ...currentLine });\n   670→      currentLine = { words: [], text: '', start: 0, end: 0 };\n   671→    }\n   672→  });\n   673→\n   674→  return lines;\n   675→}\n   676→\n   677→/**\n   678→ * Get emoji for a word based on mappings\n   679→ */\n   680→function getEmojiForWord(word, frequency = 'medium') {\n   681→  const lowerWord = word.toLowerCase().replace(/[^a-z]/g, '');\n   682→\n   683→  // Direct mapping\n   684→  if (EMOJI_MAPPINGS[lowerWord]) {\n   685→    return EMOJI_MAPPINGS[lowerWord];\n   686→  }\n   687→\n   688→  // Partial matches for high frequency\n   689→  if (frequency === 'high') {\n   690→    for (const [key, emoji] of Object.entries(EMOJI_MAPPINGS)) {\n   691→      if (lowerWord.includes(key) || key.includes(lowerWord)) {\n   692→        return emoji;\n   693→      }\n   694→    }\n   695→  }\n   696→\n   697→  return null;\n   698→}\n   699→\n   700→/**\n   701→ * Add emojis to captions\n   702→ */\n   703→function addEmojisToCaption(captions, frequency) {\n   704→  const frequencyMap = { low: 3, medium: 2, high: 1 };\n   705→  const interval = frequencyMap[frequency] || 2;\n   706→\n   707→  return captions.map((caption, index) => {\n   708→    if (index % interval === 0) {\n   709→      // Try to find an emoji for any word in the caption\n   710→      for (const word of caption.words) {\n   711→        const emoji = getEmojiForWord(word.word, frequency);\n   712→        if (emoji) {\n   713→          return {\n   714→            ...caption,\n   715→            emoji,\n   716→            text: `${caption.text} ${emoji}`\n   717→          };\n   718→        }\n   719→      }\n   720→    }\n   721→    return caption;\n   722→  });\n   723→}\n   724→\n   725→/**\n   726→ * Generate animation keyframes\n   727→ */\n   728→function generateKeyframes(caption, frameRate) {\n   729→  const { animation, start, end } = caption;\n   730→  const startFrame = Math.round(start * frameRate);\n   731→  const endFrame = Math.round(end * frameRate);\n   732→  const duration = animation.duration === 'word-timed' ?\n   733→    (end - start) / caption.words.length :\n   734→    animation.duration;\n   735→  const durationFrames = Math.round(duration * frameRate);\n   736→\n   737→  const keyframes = [];\n   738→\n   739→  switch (animation.type) {\n   740→    case 'shake-scale':\n   741→      keyframes.push(\n   742→        { frame: startFrame, scale: 1, x: 0, y: 0, opacity: 0 },\n   743→        { frame: startFrame + 2, scale: animation.scale || 1.2, x: 0, y: 0, opacity: 1 },\n   744→        { frame: startFrame + 4, scale: 1, x: animation.shake || 3, y: 0, opacity: 1 },\n   745→        { frame: startFrame + 6, scale: 1, x: -(animation.shake || 3), y: 0, opacity: 1 },\n   746→        { frame: startFrame + 8, scale: 1, x: 0, y: 0, opacity: 1 },\n   747→        { frame: endFrame - 2, scale: 1, x: 0, y: 0, opacity: 1 },\n   748→        { frame: endFrame, scale: 0.8, x: 0, y: 0, opacity: 0 }\n   749→      );\n   750→      break;\n   751→\n   752→    case 'slide-in':\n   753→      const direction = animation.direction || 'left';\n   754→      const offset = direction === 'left' ? -100 : 100;\n   755→      keyframes.push(\n   756→        { frame: startFrame, x: offset, opacity: 0 },\n   757→        { frame: startFrame + durationFrames, x: 0, opacity: 1 },\n   758→        { frame: endFrame - durationFrames, x: 0, opacity: 1 },\n   759→        { frame: endFrame, x: -offset, opacity: 0 }\n   760→      );\n   761→      break;\n   762→\n   763→    case 'glitch':\n   764→      keyframes.push(\n   765→        { frame: startFrame, opacity: 0, glitch: 0 },\n   766→        { frame: startFrame + 1, opacity: 1, glitch: animation.glitchIntensity || 5 },\n   767→        { frame: startFrame + 2, opacity: 1, glitch: 0 },\n   768→        { frame: startFrame + 3, opacity: 0.5, glitch: animation.glitchIntensity || 5 },\n   769→        { frame: startFrame + 4, opacity: 1, glitch: 0 },\n   770→        { frame: endFrame, opacity: 0, glitch: 0 }\n   771→      );\n   772→      break;\n   773→\n   774→    case 'fade':\n   775→      keyframes.push(\n   776→        { frame: startFrame, opacity: 0 },\n   777→        { frame: startFrame + durationFrames, opacity: 1 },\n   778→        { frame: endFrame - durationFrames, opacity: 1 },\n   779→        { frame: endFrame, opacity: 0 }\n   780→      );\n   781→      break;\n   782→\n   783→    case 'progressive-fill':\n   784→      // For karaoke-style, generate keyframe per word\n   785→      caption.words.forEach((word, i) => {\n   786→        const wordStart = Math.round(word.start * frameRate);\n   787→        const wordEnd = Math.round(word.end * frameRate);\n   788→        keyframes.push({\n   789→          frame: wordStart,\n   790→          wordIndex: i,\n   791→          fillProgress: i / caption.words.length,\n   792→          highlighted: true\n   793→        });\n   794→      });\n   795→      break;\n   796→\n   797→    default:\n   798→      keyframes.push(\n   799→        { frame: startFrame, opacity: 1 },\n   800→        { frame: endFrame, opacity: 1 }\n   801→      );\n   802→  }\n   803→\n   804→  return keyframes;\n   805→}\n   806→\n   807→/**\n   808→ * Generate ASS subtitle format\n   809→ */\n   810→function generateASSSubtitles(captions) {\n   811→  const template = captions[0]?.style || TEMPLATES.mrbeast.style;\n   812→\n   813→  let ass = `[Script Info]\n   814→Title: SPLICE Animated Captions\n   815→ScriptType: v4.00+\n   816→WrapStyle: 0\n   817→ScaledBorderAndShadow: yes\n   818→PlayResX: 1920\n   819→PlayResY: 1080\n   820→\n   821→[V4+ Styles]\n   822→Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\n   823→Style: Default,${template.fontFamily || 'Arial'},${template.fontSize || 64},&H00FFFFFF,&H000000FF,&H00000000,&H80000000,1,0,0,0,100,100,0,0,1,3,2,2,10,10,50,1\n   824→\n   825→[Events]\n   826→Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n   827→`;\n   828→\n   829→  captions.forEach(caption => {\n   830→    const start = formatASSTime(caption.start);\n   831→    const end = formatASSTime(caption.end);\n   832→    // Escape special characters and add styling\n   833→    const text = caption.text.replace(/\\\\/g, '\\\\\\\\').replace(/\\n/g, '\\\\N');\n   834→    ass += `Dialogue: 0,${start},${end},Default,,0,0,0,,${text}\\n`;\n   835→  });\n   836→\n   837→  return ass;\n   838→}\n   839→\n   840→/**\n   841→ * Format time for ASS subtitles\n   842→ */\n   843→function formatASSTime(seconds) {\n   844→  const h = Math.floor(seconds / 3600);\n   845→  const m = Math.floor((seconds % 3600) / 60);\n   846→  const s = Math.floor(seconds % 60);\n   847→  const cs = Math.floor((seconds % 1) * 100);\n   848→  return `${h}:${m.toString().padStart(2, '0')}:${s.toString().padStart(2, '0')}.${cs.toString().padStart(2, '0')}`;\n   849→}\n   850→\n   851→// ============================================================================\n   852→// EXPORTS\n   853→// ============================================================================\n   854→\n   855→module.exports = {\n   856→  generateAnimatedCaptions,\n   857→  detectKeywords,\n   858→  insertEmojis,\n   859→  applyTemplate,\n   860→  generateMOGRTData,\n   861→  exportBurnedInCaptions,\n   862→  getTemplates,\n   863→  getTemplateById,\n   864→  // Helpers exposed for testing\n   865→  extractWords,\n   866→  extractText,\n   867→  groupWordsIntoLines,\n   868→  getEmojiForWord,\n   869→  TEMPLATES,\n   870→  EMOJI_MAPPINGS\n   871→};\n   872→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"00b44e59-aab3-4509-90a3-41ec2933cee0","timestamp":"2025-12-26T07:40:23.410Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/animatedCaptions.js","content":"/**\n * SPLICE Animated Captions Service\n *\n * Generates word-by-word animated captions with popular styles.\n * Supports keyword highlighting, emoji insertion, and multiple export formats.\n */\n\nconst OpenAI = require('openai');\n\n// Lazy-load OpenAI client\nlet openai = null;\nfunction getOpenAI() {\n  if (!openai) {\n    openai = new OpenAI();\n  }\n  return openai;\n}\n\n// ============================================================================\n// TEMPLATE DEFINITIONS\n// ============================================================================\n\nconst TEMPLATES = {\n  mrbeast: {\n    id: 'mrbeast',\n    name: 'MrBeast',\n    description: 'Bold, uppercase, colorful with shake animation',\n    style: {\n      fontFamily: 'Impact',\n      fontSize: 72,\n      fontWeight: 'bold',\n      textTransform: 'uppercase',\n      color: '#FFFFFF',\n      strokeColor: '#000000',\n      strokeWidth: 4,\n      shadowColor: 'rgba(0,0,0,0.8)',\n      shadowBlur: 8,\n      position: 'bottom-center',\n      padding: 20\n    },\n    animation: {\n      type: 'shake-scale',\n      duration: 0.15,\n      scale: 1.2,\n      shake: 3,\n      easing: 'ease-out'\n    },\n    highlight: {\n      enabled: true,\n      color: '#FFFF00',\n      keywords: ['amazing', 'insane', 'crazy', 'million', 'thousand', 'win', 'lose', 'money', 'challenge']\n    }\n  },\n\n  hormozi: {\n    id: 'hormozi',\n    name: 'Hormozi',\n    description: 'Clean, yellow highlight bar, professional',\n    style: {\n      fontFamily: 'Arial Black',\n      fontSize: 64,\n      fontWeight: 'bold',\n      textTransform: 'none',\n      color: '#FFFFFF',\n      strokeColor: '#000000',\n      strokeWidth: 2,\n      backgroundColor: 'rgba(0,0,0,0.7)',\n      position: 'bottom-center',\n      padding: 16\n    },\n    animation: {\n      type: 'slide-in',\n      duration: 0.2,\n      direction: 'left',\n      easing: 'ease-out'\n    },\n    highlight: {\n      enabled: true,\n      color: '#FFD700',\n      backgroundColor: '#FFD700',\n      textColor: '#000000',\n      keywords: ['money', 'business', 'million', 'profit', 'revenue', 'growth', 'scale']\n    }\n  },\n\n  gaming: {\n    id: 'gaming',\n    name: 'Gaming',\n    description: 'Neon glow, glitch effects, fast transitions',\n    style: {\n      fontFamily: 'Orbitron',\n      fontSize: 56,\n      fontWeight: 'bold',\n      textTransform: 'uppercase',\n      color: '#00FF00',\n      strokeColor: '#000000',\n      strokeWidth: 2,\n      glowColor: '#00FF00',\n      glowBlur: 15,\n      position: 'bottom-center',\n      padding: 12\n    },\n    animation: {\n      type: 'glitch',\n      duration: 0.1,\n      glitchIntensity: 5,\n      chromatic: true,\n      easing: 'linear'\n    },\n    highlight: {\n      enabled: true,\n      color: '#FF00FF',\n      glowColor: '#FF00FF',\n      keywords: ['kill', 'win', 'gg', 'clutch', 'epic', 'legendary', 'headshot', 'victory']\n    }\n  },\n\n  corporate: {\n    id: 'corporate',\n    name: 'Corporate',\n    description: 'Subtle, professional, clean fade animations',\n    style: {\n      fontFamily: 'Helvetica Neue',\n      fontSize: 48,\n      fontWeight: '500',\n      textTransform: 'none',\n      color: '#FFFFFF',\n      strokeColor: 'transparent',\n      strokeWidth: 0,\n      backgroundColor: 'rgba(0,0,0,0.6)',\n      borderRadius: 8,\n      position: 'bottom-center',\n      padding: 20\n    },\n    animation: {\n      type: 'fade',\n      duration: 0.3,\n      easing: 'ease-in-out'\n    },\n    highlight: {\n      enabled: false,\n      color: '#3B82F6',\n      keywords: []\n    }\n  },\n\n  karaoke: {\n    id: 'karaoke',\n    name: 'Karaoke',\n    description: 'Word-by-word highlight bar, progressive fill',\n    style: {\n      fontFamily: 'Arial',\n      fontSize: 60,\n      fontWeight: 'bold',\n      textTransform: 'none',\n      color: '#FFFFFF',\n      highlightColor: '#FFD700',\n      strokeColor: '#000000',\n      strokeWidth: 3,\n      position: 'bottom-center',\n      padding: 16\n    },\n    animation: {\n      type: 'progressive-fill',\n      duration: 'word-timed',\n      fillDirection: 'left-to-right',\n      easing: 'linear'\n    },\n    highlight: {\n      enabled: true,\n      mode: 'progressive',\n      color: '#FFD700',\n      keywords: []\n    }\n  }\n};\n\n// ============================================================================\n// EMOJI MAPPINGS\n// ============================================================================\n\nconst EMOJI_MAPPINGS = {\n  // Emotions\n  happy: '😊', excited: '🎉', sad: '😢', angry: '😠', love: '❤️', laugh: '😂',\n  surprise: '😮', think: '🤔', cool: '😎', fire: '🔥', hot: '🔥',\n\n  // Actions\n  win: '🏆', lose: '😞', money: '💰', work: '💼', sleep: '😴', eat: '🍽️',\n  drink: '🍺', play: '🎮', music: '🎵', video: '🎬', phone: '📱',\n\n  // Objects\n  car: '🚗', house: '🏠', computer: '💻', camera: '📷', book: '📚',\n\n  // Emphasis\n  important: '⚠️', question: '❓', check: '✅', cross: '❌', star: '⭐',\n  rocket: '🚀', target: '🎯', idea: '💡', time: '⏰', point: '👉'\n};\n\n// ============================================================================\n// MAIN FUNCTIONS\n// ============================================================================\n\n/**\n * Generate animated captions from transcript\n * @param {Object} transcript - Transcript with words array (word, start, end)\n * @param {string} templateId - Template to apply\n * @param {Object} settings - Custom settings\n * @returns {Promise<Object>} Animated caption data\n */\nasync function generateAnimatedCaptions(transcript, templateId = 'mrbeast', settings = {}) {\n  const {\n    maxWordsPerLine = 4,\n    highlightKeywords = true,\n    insertEmojis = false,\n    emojiFrequency = 'medium', // low, medium, high\n    customKeywords = [],\n    language = 'en'\n  } = settings;\n\n  // Get template\n  const template = TEMPLATES[templateId] || TEMPLATES.mrbeast;\n\n  // Extract words from transcript\n  const words = extractWords(transcript);\n\n  if (!words || words.length === 0) {\n    return {\n      success: false,\n      error: 'No words found in transcript',\n      captions: []\n    };\n  }\n\n  console.log(`[SPLICE Captions] Generating ${templateId} captions for ${words.length} words`);\n\n  // Group words into caption lines\n  const lines = groupWordsIntoLines(words, maxWordsPerLine);\n\n  // Detect keywords if enabled\n  let keywords = [];\n  if (highlightKeywords) {\n    keywords = await detectKeywords(transcript, {\n      customKeywords,\n      templateKeywords: template.highlight.keywords || []\n    });\n  }\n\n  // Generate caption data for each line\n  const captions = lines.map((line, index) => {\n    const lineWords = line.words.map(word => {\n      const isKeyword = keywords.some(kw =>\n        word.word.toLowerCase().includes(kw.toLowerCase())\n      );\n\n      return {\n        word: word.word,\n        start: word.start,\n        end: word.end,\n        isKeyword,\n        emoji: insertEmojis ? getEmojiForWord(word.word, emojiFrequency) : null\n      };\n    });\n\n    return {\n      id: `caption_${index}`,\n      lineNumber: index + 1,\n      text: line.text,\n      words: lineWords,\n      start: line.start,\n      end: line.end,\n      duration: line.end - line.start,\n      template: templateId,\n      style: { ...template.style },\n      animation: { ...template.animation }\n    };\n  });\n\n  // Insert emojis if enabled\n  let finalCaptions = captions;\n  if (insertEmojis) {\n    finalCaptions = addEmojisToCaption(captions, emojiFrequency);\n  }\n\n  return {\n    success: true,\n    template: templateId,\n    templateInfo: {\n      name: template.name,\n      description: template.description\n    },\n    totalCaptions: finalCaptions.length,\n    totalWords: words.length,\n    duration: words[words.length - 1]?.end || 0,\n    captions: finalCaptions,\n    keywords: keywords,\n    metadata: {\n      maxWordsPerLine,\n      highlightKeywords,\n      insertEmojis,\n      emojiFrequency\n    }\n  };\n}\n\n/**\n * Detect keywords using AI\n * @param {Object} transcript - Transcript data\n * @param {Object} options - Detection options\n * @returns {Promise<string[]>} Array of keywords\n */\nasync function detectKeywords(transcript, options = {}) {\n  const {\n    customKeywords = [],\n    templateKeywords = [],\n    maxKeywords = 20,\n    useAI = true\n  } = options;\n\n  // Start with template and custom keywords\n  let keywords = [...new Set([...templateKeywords, ...customKeywords])];\n\n  if (!useAI) {\n    return keywords.slice(0, maxKeywords);\n  }\n\n  // Extract text\n  const text = extractText(transcript);\n  if (!text || text.length < 50) {\n    return keywords.slice(0, maxKeywords);\n  }\n\n  try {\n    console.log('[SPLICE Captions] Detecting keywords with AI...');\n\n    const response = await getOpenAI().chat.completions.create({\n      model: 'gpt-4o-mini',\n      messages: [\n        {\n          role: 'system',\n          content: 'You are a keyword extraction expert. Extract impactful words that should be highlighted in video captions for emphasis and engagement.'\n        },\n        {\n          role: 'user',\n          content: `Extract ${maxKeywords} keywords from this transcript that would be impactful to highlight in animated captions. Focus on: numbers, emotional words, action words, important nouns, and phrases that viewers would want emphasized.\n\nTRANSCRIPT:\n${text.substring(0, 3000)}\n\nReturn JSON: { \"keywords\": [\"word1\", \"word2\", ...] }`\n        }\n      ],\n      temperature: 0.5,\n      max_tokens: 200,\n      response_format: { type: 'json_object' }\n    });\n\n    const result = JSON.parse(response.choices[0].message.content);\n    const aiKeywords = result.keywords || [];\n\n    // Merge with existing keywords\n    keywords = [...new Set([...keywords, ...aiKeywords])];\n\n    console.log(`[SPLICE Captions] Detected ${aiKeywords.length} AI keywords`);\n\n  } catch (err) {\n    console.error('[SPLICE Captions] Keyword detection error:', err.message);\n    // Continue with template/custom keywords only\n  }\n\n  return keywords.slice(0, maxKeywords);\n}\n\n/**\n * Insert emojis into transcript based on context\n * @param {Object} transcript - Transcript data\n * @param {Object} options - Emoji options\n * @returns {Promise<Object>} Transcript with emoji suggestions\n */\nasync function insertEmojis(transcript, options = {}) {\n  const {\n    frequency = 'medium', // low (~1 per 30 words), medium (~1 per 15 words), high (~1 per 8 words)\n    position = 'after', // before, after, replace\n    useAI = true\n  } = options;\n\n  const frequencyMap = { low: 30, medium: 15, high: 8 };\n  const interval = frequencyMap[frequency] || 15;\n\n  const words = extractWords(transcript);\n  if (!words || words.length === 0) {\n    return { success: false, error: 'No words found', emojis: [] };\n  }\n\n  const emojis = [];\n\n  if (useAI) {\n    // Use AI to suggest contextual emojis\n    try {\n      const text = extractText(transcript);\n      const response = await getOpenAI().chat.completions.create({\n        model: 'gpt-4o-mini',\n        messages: [\n          {\n            role: 'system',\n            content: 'You suggest contextually appropriate emojis for video captions. Return relevant emojis that match the tone and content.'\n          },\n          {\n            role: 'user',\n            content: `Suggest emojis to insert into this transcript at approximately every ${interval} words. Return the word index and emoji.\n\nTRANSCRIPT (${words.length} words):\n${text.substring(0, 2000)}\n\nReturn JSON: { \"emojis\": [{ \"afterWordIndex\": 5, \"emoji\": \"🔥\" }, ...] }`\n          }\n        ],\n        temperature: 0.7,\n        max_tokens: 300,\n        response_format: { type: 'json_object' }\n      });\n\n      const result = JSON.parse(response.choices[0].message.content);\n      emojis.push(...(result.emojis || []));\n\n    } catch (err) {\n      console.error('[SPLICE Captions] Emoji insertion error:', err.message);\n      // Fall back to rule-based\n    }\n  }\n\n  // Rule-based fallback/supplement\n  if (emojis.length === 0) {\n    words.forEach((word, index) => {\n      if (index % interval === 0 && index > 0) {\n        const emoji = getEmojiForWord(word.word, frequency);\n        if (emoji) {\n          emojis.push({ afterWordIndex: index, emoji, word: word.word });\n        }\n      }\n    });\n  }\n\n  return {\n    success: true,\n    totalEmojis: emojis.length,\n    frequency,\n    emojis\n  };\n}\n\n/**\n * Apply template to caption data\n * @param {Object} captionData - Existing caption data\n * @param {string} templateId - Template to apply\n * @returns {Object} Caption data with new template applied\n */\nfunction applyTemplate(captionData, templateId) {\n  const template = TEMPLATES[templateId];\n  if (!template) {\n    return {\n      success: false,\n      error: `Template '${templateId}' not found`,\n      availableTemplates: Object.keys(TEMPLATES)\n    };\n  }\n\n  const updatedCaptions = captionData.captions.map(caption => ({\n    ...caption,\n    template: templateId,\n    style: { ...template.style },\n    animation: { ...template.animation }\n  }));\n\n  return {\n    success: true,\n    template: templateId,\n    templateInfo: {\n      name: template.name,\n      description: template.description\n    },\n    captions: updatedCaptions\n  };\n}\n\n/**\n * Generate MOGRT-compatible data for After Effects/Premiere\n * @param {Array} captions - Caption data array\n * @param {Object} settings - MOGRT settings\n * @returns {Object} MOGRT-compatible JSON\n */\nfunction generateMOGRTData(captions, settings = {}) {\n  const {\n    frameRate = 30,\n    width = 1920,\n    height = 1080,\n    outputFormat = 'json'\n  } = settings;\n\n  const mogrtData = {\n    version: '1.0',\n    type: 'animated_captions',\n    composition: {\n      width,\n      height,\n      frameRate,\n      duration: captions.length > 0 ? captions[captions.length - 1].end : 0\n    },\n    layers: captions.map((caption, index) => ({\n      id: `text_layer_${index}`,\n      type: 'text',\n      name: `Caption ${index + 1}`,\n      inPoint: Math.round(caption.start * frameRate),\n      outPoint: Math.round(caption.end * frameRate),\n      inPointSeconds: caption.start,\n      outPointSeconds: caption.end,\n      content: {\n        text: caption.text,\n        words: caption.words.map(w => ({\n          text: w.word,\n          inFrame: Math.round(w.start * frameRate),\n          outFrame: Math.round(w.end * frameRate),\n          isKeyword: w.isKeyword || false,\n          emoji: w.emoji || null\n        }))\n      },\n      style: caption.style,\n      animation: {\n        ...caption.animation,\n        keyframes: generateKeyframes(caption, frameRate)\n      }\n    })),\n    metadata: {\n      totalLayers: captions.length,\n      generatedAt: new Date().toISOString(),\n      generator: 'SPLICE Animated Captions'\n    }\n  };\n\n  return {\n    success: true,\n    format: outputFormat,\n    data: mogrtData\n  };\n}\n\n/**\n * Export captions burned into video (requires FFmpeg)\n * @param {string} videoPath - Source video path\n * @param {Array} captions - Caption data\n * @param {string} outputPath - Output video path\n * @returns {Promise<Object>} Export result\n */\nasync function exportBurnedInCaptions(videoPath, captions, outputPath) {\n  // This would require FFmpeg integration\n  // For now, return ASS subtitle format that can be burned in\n\n  const assContent = generateASSSubtitles(captions);\n\n  return {\n    success: true,\n    format: 'ass',\n    subtitleContent: assContent,\n    ffmpegCommand: `ffmpeg -i \"${videoPath}\" -vf \"ass=captions.ass\" -c:a copy \"${outputPath}\"`,\n    instructions: 'Save subtitleContent to captions.ass and run the ffmpegCommand'\n  };\n}\n\n/**\n * Get all available templates\n * @returns {Object} Templates with metadata\n */\nfunction getTemplates() {\n  return {\n    templates: Object.values(TEMPLATES).map(t => ({\n      id: t.id,\n      name: t.name,\n      description: t.description,\n      previewStyle: {\n        fontFamily: t.style.fontFamily,\n        color: t.style.color,\n        animation: t.animation.type\n      }\n    })),\n    count: Object.keys(TEMPLATES).length\n  };\n}\n\n/**\n * Get template by ID\n * @param {string} templateId - Template ID\n * @returns {Object|null} Template or null\n */\nfunction getTemplateById(templateId) {\n  return TEMPLATES[templateId] || null;\n}\n\n// ============================================================================\n// HELPER FUNCTIONS\n// ============================================================================\n\n/**\n * Extract words array from transcript\n */\nfunction extractWords(transcript) {\n  if (!transcript) return [];\n\n  // Direct words array\n  if (Array.isArray(transcript.words)) {\n    return transcript.words.map(w => ({\n      word: w.word || w.text || '',\n      start: w.start || 0,\n      end: w.end || 0\n    }));\n  }\n\n  // Words in segments\n  if (Array.isArray(transcript.segments)) {\n    const words = [];\n    transcript.segments.forEach(seg => {\n      if (Array.isArray(seg.words)) {\n        seg.words.forEach(w => {\n          words.push({\n            word: w.word || w.text || '',\n            start: w.start || 0,\n            end: w.end || 0\n          });\n        });\n      }\n    });\n    return words;\n  }\n\n  return [];\n}\n\n/**\n * Extract plain text from transcript\n */\nfunction extractText(transcript) {\n  if (typeof transcript === 'string') return transcript;\n  if (transcript.text) return transcript.text;\n\n  const words = extractWords(transcript);\n  return words.map(w => w.word).join(' ');\n}\n\n/**\n * Group words into caption lines\n */\nfunction groupWordsIntoLines(words, maxWordsPerLine = 4) {\n  const lines = [];\n  let currentLine = { words: [], text: '', start: 0, end: 0 };\n\n  words.forEach((word, index) => {\n    if (currentLine.words.length === 0) {\n      currentLine.start = word.start;\n    }\n\n    currentLine.words.push(word);\n    currentLine.end = word.end;\n\n    // Check if we should start a new line\n    const shouldBreak =\n      currentLine.words.length >= maxWordsPerLine ||\n      (word.word.endsWith('.') || word.word.endsWith('!') || word.word.endsWith('?'));\n\n    if (shouldBreak || index === words.length - 1) {\n      currentLine.text = currentLine.words.map(w => w.word).join(' ');\n      lines.push({ ...currentLine });\n      currentLine = { words: [], text: '', start: 0, end: 0 };\n    }\n  });\n\n  return lines;\n}\n\n/**\n * Get emoji for a word based on mappings\n */\nfunction getEmojiForWord(word, frequency = 'medium') {\n  const lowerWord = word.toLowerCase().replace(/[^a-z]/g, '');\n\n  // Direct mapping\n  if (EMOJI_MAPPINGS[lowerWord]) {\n    return EMOJI_MAPPINGS[lowerWord];\n  }\n\n  // Partial matches for high frequency\n  if (frequency === 'high') {\n    for (const [key, emoji] of Object.entries(EMOJI_MAPPINGS)) {\n      if (lowerWord.includes(key) || key.includes(lowerWord)) {\n        return emoji;\n      }\n    }\n  }\n\n  return null;\n}\n\n/**\n * Add emojis to captions\n */\nfunction addEmojisToCaption(captions, frequency) {\n  const frequencyMap = { low: 3, medium: 2, high: 1 };\n  const interval = frequencyMap[frequency] || 2;\n\n  return captions.map((caption, index) => {\n    if (index % interval === 0) {\n      // Try to find an emoji for any word in the caption\n      for (const word of caption.words) {\n        const emoji = getEmojiForWord(word.word, frequency);\n        if (emoji) {\n          return {\n            ...caption,\n            emoji,\n            text: `${caption.text} ${emoji}`\n          };\n        }\n      }\n    }\n    return caption;\n  });\n}\n\n/**\n * Generate animation keyframes\n */\nfunction generateKeyframes(caption, frameRate) {\n  const { animation, start, end } = caption;\n  const startFrame = Math.round(start * frameRate);\n  const endFrame = Math.round(end * frameRate);\n  const duration = animation.duration === 'word-timed' ?\n    (end - start) / caption.words.length :\n    animation.duration;\n  const durationFrames = Math.round(duration * frameRate);\n\n  const keyframes = [];\n\n  switch (animation.type) {\n    case 'shake-scale':\n      keyframes.push(\n        { frame: startFrame, scale: 1, x: 0, y: 0, opacity: 0 },\n        { frame: startFrame + 2, scale: animation.scale || 1.2, x: 0, y: 0, opacity: 1 },\n        { frame: startFrame + 4, scale: 1, x: animation.shake || 3, y: 0, opacity: 1 },\n        { frame: startFrame + 6, scale: 1, x: -(animation.shake || 3), y: 0, opacity: 1 },\n        { frame: startFrame + 8, scale: 1, x: 0, y: 0, opacity: 1 },\n        { frame: endFrame - 2, scale: 1, x: 0, y: 0, opacity: 1 },\n        { frame: endFrame, scale: 0.8, x: 0, y: 0, opacity: 0 }\n      );\n      break;\n\n    case 'slide-in':\n      const direction = animation.direction || 'left';\n      const offset = direction === 'left' ? -100 : 100;\n      keyframes.push(\n        { frame: startFrame, x: offset, opacity: 0 },\n        { frame: startFrame + durationFrames, x: 0, opacity: 1 },\n        { frame: endFrame - durationFrames, x: 0, opacity: 1 },\n        { frame: endFrame, x: -offset, opacity: 0 }\n      );\n      break;\n\n    case 'glitch':\n      keyframes.push(\n        { frame: startFrame, opacity: 0, glitch: 0 },\n        { frame: startFrame + 1, opacity: 1, glitch: animation.glitchIntensity || 5 },\n        { frame: startFrame + 2, opacity: 1, glitch: 0 },\n        { frame: startFrame + 3, opacity: 0.5, glitch: animation.glitchIntensity || 5 },\n        { frame: startFrame + 4, opacity: 1, glitch: 0 },\n        { frame: endFrame, opacity: 0, glitch: 0 }\n      );\n      break;\n\n    case 'fade':\n      keyframes.push(\n        { frame: startFrame, opacity: 0 },\n        { frame: startFrame + durationFrames, opacity: 1 },\n        { frame: endFrame - durationFrames, opacity: 1 },\n        { frame: endFrame, opacity: 0 }\n      );\n      break;\n\n    case 'progressive-fill':\n      // For karaoke-style, generate keyframe per word\n      caption.words.forEach((word, i) => {\n        const wordStart = Math.round(word.start * frameRate);\n        const wordEnd = Math.round(word.end * frameRate);\n        keyframes.push({\n          frame: wordStart,\n          wordIndex: i,\n          fillProgress: i / caption.words.length,\n          highlighted: true\n        });\n      });\n      break;\n\n    default:\n      keyframes.push(\n        { frame: startFrame, opacity: 1 },\n        { frame: endFrame, opacity: 1 }\n      );\n  }\n\n  return keyframes;\n}\n\n/**\n * Generate ASS subtitle format\n */\nfunction generateASSSubtitles(captions) {\n  const template = captions[0]?.style || TEMPLATES.mrbeast.style;\n\n  let ass = `[Script Info]\nTitle: SPLICE Animated Captions\nScriptType: v4.00+\nWrapStyle: 0\nScaledBorderAndShadow: yes\nPlayResX: 1920\nPlayResY: 1080\n\n[V4+ Styles]\nFormat: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding\nStyle: Default,${template.fontFamily || 'Arial'},${template.fontSize || 64},&H00FFFFFF,&H000000FF,&H00000000,&H80000000,1,0,0,0,100,100,0,0,1,3,2,2,10,10,50,1\n\n[Events]\nFormat: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text\n`;\n\n  captions.forEach(caption => {\n    const start = formatASSTime(caption.start);\n    const end = formatASSTime(caption.end);\n    // Escape special characters and add styling\n    const text = caption.text.replace(/\\\\/g, '\\\\\\\\').replace(/\\n/g, '\\\\N');\n    ass += `Dialogue: 0,${start},${end},Default,,0,0,0,,${text}\\n`;\n  });\n\n  return ass;\n}\n\n/**\n * Format time for ASS subtitles\n */\nfunction formatASSTime(seconds) {\n  const h = Math.floor(seconds / 3600);\n  const m = Math.floor((seconds % 3600) / 60);\n  const s = Math.floor(seconds % 60);\n  const cs = Math.floor((seconds % 1) * 100);\n  return `${h}:${m.toString().padStart(2, '0')}:${s.toString().padStart(2, '0')}.${cs.toString().padStart(2, '0')}`;\n}\n\n// ============================================================================\n// EXPORTS\n// ============================================================================\n\nmodule.exports = {\n  generateAnimatedCaptions,\n  detectKeywords,\n  insertEmojis,\n  applyTemplate,\n  generateMOGRTData,\n  exportBurnedInCaptions,\n  getTemplates,\n  getTemplateById,\n  // Helpers exposed for testing\n  extractWords,\n  extractText,\n  groupWordsIntoLines,\n  getEmojiForWord,\n  TEMPLATES,\n  EMOJI_MAPPINGS\n};\n","numLines":872,"startLine":1,"totalLines":872}}}
{"parentUuid":"00b44e59-aab3-4509-90a3-41ec2933cee0","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_019Sr2SxVZT9Zh37qQ2p9raj","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Text-Based Editing Service\n     3→ *\n     4→ * Enables editing video by editing transcript text.\n     5→ * Each word has unique ID linked to video timecode.\n     6→ * Diff algorithm detects deletions, insertions, and reorderings.\n     7→ * Changes convert to cut list format for timeline application.\n     8→ */\n     9→\n    10→// ============================================================================\n    11→// EDITABLE TRANSCRIPT GENERATION\n    12→// ============================================================================\n    13→\n    14→/**\n    15→ * Generate an editable transcript structure with unique word IDs\n    16→ * @param {Object} transcript - Transcript with words array\n    17→ * @returns {Object} Editable transcript with word IDs and metadata\n    18→ */\n    19→function generateEditableTranscript(transcript) {\n    20→  const words = extractWords(transcript);\n    21→\n    22→  if (!words || words.length === 0) {\n    23→    return {\n    24→      success: false,\n    25→      error: 'No words found in transcript',\n    26→      editableTranscript: null\n    27→    };\n    28→  }\n    29→\n    30→  // Generate unique ID for each word\n    31→  const editableWords = words.map((word, index) => ({\n    32→    id: `word_${index}_${Math.random().toString(36).substr(2, 6)}`,\n    33→    originalIndex: index,\n    34→    word: word.word,\n    35→    start: word.start,\n    36→    end: word.end,\n    37→    confidence: word.confidence || 1.0,\n    38→    speaker: word.speaker || null,\n    39→    // For paragraph grouping\n    40→    isStartOfSentence: index === 0 || isPunctuation(words[index - 1]?.word),\n    41→    paragraphIndex: null // Will be set when grouping\n    42→  }));\n    43→\n    44→  // Group into paragraphs (sentences or time-based groups)\n    45→  const paragraphs = groupIntoParagraphs(editableWords);\n    46→\n    47→  // Generate plain text with word IDs embedded (for tracking)\n    48→  const plainText = editableWords.map(w => w.word).join(' ');\n    49→\n    50→  return {\n    51→    success: true,\n    52→    editableTranscript: {\n    53→      version: '1.0',\n    54→      words: editableWords,\n    55→      paragraphs,\n    56→      plainText,\n    57→      totalWords: editableWords.length,\n    58→      duration: editableWords.length > 0 ? editableWords[editableWords.length - 1].end : 0,\n    59→      metadata: {\n    60→        generatedAt: new Date().toISOString(),\n    61→        sourceWordCount: words.length\n    62→      }\n    63→    }\n    64→  };\n    65→}\n    66→\n    67→/**\n    68→ * Group words into paragraphs\n    69→ */\n    70→function groupIntoParagraphs(words) {\n    71→  const paragraphs = [];\n    72→  let currentParagraph = { words: [], start: 0, end: 0, text: '' };\n    73→\n    74→  words.forEach((word, index) => {\n    75→    if (currentParagraph.words.length === 0) {\n    76→      currentParagraph.start = word.start;\n    77→    }\n    78→\n    79→    word.paragraphIndex = paragraphs.length;\n    80→    currentParagraph.words.push(word);\n    81→    currentParagraph.end = word.end;\n    82→\n    83→    // End paragraph on sentence-ending punctuation or after ~20 words\n    84→    const endsSentence = /[.!?]$/.test(word.word);\n    85→    const tooLong = currentParagraph.words.length >= 20;\n    86→\n    87→    if (endsSentence || tooLong || index === words.length - 1) {\n    88→      currentParagraph.text = currentParagraph.words.map(w => w.word).join(' ');\n    89→      currentParagraph.id = `para_${paragraphs.length}`;\n    90→      paragraphs.push({ ...currentParagraph });\n    91→      currentParagraph = { words: [], start: 0, end: 0, text: '' };\n    92→    }\n    93→  });\n    94→\n    95→  return paragraphs;\n    96→}\n    97→\n    98→// ============================================================================\n    99→// TEXT EDIT APPLICATION\n   100→// ============================================================================\n   101→\n   102→/**\n   103→ * Apply text edits and generate edit operations\n   104→ * @param {Object} originalTranscript - Original editable transcript\n   105→ * @param {string} editedText - New edited text\n   106→ * @returns {Object} Edit operations and resulting transcript\n   107→ */\n   108→function applyTextEdits(originalTranscript, editedText) {\n   109→  if (!originalTranscript || !originalTranscript.words) {\n   110→    return {\n   111→      success: false,\n   112→      error: 'Invalid original transcript'\n   113→    };\n   114→  }\n   115→\n   116→  if (typeof editedText !== 'string') {\n   117→    return {\n   118→      success: false,\n   119→      error: 'editedText must be a string'\n   120→    };\n   121→  }\n   122→\n   123→  // Tokenize edited text\n   124→  const editedWords = tokenizeText(editedText);\n   125→  const originalWords = originalTranscript.words;\n   126→\n   127→  // Run diff algorithm\n   128→  const diff = computeDiff(originalWords, editedWords);\n   129→\n   130→  // Detect specific edit types\n   131→  const deletions = detectDeletions(diff);\n   132→  const insertions = detectInsertions(diff);\n   133→  const reorderings = detectReorderings(originalWords, editedWords);\n   134→\n   135→  // Generate edit operations\n   136→  const operations = generateOperations(deletions, insertions, reorderings);\n   137→\n   138→  // Create updated transcript\n   139→  const updatedWords = applyOperationsToWords(originalWords, operations);\n   140→\n   141→  return {\n   142→    success: true,\n   143→    operations,\n   144→    stats: {\n   145→      deletions: deletions.length,\n   146→      insertions: insertions.length,\n   147→      reorderings: reorderings.length,\n   148→      originalWordCount: originalWords.length,\n   149→      newWordCount: updatedWords.length\n   150→    },\n   151→    updatedTranscript: {\n   152→      words: updatedWords,\n   153→      plainText: updatedWords.map(w => w.word).join(' ')\n   154→    }\n   155→  };\n   156→}\n   157→\n   158→/**\n   159→ * Tokenize text into words\n   160→ */\n   161→function tokenizeText(text) {\n   162→  return text\n   163→    .split(/\\s+/)\n   164→    .filter(w => w.length > 0)\n   165→    .map(word => word.trim());\n   166→}\n   167→\n   168→/**\n   169→ * Compute diff between original and edited words\n   170→ * Uses Longest Common Subsequence (LCS) based approach\n   171→ */\n   172→function computeDiff(originalWords, editedWords) {\n   173→  const orig = originalWords.map(w => w.word.toLowerCase());\n   174→  const edit = editedWords.map(w => w.toLowerCase());\n   175→\n   176→  // Build LCS matrix\n   177→  const m = orig.length;\n   178→  const n = edit.length;\n   179→  const dp = Array(m + 1).fill(null).map(() => Array(n + 1).fill(0));\n   180→\n   181→  for (let i = 1; i <= m; i++) {\n   182→    for (let j = 1; j <= n; j++) {\n   183→      if (orig[i - 1] === edit[j - 1]) {\n   184→        dp[i][j] = dp[i - 1][j - 1] + 1;\n   185→      } else {\n   186→        dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]);\n   187→      }\n   188→    }\n   189→  }\n   190→\n   191→  // Backtrack to find diff\n   192→  const diff = [];\n   193→  let i = m, j = n;\n   194→\n   195→  while (i > 0 || j > 0) {\n   196→    if (i > 0 && j > 0 && orig[i - 1] === edit[j - 1]) {\n   197→      diff.unshift({ type: 'keep', originalIndex: i - 1, editedIndex: j - 1, word: originalWords[i - 1] });\n   198→      i--;\n   199→      j--;\n   200→    } else if (j > 0 && (i === 0 || dp[i][j - 1] >= dp[i - 1][j])) {\n   201→      diff.unshift({ type: 'insert', editedIndex: j - 1, word: editedWords[j - 1] });\n   202→      j--;\n   203→    } else {\n   204→      diff.unshift({ type: 'delete', originalIndex: i - 1, word: originalWords[i - 1] });\n   205→      i--;\n   206→    }\n   207→  }\n   208→\n   209→  return diff;\n   210→}\n   211→\n   212→// ============================================================================\n   213→// EDIT DETECTION\n   214→// ============================================================================\n   215→\n   216→/**\n   217→ * Detect deletions from diff\n   218→ */\n   219→function detectDeletions(diff) {\n   220→  return diff\n   221→    .filter(d => d.type === 'delete')\n   222→    .map(d => ({\n   223→      type: 'deletion',\n   224→      wordId: d.word.id,\n   225→      word: d.word.word,\n   226→      originalIndex: d.originalIndex,\n   227→      start: d.word.start,\n   228→      end: d.word.end\n   229→    }));\n   230→}\n   231→\n   232→/**\n   233→ * Detect insertions from diff\n   234→ * Note: Insertions in text-based editing are typically not supported\n   235→ * as they don't have corresponding video content\n   236→ */\n   237→function detectInsertions(diff) {\n   238→  return diff\n   239→    .filter(d => d.type === 'insert')\n   240→    .map((d, i) => ({\n   241→      type: 'insertion',\n   242→      word: d.word,\n   243→      editedIndex: d.editedIndex,\n   244→      note: 'Text insertion - no video content available'\n   245→    }));\n   246→}\n   247→\n   248→/**\n   249→ * Detect reorderings (paragraph/sentence moves)\n   250→ */\n   251→function detectReorderings(originalWords, editedWords) {\n   252→  const reorderings = [];\n   253→  const editLower = editedWords.map(w => w.toLowerCase());\n   254→\n   255→  // Find words that exist in both but in different positions\n   256→  const originalPositions = new Map();\n   257→  originalWords.forEach((w, i) => {\n   258→    const key = w.word.toLowerCase();\n   259→    if (!originalPositions.has(key)) {\n   260→      originalPositions.set(key, []);\n   261→    }\n   262→    originalPositions.get(key).push(i);\n   263→  });\n   264→\n   265→  // Detect significant position changes\n   266→  editLower.forEach((word, newPos) => {\n   267→    const origPositions = originalPositions.get(word);\n   268→    if (origPositions && origPositions.length > 0) {\n   269→      const origPos = origPositions[0];\n   270→      const positionDiff = Math.abs(newPos - origPos);\n   271→\n   272→      // If position changed by more than 5 words, consider it a reordering\n   273→      if (positionDiff > 5) {\n   274→        const origWord = originalWords[origPos];\n   275→        reorderings.push({\n   276→          type: 'reorder',\n   277→          wordId: origWord.id,\n   278→          word: origWord.word,\n   279→          originalPosition: origPos,\n   280→          newPosition: newPos,\n   281→          start: origWord.start,\n   282→          end: origWord.end\n   283→        });\n   284→      }\n   285→      origPositions.shift(); // Remove used position\n   286→    }\n   287→  });\n   288→\n   289→  return reorderings;\n   290→}\n   291→\n   292→/**\n   293→ * Generate edit operations from detected changes\n   294→ */\n   295→function generateOperations(deletions, insertions, reorderings) {\n   296→  const operations = [];\n   297→\n   298→  // Sort deletions by time (ascending) to create cut regions\n   299→  const sortedDeletions = [...deletions].sort((a, b) => a.start - b.start);\n   300→\n   301→  // Merge consecutive deletions into cut regions\n   302→  let currentCut = null;\n   303→  sortedDeletions.forEach(del => {\n   304→    if (!currentCut) {\n   305→      currentCut = {\n   306→        type: 'cut',\n   307→        start: del.start,\n   308→        end: del.end,\n   309→        words: [del.word],\n   310→        wordIds: [del.wordId]\n   311→      };\n   312→    } else if (del.start - currentCut.end < 0.5) {\n   313→      // Extend current cut region (within 0.5s gap)\n   314→      currentCut.end = del.end;\n   315→      currentCut.words.push(del.word);\n   316→      currentCut.wordIds.push(del.wordId);\n   317→    } else {\n   318→      // Start new cut region\n   319→      operations.push(currentCut);\n   320→      currentCut = {\n   321→        type: 'cut',\n   322→        start: del.start,\n   323→        end: del.end,\n   324→        words: [del.word],\n   325→        wordIds: [del.wordId]\n   326→      };\n   327→    }\n   328→  });\n   329→  if (currentCut) {\n   330→    operations.push(currentCut);\n   331→  }\n   332→\n   333→  // Add reorder operations (these create move operations in timeline)\n   334→  reorderings.forEach(reorder => {\n   335→    operations.push({\n   336→      type: 'reorder',\n   337→      wordId: reorder.wordId,\n   338→      word: reorder.word,\n   339→      originalPosition: reorder.originalPosition,\n   340→      newPosition: reorder.newPosition,\n   341→      start: reorder.start,\n   342→      end: reorder.end\n   343→    });\n   344→  });\n   345→\n   346→  return operations;\n   347→}\n   348→\n   349→/**\n   350→ * Apply operations to create updated word list\n   351→ */\n   352→function applyOperationsToWords(originalWords, operations) {\n   353→  // Get IDs of deleted words\n   354→  const deletedIds = new Set();\n   355→  operations\n   356→    .filter(op => op.type === 'cut')\n   357→    .forEach(op => op.wordIds.forEach(id => deletedIds.add(id)));\n   358→\n   359→  // Filter out deleted words\n   360→  const remaining = originalWords.filter(w => !deletedIds.has(w.id));\n   361→\n   362→  // Handle reorderings (simplified - just mark them)\n   363→  // Full reordering would require more complex logic\n   364→  return remaining;\n   365→}\n   366→\n   367→// ============================================================================\n   368→// CUT LIST GENERATION\n   369→// ============================================================================\n   370→\n   371→/**\n   372→ * Generate cut list from edit operations\n   373→ * @param {Array} operations - Edit operations\n   374→ * @param {Object} sourceInfo - Source video/audio info\n   375→ * @returns {Object} Cut list in v3.5 format\n   376→ */\n   377→function generateEditCutList(operations, sourceInfo = {}) {\n   378→  const {\n   379→    sourceName = 'source',\n   380→    sourcePath = '',\n   381→    duration = 0,\n   382→    frameRate = 30\n   383→  } = sourceInfo;\n   384→\n   385→  // Sort operations by time\n   386→  const cuts = operations\n   387→    .filter(op => op.type === 'cut')\n   388→    .sort((a, b) => a.start - b.start);\n   389→\n   390→  // Generate segments (keep regions between cuts)\n   391→  const segments = [];\n   392→  let currentTime = 0;\n   393→\n   394→  cuts.forEach((cut, index) => {\n   395→    // Add keep segment before this cut\n   396→    if (cut.start > currentTime) {\n   397→      segments.push({\n   398→        type: 'speech',\n   399→        sourceName,\n   400→        sourcePath,\n   401→        inPoint: currentTime,\n   402→        outPoint: cut.start,\n   403→        originalInPoint: currentTime,\n   404→        originalOutPoint: cut.start\n   405→      });\n   406→    }\n   407→    currentTime = cut.end;\n   408→  });\n   409→\n   410→  // Add final segment after last cut\n   411→  if (currentTime < duration) {\n   412→    segments.push({\n   413→      type: 'speech',\n   414→      sourceName,\n   415→      sourcePath,\n   416→      inPoint: currentTime,\n   417→      outPoint: duration,\n   418→      originalInPoint: currentTime,\n   419→      originalOutPoint: duration\n   420→    });\n   421→  }\n   422→\n   423→  // If no cuts, keep entire duration\n   424→  if (segments.length === 0 && duration > 0) {\n   425→    segments.push({\n   426→      type: 'speech',\n   427→      sourceName,\n   428→      sourcePath,\n   429→      inPoint: 0,\n   430→      outPoint: duration,\n   431→      originalInPoint: 0,\n   432→      originalOutPoint: duration\n   433→    });\n   434→  }\n   435→\n   436→  return {\n   437→    success: true,\n   438→    cutList: {\n   439→      version: '3.5',\n   440→      source: { name: sourceName, path: sourcePath, duration },\n   441→      segments,\n   442→      metadata: {\n   443→        cutsApplied: cuts.length,\n   444→        frameRate,\n   445→        generatedFrom: 'text-based-editing',\n   446→        generatedAt: new Date().toISOString()\n   447→      }\n   448→    },\n   449→    summary: {\n   450→      totalCuts: cuts.length,\n   451→      originalDuration: duration,\n   452→      newDuration: segments.reduce((sum, s) => sum + (s.outPoint - s.inPoint), 0),\n   453→      segmentCount: segments.length\n   454→    }\n   455→  };\n   456→}\n   457→\n   458→// ============================================================================\n   459→// SEARCH AND REPLACE\n   460→// ============================================================================\n   461→\n   462→/**\n   463→ * Search for text and return matches with timestamps\n   464→ * @param {Object} transcript - Editable transcript\n   465→ * @param {string} searchText - Text to search for\n   466→ * @returns {Object} Search results with timestamps\n   467→ */\n   468→function searchTranscript(transcript, searchText) {\n   469→  if (!transcript || !transcript.words) {\n   470→    return { success: false, error: 'Invalid transcript', matches: [] };\n   471→  }\n   472→\n   473→  if (!searchText || searchText.trim().length === 0) {\n   474→    return { success: false, error: 'Search text is required', matches: [] };\n   475→  }\n   476→\n   477→  const searchLower = searchText.toLowerCase().trim();\n   478→  const searchWords = searchLower.split(/\\s+/);\n   479→  const matches = [];\n   480→\n   481→  // Search for phrase matches\n   482→  const words = transcript.words;\n   483→  for (let i = 0; i <= words.length - searchWords.length; i++) {\n   484→    let isMatch = true;\n   485→    for (let j = 0; j < searchWords.length; j++) {\n   486→      if (words[i + j].word.toLowerCase() !== searchWords[j]) {\n   487→        isMatch = false;\n   488→        break;\n   489→      }\n   490→    }\n   491→\n   492→    if (isMatch) {\n   493→      const matchWords = words.slice(i, i + searchWords.length);\n   494→      matches.push({\n   495→        index: matches.length,\n   496→        startWordIndex: i,\n   497→        endWordIndex: i + searchWords.length - 1,\n   498→        text: matchWords.map(w => w.word).join(' '),\n   499→        start: matchWords[0].start,\n   500→        end: matchWords[matchWords.length - 1].end,\n   501→        wordIds: matchWords.map(w => w.id)\n   502→      });\n   503→    }\n   504→  }\n   505→\n   506→  return {\n   507→    success: true,\n   508→    query: searchText,\n   509→    totalMatches: matches.length,\n   510→    matches\n   511→  };\n   512→}\n   513→\n   514→/**\n   515→ * Search and replace text (generates deletion operations for matches)\n   516→ * @param {Object} transcript - Editable transcript\n   517→ * @param {string} searchText - Text to search for\n   518→ * @param {string} replaceText - Replacement text (empty for delete)\n   519→ * @returns {Object} Operations to perform\n   520→ */\n   521→function searchAndReplace(transcript, searchText, replaceText = '') {\n   522→  const searchResult = searchTranscript(transcript, searchText);\n   523→\n   524→  if (!searchResult.success) {\n   525→    return searchResult;\n   526→  }\n   527→\n   528→  if (searchResult.matches.length === 0) {\n   529→    return {\n   530→      success: true,\n   531→      message: 'No matches found',\n   532→      operations: [],\n   533→      matchCount: 0\n   534→    };\n   535→  }\n   536→\n   537→  // For text-based video editing, \"replace\" with empty = delete\n   538→  // Replace with text would need AI-generated speech (out of scope)\n   539→  const isDelete = !replaceText || replaceText.trim().length === 0;\n   540→\n   541→  const operations = searchResult.matches.map(match => ({\n   542→    type: 'cut',\n   543→    reason: isDelete ? 'search_delete' : 'search_replace',\n   544→    searchText,\n   545→    replaceText: isDelete ? null : replaceText,\n   546→    start: match.start,\n   547→    end: match.end,\n   548→    wordIds: match.wordIds,\n   549→    words: match.text\n   550→  }));\n   551→\n   552→  return {\n   553→    success: true,\n   554→    searchText,\n   555→    replaceText: isDelete ? '(deleted)' : replaceText,\n   556→    matchCount: searchResult.matches.length,\n   557→    operations,\n   558→    note: isDelete\n   559→      ? 'Matched segments will be cut from video'\n   560→      : 'Replace requires text-to-speech (marking for deletion instead)'\n   561→  };\n   562→}\n   563→\n   564→// ============================================================================\n   565→// PREVIEW GENERATION\n   566→// ============================================================================\n   567→\n   568→/**\n   569→ * Generate preview of edits without applying\n   570→ * @param {Object} transcript - Original transcript\n   571→ * @param {string} editedText - Edited text\n   572→ * @returns {Object} Preview of changes\n   573→ */\n   574→function previewEdits(transcript, editedText) {\n   575→  const result = applyTextEdits(transcript, editedText);\n   576→\n   577→  if (!result.success) {\n   578→    return result;\n   579→  }\n   580→\n   581→  // Generate human-readable preview\n   582→  const preview = {\n   583→    success: true,\n   584→    changes: [],\n   585→    summary: result.stats\n   586→  };\n   587→\n   588→  result.operations.forEach(op => {\n   589→    if (op.type === 'cut') {\n   590→      preview.changes.push({\n   591→        type: 'deletion',\n   592→        description: `Delete \"${op.words.join(' ')}\"`,\n   593→        timeRange: `${formatTime(op.start)} - ${formatTime(op.end)}`,\n   594→        duration: op.end - op.start\n   595→      });\n   596→    } else if (op.type === 'reorder') {\n   597→      preview.changes.push({\n   598→        type: 'reorder',\n   599→        description: `Move \"${op.word}\" from position ${op.originalPosition} to ${op.newPosition}`,\n   600→        timeRange: `${formatTime(op.start)} - ${formatTime(op.end)}`\n   601→      });\n   602→    }\n   603→  });\n   604→\n   605→  // Calculate impact\n   606→  const totalCutDuration = result.operations\n   607→    .filter(op => op.type === 'cut')\n   608→    .reduce((sum, op) => sum + (op.end - op.start), 0);\n   609→\n   610→  preview.impact = {\n   611→    durationRemoved: totalCutDuration,\n   612→    percentageRemoved: transcript.duration > 0\n   613→      ? ((totalCutDuration / transcript.duration) * 100).toFixed(1)\n   614→      : 0\n   615→  };\n   616→\n   617→  return preview;\n   618→}\n   619→\n   620→/**\n   621→ * Format time for display\n   622→ */\n   623→function formatTime(seconds) {\n   624→  const mins = Math.floor(seconds / 60);\n   625→  const secs = Math.floor(seconds % 60);\n   626→  const ms = Math.floor((seconds % 1) * 100);\n   627→  return `${mins}:${secs.toString().padStart(2, '0')}.${ms.toString().padStart(2, '0')}`;\n   628→}\n   629→\n   630→// ============================================================================\n   631→// UNDO/REDO SUPPORT\n   632→// ============================================================================\n   633→\n   634→/**\n   635→ * Create undo history entry\n   636→ */\n   637→function createHistoryEntry(transcript, operations, description) {\n   638→  return {\n   639→    id: `hist_${Date.now()}_${Math.random().toString(36).substr(2, 6)}`,\n   640→    timestamp: new Date().toISOString(),\n   641→    description,\n   642→    transcriptSnapshot: JSON.stringify(transcript),\n   643→    operations: JSON.stringify(operations)\n   644→  };\n   645→}\n   646→\n   647→/**\n   648→ * Get undo stack from history\n   649→ */\n   650→function getUndoStack(history = []) {\n   651→  return history.map(entry => ({\n   652→    id: entry.id,\n   653→    description: entry.description,\n   654→    timestamp: entry.timestamp\n   655→  }));\n   656→}\n   657→\n   658→// ============================================================================\n   659→// HELPER FUNCTIONS\n   660→// ============================================================================\n   661→\n   662→/**\n   663→ * Extract words from transcript\n   664→ */\n   665→function extractWords(transcript) {\n   666→  if (!transcript) return [];\n   667→\n   668→  if (Array.isArray(transcript.words)) {\n   669→    return transcript.words.map(w => ({\n   670→      word: w.word || w.text || '',\n   671→      start: w.start || 0,\n   672→      end: w.end || 0,\n   673→      confidence: w.confidence,\n   674→      speaker: w.speaker\n   675→    }));\n   676→  }\n   677→\n   678→  if (Array.isArray(transcript.segments)) {\n   679→    const words = [];\n   680→    transcript.segments.forEach(seg => {\n   681→      if (Array.isArray(seg.words)) {\n   682→        seg.words.forEach(w => {\n   683→          words.push({\n   684→            word: w.word || w.text || '',\n   685→            start: w.start || 0,\n   686→            end: w.end || 0,\n   687→            confidence: w.confidence,\n   688→            speaker: w.speaker || seg.speaker\n   689→          });\n   690→        });\n   691→      }\n   692→    });\n   693→    return words;\n   694→  }\n   695→\n   696→  return [];\n   697→}\n   698→\n   699→/**\n   700→ * Check if word ends with punctuation\n   701→ */\n   702→function isPunctuation(word) {\n   703→  if (!word) return false;\n   704→  return /[.!?]$/.test(word);\n   705→}\n   706→\n   707→// ============================================================================\n   708→// EXPORTS\n   709→// ============================================================================\n   710→\n   711→module.exports = {\n   712→  generateEditableTranscript,\n   713→  applyTextEdits,\n   714→  detectDeletions,\n   715→  detectInsertions,\n   716→  detectReorderings,\n   717→  generateEditCutList,\n   718→  searchTranscript,\n   719→  searchAndReplace,\n   720→  previewEdits,\n   721→  createHistoryEntry,\n   722→  getUndoStack,\n   723→  // Helpers for testing\n   724→  extractWords,\n   725→  tokenizeText,\n   726→  computeDiff,\n   727→  groupIntoParagraphs,\n   728→  formatTime\n   729→};\n   730→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"763f708d-71ec-462b-bf13-f54d7bd04c1d","timestamp":"2025-12-26T07:40:23.410Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/textBasedEditing.js","content":"/**\n * SPLICE Text-Based Editing Service\n *\n * Enables editing video by editing transcript text.\n * Each word has unique ID linked to video timecode.\n * Diff algorithm detects deletions, insertions, and reorderings.\n * Changes convert to cut list format for timeline application.\n */\n\n// ============================================================================\n// EDITABLE TRANSCRIPT GENERATION\n// ============================================================================\n\n/**\n * Generate an editable transcript structure with unique word IDs\n * @param {Object} transcript - Transcript with words array\n * @returns {Object} Editable transcript with word IDs and metadata\n */\nfunction generateEditableTranscript(transcript) {\n  const words = extractWords(transcript);\n\n  if (!words || words.length === 0) {\n    return {\n      success: false,\n      error: 'No words found in transcript',\n      editableTranscript: null\n    };\n  }\n\n  // Generate unique ID for each word\n  const editableWords = words.map((word, index) => ({\n    id: `word_${index}_${Math.random().toString(36).substr(2, 6)}`,\n    originalIndex: index,\n    word: word.word,\n    start: word.start,\n    end: word.end,\n    confidence: word.confidence || 1.0,\n    speaker: word.speaker || null,\n    // For paragraph grouping\n    isStartOfSentence: index === 0 || isPunctuation(words[index - 1]?.word),\n    paragraphIndex: null // Will be set when grouping\n  }));\n\n  // Group into paragraphs (sentences or time-based groups)\n  const paragraphs = groupIntoParagraphs(editableWords);\n\n  // Generate plain text with word IDs embedded (for tracking)\n  const plainText = editableWords.map(w => w.word).join(' ');\n\n  return {\n    success: true,\n    editableTranscript: {\n      version: '1.0',\n      words: editableWords,\n      paragraphs,\n      plainText,\n      totalWords: editableWords.length,\n      duration: editableWords.length > 0 ? editableWords[editableWords.length - 1].end : 0,\n      metadata: {\n        generatedAt: new Date().toISOString(),\n        sourceWordCount: words.length\n      }\n    }\n  };\n}\n\n/**\n * Group words into paragraphs\n */\nfunction groupIntoParagraphs(words) {\n  const paragraphs = [];\n  let currentParagraph = { words: [], start: 0, end: 0, text: '' };\n\n  words.forEach((word, index) => {\n    if (currentParagraph.words.length === 0) {\n      currentParagraph.start = word.start;\n    }\n\n    word.paragraphIndex = paragraphs.length;\n    currentParagraph.words.push(word);\n    currentParagraph.end = word.end;\n\n    // End paragraph on sentence-ending punctuation or after ~20 words\n    const endsSentence = /[.!?]$/.test(word.word);\n    const tooLong = currentParagraph.words.length >= 20;\n\n    if (endsSentence || tooLong || index === words.length - 1) {\n      currentParagraph.text = currentParagraph.words.map(w => w.word).join(' ');\n      currentParagraph.id = `para_${paragraphs.length}`;\n      paragraphs.push({ ...currentParagraph });\n      currentParagraph = { words: [], start: 0, end: 0, text: '' };\n    }\n  });\n\n  return paragraphs;\n}\n\n// ============================================================================\n// TEXT EDIT APPLICATION\n// ============================================================================\n\n/**\n * Apply text edits and generate edit operations\n * @param {Object} originalTranscript - Original editable transcript\n * @param {string} editedText - New edited text\n * @returns {Object} Edit operations and resulting transcript\n */\nfunction applyTextEdits(originalTranscript, editedText) {\n  if (!originalTranscript || !originalTranscript.words) {\n    return {\n      success: false,\n      error: 'Invalid original transcript'\n    };\n  }\n\n  if (typeof editedText !== 'string') {\n    return {\n      success: false,\n      error: 'editedText must be a string'\n    };\n  }\n\n  // Tokenize edited text\n  const editedWords = tokenizeText(editedText);\n  const originalWords = originalTranscript.words;\n\n  // Run diff algorithm\n  const diff = computeDiff(originalWords, editedWords);\n\n  // Detect specific edit types\n  const deletions = detectDeletions(diff);\n  const insertions = detectInsertions(diff);\n  const reorderings = detectReorderings(originalWords, editedWords);\n\n  // Generate edit operations\n  const operations = generateOperations(deletions, insertions, reorderings);\n\n  // Create updated transcript\n  const updatedWords = applyOperationsToWords(originalWords, operations);\n\n  return {\n    success: true,\n    operations,\n    stats: {\n      deletions: deletions.length,\n      insertions: insertions.length,\n      reorderings: reorderings.length,\n      originalWordCount: originalWords.length,\n      newWordCount: updatedWords.length\n    },\n    updatedTranscript: {\n      words: updatedWords,\n      plainText: updatedWords.map(w => w.word).join(' ')\n    }\n  };\n}\n\n/**\n * Tokenize text into words\n */\nfunction tokenizeText(text) {\n  return text\n    .split(/\\s+/)\n    .filter(w => w.length > 0)\n    .map(word => word.trim());\n}\n\n/**\n * Compute diff between original and edited words\n * Uses Longest Common Subsequence (LCS) based approach\n */\nfunction computeDiff(originalWords, editedWords) {\n  const orig = originalWords.map(w => w.word.toLowerCase());\n  const edit = editedWords.map(w => w.toLowerCase());\n\n  // Build LCS matrix\n  const m = orig.length;\n  const n = edit.length;\n  const dp = Array(m + 1).fill(null).map(() => Array(n + 1).fill(0));\n\n  for (let i = 1; i <= m; i++) {\n    for (let j = 1; j <= n; j++) {\n      if (orig[i - 1] === edit[j - 1]) {\n        dp[i][j] = dp[i - 1][j - 1] + 1;\n      } else {\n        dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]);\n      }\n    }\n  }\n\n  // Backtrack to find diff\n  const diff = [];\n  let i = m, j = n;\n\n  while (i > 0 || j > 0) {\n    if (i > 0 && j > 0 && orig[i - 1] === edit[j - 1]) {\n      diff.unshift({ type: 'keep', originalIndex: i - 1, editedIndex: j - 1, word: originalWords[i - 1] });\n      i--;\n      j--;\n    } else if (j > 0 && (i === 0 || dp[i][j - 1] >= dp[i - 1][j])) {\n      diff.unshift({ type: 'insert', editedIndex: j - 1, word: editedWords[j - 1] });\n      j--;\n    } else {\n      diff.unshift({ type: 'delete', originalIndex: i - 1, word: originalWords[i - 1] });\n      i--;\n    }\n  }\n\n  return diff;\n}\n\n// ============================================================================\n// EDIT DETECTION\n// ============================================================================\n\n/**\n * Detect deletions from diff\n */\nfunction detectDeletions(diff) {\n  return diff\n    .filter(d => d.type === 'delete')\n    .map(d => ({\n      type: 'deletion',\n      wordId: d.word.id,\n      word: d.word.word,\n      originalIndex: d.originalIndex,\n      start: d.word.start,\n      end: d.word.end\n    }));\n}\n\n/**\n * Detect insertions from diff\n * Note: Insertions in text-based editing are typically not supported\n * as they don't have corresponding video content\n */\nfunction detectInsertions(diff) {\n  return diff\n    .filter(d => d.type === 'insert')\n    .map((d, i) => ({\n      type: 'insertion',\n      word: d.word,\n      editedIndex: d.editedIndex,\n      note: 'Text insertion - no video content available'\n    }));\n}\n\n/**\n * Detect reorderings (paragraph/sentence moves)\n */\nfunction detectReorderings(originalWords, editedWords) {\n  const reorderings = [];\n  const editLower = editedWords.map(w => w.toLowerCase());\n\n  // Find words that exist in both but in different positions\n  const originalPositions = new Map();\n  originalWords.forEach((w, i) => {\n    const key = w.word.toLowerCase();\n    if (!originalPositions.has(key)) {\n      originalPositions.set(key, []);\n    }\n    originalPositions.get(key).push(i);\n  });\n\n  // Detect significant position changes\n  editLower.forEach((word, newPos) => {\n    const origPositions = originalPositions.get(word);\n    if (origPositions && origPositions.length > 0) {\n      const origPos = origPositions[0];\n      const positionDiff = Math.abs(newPos - origPos);\n\n      // If position changed by more than 5 words, consider it a reordering\n      if (positionDiff > 5) {\n        const origWord = originalWords[origPos];\n        reorderings.push({\n          type: 'reorder',\n          wordId: origWord.id,\n          word: origWord.word,\n          originalPosition: origPos,\n          newPosition: newPos,\n          start: origWord.start,\n          end: origWord.end\n        });\n      }\n      origPositions.shift(); // Remove used position\n    }\n  });\n\n  return reorderings;\n}\n\n/**\n * Generate edit operations from detected changes\n */\nfunction generateOperations(deletions, insertions, reorderings) {\n  const operations = [];\n\n  // Sort deletions by time (ascending) to create cut regions\n  const sortedDeletions = [...deletions].sort((a, b) => a.start - b.start);\n\n  // Merge consecutive deletions into cut regions\n  let currentCut = null;\n  sortedDeletions.forEach(del => {\n    if (!currentCut) {\n      currentCut = {\n        type: 'cut',\n        start: del.start,\n        end: del.end,\n        words: [del.word],\n        wordIds: [del.wordId]\n      };\n    } else if (del.start - currentCut.end < 0.5) {\n      // Extend current cut region (within 0.5s gap)\n      currentCut.end = del.end;\n      currentCut.words.push(del.word);\n      currentCut.wordIds.push(del.wordId);\n    } else {\n      // Start new cut region\n      operations.push(currentCut);\n      currentCut = {\n        type: 'cut',\n        start: del.start,\n        end: del.end,\n        words: [del.word],\n        wordIds: [del.wordId]\n      };\n    }\n  });\n  if (currentCut) {\n    operations.push(currentCut);\n  }\n\n  // Add reorder operations (these create move operations in timeline)\n  reorderings.forEach(reorder => {\n    operations.push({\n      type: 'reorder',\n      wordId: reorder.wordId,\n      word: reorder.word,\n      originalPosition: reorder.originalPosition,\n      newPosition: reorder.newPosition,\n      start: reorder.start,\n      end: reorder.end\n    });\n  });\n\n  return operations;\n}\n\n/**\n * Apply operations to create updated word list\n */\nfunction applyOperationsToWords(originalWords, operations) {\n  // Get IDs of deleted words\n  const deletedIds = new Set();\n  operations\n    .filter(op => op.type === 'cut')\n    .forEach(op => op.wordIds.forEach(id => deletedIds.add(id)));\n\n  // Filter out deleted words\n  const remaining = originalWords.filter(w => !deletedIds.has(w.id));\n\n  // Handle reorderings (simplified - just mark them)\n  // Full reordering would require more complex logic\n  return remaining;\n}\n\n// ============================================================================\n// CUT LIST GENERATION\n// ============================================================================\n\n/**\n * Generate cut list from edit operations\n * @param {Array} operations - Edit operations\n * @param {Object} sourceInfo - Source video/audio info\n * @returns {Object} Cut list in v3.5 format\n */\nfunction generateEditCutList(operations, sourceInfo = {}) {\n  const {\n    sourceName = 'source',\n    sourcePath = '',\n    duration = 0,\n    frameRate = 30\n  } = sourceInfo;\n\n  // Sort operations by time\n  const cuts = operations\n    .filter(op => op.type === 'cut')\n    .sort((a, b) => a.start - b.start);\n\n  // Generate segments (keep regions between cuts)\n  const segments = [];\n  let currentTime = 0;\n\n  cuts.forEach((cut, index) => {\n    // Add keep segment before this cut\n    if (cut.start > currentTime) {\n      segments.push({\n        type: 'speech',\n        sourceName,\n        sourcePath,\n        inPoint: currentTime,\n        outPoint: cut.start,\n        originalInPoint: currentTime,\n        originalOutPoint: cut.start\n      });\n    }\n    currentTime = cut.end;\n  });\n\n  // Add final segment after last cut\n  if (currentTime < duration) {\n    segments.push({\n      type: 'speech',\n      sourceName,\n      sourcePath,\n      inPoint: currentTime,\n      outPoint: duration,\n      originalInPoint: currentTime,\n      originalOutPoint: duration\n    });\n  }\n\n  // If no cuts, keep entire duration\n  if (segments.length === 0 && duration > 0) {\n    segments.push({\n      type: 'speech',\n      sourceName,\n      sourcePath,\n      inPoint: 0,\n      outPoint: duration,\n      originalInPoint: 0,\n      originalOutPoint: duration\n    });\n  }\n\n  return {\n    success: true,\n    cutList: {\n      version: '3.5',\n      source: { name: sourceName, path: sourcePath, duration },\n      segments,\n      metadata: {\n        cutsApplied: cuts.length,\n        frameRate,\n        generatedFrom: 'text-based-editing',\n        generatedAt: new Date().toISOString()\n      }\n    },\n    summary: {\n      totalCuts: cuts.length,\n      originalDuration: duration,\n      newDuration: segments.reduce((sum, s) => sum + (s.outPoint - s.inPoint), 0),\n      segmentCount: segments.length\n    }\n  };\n}\n\n// ============================================================================\n// SEARCH AND REPLACE\n// ============================================================================\n\n/**\n * Search for text and return matches with timestamps\n * @param {Object} transcript - Editable transcript\n * @param {string} searchText - Text to search for\n * @returns {Object} Search results with timestamps\n */\nfunction searchTranscript(transcript, searchText) {\n  if (!transcript || !transcript.words) {\n    return { success: false, error: 'Invalid transcript', matches: [] };\n  }\n\n  if (!searchText || searchText.trim().length === 0) {\n    return { success: false, error: 'Search text is required', matches: [] };\n  }\n\n  const searchLower = searchText.toLowerCase().trim();\n  const searchWords = searchLower.split(/\\s+/);\n  const matches = [];\n\n  // Search for phrase matches\n  const words = transcript.words;\n  for (let i = 0; i <= words.length - searchWords.length; i++) {\n    let isMatch = true;\n    for (let j = 0; j < searchWords.length; j++) {\n      if (words[i + j].word.toLowerCase() !== searchWords[j]) {\n        isMatch = false;\n        break;\n      }\n    }\n\n    if (isMatch) {\n      const matchWords = words.slice(i, i + searchWords.length);\n      matches.push({\n        index: matches.length,\n        startWordIndex: i,\n        endWordIndex: i + searchWords.length - 1,\n        text: matchWords.map(w => w.word).join(' '),\n        start: matchWords[0].start,\n        end: matchWords[matchWords.length - 1].end,\n        wordIds: matchWords.map(w => w.id)\n      });\n    }\n  }\n\n  return {\n    success: true,\n    query: searchText,\n    totalMatches: matches.length,\n    matches\n  };\n}\n\n/**\n * Search and replace text (generates deletion operations for matches)\n * @param {Object} transcript - Editable transcript\n * @param {string} searchText - Text to search for\n * @param {string} replaceText - Replacement text (empty for delete)\n * @returns {Object} Operations to perform\n */\nfunction searchAndReplace(transcript, searchText, replaceText = '') {\n  const searchResult = searchTranscript(transcript, searchText);\n\n  if (!searchResult.success) {\n    return searchResult;\n  }\n\n  if (searchResult.matches.length === 0) {\n    return {\n      success: true,\n      message: 'No matches found',\n      operations: [],\n      matchCount: 0\n    };\n  }\n\n  // For text-based video editing, \"replace\" with empty = delete\n  // Replace with text would need AI-generated speech (out of scope)\n  const isDelete = !replaceText || replaceText.trim().length === 0;\n\n  const operations = searchResult.matches.map(match => ({\n    type: 'cut',\n    reason: isDelete ? 'search_delete' : 'search_replace',\n    searchText,\n    replaceText: isDelete ? null : replaceText,\n    start: match.start,\n    end: match.end,\n    wordIds: match.wordIds,\n    words: match.text\n  }));\n\n  return {\n    success: true,\n    searchText,\n    replaceText: isDelete ? '(deleted)' : replaceText,\n    matchCount: searchResult.matches.length,\n    operations,\n    note: isDelete\n      ? 'Matched segments will be cut from video'\n      : 'Replace requires text-to-speech (marking for deletion instead)'\n  };\n}\n\n// ============================================================================\n// PREVIEW GENERATION\n// ============================================================================\n\n/**\n * Generate preview of edits without applying\n * @param {Object} transcript - Original transcript\n * @param {string} editedText - Edited text\n * @returns {Object} Preview of changes\n */\nfunction previewEdits(transcript, editedText) {\n  const result = applyTextEdits(transcript, editedText);\n\n  if (!result.success) {\n    return result;\n  }\n\n  // Generate human-readable preview\n  const preview = {\n    success: true,\n    changes: [],\n    summary: result.stats\n  };\n\n  result.operations.forEach(op => {\n    if (op.type === 'cut') {\n      preview.changes.push({\n        type: 'deletion',\n        description: `Delete \"${op.words.join(' ')}\"`,\n        timeRange: `${formatTime(op.start)} - ${formatTime(op.end)}`,\n        duration: op.end - op.start\n      });\n    } else if (op.type === 'reorder') {\n      preview.changes.push({\n        type: 'reorder',\n        description: `Move \"${op.word}\" from position ${op.originalPosition} to ${op.newPosition}`,\n        timeRange: `${formatTime(op.start)} - ${formatTime(op.end)}`\n      });\n    }\n  });\n\n  // Calculate impact\n  const totalCutDuration = result.operations\n    .filter(op => op.type === 'cut')\n    .reduce((sum, op) => sum + (op.end - op.start), 0);\n\n  preview.impact = {\n    durationRemoved: totalCutDuration,\n    percentageRemoved: transcript.duration > 0\n      ? ((totalCutDuration / transcript.duration) * 100).toFixed(1)\n      : 0\n  };\n\n  return preview;\n}\n\n/**\n * Format time for display\n */\nfunction formatTime(seconds) {\n  const mins = Math.floor(seconds / 60);\n  const secs = Math.floor(seconds % 60);\n  const ms = Math.floor((seconds % 1) * 100);\n  return `${mins}:${secs.toString().padStart(2, '0')}.${ms.toString().padStart(2, '0')}`;\n}\n\n// ============================================================================\n// UNDO/REDO SUPPORT\n// ============================================================================\n\n/**\n * Create undo history entry\n */\nfunction createHistoryEntry(transcript, operations, description) {\n  return {\n    id: `hist_${Date.now()}_${Math.random().toString(36).substr(2, 6)}`,\n    timestamp: new Date().toISOString(),\n    description,\n    transcriptSnapshot: JSON.stringify(transcript),\n    operations: JSON.stringify(operations)\n  };\n}\n\n/**\n * Get undo stack from history\n */\nfunction getUndoStack(history = []) {\n  return history.map(entry => ({\n    id: entry.id,\n    description: entry.description,\n    timestamp: entry.timestamp\n  }));\n}\n\n// ============================================================================\n// HELPER FUNCTIONS\n// ============================================================================\n\n/**\n * Extract words from transcript\n */\nfunction extractWords(transcript) {\n  if (!transcript) return [];\n\n  if (Array.isArray(transcript.words)) {\n    return transcript.words.map(w => ({\n      word: w.word || w.text || '',\n      start: w.start || 0,\n      end: w.end || 0,\n      confidence: w.confidence,\n      speaker: w.speaker\n    }));\n  }\n\n  if (Array.isArray(transcript.segments)) {\n    const words = [];\n    transcript.segments.forEach(seg => {\n      if (Array.isArray(seg.words)) {\n        seg.words.forEach(w => {\n          words.push({\n            word: w.word || w.text || '',\n            start: w.start || 0,\n            end: w.end || 0,\n            confidence: w.confidence,\n            speaker: w.speaker || seg.speaker\n          });\n        });\n      }\n    });\n    return words;\n  }\n\n  return [];\n}\n\n/**\n * Check if word ends with punctuation\n */\nfunction isPunctuation(word) {\n  if (!word) return false;\n  return /[.!?]$/.test(word);\n}\n\n// ============================================================================\n// EXPORTS\n// ============================================================================\n\nmodule.exports = {\n  generateEditableTranscript,\n  applyTextEdits,\n  detectDeletions,\n  detectInsertions,\n  detectReorderings,\n  generateEditCutList,\n  searchTranscript,\n  searchAndReplace,\n  previewEdits,\n  createHistoryEntry,\n  getUndoStack,\n  // Helpers for testing\n  extractWords,\n  tokenizeText,\n  computeDiff,\n  groupIntoParagraphs,\n  formatTime\n};\n","numLines":730,"startLine":1,"totalLines":730}}}
{"parentUuid":"763f708d-71ec-462b-bf13-f54d7bd04c1d","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_018yjZGaCeX1UgVjmpgj3y73","type":"tool_result","content":"   500→  }\n   501→});\n   502→\n   503→/**\n   504→ * POST /silences - Detect silent gaps in audio\n   505→ *\n   506→ * Pipeline:\n   507→ * 1. Transcribe audio with Whisper (cached)\n   508→ * 2. Analyze gaps between segments\n   509→ * 3. Return silence regions\n   510→ */\n   511→app.post('/silences', requireCredits({ endpoint: 'silences' }), async (req, res) => {\n   512→  const { wavPath, threshold = 0.5 } = req.body;\n   513→\n   514→  if (!wavPath) {\n   515→    return res.status(400).json({ error: 'wavPath is required' });\n   516→  }\n   517→\n   518→  if (!(await fileExists(wavPath))) {\n   519→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   520→  }\n   521→\n   522→  console.log(`[SPLICE] Detecting silences: ${wavPath} (threshold: ${threshold}s)`);\n   523→\n   524→  try {\n   525→    const transcript = await transcribeAudio(wavPath);\n   526→    const silences = detectSilences(transcript.segments, threshold);\n   527→\n   528→    // Deduct usage based on audio duration\n   529→    const audioDuration = transcript.duration || 0;\n   530→    let balance = null;\n   531→    if (audioDuration > 0 && req.deductUsage) {\n   532→      balance = await req.deductUsage(audioDuration);\n   533→    }\n   534→\n   535→    res.json({\n   536→      success: true,\n   537→      wavPath,\n   538→      threshold,\n   539→      silences,\n   540→      count: silences.length,\n   541→      totalSilenceDuration: silences.reduce((sum, s) => sum + s.duration, 0).toFixed(2),\n   542→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   543→    });\n   544→  } catch (err) {\n   545→    console.error('[SPLICE] Silence detection error:', err);\n   546→    res.status(500).json({ error: err.message });\n   547→  }\n   548→});\n   549→\n   550→/**\n   551→ * POST /silences-audio - Detect silences using FFprobe audio analysis\n   552→ *\n   553→ * Uses actual audio levels (dB threshold) instead of transcript gaps.\n   554→ * More accurate for detecting silence vs background noise.\n   555→ */\n   556→app.post('/silences-audio', requireCredits({ endpoint: 'silences-audio' }), async (req, res) => {\n   557→  const {\n   558→    wavPath,\n   559→    threshold = -30,\n   560→    minDuration = 0.5,\n   561→    padding = 0.1\n   562→  } = req.body;\n   563→\n   564→  if (!wavPath) {\n   565→    return res.status(400).json({ error: 'wavPath is required' });\n   566→  }\n   567→\n   568→  if (!(await fileExists(wavPath))) {\n   569→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   570→  }\n   571→\n   572→  // Check FFprobe availability\n   573→  const ffprobeAvailable = await isFFprobeInstalled();\n   574→  if (!ffprobeAvailable) {\n   575→    return res.status(500).json({\n   576→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n   577→    });\n   578→  }\n   579→\n   580→  console.log(`[SPLICE] FFprobe silence detection: ${wavPath} (threshold: ${threshold}dB, min: ${minDuration}s)`);\n   581→\n   582→  try {\n   583→    const silences = await detectAudioSilences(wavPath, {\n   584→      threshold,\n   585→      minDuration,\n   586→      padding\n   587→    });\n   588→\n   589→    const totalDuration = silences.reduce((sum, s) => sum + s.duration, 0);\n   590→\n   591→    // Deduct usage based on audio duration\n   592→    let balance = null;\n   593→    try {\n   594→      const audioDuration = await getAudioDuration(wavPath);\n   595→      if (audioDuration > 0 && req.deductUsage) {\n   596→        balance = await req.deductUsage(audioDuration);\n   597→      }\n   598→    } catch (durErr) {\n   599→      console.warn('[SPLICE] Could not get audio duration for billing:', durErr.message);\n   600→    }\n   601→\n   602→    res.json({\n   603→      success: true,\n   604→      wavPath,\n   605→      threshold,\n   606→      minDuration,\n   607→      padding,\n   608→      silences,\n   609→      count: silences.length,\n   610→      totalSilenceDuration: totalDuration.toFixed(2),\n   611→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   612→    });\n   613→  } catch (err) {\n   614→    console.error('[SPLICE] FFprobe silence detection error:', err);\n   615→    res.status(500).json({ error: err.message });\n   616→  }\n   617→});\n   618→\n   619→/**\n   620→ * POST /silences-rms - Detect silences using RMS audio analysis\n   621→ *\n   622→ * Advanced silence detection with:\n   623→ * - RMS (Root Mean Square) audio level analysis\n   624→ * - Auto-threshold detection from audio histogram\n   625→ * - Configurable padding (before/after cuts)\n   626→ * - Sensitivity slider mapping (0-100)\n   627→ *\n   628→ * Options:\n   629→ * - threshold: dBFS threshold (-60 to -20, default: -30)\n   630→ * - minSilenceLength: Minimum silence duration in seconds (default: 0.5)\n   631→ * - seekStep: Analysis window step in seconds (default: 0.05)\n   632→ * - paddingStart: Buffer before silence in seconds (default: 0.1)\n   633→ * - paddingEnd: Buffer after silence in seconds (default: 0.05)\n   634→ * - autoThreshold: Auto-detect optimal threshold (default: false)\n   635→ * - sensitivity: UI sensitivity 0-100 (overrides other params if provided)\n   636→ */\n   637→app.post('/silences-rms', requireCredits({ endpoint: 'silences-rms' }), async (req, res) => {\n   638→  const { wavPath, sensitivity, ...manualOptions } = req.body;\n   639→\n   640→  if (!wavPath) {\n   641→    return res.status(400).json({ error: 'wavPath is required' });\n   642→  }\n   643→\n   644→  if (!(await fileExists(wavPath))) {\n   645→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   646→  }\n   647→\n   648→  // Validate file size to prevent OOM\n   649→  const sizeCheck = await validateFileSize(wavPath);\n   650→  if (!sizeCheck.valid) {\n   651→    return res.status(413).json({ error: sizeCheck.error });\n   652→  }\n   653→\n   654→  // Check FFprobe availability (needed for audio extraction)\n   655→  const ffprobeAvailable = await isFFprobeInstalled();\n   656→  if (!ffprobeAvailable) {\n   657→    return res.status(500).json({\n   658→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n   659→    });\n   660→  }\n   661→\n   662→  // Build options - use sensitivity if provided, otherwise use manual options\n   663→  let options = {};\n   664→  if (typeof sensitivity === 'number') {\n   665→    options = sensitivityToParams(sensitivity);\n   666→    console.log(`[SPLICE] RMS detection with sensitivity ${sensitivity}`);\n   667→  } else {\n   668→    options = {\n   669→      threshold: manualOptions.threshold ?? -30,\n   670→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n   671→      seekStep: manualOptions.seekStep ?? 0.05,\n   672→      paddingStart: manualOptions.paddingStart ?? 0.1,\n   673→      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n   674→      autoThreshold: manualOptions.autoThreshold ?? false,\n   675→      mergeDistance: manualOptions.mergeDistance ?? 0.2\n   676→    };\n   677→  }\n   678→\n   679→  console.log(`[SPLICE] RMS silence detection: ${wavPath}`);\n   680→\n   681→  try {\n   682→    const result = await detectSilencesRMS(wavPath, options);\n   683→\n   684→    // Deduct usage based on audio duration\n   685→    const audioDuration = result.metadata?.audioDuration || 0;\n   686→    let balance = null;\n   687→    if (audioDuration > 0 && req.deductUsage) {\n   688→      balance = await req.deductUsage(audioDuration);\n   689→    }\n   690→\n   691→    res.json({\n   692→      success: true,\n   693→      wavPath,\n   694→      ...result,\n   695→      count: result.silences.length,\n   696→      totalSilenceDuration: result.metadata.totalSilenceDuration.toFixed(2),\n   697→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   698→    });\n   699→  } catch (err) {\n   700→    console.error('[SPLICE] RMS silence detection error:', err);\n   701→    res.status(500).json({ error: err.message });\n   702→  }\n   703→});\n   704→\n   705→/**\n   706→ * POST /waveform - Get waveform data for visualization\n   707→ *\n   708→ * Extracts RMS waveform data from audio file for canvas visualization.\n   709→ * Returns normalized amplitude values (0-1) for drawing waveform display.\n   710→ *\n   711→ * Options:\n   712→ * - wavPath: Path to audio file (required)\n   713→ * - targetPoints: Number of data points to return (default: 400)\n   714→ * - windowMs: Window size in milliseconds for RMS calculation (default: 50)\n   715→ *\n   716→ * Response:\n   717→ * - waveform: Array of normalized RMS values (0-1)\n   718→ * - duration: Audio duration in seconds\n   719→ * - pointCount: Number of points in waveform array\n   720→ * - sampleRate: Audio sample rate\n   721→ */\n   722→app.post('/waveform', requireCredits({ endpoint: 'waveform' }), async (req, res) => {\n   723→  const { wavPath, targetPoints = 400, windowMs = 50 } = req.body;\n   724→\n   725→  if (!wavPath) {\n   726→    return res.status(400).json({ error: 'wavPath is required' });\n   727→  }\n   728→\n   729→  if (!(await fileExists(wavPath))) {\n   730→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   731→  }\n   732→\n   733→  // Validate file size to prevent OOM\n   734→  const sizeCheck = await validateFileSize(wavPath);\n   735→  if (!sizeCheck.valid) {\n   736→    return res.status(413).json({ error: sizeCheck.error });\n   737→  }\n   738→\n   739→  console.log(`[SPLICE] Waveform extraction: ${wavPath} (${targetPoints} points)`);\n   740→\n   741→  try {\n   742→    const result = await getWaveformData(wavPath, { targetPoints, windowMs });\n   743→\n   744→    res.json({\n   745→      success: true,\n   746→      wavPath,\n   747→      ...result\n   748→    });\n   749→  } catch (err) {\n   750→    console.error('[SPLICE] Waveform extraction error:', err);\n   751→    res.status(500).json({ error: err.message });\n   752→  }\n   753→});\n   754→\n   755→// =============================================================================\n   756→// Profanity Detection Routes\n   757→// =============================================================================\n   758→\n   759→/**\n   760→ * POST /profanity - Detect profanity in audio/transcript\n   761→ *\n   762→ * Transcribes audio (if needed) and detects profanity words.\n   763→ * Returns word-level and segment-level results for muting/bleeping.\n   764→ *\n   765→ * Options:\n   766→ * - wavPath: Path to audio file (required)\n   767→ * - transcript: Pre-existing transcript (optional, skips transcription)\n   768→ * - language: Language code (en, es, fr, de) - default: en\n   769→ * - customBlocklist: Array or comma-separated string of additional words to censor\n   770→ * - customAllowlist: Array or comma-separated string of words to allow\n   771→ * - frameRate: Frame rate for boundary alignment (default: 30)\n   772→ */\n   773→app.post('/profanity', requireCredits({ endpoint: 'profanity' }), async (req, res) => {\n   774→  const {\n   775→    wavPath,\n   776→    transcript: providedTranscript,\n   777→    language = 'en',\n   778→    customBlocklist = [],\n   779→    customAllowlist = [],\n   780→    frameRate = 30\n   781→  } = req.body;\n   782→\n   783→  if (!wavPath && !providedTranscript) {\n   784→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   785→  }\n   786→\n   787→  if (wavPath && !(await fileExists(wavPath))) {\n   788→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   789→  }\n   790→\n   791→  console.log(`[SPLICE] Profanity detection: ${wavPath || 'provided transcript'} (language: ${language})`);\n   792→\n   793→  try {\n   794→    // Get or create transcript with word-level timestamps\n   795→    let transcript = providedTranscript;\n   796→    if (!transcript && wavPath) {\n   797→      // Use transcribeWithWords for word-level timestamps required by profanity detection\n   798→      transcript = await transcribeWithWords(wavPath);\n   799→    }\n   800→\n   801→    // Validate transcript has words\n   802→    if (!transcript || !transcript.words || transcript.words.length === 0) {\n   803→      return res.status(400).json({\n   804→        error: 'Transcript must contain word-level timing data',\n   805→        hint: 'Ensure transcription returns words array with start/end times'\n   806→      });\n   807→    }\n   808→\n   809→    // Parse custom lists\n   810→    const blocklist = parseWordList(customBlocklist);\n   811→    const allowlist = parseWordList(customAllowlist);\n   812→\n   813→    // Detect profanity\n   814→    const result = detectProfanity(transcript, {\n   815→      language,\n   816→      customBlocklist: blocklist,\n   817→      customAllowlist: allowlist,\n   818→      frameRate\n   819→    });\n   820→\n   821→    // Deduct usage based on audio duration\n   822→    const audioDuration = transcript.duration || 0;\n   823→    let balance = null;\n   824→    if (audioDuration > 0 && req.deductUsage) {\n   825→      balance = await req.deductUsage(audioDuration);\n   826→    }\n   827→\n   828→    res.json({\n   829→      success: true,\n   830→      wavPath,\n   831→      ...result,\n   832→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   833→    });\n   834→  } catch (err) {\n   835→    console.error('[SPLICE] Profanity detection error:', err);\n   836→    res.status(500).json({ error: err.message });\n   837→  }\n   838→});\n   839→\n   840→/**\n   841→ * GET /profanity/languages - Get supported languages\n   842→ */\n   843→app.get('/profanity/languages', (req, res) => {\n   844→  res.json({\n   845→    success: true,\n   846→    languages: getSupportedLanguages()\n   847→  });\n   848→});\n   849→\n   850→/**\n   851→ * GET /profanity/bleeps - Get available bleep sounds\n   852→ */\n   853→app.get('/profanity/bleeps', (req, res) => {\n   854→  res.json({\n   855→    success: true,\n   856→    sounds: getAvailableBleepSounds()\n   857→  });\n   858→});\n   859→\n   860→/**\n   861→ * POST /profanity/generate-bleeps - Generate bleep audio files for profanity segments\n   862→ *\n   863→ * Creates WAV bleep files for each profanity segment that can be inserted into the timeline.\n   864→ *\n   865→ * Body:\n   866→ * - segments: Array of {start, end} profanity segments (required)\n   867→ * - bleepType: Type of bleep sound ('standard', 'tv', 'radio', 'duck', 'mute') (default: 'standard')\n   868→ * - volume: Volume level 0-1 (default: 0.5)\n   869→ *\n   870→ * Returns:\n   871→ * - bleeps: Array of generated bleep file info with paths\n   872→ */\n   873→app.post('/profanity/generate-bleeps', requireCredits({ endpoint: 'profanity-bleeps' }), async (req, res) => {\n   874→  const { segments, bleepType = 'standard', volume = 0.5 } = req.body;\n   875→\n   876→  if (!segments || !Array.isArray(segments) || segments.length === 0) {\n   877→    return res.status(400).json({ error: 'segments array is required and must not be empty' });\n   878→  }\n   879→\n   880→  // Validate segments have start/end\n   881→  for (let i = 0; i < segments.length; i++) {\n   882→    if (typeof segments[i].start !== 'number' || typeof segments[i].end !== 'number') {\n   883→      return res.status(400).json({ error: `Segment ${i} must have numeric start and end properties` });\n   884→    }\n   885→  }\n   886→\n   887→  console.log(`[SPLICE] Generating ${segments.length} bleep audio files (type: ${bleepType})`);\n   888→\n   889→  try {\n   890→    const bleeps = await generateBleepsForSegments(segments, {\n   891→      bleepType,\n   892→      volume,\n   893→      outputDir: '/tmp'\n   894→    });\n   895→\n   896→    // Calculate total duration for usage tracking\n   897→    const totalDuration = segments.reduce((sum, seg) => sum + (seg.end - seg.start), 0);\n   898→    let balance = null;\n   899→    if (totalDuration > 0 && req.deductUsage) {\n   900→      balance = await req.deductUsage(totalDuration);\n   901→    }\n   902→\n   903→    res.json({\n   904→      success: true,\n   905→      bleepCount: bleeps.length,\n   906→      bleepType,\n   907→      bleeps,\n   908→      totalDuration,\n   909→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n   910→    });\n   911→  } catch (err) {\n   912→    console.error('[SPLICE] Error generating bleeps:', err);\n   913→    res.status(500).json({ error: err.message });\n   914→  }\n   915→});\n   916→\n   917→/**\n   918→ * GET /profanity/list/:language - Get default profanity list for a language\n   919→ */\n   920→app.get('/profanity/list/:language', (req, res) => {\n   921→  const { language } = req.params;\n   922→  const list = getProfanityList(language);\n   923→\n   924→  res.json({\n   925→    success: true,\n   926→    language,\n   927→    wordCount: list.length,\n   928→    // Return first 50 words as sample (full list is large)\n   929→    sample: list.slice(0, 50),\n   930→    note: 'Full list available but truncated for response size'\n   931→  });\n   932→});\n   933→\n   934→// =============================================================================\n   935→// Repetition/Stutter Detection Routes\n   936→// =============================================================================\n   937→\n   938→/**\n   939→ * POST /repetitions - Detect phrase repetitions and stutters\n   940→ *\n   941→ * Analyzes transcript for repeated phrases and stutters.\n   942→ * Returns segments that can be removed to clean up the edit.\n   943→ *\n   944→ * Options:\n   945→ * - wavPath: Path to audio file (required unless transcript provided)\n   946→ * - transcript: Pre-existing transcript (optional)\n   947→ * - phraseSize: Words per comparison window (default: 5)\n   948→ * - tolerance: Similarity threshold 0-1 (default: 0.7)\n   949→ * - searchRadius: Words to search ahead (default: 100)\n   950→ * - useOpenAI: Use OpenAI for boundary refinement (default: false)\n   951→ * - includeStutters: Also detect single-word stutters (default: true)\n   952→ */\n   953→app.post('/repetitions', requireCredits({ endpoint: 'repetitions' }), async (req, res) => {\n   954→  const {\n   955→    wavPath,\n   956→    transcript: providedTranscript,\n   957→    phraseSize = 5,\n   958→    tolerance = 0.7,\n   959→    searchRadius = 100,\n   960→    useOpenAI = false,\n   961→    includeStutters = true\n   962→  } = req.body;\n   963→\n   964→  if (!wavPath && !providedTranscript) {\n   965→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n   966→  }\n   967→\n   968→  if (wavPath && !(await fileExists(wavPath))) {\n   969→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n   970→  }\n   971→\n   972→  console.log(`[SPLICE] Repetition detection: ${wavPath || 'provided transcript'}`);\n   973→\n   974→  try {\n   975→    // Get or create transcript with word-level timestamps\n   976→    let transcript = providedTranscript;\n   977→    if (!transcript && wavPath) {\n   978→      // Use transcribeWithWords for word-level timestamps required by repetition detection\n   979→      transcript = await transcribeWithWords(wavPath);\n   980→    }\n   981→\n   982→    // Validate transcript has words\n   983→    if (!transcript || !transcript.words || transcript.words.length === 0) {\n   984→      return res.status(400).json({\n   985→        error: 'Transcript must contain word-level timing data'\n   986→      });\n   987→    }\n   988→\n   989→    // Detect all repetitions (phrases + stutters)\n   990→    const result = await detectAllRepetitions(transcript, {\n   991→      phraseSize,\n   992→      tolerance,\n   993→      searchRadius,\n   994→      useOpenAI\n   995→    });\n   996→\n   997→    // Optionally filter out stutters\n   998→    if (!includeStutters) {\n   999→      result.stutters = [];\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"6eed003d-e0e5-4761-90d5-85bdd594fd4d","timestamp":"2025-12-26T07:40:23.410Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/server.js","content":"  }\n});\n\n/**\n * POST /silences - Detect silent gaps in audio\n *\n * Pipeline:\n * 1. Transcribe audio with Whisper (cached)\n * 2. Analyze gaps between segments\n * 3. Return silence regions\n */\napp.post('/silences', requireCredits({ endpoint: 'silences' }), async (req, res) => {\n  const { wavPath, threshold = 0.5 } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Detecting silences: ${wavPath} (threshold: ${threshold}s)`);\n\n  try {\n    const transcript = await transcribeAudio(wavPath);\n    const silences = detectSilences(transcript.segments, threshold);\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      threshold,\n      silences,\n      count: silences.length,\n      totalSilenceDuration: silences.reduce((sum, s) => sum + s.duration, 0).toFixed(2),\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Silence detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /silences-audio - Detect silences using FFprobe audio analysis\n *\n * Uses actual audio levels (dB threshold) instead of transcript gaps.\n * More accurate for detecting silence vs background noise.\n */\napp.post('/silences-audio', requireCredits({ endpoint: 'silences-audio' }), async (req, res) => {\n  const {\n    wavPath,\n    threshold = -30,\n    minDuration = 0.5,\n    padding = 0.1\n  } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  // Check FFprobe availability\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  console.log(`[SPLICE] FFprobe silence detection: ${wavPath} (threshold: ${threshold}dB, min: ${minDuration}s)`);\n\n  try {\n    const silences = await detectAudioSilences(wavPath, {\n      threshold,\n      minDuration,\n      padding\n    });\n\n    const totalDuration = silences.reduce((sum, s) => sum + s.duration, 0);\n\n    // Deduct usage based on audio duration\n    let balance = null;\n    try {\n      const audioDuration = await getAudioDuration(wavPath);\n      if (audioDuration > 0 && req.deductUsage) {\n        balance = await req.deductUsage(audioDuration);\n      }\n    } catch (durErr) {\n      console.warn('[SPLICE] Could not get audio duration for billing:', durErr.message);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      threshold,\n      minDuration,\n      padding,\n      silences,\n      count: silences.length,\n      totalSilenceDuration: totalDuration.toFixed(2),\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] FFprobe silence detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /silences-rms - Detect silences using RMS audio analysis\n *\n * Advanced silence detection with:\n * - RMS (Root Mean Square) audio level analysis\n * - Auto-threshold detection from audio histogram\n * - Configurable padding (before/after cuts)\n * - Sensitivity slider mapping (0-100)\n *\n * Options:\n * - threshold: dBFS threshold (-60 to -20, default: -30)\n * - minSilenceLength: Minimum silence duration in seconds (default: 0.5)\n * - seekStep: Analysis window step in seconds (default: 0.05)\n * - paddingStart: Buffer before silence in seconds (default: 0.1)\n * - paddingEnd: Buffer after silence in seconds (default: 0.05)\n * - autoThreshold: Auto-detect optimal threshold (default: false)\n * - sensitivity: UI sensitivity 0-100 (overrides other params if provided)\n */\napp.post('/silences-rms', requireCredits({ endpoint: 'silences-rms' }), async (req, res) => {\n  const { wavPath, sensitivity, ...manualOptions } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  // Validate file size to prevent OOM\n  const sizeCheck = await validateFileSize(wavPath);\n  if (!sizeCheck.valid) {\n    return res.status(413).json({ error: sizeCheck.error });\n  }\n\n  // Check FFprobe availability (needed for audio extraction)\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  // Build options - use sensitivity if provided, otherwise use manual options\n  let options = {};\n  if (typeof sensitivity === 'number') {\n    options = sensitivityToParams(sensitivity);\n    console.log(`[SPLICE] RMS detection with sensitivity ${sensitivity}`);\n  } else {\n    options = {\n      threshold: manualOptions.threshold ?? -30,\n      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,\n      seekStep: manualOptions.seekStep ?? 0.05,\n      paddingStart: manualOptions.paddingStart ?? 0.1,\n      paddingEnd: manualOptions.paddingEnd ?? 0.05,\n      autoThreshold: manualOptions.autoThreshold ?? false,\n      mergeDistance: manualOptions.mergeDistance ?? 0.2\n    };\n  }\n\n  console.log(`[SPLICE] RMS silence detection: ${wavPath}`);\n\n  try {\n    const result = await detectSilencesRMS(wavPath, options);\n\n    // Deduct usage based on audio duration\n    const audioDuration = result.metadata?.audioDuration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result,\n      count: result.silences.length,\n      totalSilenceDuration: result.metadata.totalSilenceDuration.toFixed(2),\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] RMS silence detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /waveform - Get waveform data for visualization\n *\n * Extracts RMS waveform data from audio file for canvas visualization.\n * Returns normalized amplitude values (0-1) for drawing waveform display.\n *\n * Options:\n * - wavPath: Path to audio file (required)\n * - targetPoints: Number of data points to return (default: 400)\n * - windowMs: Window size in milliseconds for RMS calculation (default: 50)\n *\n * Response:\n * - waveform: Array of normalized RMS values (0-1)\n * - duration: Audio duration in seconds\n * - pointCount: Number of points in waveform array\n * - sampleRate: Audio sample rate\n */\napp.post('/waveform', requireCredits({ endpoint: 'waveform' }), async (req, res) => {\n  const { wavPath, targetPoints = 400, windowMs = 50 } = req.body;\n\n  if (!wavPath) {\n    return res.status(400).json({ error: 'wavPath is required' });\n  }\n\n  if (!(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  // Validate file size to prevent OOM\n  const sizeCheck = await validateFileSize(wavPath);\n  if (!sizeCheck.valid) {\n    return res.status(413).json({ error: sizeCheck.error });\n  }\n\n  console.log(`[SPLICE] Waveform extraction: ${wavPath} (${targetPoints} points)`);\n\n  try {\n    const result = await getWaveformData(wavPath, { targetPoints, windowMs });\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result\n    });\n  } catch (err) {\n    console.error('[SPLICE] Waveform extraction error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Profanity Detection Routes\n// =============================================================================\n\n/**\n * POST /profanity - Detect profanity in audio/transcript\n *\n * Transcribes audio (if needed) and detects profanity words.\n * Returns word-level and segment-level results for muting/bleeping.\n *\n * Options:\n * - wavPath: Path to audio file (required)\n * - transcript: Pre-existing transcript (optional, skips transcription)\n * - language: Language code (en, es, fr, de) - default: en\n * - customBlocklist: Array or comma-separated string of additional words to censor\n * - customAllowlist: Array or comma-separated string of words to allow\n * - frameRate: Frame rate for boundary alignment (default: 30)\n */\napp.post('/profanity', requireCredits({ endpoint: 'profanity' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    language = 'en',\n    customBlocklist = [],\n    customAllowlist = [],\n    frameRate = 30\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Profanity detection: ${wavPath || 'provided transcript'} (language: ${language})`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      // Use transcribeWithWords for word-level timestamps required by profanity detection\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript has words\n    if (!transcript || !transcript.words || transcript.words.length === 0) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data',\n        hint: 'Ensure transcription returns words array with start/end times'\n      });\n    }\n\n    // Parse custom lists\n    const blocklist = parseWordList(customBlocklist);\n    const allowlist = parseWordList(customAllowlist);\n\n    // Detect profanity\n    const result = detectProfanity(transcript, {\n      language,\n      customBlocklist: blocklist,\n      customAllowlist: allowlist,\n      frameRate\n    });\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Profanity detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /profanity/languages - Get supported languages\n */\napp.get('/profanity/languages', (req, res) => {\n  res.json({\n    success: true,\n    languages: getSupportedLanguages()\n  });\n});\n\n/**\n * GET /profanity/bleeps - Get available bleep sounds\n */\napp.get('/profanity/bleeps', (req, res) => {\n  res.json({\n    success: true,\n    sounds: getAvailableBleepSounds()\n  });\n});\n\n/**\n * POST /profanity/generate-bleeps - Generate bleep audio files for profanity segments\n *\n * Creates WAV bleep files for each profanity segment that can be inserted into the timeline.\n *\n * Body:\n * - segments: Array of {start, end} profanity segments (required)\n * - bleepType: Type of bleep sound ('standard', 'tv', 'radio', 'duck', 'mute') (default: 'standard')\n * - volume: Volume level 0-1 (default: 0.5)\n *\n * Returns:\n * - bleeps: Array of generated bleep file info with paths\n */\napp.post('/profanity/generate-bleeps', requireCredits({ endpoint: 'profanity-bleeps' }), async (req, res) => {\n  const { segments, bleepType = 'standard', volume = 0.5 } = req.body;\n\n  if (!segments || !Array.isArray(segments) || segments.length === 0) {\n    return res.status(400).json({ error: 'segments array is required and must not be empty' });\n  }\n\n  // Validate segments have start/end\n  for (let i = 0; i < segments.length; i++) {\n    if (typeof segments[i].start !== 'number' || typeof segments[i].end !== 'number') {\n      return res.status(400).json({ error: `Segment ${i} must have numeric start and end properties` });\n    }\n  }\n\n  console.log(`[SPLICE] Generating ${segments.length} bleep audio files (type: ${bleepType})`);\n\n  try {\n    const bleeps = await generateBleepsForSegments(segments, {\n      bleepType,\n      volume,\n      outputDir: '/tmp'\n    });\n\n    // Calculate total duration for usage tracking\n    const totalDuration = segments.reduce((sum, seg) => sum + (seg.end - seg.start), 0);\n    let balance = null;\n    if (totalDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(totalDuration);\n    }\n\n    res.json({\n      success: true,\n      bleepCount: bleeps.length,\n      bleepType,\n      bleeps,\n      totalDuration,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Error generating bleeps:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /profanity/list/:language - Get default profanity list for a language\n */\napp.get('/profanity/list/:language', (req, res) => {\n  const { language } = req.params;\n  const list = getProfanityList(language);\n\n  res.json({\n    success: true,\n    language,\n    wordCount: list.length,\n    // Return first 50 words as sample (full list is large)\n    sample: list.slice(0, 50),\n    note: 'Full list available but truncated for response size'\n  });\n});\n\n// =============================================================================\n// Repetition/Stutter Detection Routes\n// =============================================================================\n\n/**\n * POST /repetitions - Detect phrase repetitions and stutters\n *\n * Analyzes transcript for repeated phrases and stutters.\n * Returns segments that can be removed to clean up the edit.\n *\n * Options:\n * - wavPath: Path to audio file (required unless transcript provided)\n * - transcript: Pre-existing transcript (optional)\n * - phraseSize: Words per comparison window (default: 5)\n * - tolerance: Similarity threshold 0-1 (default: 0.7)\n * - searchRadius: Words to search ahead (default: 100)\n * - useOpenAI: Use OpenAI for boundary refinement (default: false)\n * - includeStutters: Also detect single-word stutters (default: true)\n */\napp.post('/repetitions', requireCredits({ endpoint: 'repetitions' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    phraseSize = 5,\n    tolerance = 0.7,\n    searchRadius = 100,\n    useOpenAI = false,\n    includeStutters = true\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Repetition detection: ${wavPath || 'provided transcript'}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      // Use transcribeWithWords for word-level timestamps required by repetition detection\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript has words\n    if (!transcript || !transcript.words || transcript.words.length === 0) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data'\n      });\n    }\n\n    // Detect all repetitions (phrases + stutters)\n    const result = await detectAllRepetitions(transcript, {\n      phraseSize,\n      tolerance,\n      searchRadius,\n      useOpenAI\n    });\n\n    // Optionally filter out stutters\n    if (!includeStutters) {\n      result.stutters = [];","numLines":500,"startLine":500,"totalLines":3719}}}
{"parentUuid":"6eed003d-e0e5-4761-90d5-85bdd594fd4d","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01XrTEy7uekwu9yon1vtwdNf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Ke7RoFeyeXxNoBn62bh7Bx","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/server.js","offset":1000,"limit":500}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":36037,"cache_read_input_tokens":91881,"cache_creation":{"ephemeral_5m_input_tokens":36037,"ephemeral_1h_input_tokens":0},"output_tokens":22,"service_tier":"standard"}},"requestId":"req_011CWUvJuSYk7A3dGCnRKw4P","type":"assistant","uuid":"0c33ccae-93f0-4a5d-980f-0ab546b4dd19","timestamp":"2025-12-26T07:40:28.187Z"}
{"parentUuid":"0c33ccae-93f0-4a5d-980f-0ab546b4dd19","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01XrTEy7uekwu9yon1vtwdNf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016EhoD5S69mZPPENKM2vStX","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/faceDetection.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":36037,"cache_read_input_tokens":91881,"cache_creation":{"ephemeral_5m_input_tokens":36037,"ephemeral_1h_input_tokens":0},"output_tokens":22,"service_tier":"standard"}},"requestId":"req_011CWUvJuSYk7A3dGCnRKw4P","type":"assistant","uuid":"8f9455de-08b7-4c63-91cd-48aeba927eb7","timestamp":"2025-12-26T07:40:28.915Z"}
{"parentUuid":"8f9455de-08b7-4c63-91cd-48aeba927eb7","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_01XrTEy7uekwu9yon1vtwdNf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BpUrW8t4xkbx2KJaw742L5","name":"Read","input":{"file_path":"/Users/imorgado/SPLICE/splice-backend/services/socialReframe.js"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":36037,"cache_read_input_tokens":91881,"cache_creation":{"ephemeral_5m_input_tokens":36037,"ephemeral_1h_input_tokens":0},"output_tokens":22,"service_tier":"standard"}},"requestId":"req_011CWUvJuSYk7A3dGCnRKw4P","type":"assistant","uuid":"11261edb-110c-4861-baba-d2e11cc5d48b","timestamp":"2025-12-26T07:40:29.579Z"}
{"parentUuid":"11261edb-110c-4861-baba-d2e11cc5d48b","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Ke7RoFeyeXxNoBn62bh7Bx","type":"tool_result","content":"  1000→      result.removalSegments = result.removalSegments.filter(s => s.type !== 'stutter');\n  1001→    }\n  1002→\n  1003→    // Deduct usage based on audio duration\n  1004→    const audioDuration = transcript.duration || 0;\n  1005→    let balance = null;\n  1006→    if (audioDuration > 0 && req.deductUsage) {\n  1007→      balance = await req.deductUsage(audioDuration);\n  1008→    }\n  1009→\n  1010→    res.json({\n  1011→      success: true,\n  1012→      wavPath,\n  1013→      ...result,\n  1014→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n  1015→    });\n  1016→  } catch (err) {\n  1017→    console.error('[SPLICE] Repetition detection error:', err);\n  1018→    res.status(500).json({ error: err.message });\n  1019→  }\n  1020→});\n  1021→\n  1022→/**\n  1023→ * POST /fillers - Detect filler words (um, uh, like, etc.)\n  1024→ *\n  1025→ * Transcribes audio and identifies filler words with timestamps.\n  1026→ * Returns segments that can be cut or reviewed for removal.\n  1027→ * Supports frame alignment for precise video editing.\n  1028→ *\n  1029→ * Options:\n  1030→ * - wavPath: Path to audio file (required unless transcript provided)\n  1031→ * - transcript: Pre-existing transcript with word-level timing (optional)\n  1032→ * - customFillers: Additional filler words to detect (optional)\n  1033→ * - frameRate: Frame rate for alignment (0 = no alignment, 24/30/60 etc.)\n  1034→ * - paddingMs: Padding in ms around filler words for cleaner cuts (default 50)\n  1035→ */\n  1036→app.post('/fillers', requireCredits({ endpoint: 'fillers' }), async (req, res) => {\n  1037→  const {\n  1038→    wavPath,\n  1039→    transcript: providedTranscript,\n  1040→    customFillers = [],\n  1041→    frameRate = 0, // Optional: align timestamps to frames\n  1042→    paddingMs = 50 // Padding around filler words for cleaner cuts\n  1043→  } = req.body;\n  1044→\n  1045→  if (!wavPath && !providedTranscript) {\n  1046→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  1047→  }\n  1048→\n  1049→  if (wavPath && !(await fileExists(wavPath))) {\n  1050→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  1051→  }\n  1052→\n  1053→  console.log(`[SPLICE] Filler word detection: ${wavPath || 'provided transcript'}`);\n  1054→\n  1055→  try {\n  1056→    // Get or create transcript with word-level timestamps\n  1057→    let transcript = providedTranscript;\n  1058→    if (!transcript && wavPath) {\n  1059→      transcript = await transcribeWithWords(wavPath);\n  1060→    }\n  1061→\n  1062→    // Validate transcript has words\n  1063→    if (!transcript || !transcript.words || transcript.words.length === 0) {\n  1064→      return res.status(400).json({\n  1065→        error: 'Transcript must contain word-level timing data'\n  1066→      });\n  1067→    }\n  1068→\n  1069→    // Default filler words (common in English speech)\n  1070→    const defaultFillers = [\n  1071→      'um', 'uh', 'ah', 'er', 'eh',           // Hesitation sounds\n  1072→      'like', 'so', 'well', 'right',           // Discourse markers\n  1073→      'you know', 'i mean', 'basically',       // Filler phrases\n  1074→      'actually', 'literally', 'honestly',     // Overused qualifiers\n  1075→      'kind of', 'sort of', 'you see'          // Hedging phrases\n  1076→    ];\n  1077→\n  1078→    // Combine default + custom fillers (lowercase for matching)\n  1079→    const fillerSet = new Set([\n  1080→      ...defaultFillers,\n  1081→      ...customFillers.map(f => f.toLowerCase().trim())\n  1082→    ]);\n  1083→\n  1084→    // Detect filler words\n  1085→    const fillers = [];\n  1086→    const words = transcript.words;\n  1087→\n  1088→    for (let i = 0; i < words.length; i++) {\n  1089→      const word = words[i];\n  1090→      const normalizedWord = word.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim();\n  1091→\n  1092→      // Check single-word fillers\n  1093→      if (fillerSet.has(normalizedWord)) {\n  1094→        fillers.push({\n  1095→          word: word.word,\n  1096→          normalizedWord,\n  1097→          start: word.start,\n  1098→          end: word.end,\n  1099→          duration: word.end - word.start,\n  1100→          index: i,\n  1101→          type: 'filler'\n  1102→        });\n  1103→        continue;\n  1104→      }\n  1105→\n  1106→      // Check two-word phrases (e.g., \"you know\", \"kind of\")\n  1107→      if (i < words.length - 1) {\n  1108→        const nextWord = words[i + 1];\n  1109→        const twoWordPhrase = `${normalizedWord} ${nextWord.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim()}`;\n  1110→        if (fillerSet.has(twoWordPhrase)) {\n  1111→          fillers.push({\n  1112→            word: `${word.word} ${nextWord.word}`,\n  1113→            normalizedWord: twoWordPhrase,\n  1114→            start: word.start,\n  1115→            end: nextWord.end,\n  1116→            duration: nextWord.end - word.start,\n  1117→            index: i,\n  1118→            type: 'filler_phrase'\n  1119→          });\n  1120→          // Skip next word since it's part of this phrase\n  1121→          i++;\n  1122→        }\n  1123→      }\n  1124→    }\n  1125→\n  1126→    // Apply frame alignment if requested\n  1127→    const hasFrameAlignment = frameRate > 0;\n  1128→    const paddingSec = paddingMs / 1000;\n  1129→\n  1130→    // Enhance fillers with aligned timestamps and padding\n  1131→    const enhancedFillers = fillers.map(f => {\n  1132→      // Add padding for cleaner cuts\n  1133→      const paddedStart = Math.max(0, f.start - paddingSec);\n  1134→      const paddedEnd = f.end + paddingSec;\n  1135→\n  1136→      // Apply frame alignment if requested\n  1137→      let startAligned = paddedStart;\n  1138→      let endAligned = paddedEnd;\n  1139→      if (hasFrameAlignment) {\n  1140→        startAligned = alignToFrameFloor(paddedStart, frameRate);\n  1141→        endAligned = alignToFrameCeil(paddedEnd, frameRate);\n  1142→      }\n  1143→\n  1144→      return {\n  1145→        ...f,\n  1146→        paddedStart,\n  1147→        paddedEnd,\n  1148→        startAligned: parseFloat(startAligned.toFixed(4)),\n  1149→        endAligned: parseFloat(endAligned.toFixed(4)),\n  1150→        durationAligned: parseFloat((endAligned - startAligned).toFixed(4))\n  1151→      };\n  1152→    });\n  1153→\n  1154→    // Calculate total filler time\n  1155→    const totalFillerDuration = fillers.reduce((sum, f) => sum + f.duration, 0);\n  1156→    const audioDuration = transcript.duration || (words.length > 0 ? words[words.length - 1].end : 0);\n  1157→    const fillerPercentage = audioDuration > 0 ? (totalFillerDuration / audioDuration) * 100 : 0;\n  1158→\n  1159→    // Deduct usage based on audio duration\n  1160→    let balance = null;\n  1161→    if (audioDuration > 0 && req.deductUsage) {\n  1162→      balance = await req.deductUsage(audioDuration);\n  1163→    }\n  1164→\n  1165→    res.json({\n  1166→      success: true,\n  1167→      wavPath,\n  1168→      fillers: enhancedFillers,\n  1169→      metadata: {\n  1170→        totalWords: words.length,\n  1171→        fillerCount: enhancedFillers.length,\n  1172→        totalFillerDuration: parseFloat(totalFillerDuration.toFixed(3)),\n  1173→        audioDuration: parseFloat(audioDuration.toFixed(3)),\n  1174→        fillerPercentage: parseFloat(fillerPercentage.toFixed(2)),\n  1175→        fillersPerMinute: audioDuration > 0 ? parseFloat((enhancedFillers.length / (audioDuration / 60)).toFixed(2)) : 0,\n  1176→        frameRate: hasFrameAlignment ? frameRate : null,\n  1177→        frameAligned: hasFrameAlignment,\n  1178→        paddingMs\n  1179→      },\n  1180→      removalSegments: enhancedFillers.map(f => ({\n  1181→        start: f.startAligned,\n  1182→        end: f.endAligned,\n  1183→        duration: f.durationAligned,\n  1184→        reason: `Filler: \"${f.word}\"`,\n  1185→        type: f.type\n  1186→      })),\n  1187→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n  1188→    });\n  1189→  } catch (err) {\n  1190→    console.error('[SPLICE] Filler detection error:', err);\n  1191→    res.status(500).json({ error: err.message });\n  1192→  }\n  1193→});\n  1194→\n  1195→/**\n  1196→ * POST /stutters - Detect single-word stutters only\n  1197→ *\n  1198→ * Focused detection for word-level stutters (e.g., \"I I I think\").\n  1199→ * Faster than full repetition detection.\n  1200→ */\n  1201→app.post('/stutters', requireCredits({ endpoint: 'stutters' }), async (req, res) => {\n  1202→  const {\n  1203→    wavPath,\n  1204→    transcript: providedTranscript,\n  1205→    options = {},\n  1206→    // Support both top-level and nested options for flexibility\n  1207→    minRepeats = options.minRepeats ?? 2,\n  1208→    maxGapMs = options.maxGapMs ?? 500,\n  1209→    ignoreFillers = options.ignoreFillers ?? true,\n  1210→    minWordLength = options.minWordLength ?? 1\n  1211→  } = req.body;\n  1212→\n  1213→  if (!wavPath && !providedTranscript) {\n  1214→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  1215→  }\n  1216→\n  1217→  if (wavPath && !(await fileExists(wavPath))) {\n  1218→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  1219→  }\n  1220→\n  1221→  console.log(`[SPLICE] Stutter detection: ${wavPath || 'provided transcript'}`);\n  1222→\n  1223→  try {\n  1224→    // Get or create transcript with word-level timestamps\n  1225→    let transcript = providedTranscript;\n  1226→    if (!transcript && wavPath) {\n  1227→      // Use transcribeWithWords for word-level timestamps required by stutter detection\n  1228→      transcript = await transcribeWithWords(wavPath);\n  1229→    }\n  1230→\n  1231→    // Validate transcript exists and has words array\n  1232→    if (!transcript || !transcript.words) {\n  1233→      return res.status(400).json({\n  1234→        error: 'Transcript must contain word-level timing data'\n  1235→      });\n  1236→    }\n  1237→\n  1238→    // Empty transcript returns empty result (not an error)\n  1239→    if (transcript.words.length === 0) {\n  1240→      return res.json({\n  1241→        success: true,\n  1242→        stutters: [],\n  1243→        metadata: { type: 'stutters', totalWords: 0, stutterCount: 0, totalRepeatedWords: 0 }\n  1244→      });\n  1245→    }\n  1246→\n  1247→    // Detect stutters only\n  1248→    const result = detectStutters(transcript, {\n  1249→      minRepeats,\n  1250→      maxGapMs,\n  1251→      ignoreFillers,\n  1252→      minWordLength\n  1253→    });\n  1254→\n  1255→    // Deduct usage based on audio duration\n  1256→    const audioDuration = transcript.duration || 0;\n  1257→    let balance = null;\n  1258→    if (audioDuration > 0 && req.deductUsage) {\n  1259→      balance = await req.deductUsage(audioDuration);\n  1260→    }\n  1261→\n  1262→    res.json({\n  1263→      success: true,\n  1264→      wavPath,\n  1265→      ...result,\n  1266→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n  1267→    });\n  1268→  } catch (err) {\n  1269→    console.error('[SPLICE] Stutter detection error:', err);\n  1270→    res.status(500).json({ error: err.message });\n  1271→  }\n  1272→});\n  1273→\n  1274→// =============================================================================\n  1275→// Caption Export Routes\n  1276→// =============================================================================\n  1277→\n  1278→/**\n  1279→ * POST /export/captions - Export transcript to caption format (SRT, VTT, etc.)\n  1280→ *\n  1281→ * Converts a transcript to the specified caption format.\n  1282→ * Can optionally save to file.\n  1283→ *\n  1284→ * Options:\n  1285→ * - wavPath: Path to audio file (to transcribe first)\n  1286→ * - transcript: Pre-existing transcript with word-level timing\n  1287→ * - format: Export format (srt, vtt, txt, json) - default: srt\n  1288→ * - outputPath: Optional file path to save to\n  1289→ * - maxWordsPerCaption: Max words per caption (default: 8)\n  1290→ * - maxDuration: Max duration per caption in seconds (default: 5)\n  1291→ */\n  1292→app.post('/export/captions', requireCredits({ endpoint: 'export-captions' }), async (req, res) => {\n  1293→  const {\n  1294→    wavPath,\n  1295→    transcript: providedTranscript,\n  1296→    format = 'srt',\n  1297→    outputPath = null,\n  1298→    maxWordsPerCaption = 8,\n  1299→    maxDuration = 5\n  1300→  } = req.body;\n  1301→\n  1302→  if (!wavPath && !providedTranscript) {\n  1303→    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  1304→  }\n  1305→\n  1306→  if (wavPath && !(await fileExists(wavPath))) {\n  1307→    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  1308→  }\n  1309→\n  1310→  console.log(`[SPLICE] Caption export: ${wavPath || 'provided transcript'} -> ${format}`);\n  1311→\n  1312→  try {\n  1313→    // Get or create transcript with word-level timestamps\n  1314→    let transcript = providedTranscript;\n  1315→    if (!transcript && wavPath) {\n  1316→      transcript = await transcribeWithWords(wavPath);\n  1317→    }\n  1318→\n  1319→    const exportOptions = { maxWordsPerCaption, maxDuration };\n  1320→\n  1321→    // Generate caption content based on format\n  1322→    let content;\n  1323→    let mimeType;\n  1324→\n  1325→    switch (format.toLowerCase()) {\n  1326→      case 'srt':\n  1327→        content = toSRT(transcript, exportOptions);\n  1328→        mimeType = 'application/x-subrip';\n  1329→        break;\n  1330→      case 'vtt':\n  1331→      case 'webvtt':\n  1332→        content = toVTT(transcript, exportOptions);\n  1333→        mimeType = 'text/vtt';\n  1334→        break;\n  1335→      case 'txt':\n  1336→      case 'text':\n  1337→        content = toPlainText(transcript, { ...exportOptions, includeTimestamps: true });\n  1338→        mimeType = 'text/plain';\n  1339→        break;\n  1340→      case 'json':\n  1341→        content = toJSON(transcript);\n  1342→        mimeType = 'application/json';\n  1343→        break;\n  1344→      default:\n  1345→        return res.status(400).json({\n  1346→          error: `Unsupported format: ${format}`,\n  1347→          supportedFormats: getSupportedFormats()\n  1348→        });\n  1349→    }\n  1350→\n  1351→    // Save to file if outputPath provided\n  1352→    let savedPath = null;\n  1353→    if (outputPath) {\n  1354→      const result = await exportToFile(transcript, outputPath, format, exportOptions);\n  1355→      savedPath = result.path;\n  1356→      console.log(`[SPLICE] Saved captions to: ${savedPath}`);\n  1357→    }\n  1358→\n  1359→    // Deduct usage based on audio duration\n  1360→    const audioDuration = transcript.duration || 0;\n  1361→    let balance = null;\n  1362→    if (audioDuration > 0 && req.deductUsage) {\n  1363→      balance = await req.deductUsage(audioDuration);\n  1364→    }\n  1365→\n  1366→    res.json({\n  1367→      success: true,\n  1368→      format,\n  1369→      content,\n  1370→      mimeType,\n  1371→      savedPath,\n  1372→      wordCount: transcript.words?.length || 0,\n  1373→      duration: transcript.duration || 0,\n  1374→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n  1375→    });\n  1376→  } catch (err) {\n  1377→    console.error('[SPLICE] Caption export error:', err);\n  1378→    res.status(500).json({ error: err.message });\n  1379→  }\n  1380→});\n  1381→\n  1382→/**\n  1383→ * GET /export/formats - Get supported export formats\n  1384→ */\n  1385→app.get('/export/formats', (req, res) => {\n  1386→  res.json({\n  1387→    success: true,\n  1388→    formats: getSupportedFormats()\n  1389→  });\n  1390→});\n  1391→\n  1392→// =============================================================================\n  1393→// Multitrack/Multicam Analysis Routes\n  1394→// =============================================================================\n  1395→\n  1396→/**\n  1397→ * POST /multitrack - Analyze multiple audio tracks for multicam editing\n  1398→ *\n  1399→ * Analyzes audio levels across multiple tracks to determine optimal\n  1400→ * video angle selection based on who is speaking.\n  1401→ *\n  1402→ * Options:\n  1403→ * - audioPaths: Array of paths to audio files (one per speaker) - required\n  1404→ * - speakerNames: Array of speaker names (optional)\n  1405→ * - videoTrackMapping: Object mapping speaker index to video track { 0: 0, 1: 1 }\n  1406→ * - minShotDuration: Minimum seconds before next cut (default: 2.0)\n  1407→ * - switchingFrequency: How often to allow cuts 0-100 (default: 50)\n  1408→ * - wideShotEnabled: Enable wide shot detection (default: true)\n  1409→ * - wideShotPercentage: Target % of wide shots (default: 20)\n  1410→ * - wideShotTracks: Video track indices for wide shots\n  1411→ * - cutawayEnabled: Enable cutaway insertion (default: false)\n  1412→ * - cutawayTracks: Video track indices for cutaways\n  1413→ * - speakerBoosts: Per-speaker dB adjustments { \"Speaker 1\": 5 }\n  1414→ */\n  1415→app.post('/multitrack', requireCredits({ endpoint: 'multitrack' }), async (req, res) => {\n  1416→  const {\n  1417→    audioPaths,\n  1418→    speakerNames,\n  1419→    videoTrackMapping = {},\n  1420→    minShotDuration = 2.0,\n  1421→    switchingFrequency = 50,\n  1422→    wideShotEnabled = true,\n  1423→    wideShotPercentage = 20,\n  1424→    wideShotTracks = [],\n  1425→    cutawayEnabled = false,\n  1426→    cutawayTracks = [],\n  1427→    speakerBoosts = {},\n  1428→    frameRate = 30\n  1429→  } = req.body;\n  1430→\n  1431→  // Validate audioPaths\n  1432→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length === 0) {\n  1433→    return res.status(400).json({ error: 'audioPaths array is required (at least 1 path)' });\n  1434→  }\n  1435→\n  1436→  // Validate all files exist\n  1437→  for (const audioPath of audioPaths) {\n  1438→    if (!fs.existsSync(audioPath)) {\n  1439→      return res.status(404).json({ error: `File not found: ${audioPath}` });\n  1440→    }\n  1441→  }\n  1442→\n  1443→  // Check FFprobe availability\n  1444→  const ffprobeAvailable = await isFFprobeInstalled();\n  1445→  if (!ffprobeAvailable) {\n  1446→    return res.status(500).json({\n  1447→      error: 'FFprobe not installed. Run: brew install ffmpeg'\n  1448→    });\n  1449→  }\n  1450→\n  1451→  console.log(`[SPLICE] Multitrack analysis: ${audioPaths.length} track(s)`);\n  1452→\n  1453→  try {\n  1454→    const result = await analyzeMultitrack(audioPaths, {\n  1455→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n  1456→      videoTrackMapping,\n  1457→      minShotDuration,\n  1458→      switchingFrequency,\n  1459→      wideShotEnabled,\n  1460→      wideShotPercentage,\n  1461→      wideShotTracks,\n  1462→      cutawayEnabled,\n  1463→      cutawayTracks,\n  1464→      speakerBoosts,\n  1465→      frameRate\n  1466→    });\n  1467→\n  1468→    // Deduct usage based on total duration (use longest track)\n  1469→    const audioDuration = result.metadata?.totalDuration || 0;\n  1470→    let balance = null;\n  1471→    if (audioDuration > 0 && req.deductUsage) {\n  1472→      balance = await req.deductUsage(audioDuration);\n  1473→    }\n  1474→\n  1475→    res.json({\n  1476→      success: true,\n  1477→      ...result,\n  1478→      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n  1479→    });\n  1480→  } catch (err) {\n  1481→    console.error('[SPLICE] Multitrack analysis error:', err);\n  1482→    res.status(500).json({ error: err.message });\n  1483→  }\n  1484→});\n  1485→\n  1486→/**\n  1487→ * POST /multitrack/auto-balance - Auto-balance speaker screentime\n  1488→ *\n  1489→ * Automatically adjusts speaker boosts to achieve equal screentime distribution.\n  1490→ * Runs multiple iterations to find optimal parameters.\n  1491→ */\n  1492→app.post('/multitrack/auto-balance', requireCredits({ endpoint: 'multitrack-auto-balance' }), async (req, res) => {\n  1493→  const {\n  1494→    audioPaths,\n  1495→    speakerNames,\n  1496→    videoTrackMapping = {},\n  1497→    minShotDuration = 2.0,\n  1498→    switchingFrequency = 50,\n  1499→    wideShotEnabled = false, // Disable wide shots for balance calc\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"328fc6ff-040e-4c14-99ea-c7e0e2f28cf8","timestamp":"2025-12-26T07:40:29.633Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/server.js","content":"      result.removalSegments = result.removalSegments.filter(s => s.type !== 'stutter');\n    }\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Repetition detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /fillers - Detect filler words (um, uh, like, etc.)\n *\n * Transcribes audio and identifies filler words with timestamps.\n * Returns segments that can be cut or reviewed for removal.\n * Supports frame alignment for precise video editing.\n *\n * Options:\n * - wavPath: Path to audio file (required unless transcript provided)\n * - transcript: Pre-existing transcript with word-level timing (optional)\n * - customFillers: Additional filler words to detect (optional)\n * - frameRate: Frame rate for alignment (0 = no alignment, 24/30/60 etc.)\n * - paddingMs: Padding in ms around filler words for cleaner cuts (default 50)\n */\napp.post('/fillers', requireCredits({ endpoint: 'fillers' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    customFillers = [],\n    frameRate = 0, // Optional: align timestamps to frames\n    paddingMs = 50 // Padding around filler words for cleaner cuts\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Filler word detection: ${wavPath || 'provided transcript'}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript has words\n    if (!transcript || !transcript.words || transcript.words.length === 0) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data'\n      });\n    }\n\n    // Default filler words (common in English speech)\n    const defaultFillers = [\n      'um', 'uh', 'ah', 'er', 'eh',           // Hesitation sounds\n      'like', 'so', 'well', 'right',           // Discourse markers\n      'you know', 'i mean', 'basically',       // Filler phrases\n      'actually', 'literally', 'honestly',     // Overused qualifiers\n      'kind of', 'sort of', 'you see'          // Hedging phrases\n    ];\n\n    // Combine default + custom fillers (lowercase for matching)\n    const fillerSet = new Set([\n      ...defaultFillers,\n      ...customFillers.map(f => f.toLowerCase().trim())\n    ]);\n\n    // Detect filler words\n    const fillers = [];\n    const words = transcript.words;\n\n    for (let i = 0; i < words.length; i++) {\n      const word = words[i];\n      const normalizedWord = word.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim();\n\n      // Check single-word fillers\n      if (fillerSet.has(normalizedWord)) {\n        fillers.push({\n          word: word.word,\n          normalizedWord,\n          start: word.start,\n          end: word.end,\n          duration: word.end - word.start,\n          index: i,\n          type: 'filler'\n        });\n        continue;\n      }\n\n      // Check two-word phrases (e.g., \"you know\", \"kind of\")\n      if (i < words.length - 1) {\n        const nextWord = words[i + 1];\n        const twoWordPhrase = `${normalizedWord} ${nextWord.word.toLowerCase().replace(/[.,!?;:'\"]/g, '').trim()}`;\n        if (fillerSet.has(twoWordPhrase)) {\n          fillers.push({\n            word: `${word.word} ${nextWord.word}`,\n            normalizedWord: twoWordPhrase,\n            start: word.start,\n            end: nextWord.end,\n            duration: nextWord.end - word.start,\n            index: i,\n            type: 'filler_phrase'\n          });\n          // Skip next word since it's part of this phrase\n          i++;\n        }\n      }\n    }\n\n    // Apply frame alignment if requested\n    const hasFrameAlignment = frameRate > 0;\n    const paddingSec = paddingMs / 1000;\n\n    // Enhance fillers with aligned timestamps and padding\n    const enhancedFillers = fillers.map(f => {\n      // Add padding for cleaner cuts\n      const paddedStart = Math.max(0, f.start - paddingSec);\n      const paddedEnd = f.end + paddingSec;\n\n      // Apply frame alignment if requested\n      let startAligned = paddedStart;\n      let endAligned = paddedEnd;\n      if (hasFrameAlignment) {\n        startAligned = alignToFrameFloor(paddedStart, frameRate);\n        endAligned = alignToFrameCeil(paddedEnd, frameRate);\n      }\n\n      return {\n        ...f,\n        paddedStart,\n        paddedEnd,\n        startAligned: parseFloat(startAligned.toFixed(4)),\n        endAligned: parseFloat(endAligned.toFixed(4)),\n        durationAligned: parseFloat((endAligned - startAligned).toFixed(4))\n      };\n    });\n\n    // Calculate total filler time\n    const totalFillerDuration = fillers.reduce((sum, f) => sum + f.duration, 0);\n    const audioDuration = transcript.duration || (words.length > 0 ? words[words.length - 1].end : 0);\n    const fillerPercentage = audioDuration > 0 ? (totalFillerDuration / audioDuration) * 100 : 0;\n\n    // Deduct usage based on audio duration\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      fillers: enhancedFillers,\n      metadata: {\n        totalWords: words.length,\n        fillerCount: enhancedFillers.length,\n        totalFillerDuration: parseFloat(totalFillerDuration.toFixed(3)),\n        audioDuration: parseFloat(audioDuration.toFixed(3)),\n        fillerPercentage: parseFloat(fillerPercentage.toFixed(2)),\n        fillersPerMinute: audioDuration > 0 ? parseFloat((enhancedFillers.length / (audioDuration / 60)).toFixed(2)) : 0,\n        frameRate: hasFrameAlignment ? frameRate : null,\n        frameAligned: hasFrameAlignment,\n        paddingMs\n      },\n      removalSegments: enhancedFillers.map(f => ({\n        start: f.startAligned,\n        end: f.endAligned,\n        duration: f.durationAligned,\n        reason: `Filler: \"${f.word}\"`,\n        type: f.type\n      })),\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Filler detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /stutters - Detect single-word stutters only\n *\n * Focused detection for word-level stutters (e.g., \"I I I think\").\n * Faster than full repetition detection.\n */\napp.post('/stutters', requireCredits({ endpoint: 'stutters' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    options = {},\n    // Support both top-level and nested options for flexibility\n    minRepeats = options.minRepeats ?? 2,\n    maxGapMs = options.maxGapMs ?? 500,\n    ignoreFillers = options.ignoreFillers ?? true,\n    minWordLength = options.minWordLength ?? 1\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Stutter detection: ${wavPath || 'provided transcript'}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      // Use transcribeWithWords for word-level timestamps required by stutter detection\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    // Validate transcript exists and has words array\n    if (!transcript || !transcript.words) {\n      return res.status(400).json({\n        error: 'Transcript must contain word-level timing data'\n      });\n    }\n\n    // Empty transcript returns empty result (not an error)\n    if (transcript.words.length === 0) {\n      return res.json({\n        success: true,\n        stutters: [],\n        metadata: { type: 'stutters', totalWords: 0, stutterCount: 0, totalRepeatedWords: 0 }\n      });\n    }\n\n    // Detect stutters only\n    const result = detectStutters(transcript, {\n      minRepeats,\n      maxGapMs,\n      ignoreFillers,\n      minWordLength\n    });\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      wavPath,\n      ...result,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Stutter detection error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n// =============================================================================\n// Caption Export Routes\n// =============================================================================\n\n/**\n * POST /export/captions - Export transcript to caption format (SRT, VTT, etc.)\n *\n * Converts a transcript to the specified caption format.\n * Can optionally save to file.\n *\n * Options:\n * - wavPath: Path to audio file (to transcribe first)\n * - transcript: Pre-existing transcript with word-level timing\n * - format: Export format (srt, vtt, txt, json) - default: srt\n * - outputPath: Optional file path to save to\n * - maxWordsPerCaption: Max words per caption (default: 8)\n * - maxDuration: Max duration per caption in seconds (default: 5)\n */\napp.post('/export/captions', requireCredits({ endpoint: 'export-captions' }), async (req, res) => {\n  const {\n    wavPath,\n    transcript: providedTranscript,\n    format = 'srt',\n    outputPath = null,\n    maxWordsPerCaption = 8,\n    maxDuration = 5\n  } = req.body;\n\n  if (!wavPath && !providedTranscript) {\n    return res.status(400).json({ error: 'wavPath or transcript is required' });\n  }\n\n  if (wavPath && !(await fileExists(wavPath))) {\n    return res.status(404).json({ error: `File not found: ${wavPath}` });\n  }\n\n  console.log(`[SPLICE] Caption export: ${wavPath || 'provided transcript'} -> ${format}`);\n\n  try {\n    // Get or create transcript with word-level timestamps\n    let transcript = providedTranscript;\n    if (!transcript && wavPath) {\n      transcript = await transcribeWithWords(wavPath);\n    }\n\n    const exportOptions = { maxWordsPerCaption, maxDuration };\n\n    // Generate caption content based on format\n    let content;\n    let mimeType;\n\n    switch (format.toLowerCase()) {\n      case 'srt':\n        content = toSRT(transcript, exportOptions);\n        mimeType = 'application/x-subrip';\n        break;\n      case 'vtt':\n      case 'webvtt':\n        content = toVTT(transcript, exportOptions);\n        mimeType = 'text/vtt';\n        break;\n      case 'txt':\n      case 'text':\n        content = toPlainText(transcript, { ...exportOptions, includeTimestamps: true });\n        mimeType = 'text/plain';\n        break;\n      case 'json':\n        content = toJSON(transcript);\n        mimeType = 'application/json';\n        break;\n      default:\n        return res.status(400).json({\n          error: `Unsupported format: ${format}`,\n          supportedFormats: getSupportedFormats()\n        });\n    }\n\n    // Save to file if outputPath provided\n    let savedPath = null;\n    if (outputPath) {\n      const result = await exportToFile(transcript, outputPath, format, exportOptions);\n      savedPath = result.path;\n      console.log(`[SPLICE] Saved captions to: ${savedPath}`);\n    }\n\n    // Deduct usage based on audio duration\n    const audioDuration = transcript.duration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      format,\n      content,\n      mimeType,\n      savedPath,\n      wordCount: transcript.words?.length || 0,\n      duration: transcript.duration || 0,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Caption export error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * GET /export/formats - Get supported export formats\n */\napp.get('/export/formats', (req, res) => {\n  res.json({\n    success: true,\n    formats: getSupportedFormats()\n  });\n});\n\n// =============================================================================\n// Multitrack/Multicam Analysis Routes\n// =============================================================================\n\n/**\n * POST /multitrack - Analyze multiple audio tracks for multicam editing\n *\n * Analyzes audio levels across multiple tracks to determine optimal\n * video angle selection based on who is speaking.\n *\n * Options:\n * - audioPaths: Array of paths to audio files (one per speaker) - required\n * - speakerNames: Array of speaker names (optional)\n * - videoTrackMapping: Object mapping speaker index to video track { 0: 0, 1: 1 }\n * - minShotDuration: Minimum seconds before next cut (default: 2.0)\n * - switchingFrequency: How often to allow cuts 0-100 (default: 50)\n * - wideShotEnabled: Enable wide shot detection (default: true)\n * - wideShotPercentage: Target % of wide shots (default: 20)\n * - wideShotTracks: Video track indices for wide shots\n * - cutawayEnabled: Enable cutaway insertion (default: false)\n * - cutawayTracks: Video track indices for cutaways\n * - speakerBoosts: Per-speaker dB adjustments { \"Speaker 1\": 5 }\n */\napp.post('/multitrack', requireCredits({ endpoint: 'multitrack' }), async (req, res) => {\n  const {\n    audioPaths,\n    speakerNames,\n    videoTrackMapping = {},\n    minShotDuration = 2.0,\n    switchingFrequency = 50,\n    wideShotEnabled = true,\n    wideShotPercentage = 20,\n    wideShotTracks = [],\n    cutawayEnabled = false,\n    cutawayTracks = [],\n    speakerBoosts = {},\n    frameRate = 30\n  } = req.body;\n\n  // Validate audioPaths\n  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length === 0) {\n    return res.status(400).json({ error: 'audioPaths array is required (at least 1 path)' });\n  }\n\n  // Validate all files exist\n  for (const audioPath of audioPaths) {\n    if (!fs.existsSync(audioPath)) {\n      return res.status(404).json({ error: `File not found: ${audioPath}` });\n    }\n  }\n\n  // Check FFprobe availability\n  const ffprobeAvailable = await isFFprobeInstalled();\n  if (!ffprobeAvailable) {\n    return res.status(500).json({\n      error: 'FFprobe not installed. Run: brew install ffmpeg'\n    });\n  }\n\n  console.log(`[SPLICE] Multitrack analysis: ${audioPaths.length} track(s)`);\n\n  try {\n    const result = await analyzeMultitrack(audioPaths, {\n      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),\n      videoTrackMapping,\n      minShotDuration,\n      switchingFrequency,\n      wideShotEnabled,\n      wideShotPercentage,\n      wideShotTracks,\n      cutawayEnabled,\n      cutawayTracks,\n      speakerBoosts,\n      frameRate\n    });\n\n    // Deduct usage based on total duration (use longest track)\n    const audioDuration = result.metadata?.totalDuration || 0;\n    let balance = null;\n    if (audioDuration > 0 && req.deductUsage) {\n      balance = await req.deductUsage(audioDuration);\n    }\n\n    res.json({\n      success: true,\n      ...result,\n      balance: balance ? { hoursRemaining: balance.hoursRemaining, tier: balance.tier } : undefined\n    });\n  } catch (err) {\n    console.error('[SPLICE] Multitrack analysis error:', err);\n    res.status(500).json({ error: err.message });\n  }\n});\n\n/**\n * POST /multitrack/auto-balance - Auto-balance speaker screentime\n *\n * Automatically adjusts speaker boosts to achieve equal screentime distribution.\n * Runs multiple iterations to find optimal parameters.\n */\napp.post('/multitrack/auto-balance', requireCredits({ endpoint: 'multitrack-auto-balance' }), async (req, res) => {\n  const {\n    audioPaths,\n    speakerNames,\n    videoTrackMapping = {},\n    minShotDuration = 2.0,\n    switchingFrequency = 50,\n    wideShotEnabled = false, // Disable wide shots for balance calc","numLines":500,"startLine":1000,"totalLines":3719}}}
{"parentUuid":"328fc6ff-040e-4c14-99ea-c7e0e2f28cf8","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016EhoD5S69mZPPENKM2vStX","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Face Detection Service\n     3→ *\n     4→ * Detects and tracks faces in video frames for social reframe.\n     5→ * Uses lightweight frame sampling approach for performance.\n     6→ */\n     7→\n     8→const { spawn } = require('child_process');\n     9→const fs = require('fs');\n    10→const path = require('path');\n    11→const os = require('os');\n    12→\n    13→// ============================================================================\n    14→// FACE DETECTION\n    15→// ============================================================================\n    16→\n    17→/**\n    18→ * Detect faces in video using FFmpeg frame extraction\n    19→ * @param {string} videoPath - Path to video file\n    20→ * @param {Object} options - Detection options\n    21→ * @returns {Promise<Object>} Face detection results\n    22→ */\n    23→async function detectFaces(videoPath, options = {}) {\n    24→  const {\n    25→    sampleRate = 1, // Sample every N seconds\n    26→    maxFrames = 100,\n    27→    minConfidence = 0.5\n    28→  } = options;\n    29→\n    30→  if (!fs.existsSync(videoPath)) {\n    31→    return {\n    32→      success: false,\n    33→      error: `Video file not found: ${videoPath}`,\n    34→      faces: []\n    35→    };\n    36→  }\n    37→\n    38→  console.log(`[SPLICE Face] Analyzing video: ${videoPath}`);\n    39→\n    40→  try {\n    41→    // Get video info\n    42→    const videoInfo = await getVideoInfo(videoPath);\n    43→\n    44→    // Sample frames\n    45→    const frames = await sampleFrames(videoPath, {\n    46→      sampleRate,\n    47→      maxFrames,\n    48→      duration: videoInfo.duration\n    49→    });\n    50→\n    51→    // Analyze frames for faces using center-of-mass approach\n    52→    // (Simulated - in production would use face-api.js or similar)\n    53→    const faceData = analyzeFramesForFaces(frames, videoInfo);\n    54→\n    55→    // Generate face tracks\n    56→    const tracks = generateFaceTracks(faceData, videoInfo);\n    57→\n    58→    return {\n    59→      success: true,\n    60→      videoInfo: {\n    61→        width: videoInfo.width,\n    62→        height: videoInfo.height,\n    63→        duration: videoInfo.duration,\n    64→        frameRate: videoInfo.frameRate\n    65→      },\n    66→      facesDetected: tracks.length,\n    67→      tracks,\n    68→      metadata: {\n    69→        sampleRate,\n    70→        framesAnalyzed: frames.length,\n    71→        processingTime: Date.now()\n    72→      }\n    73→    };\n    74→\n    75→  } catch (err) {\n    76→    console.error('[SPLICE Face] Detection error:', err);\n    77→    return {\n    78→      success: false,\n    79→      error: err.message,\n    80→      faces: []\n    81→    };\n    82→  }\n    83→}\n    84→\n    85→/**\n    86→ * Track faces throughout video\n    87→ * @param {string} videoPath - Path to video\n    88→ * @param {Object} options - Tracking options\n    89→ * @returns {Promise<Object>} Face tracking data\n    90→ */\n    91→async function trackFaces(videoPath, options = {}) {\n    92→  const {\n    93→    sampleRate = 0.5, // Sample every 0.5 seconds for smoother tracking\n    94→    smoothing = 0.3 // Smoothing factor for position interpolation\n    95→  } = options;\n    96→\n    97→  const detection = await detectFaces(videoPath, { sampleRate });\n    98→\n    99→  if (!detection.success) {\n   100→    return detection;\n   101→  }\n   102→\n   103→  // Apply smoothing to face positions\n   104→  const smoothedTracks = detection.tracks.map(track => ({\n   105→    ...track,\n   106→    positions: smoothPositions(track.positions, smoothing)\n   107→  }));\n   108→\n   109→  return {\n   110→    success: true,\n   111→    videoInfo: detection.videoInfo,\n   112→    tracks: smoothedTracks,\n   113→    totalFrames: Math.ceil(detection.videoInfo.duration * detection.videoInfo.frameRate),\n   114→    metadata: {\n   115→      ...detection.metadata,\n   116→      smoothing\n   117→    }\n   118→  };\n   119→}\n   120→\n   121→/**\n   122→ * Identify primary speaker based on audio/face correlation\n   123→ * @param {Array} faces - Face track data\n   124→ * @param {Object} audioAnalysis - Audio analysis with speaker segments\n   125→ * @returns {Object} Speaker identification\n   126→ */\n   127→function identifySpeaker(faces, audioAnalysis = null) {\n   128→  if (!faces || faces.length === 0) {\n   129→    return {\n   130→      success: false,\n   131→      error: 'No faces to identify',\n   132→      primarySpeaker: null\n   133→    };\n   134→  }\n   135→\n   136→  // Without audio, use largest/most-centered face as primary\n   137→  if (!audioAnalysis) {\n   138→    const scored = faces.map((face, index) => {\n   139→      // Score based on size and center position\n   140→      const avgSize = face.positions.reduce((sum, p) => sum + (p.width * p.height), 0) / face.positions.length;\n   141→      const avgCenterX = face.positions.reduce((sum, p) => sum + (p.x + p.width / 2), 0) / face.positions.length;\n   142→      const centerScore = 1 - Math.abs(avgCenterX - 0.5); // 0.5 = center\n   143→\n   144→      return {\n   145→        faceIndex: index,\n   146→        trackId: face.trackId,\n   147→        score: avgSize * centerScore,\n   148→        avgSize,\n   149→        centerScore\n   150→      };\n   151→    });\n   152→\n   153→    scored.sort((a, b) => b.score - a.score);\n   154→\n   155→    return {\n   156→      success: true,\n   157→      primarySpeaker: scored[0]?.trackId || null,\n   158→      rankings: scored,\n   159→      method: 'size-center'\n   160→    };\n   161→  }\n   162→\n   163→  // With audio, correlate face movement with speech\n   164→  // (Simplified - full implementation would analyze lip movement)\n   165→  return {\n   166→    success: true,\n   167→    primarySpeaker: faces[0]?.trackId,\n   168→    method: 'audio-correlation',\n   169→    note: 'Audio correlation requires additional processing'\n   170→  };\n   171→}\n   172→\n   173→// ============================================================================\n   174→// HELPER FUNCTIONS\n   175→// ============================================================================\n   176→\n   177→/**\n   178→ * Get video information using FFprobe\n   179→ */\n   180→async function getVideoInfo(videoPath) {\n   181→  return new Promise((resolve, reject) => {\n   182→    const ffprobe = spawn('ffprobe', [\n   183→      '-v', 'quiet',\n   184→      '-print_format', 'json',\n   185→      '-show_format',\n   186→      '-show_streams',\n   187→      videoPath\n   188→    ]);\n   189→\n   190→    let output = '';\n   191→    let error = '';\n   192→\n   193→    ffprobe.stdout.on('data', data => output += data);\n   194→    ffprobe.stderr.on('data', data => error += data);\n   195→\n   196→    ffprobe.on('close', code => {\n   197→      if (code !== 0) {\n   198→        // Return defaults if ffprobe fails\n   199→        resolve({\n   200→          width: 1920,\n   201→          height: 1080,\n   202→          duration: 60,\n   203→          frameRate: 30\n   204→        });\n   205→        return;\n   206→      }\n   207→\n   208→      try {\n   209→        const info = JSON.parse(output);\n   210→        const videoStream = info.streams?.find(s => s.codec_type === 'video') || {};\n   211→        const format = info.format || {};\n   212→\n   213→        const frameRateStr = videoStream.r_frame_rate || '30/1';\n   214→        const [num, den] = frameRateStr.split('/').map(Number);\n   215→        const frameRate = den ? num / den : 30;\n   216→\n   217→        resolve({\n   218→          width: videoStream.width || 1920,\n   219→          height: videoStream.height || 1080,\n   220→          duration: parseFloat(format.duration) || 60,\n   221→          frameRate: Math.round(frameRate)\n   222→        });\n   223→      } catch (e) {\n   224→        resolve({\n   225→          width: 1920,\n   226→          height: 1080,\n   227→          duration: 60,\n   228→          frameRate: 30\n   229→        });\n   230→      }\n   231→    });\n   232→  });\n   233→}\n   234→\n   235→/**\n   236→ * Sample frames from video\n   237→ */\n   238→async function sampleFrames(videoPath, options) {\n   239→  const { sampleRate, maxFrames, duration } = options;\n   240→\n   241→  const frameCount = Math.min(maxFrames, Math.floor(duration / sampleRate));\n   242→  const frames = [];\n   243→\n   244→  for (let i = 0; i < frameCount; i++) {\n   245→    const timestamp = i * sampleRate;\n   246→    frames.push({\n   247→      index: i,\n   248→      timestamp,\n   249→      // In production, would extract actual frame data\n   250→      // For now, simulate frame info\n   251→      analyzed: true\n   252→    });\n   253→  }\n   254→\n   255→  return frames;\n   256→}\n   257→\n   258→/**\n   259→ * Analyze frames for face positions\n   260→ * Simulated face detection - in production would use face-api.js or ML model\n   261→ */\n   262→function analyzeFramesForFaces(frames, videoInfo) {\n   263→  const faceData = [];\n   264→\n   265→  // Simulate detecting a centered face (typical talking-head scenario)\n   266→  const centerX = 0.5;\n   267→  const centerY = 0.4; // Slightly above center\n   268→  const faceWidth = 0.25; // 25% of frame width\n   269→  const faceHeight = 0.35; // 35% of frame height\n   270→\n   271→  frames.forEach((frame, index) => {\n   272→    // Add slight movement variation\n   273→    const jitterX = (Math.random() - 0.5) * 0.02;\n   274→    const jitterY = (Math.random() - 0.5) * 0.02;\n   275→\n   276→    faceData.push({\n   277→      frameIndex: index,\n   278→      timestamp: frame.timestamp,\n   279→      faces: [{\n   280→        x: centerX - faceWidth / 2 + jitterX,\n   281→        y: centerY - faceHeight / 2 + jitterY,\n   282→        width: faceWidth,\n   283→        height: faceHeight,\n   284→        confidence: 0.95\n   285→      }]\n   286→    });\n   287→  });\n   288→\n   289→  return faceData;\n   290→}\n   291→\n   292→/**\n   293→ * Generate face tracks from frame-by-frame detections\n   294→ */\n   295→function generateFaceTracks(faceData, videoInfo) {\n   296→  if (faceData.length === 0) return [];\n   297→\n   298→  // Simple single-track approach (assumes one primary face)\n   299→  const track = {\n   300→    trackId: 'face_0',\n   301→    startTime: faceData[0].timestamp,\n   302→    endTime: faceData[faceData.length - 1].timestamp,\n   303→    positions: faceData.map(fd => ({\n   304→      timestamp: fd.timestamp,\n   305→      ...fd.faces[0]\n   306→    })),\n   307→    averageConfidence: faceData.reduce((sum, fd) =>\n   308→      sum + (fd.faces[0]?.confidence || 0), 0) / faceData.length\n   309→  };\n   310→\n   311→  return [track];\n   312→}\n   313→\n   314→/**\n   315→ * Smooth face positions using exponential moving average\n   316→ */\n   317→function smoothPositions(positions, smoothing) {\n   318→  if (positions.length < 2) return positions;\n   319→\n   320→  const smoothed = [positions[0]];\n   321→\n   322→  for (let i = 1; i < positions.length; i++) {\n   323→    const prev = smoothed[i - 1];\n   324→    const curr = positions[i];\n   325→\n   326→    smoothed.push({\n   327→      timestamp: curr.timestamp,\n   328→      x: prev.x * smoothing + curr.x * (1 - smoothing),\n   329→      y: prev.y * smoothing + curr.y * (1 - smoothing),\n   330→      width: prev.width * smoothing + curr.width * (1 - smoothing),\n   331→      height: prev.height * smoothing + curr.height * (1 - smoothing),\n   332→      confidence: curr.confidence\n   333→    });\n   334→  }\n   335→\n   336→  return smoothed;\n   337→}\n   338→\n   339→/**\n   340→ * Get face bounding box at specific time\n   341→ */\n   342→function getFaceAtTime(track, timestamp) {\n   343→  if (!track || !track.positions || track.positions.length === 0) {\n   344→    return null;\n   345→  }\n   346→\n   347→  // Find closest position\n   348→  let closest = track.positions[0];\n   349→  let minDiff = Math.abs(timestamp - closest.timestamp);\n   350→\n   351→  for (const pos of track.positions) {\n   352→    const diff = Math.abs(timestamp - pos.timestamp);\n   353→    if (diff < minDiff) {\n   354→      minDiff = diff;\n   355→      closest = pos;\n   356→    }\n   357→  }\n   358→\n   359→  return closest;\n   360→}\n   361→\n   362→/**\n   363→ * Calculate optimal crop region for aspect ratio\n   364→ */\n   365→function calculateCropRegion(facePosition, targetAspect, videoInfo, padding = 0.2) {\n   366→  if (!facePosition) {\n   367→    // Default center crop\n   368→    return calculateCenterCrop(targetAspect, videoInfo);\n   369→  }\n   370→\n   371→  const { width: videoWidth, height: videoHeight } = videoInfo;\n   372→  const targetHeight = videoHeight;\n   373→  const targetWidth = targetHeight * targetAspect;\n   374→\n   375→  // Face center in pixels\n   376→  const faceCenterX = (facePosition.x + facePosition.width / 2) * videoWidth;\n   377→  const faceCenterY = (facePosition.y + facePosition.height / 2) * videoHeight;\n   378→\n   379→  // Calculate crop region centered on face\n   380→  let cropX = faceCenterX - targetWidth / 2;\n   381→  let cropY = faceCenterY - targetHeight * 0.4; // Keep face in upper portion\n   382→\n   383→  // Apply bounds\n   384→  cropX = Math.max(0, Math.min(cropX, videoWidth - targetWidth));\n   385→  cropY = Math.max(0, Math.min(cropY, videoHeight - targetHeight));\n   386→\n   387→  return {\n   388→    x: cropX / videoWidth,\n   389→    y: cropY / videoHeight,\n   390→    width: targetWidth / videoWidth,\n   391→    height: targetHeight / videoHeight,\n   392→    pixelX: Math.round(cropX),\n   393→    pixelY: Math.round(cropY),\n   394→    pixelWidth: Math.round(targetWidth),\n   395→    pixelHeight: Math.round(targetHeight)\n   396→  };\n   397→}\n   398→\n   399→/**\n   400→ * Calculate center crop for aspect ratio\n   401→ */\n   402→function calculateCenterCrop(targetAspect, videoInfo) {\n   403→  const { width: videoWidth, height: videoHeight } = videoInfo;\n   404→  const videoAspect = videoWidth / videoHeight;\n   405→\n   406→  let cropWidth, cropHeight, cropX, cropY;\n   407→\n   408→  if (targetAspect < videoAspect) {\n   409→    // Target is narrower, crop sides\n   410→    cropHeight = videoHeight;\n   411→    cropWidth = cropHeight * targetAspect;\n   412→    cropY = 0;\n   413→    cropX = (videoWidth - cropWidth) / 2;\n   414→  } else {\n   415→    // Target is wider, crop top/bottom\n   416→    cropWidth = videoWidth;\n   417→    cropHeight = cropWidth / targetAspect;\n   418→    cropX = 0;\n   419→    cropY = (videoHeight - cropHeight) / 2;\n   420→  }\n   421→\n   422→  return {\n   423→    x: cropX / videoWidth,\n   424→    y: cropY / videoHeight,\n   425→    width: cropWidth / videoWidth,\n   426→    height: cropHeight / videoHeight,\n   427→    pixelX: Math.round(cropX),\n   428→    pixelY: Math.round(cropY),\n   429→    pixelWidth: Math.round(cropWidth),\n   430→    pixelHeight: Math.round(cropHeight)\n   431→  };\n   432→}\n   433→\n   434→// ============================================================================\n   435→// EXPORTS\n   436→// ============================================================================\n   437→\n   438→module.exports = {\n   439→  detectFaces,\n   440→  trackFaces,\n   441→  identifySpeaker,\n   442→  getFaceAtTime,\n   443→  calculateCropRegion,\n   444→  calculateCenterCrop,\n   445→  // Helpers for testing\n   446→  getVideoInfo,\n   447→  smoothPositions,\n   448→  generateFaceTracks\n   449→};\n   450→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"07118f47-089e-468e-ad1a-57779cb709e9","timestamp":"2025-12-26T07:40:29.633Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/faceDetection.js","content":"/**\n * SPLICE Face Detection Service\n *\n * Detects and tracks faces in video frames for social reframe.\n * Uses lightweight frame sampling approach for performance.\n */\n\nconst { spawn } = require('child_process');\nconst fs = require('fs');\nconst path = require('path');\nconst os = require('os');\n\n// ============================================================================\n// FACE DETECTION\n// ============================================================================\n\n/**\n * Detect faces in video using FFmpeg frame extraction\n * @param {string} videoPath - Path to video file\n * @param {Object} options - Detection options\n * @returns {Promise<Object>} Face detection results\n */\nasync function detectFaces(videoPath, options = {}) {\n  const {\n    sampleRate = 1, // Sample every N seconds\n    maxFrames = 100,\n    minConfidence = 0.5\n  } = options;\n\n  if (!fs.existsSync(videoPath)) {\n    return {\n      success: false,\n      error: `Video file not found: ${videoPath}`,\n      faces: []\n    };\n  }\n\n  console.log(`[SPLICE Face] Analyzing video: ${videoPath}`);\n\n  try {\n    // Get video info\n    const videoInfo = await getVideoInfo(videoPath);\n\n    // Sample frames\n    const frames = await sampleFrames(videoPath, {\n      sampleRate,\n      maxFrames,\n      duration: videoInfo.duration\n    });\n\n    // Analyze frames for faces using center-of-mass approach\n    // (Simulated - in production would use face-api.js or similar)\n    const faceData = analyzeFramesForFaces(frames, videoInfo);\n\n    // Generate face tracks\n    const tracks = generateFaceTracks(faceData, videoInfo);\n\n    return {\n      success: true,\n      videoInfo: {\n        width: videoInfo.width,\n        height: videoInfo.height,\n        duration: videoInfo.duration,\n        frameRate: videoInfo.frameRate\n      },\n      facesDetected: tracks.length,\n      tracks,\n      metadata: {\n        sampleRate,\n        framesAnalyzed: frames.length,\n        processingTime: Date.now()\n      }\n    };\n\n  } catch (err) {\n    console.error('[SPLICE Face] Detection error:', err);\n    return {\n      success: false,\n      error: err.message,\n      faces: []\n    };\n  }\n}\n\n/**\n * Track faces throughout video\n * @param {string} videoPath - Path to video\n * @param {Object} options - Tracking options\n * @returns {Promise<Object>} Face tracking data\n */\nasync function trackFaces(videoPath, options = {}) {\n  const {\n    sampleRate = 0.5, // Sample every 0.5 seconds for smoother tracking\n    smoothing = 0.3 // Smoothing factor for position interpolation\n  } = options;\n\n  const detection = await detectFaces(videoPath, { sampleRate });\n\n  if (!detection.success) {\n    return detection;\n  }\n\n  // Apply smoothing to face positions\n  const smoothedTracks = detection.tracks.map(track => ({\n    ...track,\n    positions: smoothPositions(track.positions, smoothing)\n  }));\n\n  return {\n    success: true,\n    videoInfo: detection.videoInfo,\n    tracks: smoothedTracks,\n    totalFrames: Math.ceil(detection.videoInfo.duration * detection.videoInfo.frameRate),\n    metadata: {\n      ...detection.metadata,\n      smoothing\n    }\n  };\n}\n\n/**\n * Identify primary speaker based on audio/face correlation\n * @param {Array} faces - Face track data\n * @param {Object} audioAnalysis - Audio analysis with speaker segments\n * @returns {Object} Speaker identification\n */\nfunction identifySpeaker(faces, audioAnalysis = null) {\n  if (!faces || faces.length === 0) {\n    return {\n      success: false,\n      error: 'No faces to identify',\n      primarySpeaker: null\n    };\n  }\n\n  // Without audio, use largest/most-centered face as primary\n  if (!audioAnalysis) {\n    const scored = faces.map((face, index) => {\n      // Score based on size and center position\n      const avgSize = face.positions.reduce((sum, p) => sum + (p.width * p.height), 0) / face.positions.length;\n      const avgCenterX = face.positions.reduce((sum, p) => sum + (p.x + p.width / 2), 0) / face.positions.length;\n      const centerScore = 1 - Math.abs(avgCenterX - 0.5); // 0.5 = center\n\n      return {\n        faceIndex: index,\n        trackId: face.trackId,\n        score: avgSize * centerScore,\n        avgSize,\n        centerScore\n      };\n    });\n\n    scored.sort((a, b) => b.score - a.score);\n\n    return {\n      success: true,\n      primarySpeaker: scored[0]?.trackId || null,\n      rankings: scored,\n      method: 'size-center'\n    };\n  }\n\n  // With audio, correlate face movement with speech\n  // (Simplified - full implementation would analyze lip movement)\n  return {\n    success: true,\n    primarySpeaker: faces[0]?.trackId,\n    method: 'audio-correlation',\n    note: 'Audio correlation requires additional processing'\n  };\n}\n\n// ============================================================================\n// HELPER FUNCTIONS\n// ============================================================================\n\n/**\n * Get video information using FFprobe\n */\nasync function getVideoInfo(videoPath) {\n  return new Promise((resolve, reject) => {\n    const ffprobe = spawn('ffprobe', [\n      '-v', 'quiet',\n      '-print_format', 'json',\n      '-show_format',\n      '-show_streams',\n      videoPath\n    ]);\n\n    let output = '';\n    let error = '';\n\n    ffprobe.stdout.on('data', data => output += data);\n    ffprobe.stderr.on('data', data => error += data);\n\n    ffprobe.on('close', code => {\n      if (code !== 0) {\n        // Return defaults if ffprobe fails\n        resolve({\n          width: 1920,\n          height: 1080,\n          duration: 60,\n          frameRate: 30\n        });\n        return;\n      }\n\n      try {\n        const info = JSON.parse(output);\n        const videoStream = info.streams?.find(s => s.codec_type === 'video') || {};\n        const format = info.format || {};\n\n        const frameRateStr = videoStream.r_frame_rate || '30/1';\n        const [num, den] = frameRateStr.split('/').map(Number);\n        const frameRate = den ? num / den : 30;\n\n        resolve({\n          width: videoStream.width || 1920,\n          height: videoStream.height || 1080,\n          duration: parseFloat(format.duration) || 60,\n          frameRate: Math.round(frameRate)\n        });\n      } catch (e) {\n        resolve({\n          width: 1920,\n          height: 1080,\n          duration: 60,\n          frameRate: 30\n        });\n      }\n    });\n  });\n}\n\n/**\n * Sample frames from video\n */\nasync function sampleFrames(videoPath, options) {\n  const { sampleRate, maxFrames, duration } = options;\n\n  const frameCount = Math.min(maxFrames, Math.floor(duration / sampleRate));\n  const frames = [];\n\n  for (let i = 0; i < frameCount; i++) {\n    const timestamp = i * sampleRate;\n    frames.push({\n      index: i,\n      timestamp,\n      // In production, would extract actual frame data\n      // For now, simulate frame info\n      analyzed: true\n    });\n  }\n\n  return frames;\n}\n\n/**\n * Analyze frames for face positions\n * Simulated face detection - in production would use face-api.js or ML model\n */\nfunction analyzeFramesForFaces(frames, videoInfo) {\n  const faceData = [];\n\n  // Simulate detecting a centered face (typical talking-head scenario)\n  const centerX = 0.5;\n  const centerY = 0.4; // Slightly above center\n  const faceWidth = 0.25; // 25% of frame width\n  const faceHeight = 0.35; // 35% of frame height\n\n  frames.forEach((frame, index) => {\n    // Add slight movement variation\n    const jitterX = (Math.random() - 0.5) * 0.02;\n    const jitterY = (Math.random() - 0.5) * 0.02;\n\n    faceData.push({\n      frameIndex: index,\n      timestamp: frame.timestamp,\n      faces: [{\n        x: centerX - faceWidth / 2 + jitterX,\n        y: centerY - faceHeight / 2 + jitterY,\n        width: faceWidth,\n        height: faceHeight,\n        confidence: 0.95\n      }]\n    });\n  });\n\n  return faceData;\n}\n\n/**\n * Generate face tracks from frame-by-frame detections\n */\nfunction generateFaceTracks(faceData, videoInfo) {\n  if (faceData.length === 0) return [];\n\n  // Simple single-track approach (assumes one primary face)\n  const track = {\n    trackId: 'face_0',\n    startTime: faceData[0].timestamp,\n    endTime: faceData[faceData.length - 1].timestamp,\n    positions: faceData.map(fd => ({\n      timestamp: fd.timestamp,\n      ...fd.faces[0]\n    })),\n    averageConfidence: faceData.reduce((sum, fd) =>\n      sum + (fd.faces[0]?.confidence || 0), 0) / faceData.length\n  };\n\n  return [track];\n}\n\n/**\n * Smooth face positions using exponential moving average\n */\nfunction smoothPositions(positions, smoothing) {\n  if (positions.length < 2) return positions;\n\n  const smoothed = [positions[0]];\n\n  for (let i = 1; i < positions.length; i++) {\n    const prev = smoothed[i - 1];\n    const curr = positions[i];\n\n    smoothed.push({\n      timestamp: curr.timestamp,\n      x: prev.x * smoothing + curr.x * (1 - smoothing),\n      y: prev.y * smoothing + curr.y * (1 - smoothing),\n      width: prev.width * smoothing + curr.width * (1 - smoothing),\n      height: prev.height * smoothing + curr.height * (1 - smoothing),\n      confidence: curr.confidence\n    });\n  }\n\n  return smoothed;\n}\n\n/**\n * Get face bounding box at specific time\n */\nfunction getFaceAtTime(track, timestamp) {\n  if (!track || !track.positions || track.positions.length === 0) {\n    return null;\n  }\n\n  // Find closest position\n  let closest = track.positions[0];\n  let minDiff = Math.abs(timestamp - closest.timestamp);\n\n  for (const pos of track.positions) {\n    const diff = Math.abs(timestamp - pos.timestamp);\n    if (diff < minDiff) {\n      minDiff = diff;\n      closest = pos;\n    }\n  }\n\n  return closest;\n}\n\n/**\n * Calculate optimal crop region for aspect ratio\n */\nfunction calculateCropRegion(facePosition, targetAspect, videoInfo, padding = 0.2) {\n  if (!facePosition) {\n    // Default center crop\n    return calculateCenterCrop(targetAspect, videoInfo);\n  }\n\n  const { width: videoWidth, height: videoHeight } = videoInfo;\n  const targetHeight = videoHeight;\n  const targetWidth = targetHeight * targetAspect;\n\n  // Face center in pixels\n  const faceCenterX = (facePosition.x + facePosition.width / 2) * videoWidth;\n  const faceCenterY = (facePosition.y + facePosition.height / 2) * videoHeight;\n\n  // Calculate crop region centered on face\n  let cropX = faceCenterX - targetWidth / 2;\n  let cropY = faceCenterY - targetHeight * 0.4; // Keep face in upper portion\n\n  // Apply bounds\n  cropX = Math.max(0, Math.min(cropX, videoWidth - targetWidth));\n  cropY = Math.max(0, Math.min(cropY, videoHeight - targetHeight));\n\n  return {\n    x: cropX / videoWidth,\n    y: cropY / videoHeight,\n    width: targetWidth / videoWidth,\n    height: targetHeight / videoHeight,\n    pixelX: Math.round(cropX),\n    pixelY: Math.round(cropY),\n    pixelWidth: Math.round(targetWidth),\n    pixelHeight: Math.round(targetHeight)\n  };\n}\n\n/**\n * Calculate center crop for aspect ratio\n */\nfunction calculateCenterCrop(targetAspect, videoInfo) {\n  const { width: videoWidth, height: videoHeight } = videoInfo;\n  const videoAspect = videoWidth / videoHeight;\n\n  let cropWidth, cropHeight, cropX, cropY;\n\n  if (targetAspect < videoAspect) {\n    // Target is narrower, crop sides\n    cropHeight = videoHeight;\n    cropWidth = cropHeight * targetAspect;\n    cropY = 0;\n    cropX = (videoWidth - cropWidth) / 2;\n  } else {\n    // Target is wider, crop top/bottom\n    cropWidth = videoWidth;\n    cropHeight = cropWidth / targetAspect;\n    cropX = 0;\n    cropY = (videoHeight - cropHeight) / 2;\n  }\n\n  return {\n    x: cropX / videoWidth,\n    y: cropY / videoHeight,\n    width: cropWidth / videoWidth,\n    height: cropHeight / videoHeight,\n    pixelX: Math.round(cropX),\n    pixelY: Math.round(cropY),\n    pixelWidth: Math.round(cropWidth),\n    pixelHeight: Math.round(cropHeight)\n  };\n}\n\n// ============================================================================\n// EXPORTS\n// ============================================================================\n\nmodule.exports = {\n  detectFaces,\n  trackFaces,\n  identifySpeaker,\n  getFaceAtTime,\n  calculateCropRegion,\n  calculateCenterCrop,\n  // Helpers for testing\n  getVideoInfo,\n  smoothPositions,\n  generateFaceTracks\n};\n","numLines":450,"startLine":1,"totalLines":450}}}
{"parentUuid":"07118f47-089e-468e-ad1a-57779cb709e9","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BpUrW8t4xkbx2KJaw742L5","type":"tool_result","content":"     1→/**\n     2→ * SPLICE Social Reframe Service\n     3→ *\n     4→ * Auto-crops video to vertical/square formats with face tracking.\n     5→ * Supports TikTok (9:16), Instagram (1:1, 4:5), YouTube Shorts.\n     6→ */\n     7→\n     8→const { trackFaces, identifySpeaker, calculateCropRegion, calculateCenterCrop } = require('./faceDetection');\n     9→const fs = require('fs');\n    10→const path = require('path');\n    11→\n    12→// ============================================================================\n    13→// ASPECT RATIO PRESETS\n    14→// ============================================================================\n    15→\n    16→const ASPECT_RATIOS = {\n    17→  portrait: {\n    18→    id: 'portrait',\n    19→    name: 'Portrait (9:16)',\n    20→    ratio: 9 / 16,\n    21→    width: 1080,\n    22→    height: 1920,\n    23→    platforms: ['tiktok', 'reels', 'shorts']\n    24→  },\n    25→  square: {\n    26→    id: 'square',\n    27→    name: 'Square (1:1)',\n    28→    ratio: 1,\n    29→    width: 1080,\n    30→    height: 1080,\n    31→    platforms: ['instagram_feed', 'linkedin']\n    32→  },\n    33→  portrait_4_5: {\n    34→    id: 'portrait_4_5',\n    35→    name: 'Portrait (4:5)',\n    36→    ratio: 4 / 5,\n    37→    width: 1080,\n    38→    height: 1350,\n    39→    platforms: ['instagram_portrait']\n    40→  },\n    41→  landscape: {\n    42→    id: 'landscape',\n    43→    name: 'Landscape (16:9)',\n    44→    ratio: 16 / 9,\n    45→    width: 1920,\n    46→    height: 1080,\n    47→    platforms: ['youtube', 'twitter']\n    48→  }\n    49→};\n    50→\n    51→// ============================================================================\n    52→// PLATFORM PRESETS\n    53→// ============================================================================\n    54→\n    55→const PLATFORM_PRESETS = {\n    56→  tiktok: {\n    57→    id: 'tiktok',\n    58→    name: 'TikTok',\n    59→    aspectRatio: 'portrait',\n    60→    safeZone: { top: 0.15, bottom: 0.2, left: 0.05, right: 0.05 },\n    61→    maxDuration: 180\n    62→  },\n    63→  reels: {\n    64→    id: 'reels',\n    65→    name: 'Instagram Reels',\n    66→    aspectRatio: 'portrait',\n    67→    safeZone: { top: 0.1, bottom: 0.15, left: 0.05, right: 0.05 },\n    68→    maxDuration: 90\n    69→  },\n    70→  shorts: {\n    71→    id: 'shorts',\n    72→    name: 'YouTube Shorts',\n    73→    aspectRatio: 'portrait',\n    74→    safeZone: { top: 0.1, bottom: 0.12, left: 0.05, right: 0.05 },\n    75→    maxDuration: 60\n    76→  },\n    77→  instagram_feed: {\n    78→    id: 'instagram_feed',\n    79→    name: 'Instagram Feed',\n    80→    aspectRatio: 'square',\n    81→    safeZone: { top: 0.05, bottom: 0.05, left: 0.05, right: 0.05 },\n    82→    maxDuration: 60\n    83→  },\n    84→  instagram_portrait: {\n    85→    id: 'instagram_portrait',\n    86→    name: 'Instagram Portrait',\n    87→    aspectRatio: 'portrait_4_5',\n    88→    safeZone: { top: 0.05, bottom: 0.1, left: 0.05, right: 0.05 },\n    89→    maxDuration: 60\n    90→  },\n    91→  linkedin: {\n    92→    id: 'linkedin',\n    93→    name: 'LinkedIn',\n    94→    aspectRatio: 'square',\n    95→    safeZone: { top: 0.05, bottom: 0.08, left: 0.05, right: 0.05 },\n    96→    maxDuration: 600\n    97→  }\n    98→};\n    99→\n   100→// ============================================================================\n   101→// MAIN FUNCTIONS\n   102→// ============================================================================\n   103→\n   104→/**\n   105→ * Analyze video for reframe possibilities\n   106→ * @param {string} videoPath - Path to video file\n   107→ * @param {Object} options - Analysis options\n   108→ * @returns {Promise<Object>} Analysis results with face positions\n   109→ */\n   110→async function analyzeForReframe(videoPath, options = {}) {\n   111→  const {\n   112→    sampleRate = 0.5,\n   113→    smoothing = 0.3\n   114→  } = options;\n   115→\n   116→  if (!fs.existsSync(videoPath)) {\n   117→    return {\n   118→      success: false,\n   119→      error: `Video file not found: ${videoPath}`\n   120→    };\n   121→  }\n   122→\n   123→  console.log(`[SPLICE Reframe] Analyzing: ${videoPath}`);\n   124→\n   125→  try {\n   126→    // Track faces in video\n   127→    const tracking = await trackFaces(videoPath, { sampleRate, smoothing });\n   128→\n   129→    if (!tracking.success) {\n   130→      return tracking;\n   131→    }\n   132→\n   133→    // Identify primary speaker\n   134→    const speaker = identifySpeaker(tracking.tracks);\n   135→\n   136→    // Calculate suggested crops for each aspect ratio\n   137→    const suggestions = {};\n   138→    for (const [key, preset] of Object.entries(ASPECT_RATIOS)) {\n   139→      suggestions[key] = calculateReframeCrops(tracking, preset.ratio);\n   140→    }\n   141→\n   142→    return {\n   143→      success: true,\n   144→      videoInfo: tracking.videoInfo,\n   145→      faceTracking: {\n   146→        tracksFound: tracking.tracks.length,\n   147→        primarySpeaker: speaker.primarySpeaker,\n   148→        tracks: tracking.tracks\n   149→      },\n   150→      suggestions,\n   151→      metadata: {\n   152→        analyzedAt: new Date().toISOString(),\n   153→        sampleRate,\n   154→        smoothing\n   155→      }\n   156→    };\n   157→\n   158→  } catch (err) {\n   159→    console.error('[SPLICE Reframe] Analysis error:', err);\n   160→    return {\n   161→      success: false,\n   162→      error: err.message\n   163→    };\n   164→  }\n   165→}\n   166→\n   167→/**\n   168→ * Calculate crop positions for target aspect ratio\n   169→ * @param {Object} tracking - Face tracking data\n   170→ * @param {number} targetAspect - Target aspect ratio\n   171→ * @returns {Object} Crop positions over time\n   172→ */\n   173→function calculateReframeCrops(tracking, targetAspect) {\n   174→  const { videoInfo, tracks } = tracking;\n   175→\n   176→  if (!tracks || tracks.length === 0) {\n   177→    // No faces - use center crop\n   178→    const crop = calculateCenterCrop(targetAspect, videoInfo);\n   179→    return {\n   180→      method: 'center',\n   181→      static: true,\n   182→      crop,\n   183→      keyframes: []\n   184→    };\n   185→  }\n   186→\n   187→  // Use primary track\n   188→  const primaryTrack = tracks[0];\n   189→  const keyframes = [];\n   190→\n   191→  // Generate keyframes at each tracked position\n   192→  primaryTrack.positions.forEach((pos, index) => {\n   193→    const crop = calculateCropRegion(pos, targetAspect, videoInfo);\n   194→    keyframes.push({\n   195→      time: pos.timestamp,\n   196→      frame: Math.round(pos.timestamp * videoInfo.frameRate),\n   197→      crop\n   198→    });\n   199→  });\n   200→\n   201→  return {\n   202→    method: 'face-tracking',\n   203→    static: false,\n   204→    trackId: primaryTrack.trackId,\n   205→    keyframes,\n   206→    totalKeyframes: keyframes.length\n   207→  };\n   208→}\n   209→\n   210→/**\n   211→ * Generate motion path with smoothing\n   212→ * @param {Array} crops - Array of crop positions\n   213→ * @param {number} duration - Video duration\n   214→ * @param {number} frameRate - Video frame rate\n   215→ * @returns {Object} Smooth motion path\n   216→ */\n   217→function generateMotionPath(crops, duration, frameRate) {\n   218→  if (!crops || crops.length === 0) {\n   219→    return {\n   220→      success: false,\n   221→      error: 'No crop data provided'\n   222→    };\n   223→  }\n   224→\n   225→  const totalFrames = Math.ceil(duration * frameRate);\n   226→  const path = [];\n   227→\n   228→  // Interpolate between keyframes\n   229→  for (let frame = 0; frame < totalFrames; frame++) {\n   230→    const time = frame / frameRate;\n   231→\n   232→    // Find surrounding keyframes\n   233→    let before = crops[0];\n   234→    let after = crops[crops.length - 1];\n   235→\n   236→    for (let i = 0; i < crops.length - 1; i++) {\n   237→      if (crops[i].time <= time && crops[i + 1].time >= time) {\n   238→        before = crops[i];\n   239→        after = crops[i + 1];\n   240→        break;\n   241→      }\n   242→    }\n   243→\n   244→    // Interpolate\n   245→    const t = after.time !== before.time\n   246→      ? (time - before.time) / (after.time - before.time)\n   247→      : 0;\n   248→\n   249→    // Ease function (smooth step)\n   250→    const ease = t * t * (3 - 2 * t);\n   251→\n   252→    path.push({\n   253→      frame,\n   254→      time,\n   255→      x: lerp(before.crop.x, after.crop.x, ease),\n   256→      y: lerp(before.crop.y, after.crop.y, ease),\n   257→      width: lerp(before.crop.width, after.crop.width, ease),\n   258→      height: lerp(before.crop.height, after.crop.height, ease)\n   259→    });\n   260→  }\n   261→\n   262→  return {\n   263→    success: true,\n   264→    totalFrames,\n   265→    duration,\n   266→    frameRate,\n   267→    path\n   268→  };\n   269→}\n   270→\n   271→/**\n   272→ * Get safe zones for platform UI elements\n   273→ * @param {string} platform - Platform ID\n   274→ * @returns {Object} Safe zone information\n   275→ */\n   276→function getSafeZones(platform) {\n   277→  const preset = PLATFORM_PRESETS[platform];\n   278→\n   279→  if (!preset) {\n   280→    return {\n   281→      success: false,\n   282→      error: `Unknown platform: ${platform}`,\n   283→      availablePlatforms: Object.keys(PLATFORM_PRESETS)\n   284→    };\n   285→  }\n   286→\n   287→  const aspect = ASPECT_RATIOS[preset.aspectRatio];\n   288→\n   289→  return {\n   290→    success: true,\n   291→    platform: preset.name,\n   292→    aspectRatio: preset.aspectRatio,\n   293→    dimensions: {\n   294→      width: aspect.width,\n   295→      height: aspect.height,\n   296→      ratio: aspect.ratio\n   297→    },\n   298→    safeZone: preset.safeZone,\n   299→    safeArea: {\n   300→      top: preset.safeZone.top * aspect.height,\n   301→      bottom: preset.safeZone.bottom * aspect.height,\n   302→      left: preset.safeZone.left * aspect.width,\n   303→      right: preset.safeZone.right * aspect.width,\n   304→      contentWidth: aspect.width * (1 - preset.safeZone.left - preset.safeZone.right),\n   305→      contentHeight: aspect.height * (1 - preset.safeZone.top - preset.safeZone.bottom)\n   306→    },\n   307→    maxDuration: preset.maxDuration\n   308→  };\n   309→}\n   310→\n   311→/**\n   312→ * Generate export settings for all formats\n   313→ * @param {string} videoPath - Source video path\n   314→ * @param {Array} formats - Array of format IDs\n   315→ * @param {Object} settings - Export settings\n   316→ * @returns {Promise<Object>} Export data for each format\n   317→ */\n   318→async function batchExportFormats(videoPath, formats = ['portrait', 'square'], settings = {}) {\n   319→  const {\n   320→    quality = 'high', // high, medium, draft\n   321→    motionSmoothing = 0.3\n   322→  } = settings;\n   323→\n   324→  // Analyze video first\n   325→  const analysis = await analyzeForReframe(videoPath, { smoothing: motionSmoothing });\n   326→\n   327→  if (!analysis.success) {\n   328→    return analysis;\n   329→  }\n   330→\n   331→  const exports = [];\n   332→\n   333→  for (const formatId of formats) {\n   334→    const aspect = ASPECT_RATIOS[formatId];\n   335→    if (!aspect) continue;\n   336→\n   337→    const crops = analysis.suggestions[formatId];\n   338→    const motionPath = crops.static\n   339→      ? null\n   340→      : generateMotionPath(crops.keyframes, analysis.videoInfo.duration, analysis.videoInfo.frameRate);\n   341→\n   342→    exports.push({\n   343→      format: formatId,\n   344→      name: aspect.name,\n   345→      dimensions: {\n   346→        width: aspect.width,\n   347→        height: aspect.height\n   348→      },\n   349→      isStatic: crops.static,\n   350→      crop: crops.static ? crops.crop : null,\n   351→      motionPath: motionPath?.success ? motionPath : null,\n   352→      ffmpegFilter: generateFFmpegFilter(crops, aspect, analysis.videoInfo),\n   353→      platforms: aspect.platforms\n   354→    });\n   355→  }\n   356→\n   357→  return {\n   358→    success: true,\n   359→    sourceVideo: videoPath,\n   360→    sourceInfo: analysis.videoInfo,\n   361→    exports,\n   362→    totalFormats: exports.length,\n   363→    metadata: {\n   364→      quality,\n   365→      motionSmoothing,\n   366→      generatedAt: new Date().toISOString()\n   367→    }\n   368→  };\n   369→}\n   370→\n   371→/**\n   372→ * Generate FFmpeg filter for reframe\n   373→ */\n   374→function generateFFmpegFilter(crops, aspect, videoInfo) {\n   375→  if (crops.static) {\n   376→    // Static crop\n   377→    const crop = crops.crop;\n   378→    return `crop=${crop.pixelWidth}:${crop.pixelHeight}:${crop.pixelX}:${crop.pixelY},scale=${aspect.width}:${aspect.height}`;\n   379→  }\n   380→\n   381→  // For motion crops, would need complex filter with keyframes\n   382→  // Return simplified version\n   383→  const firstCrop = crops.keyframes[0]?.crop;\n   384→  if (firstCrop) {\n   385→    return `crop=${firstCrop.pixelWidth}:${firstCrop.pixelHeight}:${firstCrop.pixelX}:${firstCrop.pixelY},scale=${aspect.width}:${aspect.height}`;\n   386→  }\n   387→\n   388→  // Fallback center crop\n   389→  const targetHeight = videoInfo.height;\n   390→  const targetWidth = Math.round(targetHeight * aspect.ratio);\n   391→  const cropX = Math.round((videoInfo.width - targetWidth) / 2);\n   392→\n   393→  return `crop=${targetWidth}:${targetHeight}:${cropX}:0,scale=${aspect.width}:${aspect.height}`;\n   394→}\n   395→\n   396→/**\n   397→ * Get available platforms and aspect ratios\n   398→ */\n   399→function getPlatformPresets() {\n   400→  return {\n   401→    aspectRatios: Object.values(ASPECT_RATIOS).map(a => ({\n   402→      id: a.id,\n   403→      name: a.name,\n   404→      ratio: a.ratio,\n   405→      dimensions: { width: a.width, height: a.height }\n   406→    })),\n   407→    platforms: Object.values(PLATFORM_PRESETS).map(p => ({\n   408→      id: p.id,\n   409→      name: p.name,\n   410→      aspectRatio: p.aspectRatio,\n   411→      maxDuration: p.maxDuration\n   412→    })),\n   413→    defaults: {\n   414→      quality: 'high',\n   415→      motionSmoothing: 0.3\n   416→    }\n   417→  };\n   418→}\n   419→\n   420→// ============================================================================\n   421→// HELPER FUNCTIONS\n   422→// ============================================================================\n   423→\n   424→/**\n   425→ * Linear interpolation\n   426→ */\n   427→function lerp(a, b, t) {\n   428→  return a + (b - a) * t;\n   429→}\n   430→\n   431→// ============================================================================\n   432→// EXPORTS\n   433→// ============================================================================\n   434→\n   435→module.exports = {\n   436→  analyzeForReframe,\n   437→  calculateReframeCrops,\n   438→  generateMotionPath,\n   439→  getSafeZones,\n   440→  batchExportFormats,\n   441→  getPlatformPresets,\n   442→  // Exposed for testing\n   443→  ASPECT_RATIOS,\n   444→  PLATFORM_PRESETS,\n   445→  lerp,\n   446→  generateFFmpegFilter\n   447→};\n   448→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"80bbadd8-9109-4b5d-8743-350b7391965e","timestamp":"2025-12-26T07:40:29.633Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/SPLICE/splice-backend/services/socialReframe.js","content":"/**\n * SPLICE Social Reframe Service\n *\n * Auto-crops video to vertical/square formats with face tracking.\n * Supports TikTok (9:16), Instagram (1:1, 4:5), YouTube Shorts.\n */\n\nconst { trackFaces, identifySpeaker, calculateCropRegion, calculateCenterCrop } = require('./faceDetection');\nconst fs = require('fs');\nconst path = require('path');\n\n// ============================================================================\n// ASPECT RATIO PRESETS\n// ============================================================================\n\nconst ASPECT_RATIOS = {\n  portrait: {\n    id: 'portrait',\n    name: 'Portrait (9:16)',\n    ratio: 9 / 16,\n    width: 1080,\n    height: 1920,\n    platforms: ['tiktok', 'reels', 'shorts']\n  },\n  square: {\n    id: 'square',\n    name: 'Square (1:1)',\n    ratio: 1,\n    width: 1080,\n    height: 1080,\n    platforms: ['instagram_feed', 'linkedin']\n  },\n  portrait_4_5: {\n    id: 'portrait_4_5',\n    name: 'Portrait (4:5)',\n    ratio: 4 / 5,\n    width: 1080,\n    height: 1350,\n    platforms: ['instagram_portrait']\n  },\n  landscape: {\n    id: 'landscape',\n    name: 'Landscape (16:9)',\n    ratio: 16 / 9,\n    width: 1920,\n    height: 1080,\n    platforms: ['youtube', 'twitter']\n  }\n};\n\n// ============================================================================\n// PLATFORM PRESETS\n// ============================================================================\n\nconst PLATFORM_PRESETS = {\n  tiktok: {\n    id: 'tiktok',\n    name: 'TikTok',\n    aspectRatio: 'portrait',\n    safeZone: { top: 0.15, bottom: 0.2, left: 0.05, right: 0.05 },\n    maxDuration: 180\n  },\n  reels: {\n    id: 'reels',\n    name: 'Instagram Reels',\n    aspectRatio: 'portrait',\n    safeZone: { top: 0.1, bottom: 0.15, left: 0.05, right: 0.05 },\n    maxDuration: 90\n  },\n  shorts: {\n    id: 'shorts',\n    name: 'YouTube Shorts',\n    aspectRatio: 'portrait',\n    safeZone: { top: 0.1, bottom: 0.12, left: 0.05, right: 0.05 },\n    maxDuration: 60\n  },\n  instagram_feed: {\n    id: 'instagram_feed',\n    name: 'Instagram Feed',\n    aspectRatio: 'square',\n    safeZone: { top: 0.05, bottom: 0.05, left: 0.05, right: 0.05 },\n    maxDuration: 60\n  },\n  instagram_portrait: {\n    id: 'instagram_portrait',\n    name: 'Instagram Portrait',\n    aspectRatio: 'portrait_4_5',\n    safeZone: { top: 0.05, bottom: 0.1, left: 0.05, right: 0.05 },\n    maxDuration: 60\n  },\n  linkedin: {\n    id: 'linkedin',\n    name: 'LinkedIn',\n    aspectRatio: 'square',\n    safeZone: { top: 0.05, bottom: 0.08, left: 0.05, right: 0.05 },\n    maxDuration: 600\n  }\n};\n\n// ============================================================================\n// MAIN FUNCTIONS\n// ============================================================================\n\n/**\n * Analyze video for reframe possibilities\n * @param {string} videoPath - Path to video file\n * @param {Object} options - Analysis options\n * @returns {Promise<Object>} Analysis results with face positions\n */\nasync function analyzeForReframe(videoPath, options = {}) {\n  const {\n    sampleRate = 0.5,\n    smoothing = 0.3\n  } = options;\n\n  if (!fs.existsSync(videoPath)) {\n    return {\n      success: false,\n      error: `Video file not found: ${videoPath}`\n    };\n  }\n\n  console.log(`[SPLICE Reframe] Analyzing: ${videoPath}`);\n\n  try {\n    // Track faces in video\n    const tracking = await trackFaces(videoPath, { sampleRate, smoothing });\n\n    if (!tracking.success) {\n      return tracking;\n    }\n\n    // Identify primary speaker\n    const speaker = identifySpeaker(tracking.tracks);\n\n    // Calculate suggested crops for each aspect ratio\n    const suggestions = {};\n    for (const [key, preset] of Object.entries(ASPECT_RATIOS)) {\n      suggestions[key] = calculateReframeCrops(tracking, preset.ratio);\n    }\n\n    return {\n      success: true,\n      videoInfo: tracking.videoInfo,\n      faceTracking: {\n        tracksFound: tracking.tracks.length,\n        primarySpeaker: speaker.primarySpeaker,\n        tracks: tracking.tracks\n      },\n      suggestions,\n      metadata: {\n        analyzedAt: new Date().toISOString(),\n        sampleRate,\n        smoothing\n      }\n    };\n\n  } catch (err) {\n    console.error('[SPLICE Reframe] Analysis error:', err);\n    return {\n      success: false,\n      error: err.message\n    };\n  }\n}\n\n/**\n * Calculate crop positions for target aspect ratio\n * @param {Object} tracking - Face tracking data\n * @param {number} targetAspect - Target aspect ratio\n * @returns {Object} Crop positions over time\n */\nfunction calculateReframeCrops(tracking, targetAspect) {\n  const { videoInfo, tracks } = tracking;\n\n  if (!tracks || tracks.length === 0) {\n    // No faces - use center crop\n    const crop = calculateCenterCrop(targetAspect, videoInfo);\n    return {\n      method: 'center',\n      static: true,\n      crop,\n      keyframes: []\n    };\n  }\n\n  // Use primary track\n  const primaryTrack = tracks[0];\n  const keyframes = [];\n\n  // Generate keyframes at each tracked position\n  primaryTrack.positions.forEach((pos, index) => {\n    const crop = calculateCropRegion(pos, targetAspect, videoInfo);\n    keyframes.push({\n      time: pos.timestamp,\n      frame: Math.round(pos.timestamp * videoInfo.frameRate),\n      crop\n    });\n  });\n\n  return {\n    method: 'face-tracking',\n    static: false,\n    trackId: primaryTrack.trackId,\n    keyframes,\n    totalKeyframes: keyframes.length\n  };\n}\n\n/**\n * Generate motion path with smoothing\n * @param {Array} crops - Array of crop positions\n * @param {number} duration - Video duration\n * @param {number} frameRate - Video frame rate\n * @returns {Object} Smooth motion path\n */\nfunction generateMotionPath(crops, duration, frameRate) {\n  if (!crops || crops.length === 0) {\n    return {\n      success: false,\n      error: 'No crop data provided'\n    };\n  }\n\n  const totalFrames = Math.ceil(duration * frameRate);\n  const path = [];\n\n  // Interpolate between keyframes\n  for (let frame = 0; frame < totalFrames; frame++) {\n    const time = frame / frameRate;\n\n    // Find surrounding keyframes\n    let before = crops[0];\n    let after = crops[crops.length - 1];\n\n    for (let i = 0; i < crops.length - 1; i++) {\n      if (crops[i].time <= time && crops[i + 1].time >= time) {\n        before = crops[i];\n        after = crops[i + 1];\n        break;\n      }\n    }\n\n    // Interpolate\n    const t = after.time !== before.time\n      ? (time - before.time) / (after.time - before.time)\n      : 0;\n\n    // Ease function (smooth step)\n    const ease = t * t * (3 - 2 * t);\n\n    path.push({\n      frame,\n      time,\n      x: lerp(before.crop.x, after.crop.x, ease),\n      y: lerp(before.crop.y, after.crop.y, ease),\n      width: lerp(before.crop.width, after.crop.width, ease),\n      height: lerp(before.crop.height, after.crop.height, ease)\n    });\n  }\n\n  return {\n    success: true,\n    totalFrames,\n    duration,\n    frameRate,\n    path\n  };\n}\n\n/**\n * Get safe zones for platform UI elements\n * @param {string} platform - Platform ID\n * @returns {Object} Safe zone information\n */\nfunction getSafeZones(platform) {\n  const preset = PLATFORM_PRESETS[platform];\n\n  if (!preset) {\n    return {\n      success: false,\n      error: `Unknown platform: ${platform}`,\n      availablePlatforms: Object.keys(PLATFORM_PRESETS)\n    };\n  }\n\n  const aspect = ASPECT_RATIOS[preset.aspectRatio];\n\n  return {\n    success: true,\n    platform: preset.name,\n    aspectRatio: preset.aspectRatio,\n    dimensions: {\n      width: aspect.width,\n      height: aspect.height,\n      ratio: aspect.ratio\n    },\n    safeZone: preset.safeZone,\n    safeArea: {\n      top: preset.safeZone.top * aspect.height,\n      bottom: preset.safeZone.bottom * aspect.height,\n      left: preset.safeZone.left * aspect.width,\n      right: preset.safeZone.right * aspect.width,\n      contentWidth: aspect.width * (1 - preset.safeZone.left - preset.safeZone.right),\n      contentHeight: aspect.height * (1 - preset.safeZone.top - preset.safeZone.bottom)\n    },\n    maxDuration: preset.maxDuration\n  };\n}\n\n/**\n * Generate export settings for all formats\n * @param {string} videoPath - Source video path\n * @param {Array} formats - Array of format IDs\n * @param {Object} settings - Export settings\n * @returns {Promise<Object>} Export data for each format\n */\nasync function batchExportFormats(videoPath, formats = ['portrait', 'square'], settings = {}) {\n  const {\n    quality = 'high', // high, medium, draft\n    motionSmoothing = 0.3\n  } = settings;\n\n  // Analyze video first\n  const analysis = await analyzeForReframe(videoPath, { smoothing: motionSmoothing });\n\n  if (!analysis.success) {\n    return analysis;\n  }\n\n  const exports = [];\n\n  for (const formatId of formats) {\n    const aspect = ASPECT_RATIOS[formatId];\n    if (!aspect) continue;\n\n    const crops = analysis.suggestions[formatId];\n    const motionPath = crops.static\n      ? null\n      : generateMotionPath(crops.keyframes, analysis.videoInfo.duration, analysis.videoInfo.frameRate);\n\n    exports.push({\n      format: formatId,\n      name: aspect.name,\n      dimensions: {\n        width: aspect.width,\n        height: aspect.height\n      },\n      isStatic: crops.static,\n      crop: crops.static ? crops.crop : null,\n      motionPath: motionPath?.success ? motionPath : null,\n      ffmpegFilter: generateFFmpegFilter(crops, aspect, analysis.videoInfo),\n      platforms: aspect.platforms\n    });\n  }\n\n  return {\n    success: true,\n    sourceVideo: videoPath,\n    sourceInfo: analysis.videoInfo,\n    exports,\n    totalFormats: exports.length,\n    metadata: {\n      quality,\n      motionSmoothing,\n      generatedAt: new Date().toISOString()\n    }\n  };\n}\n\n/**\n * Generate FFmpeg filter for reframe\n */\nfunction generateFFmpegFilter(crops, aspect, videoInfo) {\n  if (crops.static) {\n    // Static crop\n    const crop = crops.crop;\n    return `crop=${crop.pixelWidth}:${crop.pixelHeight}:${crop.pixelX}:${crop.pixelY},scale=${aspect.width}:${aspect.height}`;\n  }\n\n  // For motion crops, would need complex filter with keyframes\n  // Return simplified version\n  const firstCrop = crops.keyframes[0]?.crop;\n  if (firstCrop) {\n    return `crop=${firstCrop.pixelWidth}:${firstCrop.pixelHeight}:${firstCrop.pixelX}:${firstCrop.pixelY},scale=${aspect.width}:${aspect.height}`;\n  }\n\n  // Fallback center crop\n  const targetHeight = videoInfo.height;\n  const targetWidth = Math.round(targetHeight * aspect.ratio);\n  const cropX = Math.round((videoInfo.width - targetWidth) / 2);\n\n  return `crop=${targetWidth}:${targetHeight}:${cropX}:0,scale=${aspect.width}:${aspect.height}`;\n}\n\n/**\n * Get available platforms and aspect ratios\n */\nfunction getPlatformPresets() {\n  return {\n    aspectRatios: Object.values(ASPECT_RATIOS).map(a => ({\n      id: a.id,\n      name: a.name,\n      ratio: a.ratio,\n      dimensions: { width: a.width, height: a.height }\n    })),\n    platforms: Object.values(PLATFORM_PRESETS).map(p => ({\n      id: p.id,\n      name: p.name,\n      aspectRatio: p.aspectRatio,\n      maxDuration: p.maxDuration\n    })),\n    defaults: {\n      quality: 'high',\n      motionSmoothing: 0.3\n    }\n  };\n}\n\n// ============================================================================\n// HELPER FUNCTIONS\n// ============================================================================\n\n/**\n * Linear interpolation\n */\nfunction lerp(a, b, t) {\n  return a + (b - a) * t;\n}\n\n// ============================================================================\n// EXPORTS\n// ============================================================================\n\nmodule.exports = {\n  analyzeForReframe,\n  calculateReframeCrops,\n  generateMotionPath,\n  getSafeZones,\n  batchExportFormats,\n  getPlatformPresets,\n  // Exposed for testing\n  ASPECT_RATIOS,\n  PLATFORM_PRESETS,\n  lerp,\n  generateFFmpegFilter\n};\n","numLines":448,"startLine":1,"totalLines":448}}}
{"parentUuid":"80bbadd8-9109-4b5d-8743-350b7391965e","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/SPLICE","sessionId":"5d87b125-f1a0-4772-bc8a-2b15e914023b","version":"2.0.76","gitBranch":"main","agentId":"a9b41c6","slug":"noble-riding-lemon","message":{"model":"claude-opus-4-5-20251101","id":"msg_012Sg1aQpRSAW6A3NX3GRnMV","type":"message","role":"assistant","content":[{"type":"text","text":"Now I have analyzed all the key service files. Let me compile a comprehensive performance analysis report.\n\n---\n\n# SPLICE Backend Performance Analysis Report\n\n## Performance Summary\n\n| Metric | Current | Target | Gap |\n|--------|---------|--------|-----|\n| DB queries/request (typical) | 2-4 | <5 | OK |\n| Batch parallelism | 3 concurrent | Unlimited for I/O | Could improve |\n| Memory (large audio) | Unbounded | Chunked (30s) | OK |\n| Cache eviction | 50 entries, 1hr TTL | Reasonable | OK |\n| Levenshtein worst case | O(mn) | O(mn) w/early exit | Optimized |\n\n---\n\n## Critical Bottlenecks\n\n### [PERF-001] Synchronous WAV Parsing in getWaveformData()\n\n**Severity: HIGH**\n**Impact:** Blocks event loop during large file reads, causes API unresponsiveness\n**Frequency:** Every `/waveform` request\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:816-821`\n\n```javascript\n// CURRENT CODE (blocking)\nconst fs = require('fs');\n// ...\nconst buffer = fs.readFileSync(wavPath);  // <-- BLOCKING!\n```\n\n**Problem:** Uses `fs.readFileSync()` instead of async `fs.promises.readFile()`. For a 500MB file, this could block the event loop for several seconds.\n\n**Expected Improvement:** Non-blocking I/O, ~100ms to async for large files\n\n---\n\n### [PERF-002] Sequential Frame Sampling in Face Detection\n\n**Severity: MEDIUM**\n**Impact:** Slow face detection for long videos\n**Frequency:** Every `/reframe/analyze` and `/faces/detect` request\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/faceDetection.js:238-256`\n\n```javascript\n// CURRENT CODE (sequential loop - no actual FFmpeg extraction)\nfor (let i = 0; i < frameCount; i++) {\n  const timestamp = i * sampleRate;\n  frames.push({\n    index: i,\n    timestamp,\n    analyzed: true  // Simulated, no actual extraction\n  });\n}\n```\n\n**Problem:** While currently simulated, when real frame extraction is added, this loop should use parallel extraction with `Promise.all()` batched.\n\n**Expected Improvement:** 3-5x faster when real extraction is added\n\n---\n\n### [PERF-003] Spawn Without Pooling for FFprobe/FFmpeg\n\n**Severity: MEDIUM**\n**Impact:** Process spawn overhead on every audio operation\n**Frequency:** Every silence detection, multitrack analysis, waveform request\n**Location:** Multiple files including:\n- `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:260-268`\n- `/Users/imorgado/SPLICE/splice-backend/services/multitrackAnalysis.js:236-287`\n- `/Users/imorgado/SPLICE/splice-backend/services/faceDetection.js:180-233`\n\n```javascript\n// CURRENT CODE - new spawn on every call\nconst ffprobe = spawn('ffprobe', [...args]);\n```\n\n**Problem:** Each call spawns a new process. For batch operations processing 10+ files, this creates significant overhead.\n\n**Expected Improvement:** 20-30% faster batch operations with process pooling\n\n---\n\n### [PERF-004] LCS Diff Algorithm Memory Allocation\n\n**Severity: LOW**\n**Impact:** Memory pressure for large transcripts (10k+ words)\n**Frequency:** Every text-based editing operation\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/textBasedEditing.js:172-189`\n\n```javascript\n// CURRENT CODE - allocates O(mn) matrix\nconst dp = Array(m + 1).fill(null).map(() => Array(n + 1).fill(0));\n```\n\n**Problem:** For 5000-word transcripts being compared, this allocates 25 million array cells. Could use Hirschberg's algorithm for O(n) space.\n\n**Expected Improvement:** 90% memory reduction for large transcripts\n\n---\n\n### [PERF-005] Missing Parallelization in generateMotionPath\n\n**Severity: LOW**\n**Impact:** Sequential keyframe processing for long videos\n**Frequency:** Every batch export reframe\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/socialReframe.js:228-259`\n\n```javascript\n// CURRENT CODE (sequential loop)\nfor (let frame = 0; frame < totalFrames; frame++) {\n  // ... interpolation logic\n  path.push({...});\n}\n```\n\n**Problem:** For a 60-second video at 30fps (1800 frames), this runs sequentially. Could chunk and parallelize.\n\n**Expected Improvement:** Minimal for current use case, but scales poorly\n\n---\n\n## Memory Issues\n\n### [MEM-001] Large Audio Buffer in vocalIsolation\n\n**Severity: MEDIUM**\n**Impact:** OOM risk for 500MB audio files\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/vocalIsolation.js:47-50`\n\n```javascript\n// CURRENT CODE\nconst audioBuffer = await fs.promises.readFile(audioPath);  // Full file in memory\nconst base64Audio = audioBuffer.toString('base64');  // DOUBLES memory usage\n```\n\n**Problem:** A 500MB audio file becomes 1GB+ in memory (original + base64). The base64 encoding adds ~33% overhead.\n\n**Mitigation:** File size validation exists at server level (500MB limit), but streaming upload to Replicate would be better.\n\n---\n\n### [MEM-002] Unbounded Array Growth in Chunked RMS Detection\n\n**Severity: LOW**\n**Impact:** Memory growth during long file processing\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:193-196`\n\n```javascript\n// CURRENT CODE - uses push to avoid concat (good!)\nfor (let k = 0; k < dBFSValues.length; k++) {\n  allDBFSValues.push(dBFSValues[k]);\n}\n```\n\n**Note:** This was ALREADY OPTIMIZED from the original code that used `concat`. Good work!\n\n---\n\n## Database Issues\n\n### [DB-001] Missing Index on usage_log.created_at\n\n**Severity: LOW**  \n**Impact:** Slow usage history queries for heavy users\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js:101-105`\n\n```javascript\n// CURRENT - composite index exists\nCREATE INDEX IF NOT EXISTS idx_usage_log_customer_created\nON usage_log(stripe_customer_id, created_at DESC)\n```\n\n**Note:** This was ALREADY OPTIMIZED with composite index. Good work!\n\n---\n\n### [DB-002] getBalance() Calls getOrCreateUser() Every Time\n\n**Severity: LOW**\n**Impact:** Extra query on every balance check\n**Location:** `/Users/imorgado/SPLICE/splice-backend/services/usageTracking.js:149-150`\n\n```javascript\nasync function getBalance(stripeCustomerId) {\n  const user = await getOrCreateUser(stripeCustomerId);  // Always runs upsert\n  // ...\n}\n```\n\n**Problem:** Even for existing users, this runs an INSERT...ON CONFLICT which is slightly slower than a simple SELECT.\n\n**Expected Improvement:** 5-10ms per request with caching or SELECT-first approach\n\n---\n\n## Algorithm Improvements Already in Place\n\nThe codebase already has several performance optimizations:\n\n1. **Levenshtein with early exit** (`repetitionDetection.js:441-496`):\n   - Pre-allocated buffers to avoid per-call allocation\n   - `maxThreshold` parameter for early termination\n   - O(n) space using two-row DP\n\n2. **Transcription caching** (`transcription.js:17-137`):\n   - LRU cache with 50-entry limit\n   - 1-hour TTL\n   - mtime-based invalidation\n\n3. **Batch parallelism** in multitrack (`multitrackAnalysis.js:66-73`):\n   - Uses `Promise.all()` for parallel track extraction\n\n4. **RETURNING clause usage** (`usageTracking.js` throughout):\n   - Avoids extra SELECT after UPDATE/INSERT\n\n5. **Chunked audio processing** (`rmsSilenceDetection.js:147-250`):\n   - 30-second chunks for long files\n   - Memory-efficient streaming approach\n\n---\n\n## Quick Wins\n\n1. **Fix synchronous readFileSync in getWaveformData** (+100ms saved on large files)\n   - File: `/Users/imorgado/SPLICE/splice-backend/services/rmsSilenceDetection.js:821`\n   - Change: `fs.readFileSync` -> `await fs.promises.readFile`\n\n2. **Add response caching for static endpoints**\n   - `/profanity/languages`, `/profanity/bleeps`, `/export/formats`\n   - These never change, could be cached at server startup\n\n3. **Consider process pooling for FFmpeg/FFprobe**\n   - Batch operations spawn many processes\n   - A simple pool of 3-5 workers would reduce overhead\n\n---\n\n## Architecture Recommendations\n\n1. **Consider Worker Threads for CPU-intensive operations**\n   - RMS calculation, Levenshtein distance, convolution\n   - Would prevent blocking the main event loop\n\n2. **Add Redis caching for transcription results**\n   - Current in-memory cache is lost on restart\n   - Redis would persist across deployments\n\n3. **Implement request coalescing for batch operations**\n   - Multiple requests for same file could share work\n   - Especially useful for transcription (most expensive)\n\n---\n\n## Files Analyzed\n\n| File | Lines | Key Findings |\n|------|-------|--------------|\n| `server.js` | ~1600 | Middleware well-structured, rate limiting in place |\n| `usageTracking.js` | 813 | Good use of transactions, RETURNING clause |\n| `transcription.js` | 278 | Excellent caching, retry logic |\n| `multitrackAnalysis.js` | 1093 | PERF-008 caching implemented, parallel extraction |\n| `rmsSilenceDetection.js` | 913 | Sync readFile issue, otherwise well-chunked |\n| `profanityDetection.js` | 739 | Parallel bleep generation, efficient Set usage |\n| `repetitionDetection.js` | 666 | Optimized Levenshtein, pre-allocated buffers |\n| `cutListGenerator.js` | 539 | Pure functions, minimal overhead |\n| `vocalIsolation.js` | 261 | Memory concern with base64 encoding |\n| `textBasedEditing.js` | 730 | LCS memory allocation could be reduced |\n| `animatedCaptions.js` | 872 | Lazy OpenAI client, efficient |\n| `faceDetection.js` | 450 | Simulated detection, efficient when real |\n| `socialReframe.js` | 448 | Sequential motion path generation |\n\n---\n\n**Overall Assessment:** The codebase is generally well-optimized with many performance patterns already in place (caching, RETURNING clauses, parallel processing). The most impactful fix would be the synchronous file read in `getWaveformData()`. The memory concerns around large audio files are mitigated by the 500MB file size limit."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":17960,"cache_read_input_tokens":127918,"cache_creation":{"ephemeral_5m_input_tokens":17960,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWUvKNM4AHGY85tCWBrye","type":"assistant","uuid":"2ba6b602-98e7-4e20-b6ed-56c95377288a","timestamp":"2025-12-26T07:41:35.569Z"}
