     1→/**
     2→ * SPLICE Backend Server
     3→ *
     4→ * Main entry point for the SPLICE backend API.
     5→ * Orchestrates the audio analysis pipeline.
     6→ *
     7→ * Slices:
     8→ * - Slice 4: Transcription (services/transcription.js)
     9→ * - Slice 5: Take Detection (services/takeDetection.js)
    10→ */
    11→
    12→require('dotenv').config();
    13→
    14→const express = require('express');
    15→const cors = require('cors');
    16→const fs = require('fs');
    17→const https = require('https');
    18→const http = require('http');
    19→const path = require('path');
    20→
    21→// Check if running in production (Railway injects RAILWAY_ENVIRONMENT)
    22→const isProduction = process.env.NODE_ENV === 'production' || process.env.RAILWAY_ENVIRONMENT;
    23→
    24→// Import slice services
    25→const { transcribeAudio, transcribeWithWords } = require('./services/transcription');
    26→const { detectTakes } = require('./services/takeDetection');
    27→const { detectSilences } = require('./services/silenceDetection');
    28→const { detectAudioSilences, isFFprobeInstalled, getAudioDuration } = require('./services/ffprobeSilence');
    29→const { detectSilencesRMS, sensitivityToParams } = require('./services/rmsSilenceDetection');
    30→const {
    31→  detectProfanity,
    32→  getProfanityList,
    33→  getSupportedLanguages,
    34→  getAvailableBleepSounds,
    35→  parseWordList
    36→} = require('./services/profanityDetection');
    37→const {
    38→  detectRepetitionsBasic,
    39→  detectRepetitionsAdvanced,
    40→  detectStutters,
    41→  detectAllRepetitions
    42→} = require('./services/repetitionDetection');
    43→const {
    44→  analyzeMultitrack,
    45→  autoBalanceMultitrack
    46→} = require('./services/multitrackAnalysis');
    47→const { processXMLFile } = require('./services/xmlProcessor');
    48→const { isolateVocals, isReplicateConfigured } = require('./services/vocalIsolation');
    49→const { generateCutList, generateTakesCutList, validateCutList } = require('./services/cutListGenerator');
    50→
    51→// Usage tracking and billing
    52→const usageTracking = require('./services/usageTracking');
    53→// Rate limiter available for future use
    54→// const { requireCredits } = require('./middleware/rateLimiter');
    55→
    56→// Stripe for webhooks
    57→const Stripe = require('stripe');
    58→const stripe = new Stripe(process.env.STRIPE_SECRET_KEY);
    59→
    60→// =============================================================================
    61→// Server Configuration
    62→// =============================================================================
    63→
    64→const app = express();
    65→const PORT = process.env.PORT || 3847;
    66→
    67→// HTTPS certificates (generated by mkcert) - only for local development
    68→let httpsOptions = null;
    69→if (!isProduction) {
    70→  const keyPath = path.join(__dirname, 'localhost+1-key.pem');
    71→  const certPath = path.join(__dirname, 'localhost+1.pem');
    72→  if (fs.existsSync(keyPath) && fs.existsSync(certPath)) {
    73→    httpsOptions = {
    74→      key: fs.readFileSync(keyPath),
    75→      cert: fs.readFileSync(certPath)
    76→    };
    77→  }
    78→}
    79→
    80→app.use(cors());
    81→
    82→// Helper to determine tier from price ID with logging
    83→function getTierFromPriceId(priceId) {
    84→  if (priceId === process.env.STRIPE_PRICE_STARTER) return 'starter';
    85→  if (priceId === process.env.STRIPE_PRICE_PRO) return 'pro';
    86→  if (priceId === process.env.STRIPE_PRICE_TEAM) return 'team';
    87→
    88→  // Log unknown price ID for debugging
    89→  console.warn(`[SPLICE] Unknown price ID: ${priceId} - defaulting to starter tier`);
    90→  return 'starter';
    91→}
    92→
    93→// Stripe webhook needs raw body - must be before express.json()
    94→app.post('/webhooks/stripe', express.raw({ type: 'application/json' }), async (req, res) => {
    95→  const sig = req.headers['stripe-signature'];
    96→  const webhookSecret = process.env.STRIPE_WEBHOOK_SECRET;
    97→
    98→  let event;
    99→
   100→  try {
   101→    if (webhookSecret) {
   102→      event = stripe.webhooks.constructEvent(req.body, sig, webhookSecret);
   103→    } else {
   104→      // For testing without webhook secret
   105→      event = JSON.parse(req.body);
   106→      console.warn('[SPLICE] Warning: Processing webhook without signature verification');
   107→    }
   108→  } catch (err) {
   109→    console.error('[SPLICE] Webhook signature verification failed:', err.message);
   110→    return res.status(400).json({ error: 'Webhook signature verification failed' });
   111→  }
   112→
   113→  console.log(`[SPLICE] Webhook received: ${event.type} (${event.id})`);
   114→
   115→  // Idempotency check - skip if already processed
   116→  if (await usageTracking.isEventProcessed(event.id)) {
   117→    console.log(`[SPLICE] Event ${event.id} already processed, skipping`);
   118→    return res.json({ received: true, skipped: true });
   119→  }
   120→
   121→  try {
   122→    switch (event.type) {
   123→      case 'customer.subscription.created':
   124→      case 'customer.subscription.updated': {
   125→        const subscription = event.data.object;
   126→        const customerId = subscription.customer;
   127→
   128→        // Validate customerId
   129→        if (!customerId) {
   130→          console.error('[SPLICE] Missing customer ID in subscription event');
   131→          return res.status(400).json({ error: 'Missing customer ID' });
   132→        }
   133→
   134→        // Get tier from price ID
   135→        const priceId = subscription.items?.data?.[0]?.price?.id;
   136→        const tier = getTierFromPriceId(priceId);
   137→
   138→        // Update user tier and reset hours
   139→        await usageTracking.updateTier(customerId, tier);
   140→        console.log(`[SPLICE] Updated customer ${customerId} to tier: ${tier}`);
   141→        break;
   142→      }
   143→
   144→      case 'customer.subscription.deleted': {
   145→        const subscription = event.data.object;
   146→        const customerId = subscription.customer;
   147→
   148→        // Validate customerId
   149→        if (!customerId) {
   150→          console.error('[SPLICE] Missing customer ID in subscription.deleted event');
   151→          return res.status(400).json({ error: 'Missing customer ID' });
   152→        }
   153→
   154→        // Downgrade to cancelled (0 hours)
   155→        await usageTracking.updateTier(customerId, 'cancelled');
   156→        console.log(`[SPLICE] Subscription cancelled for customer ${customerId}`);
   157→        break;
   158→      }
   159→
   160→      case 'invoice.payment_succeeded': {
   161→        const invoice = event.data.object;
   162→        const customerId = invoice.customer;
   163→        const subscriptionId = invoice.subscription;
   164→
   165→        // Validate customerId
   166→        if (!customerId) {
   167→          console.error('[SPLICE] Missing customer ID in invoice event');
   168→          return res.status(400).json({ error: 'Missing customer ID' });
   169→        }
   170→
   171→        // Reset hours on successful payment (new billing period)
   172→        if (subscriptionId) {
   173→          const subscription = await stripe.subscriptions.retrieve(subscriptionId);
   174→          const priceId = subscription.items?.data?.[0]?.price?.id;
   175→          const tier = getTierFromPriceId(priceId);
   176→
   177→          await usageTracking.resetHours(customerId, tier);
   178→          console.log(`[SPLICE] Reset hours for customer ${customerId} (tier: ${tier})`);
   179→        }
   180→        break;
   181→      }
   182→
   183→      default:
   184→        console.log(`[SPLICE] Unhandled event type: ${event.type}`);
   185→    }
   186→
   187→    // Record event as processed (idempotency)
   188→    await usageTracking.recordWebhookEvent(event.id, event.type);
   189→
   190→    res.json({ received: true });
   191→  } catch (err) {
   192→    console.error('[SPLICE] Webhook handler error:', err);
   193→    res.status(500).json({ error: err.message });
   194→  }
   195→});
   196→
   197→app.use(express.json());
   198→
   199→// =============================================================================
   200→// Routes
   201→// =============================================================================
   202→
   203→/**
   204→ * GET / - API information
   205→ */
   206→app.get('/', (req, res) => {
   207→  res.json({
   208→    service: 'splice-backend',
   209→    version: '0.3.0',
   210→    endpoints: {
   211→      'GET /': 'This info',
   212→      'GET /health': 'Health check',
   213→      'GET /ffprobe-check': 'Check if FFprobe is installed',
   214→      'GET /replicate-check': 'Check if Replicate API is configured',
   215→      'POST /analyze': 'Analyze WAV file { wavPath }',
   216→      'POST /silences': 'Detect silences via Whisper gaps { wavPath, threshold: 0.5 }',
   217→      'POST /silences-audio': 'Detect silences via FFprobe { wavPath, threshold: -30, minDuration: 0.5, padding: 0.1 }',
   218→      'POST /silences-rms': 'Detect silences via RMS analysis { wavPath, threshold: -30, minSilenceLength: 0.5, paddingStart: 0.1, paddingEnd: 0.05, autoThreshold: false, sensitivity: 50 }',
   219→      'POST /profanity': 'Detect profanity in transcript { wavPath, language: "en", customBlocklist: [], customAllowlist: [] }',
   220→      'GET /profanity/languages': 'Get supported languages for profanity detection',
   221→      'GET /profanity/bleeps': 'Get available bleep sounds',
   222→      'POST /repetitions': 'Detect phrase repetitions and stutters { wavPath, phraseSize: 5, tolerance: 0.7, useOpenAI: false }',
   223→      'POST /stutters': 'Detect single-word stutters only { wavPath, minRepeats: 2 }',
   224→      'POST /multitrack': 'Analyze multiple audio tracks for multicam { audioPaths: [], speakerNames: [], videoTrackMapping: {} }',
   225→      'POST /multitrack/auto-balance': 'Auto-balance speaker screentime { audioPaths: [], speakerNames: [] }',
   226→      'POST /process-xml': 'Process FCP XML { xmlPath, silences, removeGaps: true }',
   227→      'POST /cut-list': 'Generate JSON cut list for DOM building (v3.5) { sourceName, sourcePath, duration, silences, takes?, settings? }',
   228→      'POST /cut-list/takes': 'Generate cut list keeping only takes { sourceName, sourcePath, duration, takes, settings? }',
   229→      'POST /isolate-vocals': 'Isolate vocals from audio { audioPath }',
   230→      'GET /credits': 'Get user credit balance (requires x-stripe-customer-id header)',
   231→      'GET /usage-history': 'Get usage history (requires x-stripe-customer-id header)',
   232→      'POST /webhooks/stripe': 'Stripe webhook endpoint'
   233→    }
   234→  });
   235→});
   236→
   237→/**
   238→ * GET /health - Health check
   239→ */
   240→app.get('/health', (req, res) => {
   241→  res.json({ status: 'ok', service: 'splice-backend' });
   242→});
   243→
   244→/**
   245→ * POST /analyze - Main analysis endpoint
   246→ *
   247→ * Pipeline:
   248→ * 1. Validate input (wavPath)
   249→ * 2. Slice 4: Transcribe audio with Groq Whisper
   250→ * 3. Slice 5: Detect takes with GPT-4o-mini
   251→ * 4. Return combined results
   252→ */
   253→app.post('/analyze', async (req, res) => {
   254→  const { wavPath } = req.body;
   255→
   256→  // Validate input
   257→  if (!wavPath) {
   258→    return res.status(400).json({ error: 'wavPath is required' });
   259→  }
   260→
   261→  if (!fs.existsSync(wavPath)) {
   262→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   263→  }
   264→
   265→  console.log(`[SPLICE] Analyzing: ${wavPath}`);
   266→
   267→  try {
   268→    // Slice 4 - GPT-4o-mini transcription
   269→    const transcript = await transcribeAudio(wavPath);
   270→
   271→    // Slice 5 - GPT-4o-mini take detection
   272→    const takes = await detectTakes(transcript);
   273→
   274→    res.json({
   275→      success: true,
   276→      wavPath,
   277→      transcript,
   278→      takes
   279→    });
   280→  } catch (err) {
   281→    console.error('[SPLICE] Error:', err);
   282→    res.status(500).json({ error: err.message });
   283→  }
   284→});
   285→
   286→/**
   287→ * POST /silences - Detect silent gaps in audio
   288→ *
   289→ * Pipeline:
   290→ * 1. Transcribe audio with Whisper (reuses transcription)
   291→ * 2. Analyze gaps between segments
   292→ * 3. Return silence regions
   293→ */
   294→app.post('/silences', async (req, res) => {
   295→  const { wavPath, threshold = 0.5 } = req.body;
   296→
   297→  if (!wavPath) {
   298→    return res.status(400).json({ error: 'wavPath is required' });
   299→  }
   300→
   301→  if (!fs.existsSync(wavPath)) {
   302→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   303→  }
   304→
   305→  console.log(`[SPLICE] Detecting silences: ${wavPath} (threshold: ${threshold}s)`);
   306→
   307→  try {
   308→    const transcript = await transcribeAudio(wavPath);
   309→    const silences = detectSilences(transcript.segments, threshold);
   310→
   311→    res.json({
   312→      success: true,
   313→      wavPath,
   314→      threshold,
   315→      silences,
   316→      count: silences.length,
   317→      totalSilenceDuration: silences.reduce((sum, s) => sum + s.duration, 0).toFixed(2)
   318→    });
   319→  } catch (err) {
   320→    console.error('[SPLICE] Silence detection error:', err);
   321→    res.status(500).json({ error: err.message });
   322→  }
   323→});
   324→
   325→/**
   326→ * POST /silences-audio - Detect silences using FFprobe audio analysis
   327→ *
   328→ * Uses actual audio levels (dB threshold) instead of transcript gaps.
   329→ * More accurate for detecting silence vs background noise.
   330→ */
   331→app.post('/silences-audio', async (req, res) => {
   332→  const {
   333→    wavPath,
   334→    threshold = -30,
   335→    minDuration = 0.5,
   336→    padding = 0.1
   337→  } = req.body;
   338→
   339→  if (!wavPath) {
   340→    return res.status(400).json({ error: 'wavPath is required' });
   341→  }
   342→
   343→  if (!fs.existsSync(wavPath)) {
   344→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   345→  }
   346→
   347→  // Check FFprobe availability
   348→  const ffprobeAvailable = await isFFprobeInstalled();
   349→  if (!ffprobeAvailable) {
   350→    return res.status(500).json({
   351→      error: 'FFprobe not installed. Run: brew install ffmpeg'
   352→    });
   353→  }
   354→
   355→  console.log(`[SPLICE] FFprobe silence detection: ${wavPath} (threshold: ${threshold}dB, min: ${minDuration}s)`);
   356→
   357→  try {
   358→    const silences = await detectAudioSilences(wavPath, {
   359→      threshold,
   360→      minDuration,
   361→      padding
   362→    });
   363→
   364→    const totalDuration = silences.reduce((sum, s) => sum + s.duration, 0);
   365→
   366→    res.json({
   367→      success: true,
   368→      wavPath,
   369→      threshold,
   370→      minDuration,
   371→      padding,
   372→      silences,
   373→      count: silences.length,
   374→      totalSilenceDuration: totalDuration.toFixed(2)
   375→    });
   376→  } catch (err) {
   377→    console.error('[SPLICE] FFprobe silence detection error:', err);
   378→    res.status(500).json({ error: err.message });
   379→  }
   380→});
   381→
   382→/**
   383→ * POST /silences-rms - Detect silences using RMS audio analysis
   384→ *
   385→ * Advanced silence detection with:
   386→ * - RMS (Root Mean Square) audio level analysis
   387→ * - Auto-threshold detection from audio histogram
   388→ * - Configurable padding (before/after cuts)
   389→ * - Sensitivity slider mapping (0-100)
   390→ *
   391→ * Options:
   392→ * - threshold: dBFS threshold (-60 to -20, default: -30)
   393→ * - minSilenceLength: Minimum silence duration in seconds (default: 0.5)
   394→ * - seekStep: Analysis window step in seconds (default: 0.05)
   395→ * - paddingStart: Buffer before silence in seconds (default: 0.1)
   396→ * - paddingEnd: Buffer after silence in seconds (default: 0.05)
   397→ * - autoThreshold: Auto-detect optimal threshold (default: false)
   398→ * - sensitivity: UI sensitivity 0-100 (overrides other params if provided)
   399→ */
   400→app.post('/silences-rms', async (req, res) => {
   401→  const { wavPath, sensitivity, ...manualOptions } = req.body;
   402→
   403→  if (!wavPath) {
   404→    return res.status(400).json({ error: 'wavPath is required' });
   405→  }
   406→
   407→  if (!fs.existsSync(wavPath)) {
   408→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   409→  }
   410→
   411→  // Check FFprobe availability (needed for audio extraction)
   412→  const ffprobeAvailable = await isFFprobeInstalled();
   413→  if (!ffprobeAvailable) {
   414→    return res.status(500).json({
   415→      error: 'FFprobe not installed. Run: brew install ffmpeg'
   416→    });
   417→  }
   418→
   419→  // Build options - use sensitivity if provided, otherwise use manual options
   420→  let options = {};
   421→  if (typeof sensitivity === 'number') {
   422→    options = sensitivityToParams(sensitivity);
   423→    console.log(`[SPLICE] RMS detection with sensitivity ${sensitivity}`);
   424→  } else {
   425→    options = {
   426→      threshold: manualOptions.threshold ?? -30,
   427→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,
   428→      seekStep: manualOptions.seekStep ?? 0.05,
   429→      paddingStart: manualOptions.paddingStart ?? 0.1,
   430→      paddingEnd: manualOptions.paddingEnd ?? 0.05,
   431→      autoThreshold: manualOptions.autoThreshold ?? false,
   432→      mergeDistance: manualOptions.mergeDistance ?? 0.2
   433→    };
   434→  }
   435→
   436→  console.log(`[SPLICE] RMS silence detection: ${wavPath}`);
   437→
   438→  try {
   439→    const result = await detectSilencesRMS(wavPath, options);
   440→
   441→    res.json({
   442→      success: true,
   443→      wavPath,
   444→      ...result,
   445→      count: result.silences.length,
   446→      totalSilenceDuration: result.metadata.totalSilenceDuration.toFixed(2)
   447→    });
   448→  } catch (err) {
   449→    console.error('[SPLICE] RMS silence detection error:', err);
   450→    res.status(500).json({ error: err.message });
   451→  }
   452→});
   453→
   454→// =============================================================================
   455→// Profanity Detection Routes
   456→// =============================================================================
   457→
   458→/**
   459→ * POST /profanity - Detect profanity in audio/transcript
   460→ *
   461→ * Transcribes audio (if needed) and detects profanity words.
   462→ * Returns word-level and segment-level results for muting/bleeping.
   463→ *
   464→ * Options:
   465→ * - wavPath: Path to audio file (required)
   466→ * - transcript: Pre-existing transcript (optional, skips transcription)
   467→ * - language: Language code (en, es, fr, de) - default: en
   468→ * - customBlocklist: Array or comma-separated string of additional words to censor
   469→ * - customAllowlist: Array or comma-separated string of words to allow
   470→ * - frameRate: Frame rate for boundary alignment (default: 30)
   471→ */
   472→app.post('/profanity', async (req, res) => {
   473→  const {
   474→    wavPath,
   475→    transcript: providedTranscript,
   476→    language = 'en',
   477→    customBlocklist = [],
   478→    customAllowlist = [],
   479→    frameRate = 30
   480→  } = req.body;
   481→
   482→  if (!wavPath && !providedTranscript) {
   483→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   484→  }
   485→
   486→  if (wavPath && !fs.existsSync(wavPath)) {
   487→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   488→  }
   489→
   490→  console.log(`[SPLICE] Profanity detection: ${wavPath || 'provided transcript'} (language: ${language})`);
   491→
   492→  try {
   493→    // Get or create transcript with word-level timestamps
   494→    let transcript = providedTranscript;
   495→    if (!transcript && wavPath) {
   496→      // Use transcribeWithWords for word-level timestamps required by profanity detection
   497→      transcript = await transcribeWithWords(wavPath);
   498→    }
   499→
   500→    // Validate transcript has words
   501→    if (!transcript || !transcript.words || transcript.words.length === 0) {
   502→      return res.status(400).json({
   503→        error: 'Transcript must contain word-level timing data',
   504→        hint: 'Ensure transcription returns words array with start/end times'
   505→      });
   506→    }
   507→
   508→    // Parse custom lists
   509→    const blocklist = parseWordList(customBlocklist);
   510→    const allowlist = parseWordList(customAllowlist);
   511→
   512→    // Detect profanity
   513→    const result = detectProfanity(transcript, {
   514→      language,
   515→      customBlocklist: blocklist,
   516→      customAllowlist: allowlist,
   517→      frameRate
   518→    });
   519→
   520→    res.json({
   521→      success: true,
   522→      wavPath,
   523→      ...result
   524→    });
   525→  } catch (err) {
   526→    console.error('[SPLICE] Profanity detection error:', err);
   527→    res.status(500).json({ error: err.message });
   528→  }
   529→});
   530→
   531→/**
   532→ * GET /profanity/languages - Get supported languages
   533→ */
   534→app.get('/profanity/languages', (req, res) => {
   535→  res.json({
   536→    success: true,
   537→    languages: getSupportedLanguages()
   538→  });
   539→});
   540→
   541→/**
   542→ * GET /profanity/bleeps - Get available bleep sounds
   543→ */
   544→app.get('/profanity/bleeps', (req, res) => {
   545→  res.json({
   546→    success: true,
   547→    sounds: getAvailableBleepSounds()
   548→  });
   549→});
   550→
   551→/**
   552→ * GET /profanity/list/:language - Get default profanity list for a language
   553→ */
   554→app.get('/profanity/list/:language', (req, res) => {
   555→  const { language } = req.params;
   556→  const list = getProfanityList(language);
   557→
   558→  res.json({
   559→    success: true,
   560→    language,
   561→    wordCount: list.length,
   562→    // Return first 50 words as sample (full list is large)
   563→    sample: list.slice(0, 50),
   564→    note: 'Full list available but truncated for response size'
   565→  });
   566→});
   567→
   568→// =============================================================================
   569→// Repetition/Stutter Detection Routes
   570→// =============================================================================
   571→
   572→/**
   573→ * POST /repetitions - Detect phrase repetitions and stutters
   574→ *
   575→ * Analyzes transcript for repeated phrases and stutters.
   576→ * Returns segments that can be removed to clean up the edit.
   577→ *
   578→ * Options:
   579→ * - wavPath: Path to audio file (required unless transcript provided)
   580→ * - transcript: Pre-existing transcript (optional)
   581→ * - phraseSize: Words per comparison window (default: 5)
   582→ * - tolerance: Similarity threshold 0-1 (default: 0.7)
   583→ * - searchRadius: Words to search ahead (default: 100)
   584→ * - useOpenAI: Use OpenAI for boundary refinement (default: false)
   585→ * - includeStutters: Also detect single-word stutters (default: true)
   586→ */
   587→app.post('/repetitions', async (req, res) => {
   588→  const {
   589→    wavPath,
   590→    transcript: providedTranscript,
   591→    phraseSize = 5,
   592→    tolerance = 0.7,
   593→    searchRadius = 100,
   594→    useOpenAI = false,
   595→    includeStutters = true
   596→  } = req.body;
   597→
   598→  if (!wavPath && !providedTranscript) {
   599→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   600→  }
   601→
   602→  if (wavPath && !fs.existsSync(wavPath)) {
   603→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   604→  }
   605→
   606→  console.log(`[SPLICE] Repetition detection: ${wavPath || 'provided transcript'}`);
   607→
   608→  try {
   609→    // Get or create transcript with word-level timestamps
   610→    let transcript = providedTranscript;
   611→    if (!transcript && wavPath) {
   612→      // Use transcribeWithWords for word-level timestamps required by repetition detection
   613→      transcript = await transcribeWithWords(wavPath);
   614→    }
   615→
   616→    // Validate transcript has words
   617→    if (!transcript || !transcript.words || transcript.words.length === 0) {
   618→      return res.status(400).json({
   619→        error: 'Transcript must contain word-level timing data'
   620→      });
   621→    }
   622→
   623→    // Detect all repetitions (phrases + stutters)
   624→    const result = await detectAllRepetitions(transcript, {
   625→      phraseSize,
   626→      tolerance,
   627→      searchRadius,
   628→      useOpenAI
   629→    });
   630→
   631→    // Optionally filter out stutters
   632→    if (!includeStutters) {
   633→      result.stutters = [];
   634→      result.removalSegments = result.removalSegments.filter(s => s.type !== 'stutter');
   635→    }
   636→
   637→    res.json({
   638→      success: true,
   639→      wavPath,
   640→      ...result
   641→    });
   642→  } catch (err) {
   643→    console.error('[SPLICE] Repetition detection error:', err);
   644→    res.status(500).json({ error: err.message });
   645→  }
   646→});
   647→
   648→/**
   649→ * POST /stutters - Detect single-word stutters only
   650→ *
   651→ * Focused detection for word-level stutters (e.g., "I I I think").
   652→ * Faster than full repetition detection.
   653→ */
   654→app.post('/stutters', async (req, res) => {
   655→  const {
   656→    wavPath,
   657→    transcript: providedTranscript,
   658→    options = {},
   659→    // Support both top-level and nested options for flexibility
   660→    minRepeats = options.minRepeats ?? 2,
   661→    maxGapMs = options.maxGapMs ?? 500,
   662→    ignoreFillers = options.ignoreFillers ?? true,
   663→    minWordLength = options.minWordLength ?? 1
   664→  } = req.body;
   665→
   666→  if (!wavPath && !providedTranscript) {
   667→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   668→  }
   669→
   670→  if (wavPath && !fs.existsSync(wavPath)) {
   671→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   672→  }
   673→
   674→  console.log(`[SPLICE] Stutter detection: ${wavPath || 'provided transcript'}`);
   675→
   676→  try {
   677→    // Get or create transcript with word-level timestamps
   678→    let transcript = providedTranscript;
   679→    if (!transcript && wavPath) {
   680→      // Use transcribeWithWords for word-level timestamps required by stutter detection
   681→      transcript = await transcribeWithWords(wavPath);
   682→    }
   683→
   684→    // Validate transcript exists and has words array
   685→    if (!transcript || !transcript.words) {
   686→      return res.status(400).json({
   687→        error: 'Transcript must contain word-level timing data'
   688→      });
   689→    }
   690→
   691→    // Empty transcript returns empty result (not an error)
   692→    if (transcript.words.length === 0) {
   693→      return res.json({
   694→        success: true,
   695→        stutters: [],
   696→        metadata: { type: 'stutters', totalWords: 0, stutterCount: 0, totalRepeatedWords: 0 }
   697→      });
   698→    }
   699→
   700→    // Detect stutters only
   701→    const result = detectStutters(transcript, {
   702→      minRepeats,
   703→      maxGapMs,
   704→      ignoreFillers,
   705→      minWordLength
   706→    });
   707→
   708→    res.json({
   709→      success: true,
   710→      wavPath,
   711→      ...result
   712→    });
   713→  } catch (err) {
   714→    console.error('[SPLICE] Stutter detection error:', err);
   715→    res.status(500).json({ error: err.message });
   716→  }
   717→});
   718→
   719→// =============================================================================
   720→// Multitrack/Multicam Analysis Routes
   721→// =============================================================================
   722→
   723→/**
   724→ * POST /multitrack - Analyze multiple audio tracks for multicam editing
   725→ *
   726→ * Analyzes audio levels across multiple tracks to determine optimal
   727→ * video angle selection based on who is speaking.
   728→ *
   729→ * Options:
   730→ * - audioPaths: Array of paths to audio files (one per speaker) - required
   731→ * - speakerNames: Array of speaker names (optional)
   732→ * - videoTrackMapping: Object mapping speaker index to video track { 0: 0, 1: 1 }
   733→ * - minShotDuration: Minimum seconds before next cut (default: 2.0)
   734→ * - switchingFrequency: How often to allow cuts 0-100 (default: 50)
   735→ * - wideShotEnabled: Enable wide shot detection (default: true)
   736→ * - wideShotPercentage: Target % of wide shots (default: 20)
   737→ * - wideShotTracks: Video track indices for wide shots
   738→ * - cutawayEnabled: Enable cutaway insertion (default: false)
   739→ * - cutawayTracks: Video track indices for cutaways
   740→ * - speakerBoosts: Per-speaker dB adjustments { "Speaker 1": 5 }
   741→ */
   742→app.post('/multitrack', async (req, res) => {
   743→  const {
   744→    audioPaths,
   745→    speakerNames,
   746→    videoTrackMapping = {},
   747→    minShotDuration = 2.0,
   748→    switchingFrequency = 50,
   749→    wideShotEnabled = true,
   750→    wideShotPercentage = 20,
   751→    wideShotTracks = [],
   752→    cutawayEnabled = false,
   753→    cutawayTracks = [],
   754→    speakerBoosts = {},
   755→    frameRate = 30
   756→  } = req.body;
   757→
   758→  // Validate audioPaths
   759→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length === 0) {
   760→    return res.status(400).json({ error: 'audioPaths array is required (at least 1 path)' });
   761→  }
   762→
   763→  // Validate all files exist
   764→  for (const audioPath of audioPaths) {
   765→    if (!fs.existsSync(audioPath)) {
   766→      return res.status(404).json({ error: `File not found: ${audioPath}` });
   767→    }
   768→  }
   769→
   770→  // Check FFprobe availability
   771→  const ffprobeAvailable = await isFFprobeInstalled();
   772→  if (!ffprobeAvailable) {
   773→    return res.status(500).json({
   774→      error: 'FFprobe not installed. Run: brew install ffmpeg'
   775→    });
   776→  }
   777→
   778→  console.log(`[SPLICE] Multitrack analysis: ${audioPaths.length} track(s)`);
   779→
   780→  try {
   781→    const result = await analyzeMultitrack(audioPaths, {
   782→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),
   783→      videoTrackMapping,
   784→      minShotDuration,
   785→      switchingFrequency,
   786→      wideShotEnabled,
   787→      wideShotPercentage,
   788→      wideShotTracks,
   789→      cutawayEnabled,
   790→      cutawayTracks,
   791→      speakerBoosts,
   792→      frameRate
   793→    });
   794→
   795→    res.json({
   796→      success: true,
   797→      ...result
   798→    });
   799→  } catch (err) {
   800→    console.error('[SPLICE] Multitrack analysis error:', err);
   801→    res.status(500).json({ error: err.message });
   802→  }
   803→});
   804→
   805→/**
   806→ * POST /multitrack/auto-balance - Auto-balance speaker screentime
   807→ *
   808→ * Automatically adjusts speaker boosts to achieve equal screentime distribution.
   809→ * Runs multiple iterations to find optimal parameters.
   810→ */
   811→app.post('/multitrack/auto-balance', async (req, res) => {
   812→  const {
   813→    audioPaths,
   814→    speakerNames,
   815→    videoTrackMapping = {},
   816→    minShotDuration = 2.0,
   817→    switchingFrequency = 50,
   818→    wideShotEnabled = false, // Disable wide shots for balance calc
   819→    frameRate = 30
   820→  } = req.body;
   821→
   822→  // Validate audioPaths
   823→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length < 2) {
   824→    return res.status(400).json({ error: 'audioPaths array requires at least 2 tracks for balancing' });
   825→  }
   826→
   827→  // Validate all files exist
   828→  for (const audioPath of audioPaths) {
   829→    if (!fs.existsSync(audioPath)) {
   830→      return res.status(404).json({ error: `File not found: ${audioPath}` });
   831→    }
   832→  }
   833→
   834→  // Check FFprobe availability
   835→  const ffprobeAvailable = await isFFprobeInstalled();
   836→  if (!ffprobeAvailable) {
   837→    return res.status(500).json({
   838→      error: 'FFprobe not installed. Run: brew install ffmpeg'
   839→    });
   840→  }
   841→
   842→  console.log(`[SPLICE] Auto-balancing multitrack: ${audioPaths.length} track(s)`);
   843→
   844→  try {
   845→    const result = await autoBalanceMultitrack(audioPaths, {
   846→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),
   847→      videoTrackMapping,
   848→      minShotDuration,
   849→      switchingFrequency,
   850→      wideShotEnabled,
   851→      frameRate
   852→    });
   853→
   854→    res.json({
   855→      success: true,
   856→      ...result
   857→    });
   858→  } catch (err) {
   859→    console.error('[SPLICE] Auto-balance error:', err);
   860→    res.status(500).json({ error: err.message });
   861→  }
   862→});
   863→
   864→/**
   865→ * POST /process-xml - Process FCP XML to split clips at silences
   866→ *
   867→ * Takes an FCP XML file and silence timestamps, splits clips
   868→ * at silence boundaries, and optionally removes gaps.
   869→ */
   870→app.post('/process-xml', async (req, res) => {
   871→  const {
   872→    xmlPath,
   873→    silences,
   874→    removeGaps = true,
   875→    outputPath = null
   876→  } = req.body;
   877→
   878→  if (!xmlPath) {
   879→    return res.status(400).json({ error: 'xmlPath is required' });
   880→  }
   881→
   882→  if (!silences || !Array.isArray(silences)) {
   883→    return res.status(400).json({ error: 'silences array is required' });
   884→  }
   885→
   886→  if (!fs.existsSync(xmlPath)) {
   887→    return res.status(404).json({ error: `XML file not found: ${xmlPath}` });
   888→  }
   889→
   890→  console.log(`[SPLICE] Processing XML: ${xmlPath} with ${silences.length} silence(s)`);
   891→
   892→  try {
   893→    const result = await processXMLFile(xmlPath, silences, {
   894→      outputPath,
   895→      removeGaps
   896→    });
   897→
   898→    res.json({
   899→      success: true,
   900→      inputPath: xmlPath,
   901→      outputPath: result.outputPath,
   902→      stats: result.stats
   903→    });
   904→  } catch (err) {
   905→    console.error('[SPLICE] XML processing error:', err);
   906→    res.status(500).json({ error: err.message });
   907→  }
   908→});
   909→
   910→/**
   911→ * POST /cut-list - Generate a JSON cut list for direct DOM building (v3.5)
   912→ *
   913→ * Takes silences and optionally takes, returns a cut list that the
   914→ * plugin can use to build sequences directly via UXP APIs.
   915→ *
   916→ * Body:
   917→ * - sourceName: Name of the source clip
   918→ * - sourcePath: Full path to the source file
   919→ * - duration: Total duration in seconds
   920→ * - silences: Array of silence segments [{start, end, duration}]
   921→ * - takes: (optional) Array of detected takes
   922→ * - settings: (optional) Generation settings
   923→ */
   924→app.post('/cut-list', async (req, res) => {
   925→  const {
   926→    sourceName,
   927→    sourcePath,
   928→    duration,
   929→    silences,
   930→    takes = [],
   931→    settings = {}
   932→  } = req.body;
   933→
   934→  // Validate required fields
   935→  if (!sourceName && !sourcePath) {
   936→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });
   937→  }
   938→
   939→  if (typeof duration !== 'number' || duration <= 0) {
   940→    return res.status(400).json({ error: 'duration must be a positive number' });
   941→  }
   942→
   943→  if (!silences || !Array.isArray(silences)) {
   944→    return res.status(400).json({ error: 'silences array is required' });
   945→  }
   946→
   947→  console.log(`[SPLICE] Generating cut list for ${sourceName || sourcePath} (${silences.length} silences)`);
   948→
   949→  try {
   950→    const cutList = generateCutList({
   951→      sourceName: sourceName || path.basename(sourcePath),
   952→      sourcePath,
   953→      duration,
   954→      silences,
   955→      takes,
   956→      settings
   957→    });
   958→
   959→    // Validate the generated cut list
   960→    const validation = validateCutList(cutList);
   961→    if (!validation.valid) {
   962→      return res.status(500).json({
   963→        error: 'Generated cut list is invalid',
   964→        validationErrors: validation.errors
   965→      });
   966→    }
   967→
   968→    res.json({
   969→      success: true,
   970→      cutList
   971→    });
   972→  } catch (err) {
   973→    console.error('[SPLICE] Cut list generation error:', err);
   974→    res.status(500).json({ error: err.message });
   975→  }
   976→});
   977→
   978→/**
   979→ * POST /cut-list/takes - Generate a cut list that keeps only takes
   980→ *
   981→ * Alternative endpoint for "keep best takes only" workflow.
   982→ */
   983→app.post('/cut-list/takes', async (req, res) => {
   984→  const {
   985→    sourceName,
   986→    sourcePath,
   987→    duration,
   988→    takes,
   989→    settings = {}
   990→  } = req.body;
   991→
   992→  // Validate required fields
   993→  if (!sourceName && !sourcePath) {
   994→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });
   995→  }
   996→
   997→  if (typeof duration !== 'number' || duration <= 0) {
   998→    return res.status(400).json({ error: 'duration must be a positive number' });
   999→  }
  1000→
  1001→  if (!takes || !Array.isArray(takes) || takes.length === 0) {
  1002→    return res.status(400).json({ error: 'takes array is required and must not be empty' });
  1003→  }
  1004→
  1005→  console.log(`[SPLICE] Generating takes cut list for ${sourceName || sourcePath} (${takes.length} takes)`);
  1006→
  1007→  try {
  1008→    const cutList = generateTakesCutList({
  1009→      sourceName: sourceName || path.basename(sourcePath),
  1010→      sourcePath,
  1011→      duration,
  1012→      takes,
  1013→      settings
  1014→    });
  1015→
  1016→    res.json({
  1017→      success: true,
  1018→      cutList
  1019→    });
  1020→  } catch (err) {
  1021→    console.error('[SPLICE] Takes cut list generation error:', err);
  1022→    res.status(500).json({ error: err.message });
  1023→  }
  1024→});
  1025→
  1026→/**
  1027→ * GET /ffprobe-check - Check if FFprobe is installed
  1028→ */
  1029→app.get('/ffprobe-check', async (req, res) => {
  1030→  const installed = await isFFprobeInstalled();
  1031→  res.json({
  1032→    installed,
  1033→    message: installed
  1034→      ? 'FFprobe is available'
  1035→      : 'FFprobe not found. Install with: brew install ffmpeg'
  1036→  });
  1037→});
  1038→
  1039→/**
  1040→ * GET /replicate-check - Check if Replicate API is configured
  1041→ */
  1042→app.get('/replicate-check', async (req, res) => {
  1043→  const configured = isReplicateConfigured();
  1044→  res.json({
  1045→    configured,
  1046→    message: configured
  1047→      ? 'Replicate API is configured'
  1048→      : 'REPLICATE_API_TOKEN not set. Add to .env file.'
  1049→  });
  1050→});
  1051→
  1052→/**
  1053→ * POST /isolate-vocals - Isolate vocals from audio using Demucs
  1054→ *
  1055→ * Uses Replicate's Demucs model to separate vocals from background audio.
  1056→ * Cost: ~$0.015/min of audio
  1057→ *
  1058→ * Tier access:
  1059→ * - Starter: No access (upgrade required)
  1060→ * - Pro: 2 hours included, then $0.08/min overage
  1061→ * - Team: 5 hours included, then $0.08/min overage
  1062→ */
  1063→app.post('/isolate-vocals', async (req, res) => {
  1064→  const { audioPath, stem = 'vocals', outputDir = null } = req.body;
  1065→  const stripeCustomerId = req.headers['x-stripe-customer-id'];
  1066→
  1067→  if (!audioPath) {
  1068→    return res.status(400).json({ error: 'audioPath is required' });
  1069→  }
  1070→
  1071→  if (!fs.existsSync(audioPath)) {
  1072→    return res.status(404).json({ error: `File not found: ${audioPath}` });
  1073→  }
  1074→
  1075→  // Check Replicate configuration
  1076→  if (!isReplicateConfigured()) {
  1077→    return res.status(500).json({
  1078→      error: 'Replicate API not configured. Set REPLICATE_API_TOKEN in .env'
  1079→    });
  1080→  }
  1081→
  1082→  // Get audio duration for billing
  1083→  let audioDurationSeconds = 0;
  1084→  try {
  1085→    audioDurationSeconds = await getAudioDuration(audioPath);
  1086→  } catch (err) {
  1087→    console.warn('[SPLICE] Could not get audio duration:', err.message);
  1088→  }
  1089→
  1090→  const audioDurationMinutes = audioDurationSeconds / 60;
  1091→
  1092→  // Check isolation access if customer ID provided
  1093→  if (stripeCustomerId) {
  1094→    const accessCheck = await usageTracking.checkIsolationAccess(stripeCustomerId, audioDurationMinutes);
  1095→
  1096→    if (!accessCheck.allowed) {
  1097→      return res.status(403).json({
  1098→        error: accessCheck.message,
  1099→        reason: accessCheck.reason,
  1100→        upgradeRequired: accessCheck.reason === 'upgrade_required'
  1101→      });
  1102→    }
  1103→
  1104→    console.log(`[SPLICE] Isolation access: ${accessCheck.message}`);
  1105→  }
  1106→
  1107→  console.log(`[SPLICE] Isolating vocals: ${audioPath} (${audioDurationMinutes.toFixed(1)} min)`);
  1108→
  1109→  try {
  1110→    const result = await isolateVocals(audioPath, {
  1111→      stem,
  1112→      outputDir: outputDir || undefined
  1113→    });
  1114→
  1115→    // Deduct isolation usage if customer ID provided
  1116→    let usageInfo = null;
  1117→    if (stripeCustomerId) {
  1118→      usageInfo = await usageTracking.deductIsolationUsage(
  1119→        stripeCustomerId,
  1120→        audioDurationSeconds,
  1121→        'isolate-vocals'
  1122→      );
  1123→      console.log(`[SPLICE] Isolation usage deducted: ${audioDurationMinutes.toFixed(1)} min`);
  1124→      if (usageInfo.isolationUsed?.overageCost > 0) {
  1125→        console.log(`[SPLICE] Overage cost: $${usageInfo.isolationUsed.overageCost.toFixed(2)}`);
  1126→      }
  1127→    }
  1128→
  1129→    res.json({
  1130→      success: true,
  1131→      inputPath: audioPath,
  1132→      outputPath: result.outputPath,
  1133→      stem: result.stem,
  1134→      processingTime: result.processingTime,
  1135→      availableStems: result.allStems,
  1136→      audioDurationMinutes,
  1137→      usage: usageInfo ? {
  1138→        isolationHoursRemaining: usageInfo.isolationHoursRemaining,
  1139→        overageCost: usageInfo.isolationUsed?.overageCost || 0
  1140→      } : null
  1141→    });
  1142→  } catch (err) {
  1143→    console.error('[SPLICE] Vocal isolation error:', err);
  1144→    res.status(500).json({ error: err.message });
  1145→  }
  1146→});
  1147→
  1148→// =============================================================================
  1149→// Billing & Credits Routes
  1150→// =============================================================================
  1151→
  1152→/**
  1153→ * GET /credits - Get user's credit balance
  1154→ *
  1155→ * Requires x-stripe-customer-id header
  1156→ */
  1157→app.get('/credits', async (req, res) => {
  1158→  const stripeCustomerId = req.headers['x-stripe-customer-id'];
  1159→
  1160→  if (!stripeCustomerId) {
  1161→    return res.status(401).json({
  1162→      error: 'Authentication required',
  1163→      message: 'Missing x-stripe-customer-id header'
  1164→    });
  1165→  }
  1166→
  1167→  try {
  1168→    const balance = await usageTracking.getBalance(stripeCustomerId);
  1169→    res.json({
  1170→      success: true,
  1171→      ...balance
  1172→    });
  1173→  } catch (err) {
  1174→    console.error('[SPLICE] Credits error:', err);
  1175→    res.status(500).json({ error: err.message });
  1176→  }
  1177→});
  1178→
  1179→/**
  1180→ * GET /usage-history - Get user's usage history
  1181→ */
  1182→app.get('/usage-history', async (req, res) => {
  1183→  const stripeCustomerId = req.headers['x-stripe-customer-id'];
  1184→
  1185→  if (!stripeCustomerId) {
  1186→    return res.status(401).json({
  1187→      error: 'Authentication required',
  1188→      message: 'Missing x-stripe-customer-id header'
  1189→    });
  1190→  }
  1191→
  1192→  try {
  1193→    const history = await usageTracking.getUsageHistory(stripeCustomerId);
  1194→    res.json({
  1195→      success: true,
  1196→      history
  1197→    });
  1198→  } catch (err) {
  1199→    console.error('[SPLICE] Usage history error:', err);
  1200→    res.status(500).json({ error: err.message });
  1201→  }
  1202→});
  1203→
  1204→// =============================================================================
  1205→// Start Server
  1206→// =============================================================================
  1207→
  1208→// Initialize database and start server
  1209→async function startServer() {
  1210→  try {
  1211→    await usageTracking.initDatabase();
  1212→    console.log('[SPLICE] Database initialized');
  1213→
  1214→    if (isProduction || !httpsOptions) {
  1215→      // Production: Railway provides TLS termination, use HTTP
  1216→      http.createServer(app).listen(PORT, () => {
  1217→        console.log(`[SPLICE] Backend running at http://0.0.0.0:${PORT} (production)`);
  1218→      });
  1219→    } else {
  1220→      // Development: Use HTTPS with local certificates
  1221→      https.createServer(httpsOptions, app).listen(PORT, () => {
  1222→        console.log(`[SPLICE] Backend running at https://127.0.0.1:${PORT} (development)`);
  1223→        console.log(`[SPLICE] POST /analyze with { "wavPath": "/path/to/audio.wav" }`);
  1224→      });
  1225→    }
  1226→  } catch (err) {
  1227→    console.error('[SPLICE] Failed to start server:', err);
  1228→    process.exit(1);
  1229→  }
  1230→}
  1231→
  1232→startServer();
  1233→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
