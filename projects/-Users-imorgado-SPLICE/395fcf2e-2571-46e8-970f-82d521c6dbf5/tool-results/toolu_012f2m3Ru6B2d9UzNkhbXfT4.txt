     1→/**
     2→ * SPLICE Backend Server
     3→ *
     4→ * Main entry point for the SPLICE backend API.
     5→ * Orchestrates the audio analysis pipeline.
     6→ *
     7→ * Slices:
     8→ * - Slice 4: Transcription (services/transcription.js)
     9→ * - Slice 5: Take Detection (services/takeDetection.js)
    10→ */
    11→
    12→require('dotenv').config();
    13→
    14→const express = require('express');
    15→const cors = require('cors');
    16→const fs = require('fs');
    17→const https = require('https');
    18→const http = require('http');
    19→const path = require('path');
    20→
    21→// Check if running in production (Railway injects RAILWAY_ENVIRONMENT)
    22→const isProduction = process.env.NODE_ENV === 'production' || process.env.RAILWAY_ENVIRONMENT;
    23→
    24→// Import slice services
    25→const { transcribeAudio, transcribeWithWords } = require('./services/transcription');
    26→const { detectTakes } = require('./services/takeDetection');
    27→const { detectSilences } = require('./services/silenceDetection');
    28→const { detectAudioSilences, isFFprobeInstalled, getAudioDuration } = require('./services/ffprobeSilence');
    29→const { detectSilencesRMS, sensitivityToParams } = require('./services/rmsSilenceDetection');
    30→const {
    31→  detectProfanity,
    32→  getProfanityList,
    33→  getSupportedLanguages,
    34→  getAvailableBleepSounds,
    35→  parseWordList
    36→} = require('./services/profanityDetection');
    37→const {
    38→  detectRepetitionsBasic,
    39→  detectRepetitionsAdvanced,
    40→  detectStutters,
    41→  detectAllRepetitions
    42→} = require('./services/repetitionDetection');
    43→const {
    44→  analyzeMultitrack,
    45→  autoBalanceMultitrack
    46→} = require('./services/multitrackAnalysis');
    47→const {
    48→  toSRT,
    49→  toVTT,
    50→  toPlainText,
    51→  toJSON,
    52→  exportToFile,
    53→  getSupportedFormats
    54→} = require('./services/captionExporter');
    55→const { processXMLFile } = require('./services/xmlProcessor');
    56→const { isolateVocals, isReplicateConfigured } = require('./services/vocalIsolation');
    57→const { generateCutList, generateTakesCutList, validateCutList } = require('./services/cutListGenerator');
    58→
    59→// Usage tracking and billing
    60→const usageTracking = require('./services/usageTracking');
    61→// Rate limiter for usage-based endpoints
    62→const { requireCredits } = require('./middleware/rateLimiter');
    63→
    64→// Maximum file size for audio processing (500MB)
    65→const MAX_FILE_SIZE_BYTES = 500 * 1024 * 1024;
    66→
    67→/**
    68→ * Validate file size to prevent OOM crashes
    69→ * @param {string} filePath - Path to file
    70→ * @returns {Promise<{valid: boolean, size?: number, error?: string}>}
    71→ */
    72→async function validateFileSize(filePath) {
    73→  try {
    74→    const stats = await require('fs').promises.stat(filePath);
    75→    if (stats.size > MAX_FILE_SIZE_BYTES) {
    76→      return {
    77→        valid: false,
    78→        size: stats.size,
    79→        error: `File too large (${(stats.size / 1024 / 1024).toFixed(1)}MB). Maximum allowed: ${MAX_FILE_SIZE_BYTES / 1024 / 1024}MB`
    80→      };
    81→    }
    82→    return { valid: true, size: stats.size };
    83→  } catch (err) {
    84→    return { valid: false, error: `Cannot access file: ${err.message}` };
    85→  }
    86→}
    87→
    88→// Stripe for webhooks
    89→const Stripe = require('stripe');
    90→const stripe = new Stripe(process.env.STRIPE_SECRET_KEY);
    91→
    92→// =============================================================================
    93→// Server Configuration
    94→// =============================================================================
    95→
    96→const app = express();
    97→const PORT = process.env.PORT || 3847;
    98→
    99→// HTTPS certificates (generated by mkcert) - only for local development
   100→let httpsOptions = null;
   101→if (!isProduction) {
   102→  const keyPath = path.join(__dirname, 'localhost+1-key.pem');
   103→  const certPath = path.join(__dirname, 'localhost+1.pem');
   104→  if (fs.existsSync(keyPath) && fs.existsSync(certPath)) {
   105→    httpsOptions = {
   106→      key: fs.readFileSync(keyPath),
   107→      cert: fs.readFileSync(certPath)
   108→    };
   109→  }
   110→}
   111→
   112→app.use(cors());
   113→
   114→// Helper to determine tier from price ID with logging
   115→function getTierFromPriceId(priceId) {
   116→  if (priceId === process.env.STRIPE_PRICE_STARTER) return 'starter';
   117→  if (priceId === process.env.STRIPE_PRICE_PRO) return 'pro';
   118→  if (priceId === process.env.STRIPE_PRICE_TEAM) return 'team';
   119→
   120→  // Log unknown price ID for debugging
   121→  console.warn(`[SPLICE] Unknown price ID: ${priceId} - defaulting to starter tier`);
   122→  return 'starter';
   123→}
   124→
   125→// Stripe webhook needs raw body - must be before express.json()
   126→app.post('/webhooks/stripe', express.raw({ type: 'application/json' }), async (req, res) => {
   127→  const sig = req.headers['stripe-signature'];
   128→  const webhookSecret = process.env.STRIPE_WEBHOOK_SECRET;
   129→
   130→  let event;
   131→
   132→  try {
   133→    if (webhookSecret) {
   134→      event = stripe.webhooks.constructEvent(req.body, sig, webhookSecret);
   135→    } else if (isProduction) {
   136→      // SECURITY: Reject unsigned webhooks in production
   137→      console.error('[SPLICE] CRITICAL: STRIPE_WEBHOOK_SECRET not set in production');
   138→      return res.status(500).json({ error: 'Webhook configuration error: secret not configured' });
   139→    } else {
   140→      // For local development testing only
   141→      event = JSON.parse(req.body);
   142→      console.warn('[SPLICE] Warning: Processing webhook without signature verification (dev only)');
   143→    }
   144→  } catch (err) {
   145→    console.error('[SPLICE] Webhook signature verification failed:', err.message);
   146→    return res.status(400).json({ error: 'Webhook signature verification failed' });
   147→  }
   148→
   149→  console.log(`[SPLICE] Webhook received: ${event.type} (${event.id})`);
   150→
   151→  // Idempotency check - skip if already processed
   152→  if (await usageTracking.isEventProcessed(event.id)) {
   153→    console.log(`[SPLICE] Event ${event.id} already processed, skipping`);
   154→    return res.json({ received: true, skipped: true });
   155→  }
   156→
   157→  try {
   158→    switch (event.type) {
   159→      case 'customer.subscription.created':
   160→      case 'customer.subscription.updated': {
   161→        const subscription = event.data.object;
   162→        const customerId = subscription.customer;
   163→
   164→        // Validate customerId
   165→        if (!customerId) {
   166→          console.error('[SPLICE] Missing customer ID in subscription event');
   167→          return res.status(400).json({ error: 'Missing customer ID' });
   168→        }
   169→
   170→        // Get tier from price ID
   171→        const priceId = subscription.items?.data?.[0]?.price?.id;
   172→        const tier = getTierFromPriceId(priceId);
   173→
   174→        // Update user tier and reset hours
   175→        await usageTracking.updateTier(customerId, tier);
   176→        console.log(`[SPLICE] Updated customer ${customerId} to tier: ${tier}`);
   177→        break;
   178→      }
   179→
   180→      case 'customer.subscription.deleted': {
   181→        const subscription = event.data.object;
   182→        const customerId = subscription.customer;
   183→
   184→        // Validate customerId
   185→        if (!customerId) {
   186→          console.error('[SPLICE] Missing customer ID in subscription.deleted event');
   187→          return res.status(400).json({ error: 'Missing customer ID' });
   188→        }
   189→
   190→        // Downgrade to cancelled (0 hours)
   191→        await usageTracking.updateTier(customerId, 'cancelled');
   192→        console.log(`[SPLICE] Subscription cancelled for customer ${customerId}`);
   193→        break;
   194→      }
   195→
   196→      case 'invoice.payment_succeeded': {
   197→        const invoice = event.data.object;
   198→        const customerId = invoice.customer;
   199→        const subscriptionId = invoice.subscription;
   200→
   201→        // Validate customerId
   202→        if (!customerId) {
   203→          console.error('[SPLICE] Missing customer ID in invoice event');
   204→          return res.status(400).json({ error: 'Missing customer ID' });
   205→        }
   206→
   207→        // Reset hours on successful payment (new billing period)
   208→        if (subscriptionId) {
   209→          const subscription = await stripe.subscriptions.retrieve(subscriptionId);
   210→          const priceId = subscription.items?.data?.[0]?.price?.id;
   211→          const tier = getTierFromPriceId(priceId);
   212→
   213→          await usageTracking.resetHours(customerId, tier);
   214→          console.log(`[SPLICE] Reset hours for customer ${customerId} (tier: ${tier})`);
   215→        }
   216→        break;
   217→      }
   218→
   219→      default:
   220→        console.log(`[SPLICE] Unhandled event type: ${event.type}`);
   221→    }
   222→
   223→    // Record event as processed (idempotency)
   224→    await usageTracking.recordWebhookEvent(event.id, event.type);
   225→
   226→    res.json({ received: true });
   227→  } catch (err) {
   228→    console.error('[SPLICE] Webhook handler error:', err);
   229→    res.status(500).json({ error: err.message });
   230→  }
   231→});
   232→
   233→app.use(express.json());
   234→
   235→// =============================================================================
   236→// Routes
   237→// =============================================================================
   238→
   239→/**
   240→ * GET / - API information
   241→ */
   242→app.get('/', (req, res) => {
   243→  res.json({
   244→    service: 'splice-backend',
   245→    version: '0.3.0',
   246→    endpoints: {
   247→      'GET /': 'This info',
   248→      'GET /health': 'Health check',
   249→      'GET /ffprobe-check': 'Check if FFprobe is installed',
   250→      'GET /replicate-check': 'Check if Replicate API is configured',
   251→      'POST /analyze': 'Analyze WAV file { wavPath }',
   252→      'POST /silences': 'Detect silences via Whisper gaps { wavPath, threshold: 0.5 }',
   253→      'POST /silences-audio': 'Detect silences via FFprobe { wavPath, threshold: -30, minDuration: 0.5, padding: 0.1 }',
   254→      'POST /silences-rms': 'Detect silences via RMS analysis { wavPath, threshold: -30, minSilenceLength: 0.5, paddingStart: 0.1, paddingEnd: 0.05, autoThreshold: false, sensitivity: 50 }',
   255→      'POST /profanity': 'Detect profanity in transcript { wavPath, language: "en", customBlocklist: [], customAllowlist: [] }',
   256→      'GET /profanity/languages': 'Get supported languages for profanity detection',
   257→      'GET /profanity/bleeps': 'Get available bleep sounds',
   258→      'POST /repetitions': 'Detect phrase repetitions and stutters { wavPath, phraseSize: 5, tolerance: 0.7, useOpenAI: false }',
   259→      'POST /fillers': 'Detect filler words (um, uh, like, etc.) { wavPath, customFillers: [] }',
   260→      'POST /stutters': 'Detect single-word stutters only { wavPath, minRepeats: 2 }',
   261→      'POST /export/captions': 'Export transcript to caption format { wavPath, format: srt|vtt|txt|json, outputPath? }',
   262→      'GET /export/formats': 'Get supported caption export formats',
   263→      'POST /multitrack': 'Analyze multiple audio tracks for multicam { audioPaths: [], speakerNames: [], videoTrackMapping: {} }',
   264→      'POST /multitrack/auto-balance': 'Auto-balance speaker screentime { audioPaths: [], speakerNames: [] }',
   265→      'POST /process-xml': 'Process FCP XML { xmlPath, silences, removeGaps: true }',
   266→      'POST /cut-list': 'Generate JSON cut list for DOM building (v3.5) { sourceName, sourcePath, duration, silences, takes?, settings? }',
   267→      'POST /cut-list/takes': 'Generate cut list keeping only takes { sourceName, sourcePath, duration, takes, settings? }',
   268→      'POST /isolate-vocals': 'Isolate vocals from audio { audioPath }',
   269→      'POST /batch/silences': 'Batch process multiple files for silence detection { files: [], options: {} }',
   270→      'GET /batch/status/:jobId': 'Get batch job status',
   271→      'GET /batch/results/:jobId': 'Get full batch job results',
   272→      'GET /batch/jobs': 'List all batch jobs',
   273→      'DELETE /batch/:jobId': 'Delete a batch job',
   274→      'GET /credits': 'Get user credit balance (requires x-stripe-customer-id header)',
   275→      'GET /usage-history': 'Get usage history (requires x-stripe-customer-id header)',
   276→      'POST /webhooks/stripe': 'Stripe webhook endpoint'
   277→    }
   278→  });
   279→});
   280→
   281→/**
   282→ * GET /health - Health check
   283→ */
   284→app.get('/health', (req, res) => {
   285→  res.json({ status: 'ok', service: 'splice-backend' });
   286→});
   287→
   288→/**
   289→ * POST /analyze - Main analysis endpoint
   290→ *
   291→ * Pipeline:
   292→ * 1. Validate input (wavPath)
   293→ * 2. Slice 4: Transcribe audio with Groq Whisper
   294→ * 3. Slice 5: Detect takes with GPT-4o-mini
   295→ * 4. Return combined results
   296→ */
   297→app.post('/analyze', requireCredits({ endpoint: 'analyze' }), async (req, res) => {
   298→  const { wavPath } = req.body;
   299→
   300→  // Validate input
   301→  if (!wavPath) {
   302→    return res.status(400).json({ error: 'wavPath is required' });
   303→  }
   304→
   305→  if (!fs.existsSync(wavPath)) {
   306→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   307→  }
   308→
   309→  console.log(`[SPLICE] Analyzing: ${wavPath}`);
   310→
   311→  try {
   312→    // Slice 4 - GPT-4o-mini transcription
   313→    const transcript = await transcribeAudio(wavPath);
   314→
   315→    // Slice 5 - GPT-4o-mini take detection
   316→    const takes = await detectTakes(transcript);
   317→
   318→    res.json({
   319→      success: true,
   320→      wavPath,
   321→      transcript,
   322→      takes
   323→    });
   324→  } catch (err) {
   325→    console.error('[SPLICE] Error:', err);
   326→    res.status(500).json({ error: err.message });
   327→  }
   328→});
   329→
   330→/**
   331→ * POST /silences - Detect silent gaps in audio
   332→ *
   333→ * Pipeline:
   334→ * 1. Transcribe audio with Whisper (reuses transcription)
   335→ * 2. Analyze gaps between segments
   336→ * 3. Return silence regions
   337→ */
   338→app.post('/silences', requireCredits({ endpoint: 'silences' }), async (req, res) => {
   339→  const { wavPath, threshold = 0.5 } = req.body;
   340→
   341→  if (!wavPath) {
   342→    return res.status(400).json({ error: 'wavPath is required' });
   343→  }
   344→
   345→  if (!fs.existsSync(wavPath)) {
   346→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   347→  }
   348→
   349→  console.log(`[SPLICE] Detecting silences: ${wavPath} (threshold: ${threshold}s)`);
   350→
   351→  try {
   352→    const transcript = await transcribeAudio(wavPath);
   353→    const silences = detectSilences(transcript.segments, threshold);
   354→
   355→    res.json({
   356→      success: true,
   357→      wavPath,
   358→      threshold,
   359→      silences,
   360→      count: silences.length,
   361→      totalSilenceDuration: silences.reduce((sum, s) => sum + s.duration, 0).toFixed(2)
   362→    });
   363→  } catch (err) {
   364→    console.error('[SPLICE] Silence detection error:', err);
   365→    res.status(500).json({ error: err.message });
   366→  }
   367→});
   368→
   369→/**
   370→ * POST /silences-audio - Detect silences using FFprobe audio analysis
   371→ *
   372→ * Uses actual audio levels (dB threshold) instead of transcript gaps.
   373→ * More accurate for detecting silence vs background noise.
   374→ */
   375→app.post('/silences-audio', async (req, res) => {
   376→  const {
   377→    wavPath,
   378→    threshold = -30,
   379→    minDuration = 0.5,
   380→    padding = 0.1
   381→  } = req.body;
   382→
   383→  if (!wavPath) {
   384→    return res.status(400).json({ error: 'wavPath is required' });
   385→  }
   386→
   387→  if (!fs.existsSync(wavPath)) {
   388→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   389→  }
   390→
   391→  // Check FFprobe availability
   392→  const ffprobeAvailable = await isFFprobeInstalled();
   393→  if (!ffprobeAvailable) {
   394→    return res.status(500).json({
   395→      error: 'FFprobe not installed. Run: brew install ffmpeg'
   396→    });
   397→  }
   398→
   399→  console.log(`[SPLICE] FFprobe silence detection: ${wavPath} (threshold: ${threshold}dB, min: ${minDuration}s)`);
   400→
   401→  try {
   402→    const silences = await detectAudioSilences(wavPath, {
   403→      threshold,
   404→      minDuration,
   405→      padding
   406→    });
   407→
   408→    const totalDuration = silences.reduce((sum, s) => sum + s.duration, 0);
   409→
   410→    res.json({
   411→      success: true,
   412→      wavPath,
   413→      threshold,
   414→      minDuration,
   415→      padding,
   416→      silences,
   417→      count: silences.length,
   418→      totalSilenceDuration: totalDuration.toFixed(2)
   419→    });
   420→  } catch (err) {
   421→    console.error('[SPLICE] FFprobe silence detection error:', err);
   422→    res.status(500).json({ error: err.message });
   423→  }
   424→});
   425→
   426→/**
   427→ * POST /silences-rms - Detect silences using RMS audio analysis
   428→ *
   429→ * Advanced silence detection with:
   430→ * - RMS (Root Mean Square) audio level analysis
   431→ * - Auto-threshold detection from audio histogram
   432→ * - Configurable padding (before/after cuts)
   433→ * - Sensitivity slider mapping (0-100)
   434→ *
   435→ * Options:
   436→ * - threshold: dBFS threshold (-60 to -20, default: -30)
   437→ * - minSilenceLength: Minimum silence duration in seconds (default: 0.5)
   438→ * - seekStep: Analysis window step in seconds (default: 0.05)
   439→ * - paddingStart: Buffer before silence in seconds (default: 0.1)
   440→ * - paddingEnd: Buffer after silence in seconds (default: 0.05)
   441→ * - autoThreshold: Auto-detect optimal threshold (default: false)
   442→ * - sensitivity: UI sensitivity 0-100 (overrides other params if provided)
   443→ */
   444→app.post('/silences-rms', requireCredits({ endpoint: 'silences-rms' }), async (req, res) => {
   445→  const { wavPath, sensitivity, ...manualOptions } = req.body;
   446→
   447→  if (!wavPath) {
   448→    return res.status(400).json({ error: 'wavPath is required' });
   449→  }
   450→
   451→  if (!fs.existsSync(wavPath)) {
   452→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   453→  }
   454→
   455→  // Validate file size to prevent OOM
   456→  const sizeCheck = await validateFileSize(wavPath);
   457→  if (!sizeCheck.valid) {
   458→    return res.status(413).json({ error: sizeCheck.error });
   459→  }
   460→
   461→  // Check FFprobe availability (needed for audio extraction)
   462→  const ffprobeAvailable = await isFFprobeInstalled();
   463→  if (!ffprobeAvailable) {
   464→    return res.status(500).json({
   465→      error: 'FFprobe not installed. Run: brew install ffmpeg'
   466→    });
   467→  }
   468→
   469→  // Build options - use sensitivity if provided, otherwise use manual options
   470→  let options = {};
   471→  if (typeof sensitivity === 'number') {
   472→    options = sensitivityToParams(sensitivity);
   473→    console.log(`[SPLICE] RMS detection with sensitivity ${sensitivity}`);
   474→  } else {
   475→    options = {
   476→      threshold: manualOptions.threshold ?? -30,
   477→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,
   478→      seekStep: manualOptions.seekStep ?? 0.05,
   479→      paddingStart: manualOptions.paddingStart ?? 0.1,
   480→      paddingEnd: manualOptions.paddingEnd ?? 0.05,
   481→      autoThreshold: manualOptions.autoThreshold ?? false,
   482→      mergeDistance: manualOptions.mergeDistance ?? 0.2
   483→    };
   484→  }
   485→
   486→  console.log(`[SPLICE] RMS silence detection: ${wavPath}`);
   487→
   488→  try {
   489→    const result = await detectSilencesRMS(wavPath, options);
   490→
   491→    res.json({
   492→      success: true,
   493→      wavPath,
   494→      ...result,
   495→      count: result.silences.length,
   496→      totalSilenceDuration: result.metadata.totalSilenceDuration.toFixed(2)
   497→    });
   498→  } catch (err) {
   499→    console.error('[SPLICE] RMS silence detection error:', err);
   500→    res.status(500).json({ error: err.message });
   501→  }
   502→});
   503→
   504→// =============================================================================
   505→// Profanity Detection Routes
   506→// =============================================================================
   507→
   508→/**
   509→ * POST /profanity - Detect profanity in audio/transcript
   510→ *
   511→ * Transcribes audio (if needed) and detects profanity words.
   512→ * Returns word-level and segment-level results for muting/bleeping.
   513→ *
   514→ * Options:
   515→ * - wavPath: Path to audio file (required)
   516→ * - transcript: Pre-existing transcript (optional, skips transcription)
   517→ * - language: Language code (en, es, fr, de) - default: en
   518→ * - customBlocklist: Array or comma-separated string of additional words to censor
   519→ * - customAllowlist: Array or comma-separated string of words to allow
   520→ * - frameRate: Frame rate for boundary alignment (default: 30)
   521→ */
   522→app.post('/profanity', requireCredits({ endpoint: 'profanity' }), async (req, res) => {
   523→  const {
   524→    wavPath,
   525→    transcript: providedTranscript,
   526→    language = 'en',
   527→    customBlocklist = [],
   528→    customAllowlist = [],
   529→    frameRate = 30
   530→  } = req.body;
   531→
   532→  if (!wavPath && !providedTranscript) {
   533→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   534→  }
   535→
   536→  if (wavPath && !fs.existsSync(wavPath)) {
   537→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   538→  }
   539→
   540→  console.log(`[SPLICE] Profanity detection: ${wavPath || 'provided transcript'} (language: ${language})`);
   541→
   542→  try {
   543→    // Get or create transcript with word-level timestamps
   544→    let transcript = providedTranscript;
   545→    if (!transcript && wavPath) {
   546→      // Use transcribeWithWords for word-level timestamps required by profanity detection
   547→      transcript = await transcribeWithWords(wavPath);
   548→    }
   549→
   550→    // Validate transcript has words
   551→    if (!transcript || !transcript.words || transcript.words.length === 0) {
   552→      return res.status(400).json({
   553→        error: 'Transcript must contain word-level timing data',
   554→        hint: 'Ensure transcription returns words array with start/end times'
   555→      });
   556→    }
   557→
   558→    // Parse custom lists
   559→    const blocklist = parseWordList(customBlocklist);
   560→    const allowlist = parseWordList(customAllowlist);
   561→
   562→    // Detect profanity
   563→    const result = detectProfanity(transcript, {
   564→      language,
   565→      customBlocklist: blocklist,
   566→      customAllowlist: allowlist,
   567→      frameRate
   568→    });
   569→
   570→    res.json({
   571→      success: true,
   572→      wavPath,
   573→      ...result
   574→    });
   575→  } catch (err) {
   576→    console.error('[SPLICE] Profanity detection error:', err);
   577→    res.status(500).json({ error: err.message });
   578→  }
   579→});
   580→
   581→/**
   582→ * GET /profanity/languages - Get supported languages
   583→ */
   584→app.get('/profanity/languages', (req, res) => {
   585→  res.json({
   586→    success: true,
   587→    languages: getSupportedLanguages()
   588→  });
   589→});
   590→
   591→/**
   592→ * GET /profanity/bleeps - Get available bleep sounds
   593→ */
   594→app.get('/profanity/bleeps', (req, res) => {
   595→  res.json({
   596→    success: true,
   597→    sounds: getAvailableBleepSounds()
   598→  });
   599→});
   600→
   601→/**
   602→ * GET /profanity/list/:language - Get default profanity list for a language
   603→ */
   604→app.get('/profanity/list/:language', (req, res) => {
   605→  const { language } = req.params;
   606→  const list = getProfanityList(language);
   607→
   608→  res.json({
   609→    success: true,
   610→    language,
   611→    wordCount: list.length,
   612→    // Return first 50 words as sample (full list is large)
   613→    sample: list.slice(0, 50),
   614→    note: 'Full list available but truncated for response size'
   615→  });
   616→});
   617→
   618→// =============================================================================
   619→// Repetition/Stutter Detection Routes
   620→// =============================================================================
   621→
   622→/**
   623→ * POST /repetitions - Detect phrase repetitions and stutters
   624→ *
   625→ * Analyzes transcript for repeated phrases and stutters.
   626→ * Returns segments that can be removed to clean up the edit.
   627→ *
   628→ * Options:
   629→ * - wavPath: Path to audio file (required unless transcript provided)
   630→ * - transcript: Pre-existing transcript (optional)
   631→ * - phraseSize: Words per comparison window (default: 5)
   632→ * - tolerance: Similarity threshold 0-1 (default: 0.7)
   633→ * - searchRadius: Words to search ahead (default: 100)
   634→ * - useOpenAI: Use OpenAI for boundary refinement (default: false)
   635→ * - includeStutters: Also detect single-word stutters (default: true)
   636→ */
   637→app.post('/repetitions', async (req, res) => {
   638→  const {
   639→    wavPath,
   640→    transcript: providedTranscript,
   641→    phraseSize = 5,
   642→    tolerance = 0.7,
   643→    searchRadius = 100,
   644→    useOpenAI = false,
   645→    includeStutters = true
   646→  } = req.body;
   647→
   648→  if (!wavPath && !providedTranscript) {
   649→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   650→  }
   651→
   652→  if (wavPath && !fs.existsSync(wavPath)) {
   653→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   654→  }
   655→
   656→  console.log(`[SPLICE] Repetition detection: ${wavPath || 'provided transcript'}`);
   657→
   658→  try {
   659→    // Get or create transcript with word-level timestamps
   660→    let transcript = providedTranscript;
   661→    if (!transcript && wavPath) {
   662→      // Use transcribeWithWords for word-level timestamps required by repetition detection
   663→      transcript = await transcribeWithWords(wavPath);
   664→    }
   665→
   666→    // Validate transcript has words
   667→    if (!transcript || !transcript.words || transcript.words.length === 0) {
   668→      return res.status(400).json({
   669→        error: 'Transcript must contain word-level timing data'
   670→      });
   671→    }
   672→
   673→    // Detect all repetitions (phrases + stutters)
   674→    const result = await detectAllRepetitions(transcript, {
   675→      phraseSize,
   676→      tolerance,
   677→      searchRadius,
   678→      useOpenAI
   679→    });
   680→
   681→    // Optionally filter out stutters
   682→    if (!includeStutters) {
   683→      result.stutters = [];
   684→      result.removalSegments = result.removalSegments.filter(s => s.type !== 'stutter');
   685→    }
   686→
   687→    res.json({
   688→      success: true,
   689→      wavPath,
   690→      ...result
   691→    });
   692→  } catch (err) {
   693→    console.error('[SPLICE] Repetition detection error:', err);
   694→    res.status(500).json({ error: err.message });
   695→  }
   696→});
   697→
   698→/**
   699→ * POST /fillers - Detect filler words (um, uh, like, etc.)
   700→ *
   701→ * Transcribes audio and identifies filler words with timestamps.
   702→ * Returns segments that can be cut or reviewed for removal.
   703→ *
   704→ * Options:
   705→ * - wavPath: Path to audio file (required unless transcript provided)
   706→ * - transcript: Pre-existing transcript with word-level timing (optional)
   707→ * - customFillers: Additional filler words to detect (optional)
   708→ */
   709→app.post('/fillers', async (req, res) => {
   710→  const {
   711→    wavPath,
   712→    transcript: providedTranscript,
   713→    customFillers = []
   714→  } = req.body;
   715→
   716→  if (!wavPath && !providedTranscript) {
   717→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   718→  }
   719→
   720→  if (wavPath && !fs.existsSync(wavPath)) {
   721→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   722→  }
   723→
   724→  console.log(`[SPLICE] Filler word detection: ${wavPath || 'provided transcript'}`);
   725→
   726→  try {
   727→    // Get or create transcript with word-level timestamps
   728→    let transcript = providedTranscript;
   729→    if (!transcript && wavPath) {
   730→      transcript = await transcribeWithWords(wavPath);
   731→    }
   732→
   733→    // Validate transcript has words
   734→    if (!transcript || !transcript.words || transcript.words.length === 0) {
   735→      return res.status(400).json({
   736→        error: 'Transcript must contain word-level timing data'
   737→      });
   738→    }
   739→
   740→    // Default filler words (common in English speech)
   741→    const defaultFillers = [
   742→      'um', 'uh', 'ah', 'er', 'eh',           // Hesitation sounds
   743→      'like', 'so', 'well', 'right',           // Discourse markers
   744→      'you know', 'i mean', 'basically',       // Filler phrases
   745→      'actually', 'literally', 'honestly',     // Overused qualifiers
   746→      'kind of', 'sort of', 'you see'          // Hedging phrases
   747→    ];
   748→
   749→    // Combine default + custom fillers (lowercase for matching)
   750→    const fillerSet = new Set([
   751→      ...defaultFillers,
   752→      ...customFillers.map(f => f.toLowerCase().trim())
   753→    ]);
   754→
   755→    // Detect filler words
   756→    const fillers = [];
   757→    const words = transcript.words;
   758→
   759→    for (let i = 0; i < words.length; i++) {
   760→      const word = words[i];
   761→      const normalizedWord = word.word.toLowerCase().replace(/[.,!?;:'"]/g, '').trim();
   762→
   763→      // Check single-word fillers
   764→      if (fillerSet.has(normalizedWord)) {
   765→        fillers.push({
   766→          word: word.word,
   767→          normalizedWord,
   768→          start: word.start,
   769→          end: word.end,
   770→          duration: word.end - word.start,
   771→          index: i,
   772→          type: 'filler'
   773→        });
   774→        continue;
   775→      }
   776→
   777→      // Check two-word phrases (e.g., "you know", "kind of")
   778→      if (i < words.length - 1) {
   779→        const nextWord = words[i + 1];
   780→        const twoWordPhrase = `${normalizedWord} ${nextWord.word.toLowerCase().replace(/[.,!?;:'"]/g, '').trim()}`;
   781→        if (fillerSet.has(twoWordPhrase)) {
   782→          fillers.push({
   783→            word: `${word.word} ${nextWord.word}`,
   784→            normalizedWord: twoWordPhrase,
   785→            start: word.start,
   786→            end: nextWord.end,
   787→            duration: nextWord.end - word.start,
   788→            index: i,
   789→            type: 'filler_phrase'
   790→          });
   791→          // Skip next word since it's part of this phrase
   792→          i++;
   793→        }
   794→      }
   795→    }
   796→
   797→    // Calculate total filler time
   798→    const totalFillerDuration = fillers.reduce((sum, f) => sum + f.duration, 0);
   799→    const audioDuration = transcript.duration || (words.length > 0 ? words[words.length - 1].end : 0);
   800→    const fillerPercentage = audioDuration > 0 ? (totalFillerDuration / audioDuration) * 100 : 0;
   801→
   802→    res.json({
   803→      success: true,
   804→      wavPath,
   805→      fillers,
   806→      metadata: {
   807→        totalWords: words.length,
   808→        fillerCount: fillers.length,
   809→        totalFillerDuration: parseFloat(totalFillerDuration.toFixed(3)),
   810→        audioDuration: parseFloat(audioDuration.toFixed(3)),
   811→        fillerPercentage: parseFloat(fillerPercentage.toFixed(2)),
   812→        fillersPerMinute: audioDuration > 0 ? parseFloat((fillers.length / (audioDuration / 60)).toFixed(2)) : 0
   813→      },
   814→      removalSegments: fillers.map(f => ({
   815→        start: f.start,
   816→        end: f.end,
   817→        duration: f.duration,
   818→        reason: `Filler: "${f.word}"`,
   819→        type: f.type
   820→      }))
   821→    });
   822→  } catch (err) {
   823→    console.error('[SPLICE] Filler detection error:', err);
   824→    res.status(500).json({ error: err.message });
   825→  }
   826→});
   827→
   828→/**
   829→ * POST /stutters - Detect single-word stutters only
   830→ *
   831→ * Focused detection for word-level stutters (e.g., "I I I think").
   832→ * Faster than full repetition detection.
   833→ */
   834→app.post('/stutters', async (req, res) => {
   835→  const {
   836→    wavPath,
   837→    transcript: providedTranscript,
   838→    options = {},
   839→    // Support both top-level and nested options for flexibility
   840→    minRepeats = options.minRepeats ?? 2,
   841→    maxGapMs = options.maxGapMs ?? 500,
   842→    ignoreFillers = options.ignoreFillers ?? true,
   843→    minWordLength = options.minWordLength ?? 1
   844→  } = req.body;
   845→
   846→  if (!wavPath && !providedTranscript) {
   847→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   848→  }
   849→
   850→  if (wavPath && !fs.existsSync(wavPath)) {
   851→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   852→  }
   853→
   854→  console.log(`[SPLICE] Stutter detection: ${wavPath || 'provided transcript'}`);
   855→
   856→  try {
   857→    // Get or create transcript with word-level timestamps
   858→    let transcript = providedTranscript;
   859→    if (!transcript && wavPath) {
   860→      // Use transcribeWithWords for word-level timestamps required by stutter detection
   861→      transcript = await transcribeWithWords(wavPath);
   862→    }
   863→
   864→    // Validate transcript exists and has words array
   865→    if (!transcript || !transcript.words) {
   866→      return res.status(400).json({
   867→        error: 'Transcript must contain word-level timing data'
   868→      });
   869→    }
   870→
   871→    // Empty transcript returns empty result (not an error)
   872→    if (transcript.words.length === 0) {
   873→      return res.json({
   874→        success: true,
   875→        stutters: [],
   876→        metadata: { type: 'stutters', totalWords: 0, stutterCount: 0, totalRepeatedWords: 0 }
   877→      });
   878→    }
   879→
   880→    // Detect stutters only
   881→    const result = detectStutters(transcript, {
   882→      minRepeats,
   883→      maxGapMs,
   884→      ignoreFillers,
   885→      minWordLength
   886→    });
   887→
   888→    res.json({
   889→      success: true,
   890→      wavPath,
   891→      ...result
   892→    });
   893→  } catch (err) {
   894→    console.error('[SPLICE] Stutter detection error:', err);
   895→    res.status(500).json({ error: err.message });
   896→  }
   897→});
   898→
   899→// =============================================================================
   900→// Caption Export Routes
   901→// =============================================================================
   902→
   903→/**
   904→ * POST /export/captions - Export transcript to caption format (SRT, VTT, etc.)
   905→ *
   906→ * Converts a transcript to the specified caption format.
   907→ * Can optionally save to file.
   908→ *
   909→ * Options:
   910→ * - wavPath: Path to audio file (to transcribe first)
   911→ * - transcript: Pre-existing transcript with word-level timing
   912→ * - format: Export format (srt, vtt, txt, json) - default: srt
   913→ * - outputPath: Optional file path to save to
   914→ * - maxWordsPerCaption: Max words per caption (default: 8)
   915→ * - maxDuration: Max duration per caption in seconds (default: 5)
   916→ */
   917→app.post('/export/captions', async (req, res) => {
   918→  const {
   919→    wavPath,
   920→    transcript: providedTranscript,
   921→    format = 'srt',
   922→    outputPath = null,
   923→    maxWordsPerCaption = 8,
   924→    maxDuration = 5
   925→  } = req.body;
   926→
   927→  if (!wavPath && !providedTranscript) {
   928→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   929→  }
   930→
   931→  if (wavPath && !fs.existsSync(wavPath)) {
   932→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   933→  }
   934→
   935→  console.log(`[SPLICE] Caption export: ${wavPath || 'provided transcript'} -> ${format}`);
   936→
   937→  try {
   938→    // Get or create transcript with word-level timestamps
   939→    let transcript = providedTranscript;
   940→    if (!transcript && wavPath) {
   941→      transcript = await transcribeWithWords(wavPath);
   942→    }
   943→
   944→    const exportOptions = { maxWordsPerCaption, maxDuration };
   945→
   946→    // Generate caption content based on format
   947→    let content;
   948→    let mimeType;
   949→
   950→    switch (format.toLowerCase()) {
   951→      case 'srt':
   952→        content = toSRT(transcript, exportOptions);
   953→        mimeType = 'application/x-subrip';
   954→        break;
   955→      case 'vtt':
   956→      case 'webvtt':
   957→        content = toVTT(transcript, exportOptions);
   958→        mimeType = 'text/vtt';
   959→        break;
   960→      case 'txt':
   961→      case 'text':
   962→        content = toPlainText(transcript, { ...exportOptions, includeTimestamps: true });
   963→        mimeType = 'text/plain';
   964→        break;
   965→      case 'json':
   966→        content = toJSON(transcript);
   967→        mimeType = 'application/json';
   968→        break;
   969→      default:
   970→        return res.status(400).json({
   971→          error: `Unsupported format: ${format}`,
   972→          supportedFormats: getSupportedFormats()
   973→        });
   974→    }
   975→
   976→    // Save to file if outputPath provided
   977→    let savedPath = null;
   978→    if (outputPath) {
   979→      const result = await exportToFile(transcript, outputPath, format, exportOptions);
   980→      savedPath = result.path;
   981→      console.log(`[SPLICE] Saved captions to: ${savedPath}`);
   982→    }
   983→
   984→    res.json({
   985→      success: true,
   986→      format,
   987→      content,
   988→      mimeType,
   989→      savedPath,
   990→      wordCount: transcript.words?.length || 0,
   991→      duration: transcript.duration || 0
   992→    });
   993→  } catch (err) {
   994→    console.error('[SPLICE] Caption export error:', err);
   995→    res.status(500).json({ error: err.message });
   996→  }
   997→});
   998→
   999→/**
  1000→ * GET /export/formats - Get supported export formats
  1001→ */
  1002→app.get('/export/formats', (req, res) => {
  1003→  res.json({
  1004→    success: true,
  1005→    formats: getSupportedFormats()
  1006→  });
  1007→});
  1008→
  1009→// =============================================================================
  1010→// Multitrack/Multicam Analysis Routes
  1011→// =============================================================================
  1012→
  1013→/**
  1014→ * POST /multitrack - Analyze multiple audio tracks for multicam editing
  1015→ *
  1016→ * Analyzes audio levels across multiple tracks to determine optimal
  1017→ * video angle selection based on who is speaking.
  1018→ *
  1019→ * Options:
  1020→ * - audioPaths: Array of paths to audio files (one per speaker) - required
  1021→ * - speakerNames: Array of speaker names (optional)
  1022→ * - videoTrackMapping: Object mapping speaker index to video track { 0: 0, 1: 1 }
  1023→ * - minShotDuration: Minimum seconds before next cut (default: 2.0)
  1024→ * - switchingFrequency: How often to allow cuts 0-100 (default: 50)
  1025→ * - wideShotEnabled: Enable wide shot detection (default: true)
  1026→ * - wideShotPercentage: Target % of wide shots (default: 20)
  1027→ * - wideShotTracks: Video track indices for wide shots
  1028→ * - cutawayEnabled: Enable cutaway insertion (default: false)
  1029→ * - cutawayTracks: Video track indices for cutaways
  1030→ * - speakerBoosts: Per-speaker dB adjustments { "Speaker 1": 5 }
  1031→ */
  1032→app.post('/multitrack', requireCredits({ endpoint: 'multitrack' }), async (req, res) => {
  1033→  const {
  1034→    audioPaths,
  1035→    speakerNames,
  1036→    videoTrackMapping = {},
  1037→    minShotDuration = 2.0,
  1038→    switchingFrequency = 50,
  1039→    wideShotEnabled = true,
  1040→    wideShotPercentage = 20,
  1041→    wideShotTracks = [],
  1042→    cutawayEnabled = false,
  1043→    cutawayTracks = [],
  1044→    speakerBoosts = {},
  1045→    frameRate = 30
  1046→  } = req.body;
  1047→
  1048→  // Validate audioPaths
  1049→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length === 0) {
  1050→    return res.status(400).json({ error: 'audioPaths array is required (at least 1 path)' });
  1051→  }
  1052→
  1053→  // Validate all files exist
  1054→  for (const audioPath of audioPaths) {
  1055→    if (!fs.existsSync(audioPath)) {
  1056→      return res.status(404).json({ error: `File not found: ${audioPath}` });
  1057→    }
  1058→  }
  1059→
  1060→  // Check FFprobe availability
  1061→  const ffprobeAvailable = await isFFprobeInstalled();
  1062→  if (!ffprobeAvailable) {
  1063→    return res.status(500).json({
  1064→      error: 'FFprobe not installed. Run: brew install ffmpeg'
  1065→    });
  1066→  }
  1067→
  1068→  console.log(`[SPLICE] Multitrack analysis: ${audioPaths.length} track(s)`);
  1069→
  1070→  try {
  1071→    const result = await analyzeMultitrack(audioPaths, {
  1072→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),
  1073→      videoTrackMapping,
  1074→      minShotDuration,
  1075→      switchingFrequency,
  1076→      wideShotEnabled,
  1077→      wideShotPercentage,
  1078→      wideShotTracks,
  1079→      cutawayEnabled,
  1080→      cutawayTracks,
  1081→      speakerBoosts,
  1082→      frameRate
  1083→    });
  1084→
  1085→    res.json({
  1086→      success: true,
  1087→      ...result
  1088→    });
  1089→  } catch (err) {
  1090→    console.error('[SPLICE] Multitrack analysis error:', err);
  1091→    res.status(500).json({ error: err.message });
  1092→  }
  1093→});
  1094→
  1095→/**
  1096→ * POST /multitrack/auto-balance - Auto-balance speaker screentime
  1097→ *
  1098→ * Automatically adjusts speaker boosts to achieve equal screentime distribution.
  1099→ * Runs multiple iterations to find optimal parameters.
  1100→ */
  1101→app.post('/multitrack/auto-balance', async (req, res) => {
  1102→  const {
  1103→    audioPaths,
  1104→    speakerNames,
  1105→    videoTrackMapping = {},
  1106→    minShotDuration = 2.0,
  1107→    switchingFrequency = 50,
  1108→    wideShotEnabled = false, // Disable wide shots for balance calc
  1109→    frameRate = 30
  1110→  } = req.body;
  1111→
  1112→  // Validate audioPaths
  1113→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length < 2) {
  1114→    return res.status(400).json({ error: 'audioPaths array requires at least 2 tracks for balancing' });
  1115→  }
  1116→
  1117→  // Validate all files exist
  1118→  for (const audioPath of audioPaths) {
  1119→    if (!fs.existsSync(audioPath)) {
  1120→      return res.status(404).json({ error: `File not found: ${audioPath}` });
  1121→    }
  1122→  }
  1123→
  1124→  // Check FFprobe availability
  1125→  const ffprobeAvailable = await isFFprobeInstalled();
  1126→  if (!ffprobeAvailable) {
  1127→    return res.status(500).json({
  1128→      error: 'FFprobe not installed. Run: brew install ffmpeg'
  1129→    });
  1130→  }
  1131→
  1132→  console.log(`[SPLICE] Auto-balancing multitrack: ${audioPaths.length} track(s)`);
  1133→
  1134→  try {
  1135→    const result = await autoBalanceMultitrack(audioPaths, {
  1136→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),
  1137→      videoTrackMapping,
  1138→      minShotDuration,
  1139→      switchingFrequency,
  1140→      wideShotEnabled,
  1141→      frameRate
  1142→    });
  1143→
  1144→    res.json({
  1145→      success: true,
  1146→      ...result
  1147→    });
  1148→  } catch (err) {
  1149→    console.error('[SPLICE] Auto-balance error:', err);
  1150→    res.status(500).json({ error: err.message });
  1151→  }
  1152→});
  1153→
  1154→/**
  1155→ * POST /process-xml - Process FCP XML to split clips at silences
  1156→ *
  1157→ * Takes an FCP XML file and silence timestamps, splits clips
  1158→ * at silence boundaries, and optionally removes gaps.
  1159→ */
  1160→app.post('/process-xml', async (req, res) => {
  1161→  const {
  1162→    xmlPath,
  1163→    silences,
  1164→    removeGaps = true,
  1165→    outputPath = null
  1166→  } = req.body;
  1167→
  1168→  if (!xmlPath) {
  1169→    return res.status(400).json({ error: 'xmlPath is required' });
  1170→  }
  1171→
  1172→  if (!silences || !Array.isArray(silences)) {
  1173→    return res.status(400).json({ error: 'silences array is required' });
  1174→  }
  1175→
  1176→  if (!fs.existsSync(xmlPath)) {
  1177→    return res.status(404).json({ error: `XML file not found: ${xmlPath}` });
  1178→  }
  1179→
  1180→  console.log(`[SPLICE] Processing XML: ${xmlPath} with ${silences.length} silence(s)`);
  1181→
  1182→  try {
  1183→    const result = await processXMLFile(xmlPath, silences, {
  1184→      outputPath,
  1185→      removeGaps
  1186→    });
  1187→
  1188→    res.json({
  1189→      success: true,
  1190→      inputPath: xmlPath,
  1191→      outputPath: result.outputPath,
  1192→      stats: result.stats
  1193→    });
  1194→  } catch (err) {
  1195→    console.error('[SPLICE] XML processing error:', err);
  1196→    res.status(500).json({ error: err.message });
  1197→  }
  1198→});
  1199→
  1200→/**
  1201→ * POST /cut-list - Generate a JSON cut list for direct DOM building (v3.5)
  1202→ *
  1203→ * Takes silences and optionally takes, returns a cut list that the
  1204→ * plugin can use to build sequences directly via UXP APIs.
  1205→ *
  1206→ * Body:
  1207→ * - sourceName: Name of the source clip
  1208→ * - sourcePath: Full path to the source file
  1209→ * - duration: Total duration in seconds
  1210→ * - silences: Array of silence segments [{start, end, duration}]
  1211→ * - takes: (optional) Array of detected takes
  1212→ * - settings: (optional) Generation settings
  1213→ */
  1214→app.post('/cut-list', async (req, res) => {
  1215→  const {
  1216→    sourceName,
  1217→    sourcePath,
  1218→    duration,
  1219→    silences,
  1220→    takes = [],
  1221→    settings = {}
  1222→  } = req.body;
  1223→
  1224→  // Validate required fields
  1225→  if (!sourceName && !sourcePath) {
  1226→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });
  1227→  }
  1228→
  1229→  if (typeof duration !== 'number' || duration <= 0) {
  1230→    return res.status(400).json({ error: 'duration must be a positive number' });
  1231→  }
  1232→
  1233→  if (!silences || !Array.isArray(silences)) {
  1234→    return res.status(400).json({ error: 'silences array is required' });
  1235→  }
  1236→
  1237→  console.log(`[SPLICE] Generating cut list for ${sourceName || sourcePath} (${silences.length} silences)`);
  1238→
  1239→  try {
  1240→    const cutList = generateCutList({
  1241→      sourceName: sourceName || path.basename(sourcePath),
  1242→      sourcePath,
  1243→      duration,
  1244→      silences,
  1245→      takes,
  1246→      settings
  1247→    });
  1248→
  1249→    // Validate the generated cut list
  1250→    const validation = validateCutList(cutList);
  1251→    if (!validation.valid) {
  1252→      return res.status(500).json({
  1253→        error: 'Generated cut list is invalid',
  1254→        validationErrors: validation.errors
  1255→      });
  1256→    }
  1257→
  1258→    res.json({
  1259→      success: true,
  1260→      cutList
  1261→    });
  1262→  } catch (err) {
  1263→    console.error('[SPLICE] Cut list generation error:', err);
  1264→    res.status(500).json({ error: err.message });
  1265→  }
  1266→});
  1267→
  1268→/**
  1269→ * POST /cut-list/takes - Generate a cut list that keeps only takes
  1270→ *
  1271→ * Alternative endpoint for "keep best takes only" workflow.
  1272→ */
  1273→app.post('/cut-list/takes', async (req, res) => {
  1274→  const {
  1275→    sourceName,
  1276→    sourcePath,
  1277→    duration,
  1278→    takes,
  1279→    settings = {}
  1280→  } = req.body;
  1281→
  1282→  // Validate required fields
  1283→  if (!sourceName && !sourcePath) {
  1284→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });
  1285→  }
  1286→
  1287→  if (typeof duration !== 'number' || duration <= 0) {
  1288→    return res.status(400).json({ error: 'duration must be a positive number' });
  1289→  }
  1290→
  1291→  if (!takes || !Array.isArray(takes) || takes.length === 0) {
  1292→    return res.status(400).json({ error: 'takes array is required and must not be empty' });
  1293→  }
  1294→
  1295→  console.log(`[SPLICE] Generating takes cut list for ${sourceName || sourcePath} (${takes.length} takes)`);
  1296→
  1297→  try {
  1298→    const cutList = generateTakesCutList({
  1299→      sourceName: sourceName || path.basename(sourcePath),
  1300→      sourcePath,
  1301→      duration,
  1302→      takes,
  1303→      settings
  1304→    });
  1305→
  1306→    res.json({
  1307→      success: true,
  1308→      cutList
  1309→    });
  1310→  } catch (err) {
  1311→    console.error('[SPLICE] Takes cut list generation error:', err);
  1312→    res.status(500).json({ error: err.message });
  1313→  }
  1314→});
  1315→
  1316→/**
  1317→ * GET /ffprobe-check - Check if FFprobe is installed
  1318→ */
  1319→app.get('/ffprobe-check', async (req, res) => {
  1320→  const installed = await isFFprobeInstalled();
  1321→  res.json({
  1322→    installed,
  1323→    message: installed
  1324→      ? 'FFprobe is available'
  1325→      : 'FFprobe not found. Install with: brew install ffmpeg'
  1326→  });
  1327→});
  1328→
  1329→/**
  1330→ * GET /replicate-check - Check if Replicate API is configured
  1331→ */
  1332→app.get('/replicate-check', async (req, res) => {
  1333→  const configured = isReplicateConfigured();
  1334→  res.json({
  1335→    configured,
  1336→    message: configured
  1337→      ? 'Replicate API is configured'
  1338→      : 'REPLICATE_API_TOKEN not set. Add to .env file.'
  1339→  });
  1340→});
  1341→
  1342→/**
  1343→ * POST /isolate-vocals - Isolate vocals from audio using Demucs
  1344→ *
  1345→ * Uses Replicate's Demucs model to separate vocals from background audio.
  1346→ * Cost: ~$0.015/min of audio
  1347→ *
  1348→ * Tier access:
  1349→ * - Starter: No access (upgrade required)
  1350→ * - Pro: 2 hours included, then $0.08/min overage
  1351→ * - Team: 5 hours included, then $0.08/min overage
  1352→ */
  1353→app.post('/isolate-vocals', async (req, res) => {
  1354→  const { audioPath, stem = 'vocals', outputDir = null } = req.body;
  1355→  const stripeCustomerId = req.headers['x-stripe-customer-id'];
  1356→
  1357→  if (!audioPath) {
  1358→    return res.status(400).json({ error: 'audioPath is required' });
  1359→  }
  1360→
  1361→  if (!fs.existsSync(audioPath)) {
  1362→    return res.status(404).json({ error: `File not found: ${audioPath}` });
  1363→  }
  1364→
  1365→  // Check Replicate configuration
  1366→  if (!isReplicateConfigured()) {
  1367→    return res.status(500).json({
  1368→      error: 'Replicate API not configured. Set REPLICATE_API_TOKEN in .env'
  1369→    });
  1370→  }
  1371→
  1372→  // Get audio duration for billing
  1373→  let audioDurationSeconds = 0;
  1374→  try {
  1375→    audioDurationSeconds = await getAudioDuration(audioPath);
  1376→  } catch (err) {
  1377→    console.warn('[SPLICE] Could not get audio duration:', err.message);
  1378→  }
  1379→
  1380→  const audioDurationMinutes = audioDurationSeconds / 60;
  1381→
  1382→  // Check isolation access if customer ID provided
  1383→  if (stripeCustomerId) {
  1384→    const accessCheck = await usageTracking.checkIsolationAccess(stripeCustomerId, audioDurationMinutes);
  1385→
  1386→    if (!accessCheck.allowed) {
  1387→      return res.status(403).json({
  1388→        error: accessCheck.message,
  1389→        reason: accessCheck.reason,
  1390→        upgradeRequired: accessCheck.reason === 'upgrade_required'
  1391→      });
  1392→    }
  1393→
  1394→    console.log(`[SPLICE] Isolation access: ${accessCheck.message}`);
  1395→  }
  1396→
  1397→  console.log(`[SPLICE] Isolating vocals: ${audioPath} (${audioDurationMinutes.toFixed(1)} min)`);
  1398→
  1399→  try {
  1400→    const result = await isolateVocals(audioPath, {
  1401→      stem,
  1402→      outputDir: outputDir || undefined
  1403→    });
  1404→
  1405→    // Deduct isolation usage if customer ID provided
  1406→    let usageInfo = null;
  1407→    if (stripeCustomerId) {
  1408→      usageInfo = await usageTracking.deductIsolationUsage(
  1409→        stripeCustomerId,
  1410→        audioDurationSeconds,
  1411→        'isolate-vocals'
  1412→      );
  1413→      console.log(`[SPLICE] Isolation usage deducted: ${audioDurationMinutes.toFixed(1)} min`);
  1414→      if (usageInfo.isolationUsed?.overageCost > 0) {
  1415→        console.log(`[SPLICE] Overage cost: $${usageInfo.isolationUsed.overageCost.toFixed(2)}`);
  1416→      }
  1417→    }
  1418→
  1419→    res.json({
  1420→      success: true,
  1421→      inputPath: audioPath,
  1422→      outputPath: result.outputPath,
  1423→      stem: result.stem,
  1424→      processingTime: result.processingTime,
  1425→      availableStems: result.allStems,
  1426→      audioDurationMinutes,
  1427→      usage: usageInfo ? {
  1428→        isolationHoursRemaining: usageInfo.isolationHoursRemaining,
  1429→        overageCost: usageInfo.isolationUsed?.overageCost || 0
  1430→      } : null
  1431→    });
  1432→  } catch (err) {
  1433→    console.error('[SPLICE] Vocal isolation error:', err);
  1434→    res.status(500).json({ error: err.message });
  1435→  }
  1436→});
  1437→
  1438→// =============================================================================
  1439→// Batch Processing Routes
  1440→// =============================================================================
  1441→
  1442→// In-memory job queue for batch processing
  1443→const batchJobs = new Map();
  1444→
  1445→// Batch job limits to prevent memory leak
  1446→const MAX_BATCH_JOBS = 10000;
  1447→const BATCH_JOB_MAX_AGE_MS = 24 * 60 * 60 * 1000; // 24 hours
  1448→
  1449→/**
  1450→ * Generate a unique job ID
  1451→ */
  1452→function generateJobId() {
  1453→  return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  1454→}
  1455→
  1456→/**
  1457→ * Clean up old batch jobs to prevent memory leak
  1458→ * Removes jobs older than 24 hours
  1459→ */
  1460→function cleanupOldBatchJobs() {
  1461→  const now = Date.now();
  1462→  let removedCount = 0;
  1463→
  1464→  for (const [jobId, job] of batchJobs.entries()) {
  1465→    const createdAt = new Date(job.createdAt).getTime();
  1466→    if (now - createdAt > BATCH_JOB_MAX_AGE_MS) {
  1467→      batchJobs.delete(jobId);
  1468→      removedCount++;
  1469→    }
  1470→  }
  1471→
  1472→  if (removedCount > 0) {
  1473→    console.log(`[SPLICE] Cleaned up ${removedCount} old batch job(s)`);
  1474→  }
  1475→}
  1476→
  1477→/**
  1478→ * Enforce max job limit by removing oldest completed jobs
  1479→ */
  1480→function enforceJobLimit() {
  1481→  if (batchJobs.size < MAX_BATCH_JOBS) return;
  1482→
  1483→  // Get completed jobs sorted by creation date (oldest first)
  1484→  const completedJobs = Array.from(batchJobs.entries())
  1485→    .filter(([_, job]) => job.status !== 'processing')
  1486→    .sort((a, b) => new Date(a[1].createdAt) - new Date(b[1].createdAt));
  1487→
  1488→  // Remove oldest completed jobs until under limit
  1489→  const toRemove = batchJobs.size - MAX_BATCH_JOBS + 1;
  1490→  for (let i = 0; i < Math.min(toRemove, completedJobs.length); i++) {
  1491→    batchJobs.delete(completedJobs[i][0]);
  1492→  }
  1493→
  1494→  console.log(`[SPLICE] Enforced job limit, removed ${Math.min(toRemove, completedJobs.length)} job(s)`);
  1495→}
  1496→
  1497→// Run cleanup every hour
  1498→setInterval(cleanupOldBatchJobs, 60 * 60 * 1000);
  1499→
  1500→/**
  1501→ * POST /batch/silences - Process multiple files for silence detection
  1502→ *
  1503→ * Creates a batch job that processes multiple audio files.
  1504→ * Returns a job ID for tracking progress.
  1505→ *
  1506→ * Body:
  1507→ * - files: Array of file paths to process
  1508→ * - options: Detection options (sensitivity, threshold, etc.)
  1509→ */
  1510→app.post('/batch/silences', async (req, res) => {
  1511→  const { files, options = {} } = req.body;
  1512→
  1513→  if (!files || !Array.isArray(files) || files.length === 0) {
  1514→    return res.status(400).json({ error: 'files array is required' });
  1515→  }
  1516→
  1517→  // Validate all files exist
  1518→  const missingFiles = files.filter(f => !fs.existsSync(f));
  1519→  if (missingFiles.length > 0) {
  1520→    return res.status(404).json({
  1521→      error: 'Some files not found',
  1522→      missingFiles
  1523→    });
  1524→  }
  1525→
  1526→  // Enforce job limit before creating new job
  1527→  enforceJobLimit();
  1528→
  1529→  const jobId = generateJobId();
  1530→
  1531→  // Initialize job
  1532→  const job = {
  1533→    id: jobId,
  1534→    type: 'silences',
  1535→    status: 'processing',
  1536→    createdAt: new Date().toISOString(),
  1537→    files: files.map(f => ({
  1538→      path: f,
  1539→      status: 'pending',
  1540→      result: null,
  1541→      error: null
  1542→    })),
  1543→    options,
  1544→    progress: {
  1545→      total: files.length,
  1546→      completed: 0,
  1547→      failed: 0,
  1548→      percentage: 0
  1549→    },
  1550→    results: [],
  1551→    errors: []
  1552→  };
  1553→
  1554→  batchJobs.set(jobId, job);
  1555→  console.log(`[SPLICE] Batch job ${jobId} created with ${files.length} files`);
  1556→
  1557→  // Start processing in background
  1558→  processBatchJob(jobId);
  1559→
  1560→  res.json({
  1561→    success: true,
  1562→    jobId,
  1563→    message: `Batch job created with ${files.length} files`,
  1564→    statusUrl: `/batch/status/${jobId}`
  1565→  });
  1566→});
  1567→
  1568→/**
  1569→ * Process a batch job (runs in background)
  1570→ */
  1571→async function processBatchJob(jobId) {
  1572→  const job = batchJobs.get(jobId);
  1573→  if (!job) return;
  1574→
  1575→  const { sensitivity, ...manualOptions } = job.options;
  1576→
  1577→  // Build detection options
  1578→  let detectionOptions = {};
  1579→  if (typeof sensitivity === 'number') {
  1580→    detectionOptions = sensitivityToParams(sensitivity);
  1581→  } else {
  1582→    detectionOptions = {
  1583→      threshold: manualOptions.threshold ?? -30,
  1584→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,
  1585→      paddingStart: manualOptions.paddingStart ?? 0.1,
  1586→      paddingEnd: manualOptions.paddingEnd ?? 0.05,
  1587→      autoThreshold: manualOptions.autoThreshold ?? false
  1588→    };
  1589→  }
  1590→
  1591→  // Process files sequentially to avoid overwhelming the system
  1592→  for (let i = 0; i < job.files.length; i++) {
  1593→    const fileEntry = job.files[i];
  1594→    fileEntry.status = 'processing';
  1595→
  1596→    try {
  1597→      const result = await detectSilencesRMS(fileEntry.path, detectionOptions);
  1598→
  1599→      fileEntry.status = 'completed';
  1600→      fileEntry.result = {
  1601→        silences: result.silences,
  1602→        count: result.silences.length,
  1603→        totalSilenceDuration: result.metadata.totalSilenceDuration,
  1604→        audioDuration: result.metadata.audioDuration
  1605→      };
  1606→
  1607→      job.results.push({
  1608→        file: fileEntry.path,
  1609→        ...fileEntry.result
  1610→      });
  1611→
  1612→      job.progress.completed++;
  1613→      console.log(`[SPLICE] Batch ${jobId}: ${i + 1}/${job.files.length} completed`);
  1614→    } catch (err) {
  1615→      fileEntry.status = 'failed';
  1616→      fileEntry.error = err.message;
  1617→
  1618→      job.errors.push({
  1619→        file: fileEntry.path,
  1620→        error: err.message
  1621→      });
  1622→
  1623→      job.progress.failed++;
  1624→      console.error(`[SPLICE] Batch ${jobId}: ${fileEntry.path} failed:`, err.message);
  1625→    }
  1626→
  1627→    // Update progress
  1628→    job.progress.percentage = Math.round(
  1629→      ((job.progress.completed + job.progress.failed) / job.progress.total) * 100
  1630→    );
  1631→  }
  1632→
  1633→  // Mark job as complete
  1634→  job.status = job.progress.failed === job.progress.total ? 'failed' :
  1635→               job.progress.failed > 0 ? 'completed_with_errors' : 'completed';
  1636→  job.completedAt = new Date().toISOString();
  1637→
  1638→  console.log(`[SPLICE] Batch job ${jobId} ${job.status}`);
  1639→}
  1640→
  1641→/**
  1642→ * GET /batch/status/:jobId - Get batch job status and results
  1643→ */
  1644→app.get('/batch/status/:jobId', (req, res) => {
  1645→  const { jobId } = req.params;
  1646→  const job = batchJobs.get(jobId);
  1647→
  1648→  if (!job) {
  1649→    return res.status(404).json({ error: 'Job not found' });
  1650→  }
  1651→
  1652→  res.json({
  1653→    success: true,
  1654→    job: {
  1655→      id: job.id,
  1656→      type: job.type,
  1657→      status: job.status,
  1658→      createdAt: job.createdAt,
  1659→      completedAt: job.completedAt,
  1660→      progress: job.progress,
  1661→      files: job.files.map(f => ({
  1662→        path: f.path,
  1663→        status: f.status,
  1664→        silenceCount: f.result?.count,
  1665→        error: f.error
  1666→      }))
  1667→    }
  1668→  });
  1669→});
  1670→
  1671→/**
  1672→ * GET /batch/results/:jobId - Get full results for a completed batch job
  1673→ */
  1674→app.get('/batch/results/:jobId', (req, res) => {
  1675→  const { jobId } = req.params;
  1676→  const job = batchJobs.get(jobId);
  1677→
  1678→  if (!job) {
  1679→    return res.status(404).json({ error: 'Job not found' });
  1680→  }
  1681→
  1682→  if (job.status === 'processing') {
  1683→    return res.status(202).json({
  1684→      success: false,
  1685→      message: 'Job still processing',
  1686→      progress: job.progress
  1687→    });
  1688→  }
  1689→
  1690→  res.json({
  1691→    success: true,
  1692→    jobId: job.id,
  1693→    status: job.status,
  1694→    progress: job.progress,
  1695→    results: job.results,
  1696→    errors: job.errors,
  1697→    summary: {
  1698→      totalFiles: job.progress.total,
  1699→      successful: job.progress.completed,
  1700→      failed: job.progress.failed,
  1701→      totalSilences: job.results.reduce((sum, r) => sum + (r.count || 0), 0),
  1702→      totalSilenceDuration: job.results.reduce((sum, r) => sum + (r.totalSilenceDuration || 0), 0)
  1703→    }
  1704→  });
  1705→});
  1706→
  1707→/**
  1708→ * DELETE /batch/:jobId - Cancel or delete a batch job
  1709→ */
  1710→app.delete('/batch/:jobId', (req, res) => {
  1711→  const { jobId } = req.params;
  1712→  const job = batchJobs.get(jobId);
  1713→
  1714→  if (!job) {
  1715→    return res.status(404).json({ error: 'Job not found' });
  1716→  }
  1717→
  1718→  // Note: This doesn't actually cancel in-progress processing
  1719→  // but prevents the job from being queried
  1720→  batchJobs.delete(jobId);
  1721→
  1722→  res.json({
  1723→    success: true,
  1724→    message: `Job ${jobId} deleted`
  1725→  });
  1726→});
  1727→
  1728→/**
  1729→ * GET /batch/jobs - List all batch jobs
  1730→ */
  1731→app.get('/batch/jobs', (req, res) => {
  1732→  const jobs = Array.from(batchJobs.values()).map(job => ({
  1733→    id: job.id,
  1734→    type: job.type,
  1735→    status: job.status,
  1736→    createdAt: job.createdAt,
  1737→    completedAt: job.completedAt,
  1738→    progress: job.progress
  1739→  }));
  1740→
  1741→  // Sort by creation date (newest first)
  1742→  jobs.sort((a, b) => new Date(b.createdAt) - new Date(a.createdAt));
  1743→
  1744→  res.json({
  1745→    success: true,
  1746→    count: jobs.length,
  1747→    jobs
  1748→  });
  1749→});
  1750→
  1751→// =============================================================================
  1752→// Billing & Credits Routes
  1753→// =============================================================================
  1754→
  1755→/**
  1756→ * GET /credits - Get user's credit balance
  1757→ *
  1758→ * Requires x-stripe-customer-id header
  1759→ */
  1760→app.get('/credits', async (req, res) => {
  1761→  const stripeCustomerId = req.headers['x-stripe-customer-id'];
  1762→
  1763→  if (!stripeCustomerId) {
  1764→    return res.status(401).json({
  1765→      error: 'Authentication required',
  1766→      message: 'Missing x-stripe-customer-id header'
  1767→    });
  1768→  }
  1769→
  1770→  try {
  1771→    const balance = await usageTracking.getBalance(stripeCustomerId);
  1772→    res.json({
  1773→      success: true,
  1774→      ...balance
  1775→    });
  1776→  } catch (err) {
  1777→    console.error('[SPLICE] Credits error:', err);
  1778→    res.status(500).json({ error: err.message });
  1779→  }
  1780→});
  1781→
  1782→/**
  1783→ * GET /usage-history - Get user's usage history
  1784→ */
  1785→app.get('/usage-history', async (req, res) => {
  1786→  const stripeCustomerId = req.headers['x-stripe-customer-id'];
  1787→
  1788→  if (!stripeCustomerId) {
  1789→    return res.status(401).json({
  1790→      error: 'Authentication required',
  1791→      message: 'Missing x-stripe-customer-id header'
  1792→    });
  1793→  }
  1794→
  1795→  try {
  1796→    const history = await usageTracking.getUsageHistory(stripeCustomerId);
  1797→    res.json({
  1798→      success: true,
  1799→      history
  1800→    });
  1801→  } catch (err) {
  1802→    console.error('[SPLICE] Usage history error:', err);
  1803→    res.status(500).json({ error: err.message });
  1804→  }
  1805→});
  1806→
  1807→// =============================================================================
  1808→// Start Server
  1809→// =============================================================================
  1810→
  1811→// Initialize database and start server
  1812→async function startServer() {
  1813→  try {
  1814→    await usageTracking.initDatabase();
  1815→    console.log('[SPLICE] Database initialized');
  1816→
  1817→    if (isProduction || !httpsOptions) {
  1818→      // Production: Railway provides TLS termination, use HTTP
  1819→      http.createServer(app).listen(PORT, () => {
  1820→        console.log(`[SPLICE] Backend running at http://0.0.0.0:${PORT} (production)`);
  1821→      });
  1822→    } else {
  1823→      // Development: Use HTTPS with local certificates
  1824→      https.createServer(httpsOptions, app).listen(PORT, () => {
  1825→        console.log(`[SPLICE] Backend running at https://127.0.0.1:${PORT} (development)`);
  1826→        console.log(`[SPLICE] POST /analyze with { "wavPath": "/path/to/audio.wav" }`);
  1827→      });
  1828→    }
  1829→  } catch (err) {
  1830→    console.error('[SPLICE] Failed to start server:', err);
  1831→    process.exit(1);
  1832→  }
  1833→}
  1834→
  1835→startServer();
  1836→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
