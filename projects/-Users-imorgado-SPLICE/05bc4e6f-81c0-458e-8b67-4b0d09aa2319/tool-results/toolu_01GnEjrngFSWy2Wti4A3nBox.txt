     1→# SPLICE Feature Integration Plan
     2→## Based on FireCut Analysis & Current SPLICE Architecture
     3→
     4→**Generated**: 2025-12-25
     5→**Target Version**: SPLICE v4.0
     6→**Current Status**: v3.5 (82% FireCut Parity)
     7→
     8→---
     9→
    10→## Executive Summary
    11→
    12→This plan integrates 5 priority features into SPLICE based on comprehensive analysis of:
    13→- FireCut deobfuscated source code (`/Users/imorgado/Desktop/Fireside_Deobstuficated`)
    14→- Current SPLICE backend (14 services, 5,200+ LOC)
    15→- Current SPLICE plugin (14 files, 3,700+ LOC)
    16→
    17→### Features to Implement
    18→
    19→| Feature | Priority | Effort | Impact |
    20→|---------|----------|--------|--------|
    21→| **1. Auto Zoom** | HIGH | 8-10 hrs | Major differentiator |
    22→| **2. Multitrack UI** | HIGH | 6-8 hrs | Backend done, UI needed |
    23→| **3. J-Cut Support** | HIGH | 4-6 hrs | Podcast workflow |
    24→| **4. Chapter Detection** | MEDIUM | 5-7 hrs | YouTube optimization |
    25→| **5. Takes Labeling & Color Coding** | HIGH | 3-4 hrs | Already partially done |
    26→
    27→**Total Estimated Effort**: 26-35 hours
    28→
    29→### Features NOT Being Implemented
    30→- B-Roll Auto-Insert (excluded per user request)
    31→- Highlight Extraction (excluded per user request)
    32→- Auto-Reframe 9:16 (excluded per user request)
    33→
    34→---
    35→
    36→## 1. AUTO ZOOM IMPLEMENTATION
    37→
    38→### FireCut Analysis
    39→
    40→**Source**: `/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx`
    41→**Lines**: 1847-2100
    42→
    43→**FireCut Approach**:
    44→1. Uses **Adjustment Layers** (AI Adjustment Layer) for non-destructive zooms
    45→2. Applies **Transform effect** with keyframes for Scale and Anchor Point
    46→3. Implements **easing curves** with configurable power parameter
    47→4. Supports asymmetric easing (different in/out animation speeds)
    48→
    49→**Key FireCut Functions**:
    50→```javascript
    51→addZoomAtTime(trackIndex, start, scale, duration, center)
    52→addZoomAnimation(trackIndex, start, duration, scaleStart, scalePeak, scaleEnd, easing)
    53→addZoomAtTimeAndCenterAsymmetric(trackIndex, start, duration, scaleStart,
    54→    scalePeak, scaleEnd, centerX, centerY, easingIn, easingOut)
    55→```
    56→
    57→**Easing Curve Implementation**:
    58→```javascript
    59→function easing_curve(t, power) {
    60→    const coefficient = Math.pow(2, power - 1);
    61→    return (1 - Math.pow(2, -power * t)) / coefficient;
    62→}
    63→```
    64→
    65→### SPLICE Implementation Plan
    66→
    67→#### Backend Changes (`splice-backend/services/`)
    68→
    69→**New File**: `zoomGenerator.js`
    70→```javascript
    71→/**
    72→ * Zoom Generator Service
    73→ * Generates zoom keyframe data for cut list integration
    74→ */
    75→
    76→const ZOOM_PRESETS = {
    77→  subtle: { scale: 110, duration: 0.6, easing: 2 },
    78→  medium: { scale: 120, duration: 0.8, easing: 3 },
    79→  dramatic: { scale: 140, duration: 1.0, easing: 4 }
    80→};
    81→
    82→const ZOOM_FREQUENCIES = {
    83→  low: 60,      // ~1 per minute
    84→  medium: 30,   // ~2 per minute
    85→  high: 15      // ~4 per minute
    86→};
    87→
    88→function generateZoomPoints(transcript, settings) {
    89→  const { frequency, preset, placement } = settings;
    90→  const zoomPoints = [];
    91→
    92→  // Find emphasis points (sentence starts, keywords, speaker changes)
    93→  const emphasisPoints = findEmphasisPoints(transcript, placement);
    94→
    95→  // Select zoom points based on frequency
    96→  const intervalSeconds = ZOOM_FREQUENCIES[frequency] || 30;
    97→  let lastZoomTime = 0;
    98→
    99→  for (const point of emphasisPoints) {
   100→    if (point.time - lastZoomTime >= intervalSeconds) {
   101→      zoomPoints.push({
   102→        type: 'zoom',
   103→        startTime: point.time,
   104→        duration: ZOOM_PRESETS[preset].duration,
   105→        scale: ZOOM_PRESETS[preset].scale,
   106→        easing: ZOOM_PRESETS[preset].easing,
   107→        centerPoint: point.center || { x: 0.5, y: 0.5 },
   108→        reason: point.reason
   109→      });
   110→      lastZoomTime = point.time;
   111→    }
   112→  }
   113→
   114→  return zoomPoints;
   115→}
   116→
   117→function findEmphasisPoints(transcript, placement) {
   118→  // Placement options: 'sentence_start', 'keywords', 'random', 'speaker_change'
   119→  // Implementation varies by placement type
   120→}
   121→
   122→module.exports = { generateZoomPoints, ZOOM_PRESETS, ZOOM_FREQUENCIES };
   123→```
   124→
   125→**Modify**: `cutListGenerator.js`
   126→```javascript
   127→// Add zoom data to cut list format
   128→function generateCutList(options) {
   129→  // ... existing code ...
   130→
   131→  // Add zoom points if enabled
   132→  if (options.zoomSettings?.enabled) {
   133→    const zoomPoints = generateZoomPoints(
   134→      options.transcript,
   135→      options.zoomSettings
   136→    );
   137→    cutList.zooms = zoomPoints;
   138→  }
   139→
   140→  return cutList;
   141→}
   142→```
   143→
   144→#### Plugin Changes (`splice-plugin/js/`)
   145→
   146→**Modify**: `builder.js` - Add zoom application
   147→```javascript
   148→/**
   149→ * Apply zoom keyframes to adjustment layer
   150→ * Uses UXP APIs for non-destructive editing
   151→ */
   152→async function applyZooms(sequence, zoomPoints, project) {
   153→  if (!zoomPoints || zoomPoints.length === 0) return;
   154→
   155→  // Get or create adjustment layer
   156→  const adjustmentLayer = await getOrCreateAdjustmentLayer(project, sequence);
   157→
   158→  for (const zoom of zoomPoints) {
   159→    await project.lockedAccess(async () => {
   160→      await project.executeTransaction((compoundAction) => {
   161→        const startTime = pproBuilder.TickTime.createWithSeconds(zoom.startTime);
   162→        const endTime = pproBuilder.TickTime.createWithSeconds(
   163→          zoom.startTime + zoom.duration
   164→        );
   165→
   166→        // Generate keyframes
   167→        const keyframes = generateZoomKeyframes(zoom);
   168→
   169→        // Apply to Transform effect
   170→        for (const kf of keyframes) {
   171→          // Scale keyframes
   172→          const scaleAction = adjustmentLayer.createSetPropertyKeyframeAction(
   173→            'Transform', 'Scale', kf.time, kf.scale
   174→          );
   175→          compoundAction.addAction(scaleAction);
   176→
   177→          // Anchor point keyframes (for center point)
   178→          if (kf.anchor) {
   179→            const anchorAction = adjustmentLayer.createSetPropertyKeyframeAction(
   180→              'Transform', 'Anchor Point', kf.time, kf.anchor
   181→            );
   182→            compoundAction.addAction(anchorAction);
   183→          }
   184→        }
   185→      }, 'SPLICE: Apply Zoom Effects');
   186→    });
   187→  }
   188→}
   189→
   190→function generateZoomKeyframes(zoom) {
   191→  const keyframes = [];
   192→  const frameCount = Math.ceil(zoom.duration * 30); // Assume 30fps
   193→
   194→  for (let i = 0; i <= frameCount; i++) {
   195→    const t = i / frameCount;
   196→    const easedT = easingCurve(t, zoom.easing);
   197→
   198→    // Calculate scale: start -> peak -> end
   199→    let scale;
   200→    if (t < 0.5) {
   201→      scale = 100 + (zoom.scale - 100) * easingCurve(t * 2, zoom.easing);
   202→    } else {
   203→      scale = zoom.scale - (zoom.scale - 100) * easingCurve((t - 0.5) * 2, zoom.easing);
   204→    }
   205→
   206→    keyframes.push({
   207→      time: zoom.startTime + (i / 30),
   208→      scale: [scale, scale],
   209→      anchor: zoom.centerPoint ? [
   210→        zoom.centerPoint.x * 1920, // Assume 1080p
   211→        zoom.centerPoint.y * 1080
   212→      ] : null
   213→    });
   214→  }
   215→
   216→  return keyframes;
   217→}
   218→
   219→function easingCurve(t, power) {
   220→  const coefficient = Math.pow(2, power - 1);
   221→  return (1 - Math.pow(2, -power * t)) / coefficient;
   222→}
   223→```
   224→
   225→**Modify**: `index.html` - Add zoom UI
   226→```html
   227→<!-- Add to Options Panel -->
   228→<div class="option-group zoom-options">
   229→  <label>
   230→    <input type="checkbox" id="enableZoom"> Enable Auto Zoom
   231→  </label>
   232→
   233→  <div id="zoomSettings" class="collapsed">
   234→    <div class="option-row">
   235→      <label for="zoomFrequency">Frequency:</label>
   236→      <select id="zoomFrequency">
   237→        <option value="low">Low (~1/min)</option>
   238→        <option value="medium" selected>Medium (~2/min)</option>
   239→        <option value="high">High (~4/min)</option>
   240→      </select>
   241→    </div>
   242→
   243→    <div class="option-row">
   244→      <label for="zoomPreset">Intensity:</label>
   245→      <select id="zoomPreset">
   246→        <option value="subtle">Subtle (110%)</option>
   247→        <option value="medium" selected>Medium (120%)</option>
   248→        <option value="dramatic">Dramatic (140%)</option>
   249→      </select>
   250→    </div>
   251→
   252→    <div class="option-row">
   253→      <label for="zoomPlacement">Placement:</label>
   254→      <select id="zoomPlacement">
   255→        <option value="sentence_start" selected>Sentence Starts</option>
   256→        <option value="keywords">Keywords</option>
   257→        <option value="random">Random</option>
   258→      </select>
   259→    </div>
   260→  </div>
   261→</div>
   262→```
   263→
   264→---
   265→
   266→## 2. MULTITRACK UI IMPLEMENTATION
   267→
   268→### Current SPLICE Status
   269→
   270→**Backend**: FULLY IMPLEMENTED in `multitrackAnalysis.js` (725 LOC)
   271→- Speaker diarization via RMS analysis
   272→- Gaussian smoothing for speaker detection
   273→- Wide shot detection logic
   274→- Auto-balance algorithm
   275→
   276→**Missing**: UI panel to configure and trigger multitrack analysis
   277→
   278→### FireCut Analysis
   279→
   280→**Source**: `/Users/imorgado/Desktop/Fireside_Deobstuficated/multitrack/`
   281→**Key Files**: `main_clean.js`, `helpers_clean.js`, `ui_clean.js`
   282→
   283→**FireCut UI Elements**:
   284→1. Speaker-to-track mapping dropdowns
   285→2. Audio threshold sliders per speaker
   286→3. Speaker boost controls (-50 to +50 dB)
   287→4. Wide shot percentage slider
   288→5. Min shot duration control
   289→6. Cutaway configuration
   290→
   291→### SPLICE Implementation Plan
   292→
   293→#### Plugin Changes (`splice-plugin/`)
   294→
   295→**Modify**: `index.html` - Add Multitrack Panel
   296→```html
   297→<!-- Add new section after Advanced Options -->
   298→<div id="multitrackSection" class="feature-section collapsed">
   299→  <button id="multitrackToggle" class="section-toggle">
   300→    Multitrack Editing <span class="toggle-icon">+</span>
   301→  </button>
   302→
   303→  <div id="multitrackPanel" class="section-content">
   304→    <!-- Speaker Configuration -->
   305→    <div class="speaker-config">
   306→      <h4>Speaker Configuration</h4>
   307→      <div id="speakerList">
   308→        <!-- Dynamically populated -->
   309→      </div>
   310→      <button id="addSpeaker" class="btn-secondary">+ Add Speaker</button>
   311→    </div>
   312→
   313→    <!-- Track Mapping -->
   314→    <div class="track-mapping">
   315→      <h4>Video Track Mapping</h4>
   316→      <div id="trackMappingList">
   317→        <!-- Shows: Speaker 1 -> Track 3 dropdown -->
   318→      </div>
   319→    </div>
   320→
   321→    <!-- Parameters -->
   322→    <div class="multitrack-params">
   323→      <div class="param-row">
   324→        <label>Min Shot Duration:</label>
   325→        <input type="range" id="minShotDuration" min="0.5" max="5" step="0.25" value="2">
   326→        <span id="minShotDurationValue">2.0s</span>
   327→      </div>
   328→
   329→      <div class="param-row">
   330→        <label>Wide Shot %:</label>
   331→        <input type="range" id="wideShot" min="0" max="50" value="20">
   332→        <span id="wideShotValue">20%</span>
   333→      </div>
   334→
   335→      <div class="param-row">
   336→        <label>Switching Smoothing:</label>
   337→        <input type="range" id="switchSmoothing" min="0" max="100" value="50">
   338→        <span id="switchSmoothingValue">50</span>
   339→      </div>
   340→    </div>
   341→
   342→    <!-- Actions -->
   343→    <div class="multitrack-actions">
   344→      <button id="analyzeMultitrack" class="btn-primary">Analyze Speakers</button>
   345→      <button id="autoBalance" class="btn-secondary">Auto-Balance</button>
   346→      <button id="applyMultitrack" class="btn-success" disabled>Apply Cuts</button>
   347→    </div>
   348→
   349→    <!-- Preview -->
   350→    <div id="multitrackPreview" class="hidden">
   351→      <h4>Speaker Distribution</h4>
   352→      <div id="distributionChart">
   353→        <!-- Bar chart showing speaker screentime -->
   354→      </div>
   355→      <div id="decisionList">
   356→        <!-- List of cut decisions with timestamps -->
   357→      </div>
   358→    </div>
   359→  </div>
   360→</div>
   361→```
   362→
   363→**New File**: `multitrack.js`
   364→```javascript
   365→/**
   366→ * Multitrack UI Module
   367→ * Connects multitrackAnalysis.js backend to user interface
   368→ */
   369→
   370→let speakerConfig = [];
   371→let trackMapping = {};
   372→let analysisResults = null;
   373→
   374→function initMultitrackUI() {
   375→  // Toggle section
   376→  document.getElementById('multitrackToggle').addEventListener('click', () => {
   377→    document.getElementById('multitrackSection').classList.toggle('collapsed');
   378→  });
   379→
   380→  // Add speaker
   381→  document.getElementById('addSpeaker').addEventListener('click', addSpeaker);
   382→
   383→  // Analyze button
   384→  document.getElementById('analyzeMultitrack').addEventListener('click', analyzeMultitrack);
   385→
   386→  // Auto-balance
   387→  document.getElementById('autoBalance').addEventListener('click', autoBalance);
   388→
   389→  // Apply cuts
   390→  document.getElementById('applyMultitrack').addEventListener('click', applyMultitrackCuts);
   391→
   392→  // Parameter updates
   393→  setupParameterListeners();
   394→}
   395→
   396→async function analyzeMultitrack() {
   397→  const settings = getMultitrackSettings();
   398→  setStatus('Analyzing speakers...');
   399→
   400→  try {
   401→    const response = await fetchWithTimeout(`${getBackendUrl()}/multitrack`, {
   402→      method: 'POST',
   403→      headers: getAuthHeaders(),
   404→      body: JSON.stringify({
   405→        wavPath: currentWavPath,
   406→        speakers: speakerConfig,
   407→        trackMapping: trackMapping,
   408→        settings: settings
   409→      })
   410→    });
   411→
   412→    analysisResults = await response.json();
   413→    displayMultitrackResults(analysisResults);
   414→    document.getElementById('applyMultitrack').disabled = false;
   415→    setStatus('Analysis complete - review and apply');
   416→  } catch (err) {
   417→    setStatus('Analysis failed: ' + err.message);
   418→  }
   419→}
   420→
   421→function displayMultitrackResults(results) {
   422→  // Show distribution chart
   423→  const chartContainer = document.getElementById('distributionChart');
   424→  chartContainer.innerHTML = '';
   425→
   426→  for (const speaker of results.speakerStats) {
   427→    const bar = document.createElement('div');
   428→    bar.className = 'distribution-bar';
   429→    bar.style.width = `${speaker.percentage}%`;
   430→    bar.style.backgroundColor = getSpeakerColor(speaker.index);
   431→    bar.innerHTML = `<span>${speaker.name}: ${speaker.percentage.toFixed(1)}%</span>`;
   432→    chartContainer.appendChild(bar);
   433→  }
   434→
   435→  // Show decision list
   436→  const decisionList = document.getElementById('decisionList');
   437→  decisionList.innerHTML = '';
   438→
   439→  for (const decision of results.decisions) {
   440→    const item = document.createElement('div');
   441→    item.className = 'decision-item';
   442→    item.innerHTML = `
   443→      <span class="decision-time">${formatTime(decision.start)} - ${formatTime(decision.end)}</span>
   444→      <span class="decision-speaker">${decision.speaker}</span>
   445→      <span class="decision-reason">${decision.reason}</span>
   446→      <button class="decision-seek" data-time="${decision.start}">></button>
   447→    `;
   448→    decisionList.appendChild(item);
   449→  }
   450→
   451→  document.getElementById('multitrackPreview').classList.remove('hidden');
   452→}
   453→
   454→async function applyMultitrackCuts() {
   455→  if (!analysisResults) return;
   456→
   457→  setStatus('Applying multitrack cuts...');
   458→
   459→  try {
   460→    // Generate cut list from decisions
   461→    const cutList = {
   462→      version: '3.5',
   463→      source: analysisResults.source,
   464→      segments: analysisResults.decisions.map(d => ({
   465→        type: d.speaker === 'Wide Shot' ? 'wide_shot' : 'speaker',
   466→        sourceName: d.trackName,
   467→        sourcePath: d.trackPath,
   468→        inPoint: d.start,
   469→        outPoint: d.end,
   470→        speaker: d.speaker,
   471→        videoTrack: d.videoTrack
   472→      })),
   473→      metadata: {
   474→        isMultitrack: true,
   475→        speakerCount: speakerConfig.length
   476→      }
   477→    };
   478→
   479→    // Build sequence using existing builder
   480→    await window.spliceBuilder.buildSequenceFromCutList(cutList, {
   481→      colorCode: true,
   482→      suffix: '_MULTITRACK'
   483→    });
   484→
   485→    setStatus('Multitrack sequence created');
   486→  } catch (err) {
   487→    setStatus('Failed to apply: ' + err.message);
   488→  }
   489→}
   490→```
   491→
   492→---
   493→
   494→## 3. J-CUT SUPPORT (PODCAST SYNC)
   495→
   496→### FireCut Analysis
   497→
   498→**Source**: `/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx`
   499→**Key Function**: `moveAllClipsInTrack()` (lines 892-932)
   500→
   501→**FireCut J-Cut Approach**:
   502→1. Shift audio track earlier/later than video
   503→2. Cut clips at content boundaries
   504→3. Shift back to original alignment
   505→4. Creates natural audio lead-in/lead-out
   506→
   507→```javascript
   508→// FireCut J-cut flow
   509→moveAllClipsInTrack('audio', trackIndex, -0.5);  // Shift audio 0.5s earlier
   510→// ... perform cuts ...
   511→moveAllClipsInTrack('audio', trackIndex, +0.5);  // Shift back
   512→```
   513→
   514→### SPLICE Implementation Plan
   515→
   516→#### Backend Changes
   517→
   518→**Modify**: `cutListGenerator.js`
   519→```javascript
   520→// Add J-cut parameters to cut list format
   521→function generateCutList(options) {
   522→  const { jcutSettings } = options;
   523→
   524→  // For each segment, calculate audio offset
   525→  for (const segment of segments) {
   526→    if (jcutSettings?.enabled) {
   527→      segment.audioInPoint = segment.inPoint - jcutSettings.leadIn;
   528→      segment.audioOutPoint = segment.outPoint + jcutSettings.leadOut;
   529→      segment.hasAudioOffset = true;
   530→    }
   531→  }
   532→
   533→  return cutList;
   534→}
   535→```
   536→
   537→#### Plugin Changes
   538→
   539→**Modify**: `builder.js`
   540→```javascript
   541→// Handle J-cut segments with separate audio/video timing
   542→async function insertSegmentWithJCut(segment, editor, project) {
   543→  const videoPosition = pproBuilder.TickTime.createWithSeconds(segment.position);
   544→  const videoIn = pproBuilder.TickTime.createWithSeconds(segment.inPoint);
   545→  const videoOut = pproBuilder.TickTime.createWithSeconds(segment.outPoint);
   546→
   547→  // Insert video
   548→  const videoAction = editor.createInsertProjectItemAction(
   549→    segment.sourceItem,
   550→    videoPosition,
   551→    segment.videoTrack,
   552→    -1,  // No audio on video track
   553→    false
   554→  );
   555→
   556→  // Insert audio with offset
   557→  if (segment.hasAudioOffset) {
   558→    const audioIn = pproBuilder.TickTime.createWithSeconds(segment.audioInPoint);
   559→    const audioOut = pproBuilder.TickTime.createWithSeconds(segment.audioOutPoint);
   560→
   561→    const audioAction = editor.createInsertProjectItemAction(
   562→      segment.sourceItem,
   563→      videoPosition,  // Same position
   564→      -1,  // No video
   565→      segment.audioTrack,
   566→      false
   567→    );
   568→
   569→    // Set audio in/out points separately
   570→    // This creates the J-cut effect
   571→  }
   572→}
   573→```
   574→
   575→**Modify**: `index.html` - Add J-Cut UI
   576→```html
   577→<div class="jcut-options">
   578→  <label>
   579→    <input type="checkbox" id="enableJCut"> Enable J-Cuts (Podcast Mode)
   580→  </label>
   581→
   582→  <div id="jcutSettings" class="collapsed">
   583→    <div class="option-row">
   584→      <label>Audio Lead-in:</label>
   585→      <input type="range" id="jcutLeadIn" min="0" max="1" step="0.1" value="0.3">
   586→      <span id="jcutLeadInValue">0.3s</span>
   587→    </div>
   588→    <div class="option-row">
   589→      <label>Audio Lead-out:</label>
   590→      <input type="range" id="jcutLeadOut" min="0" max="1" step="0.1" value="0.2">
   591→      <span id="jcutLeadOutValue">0.2s</span>
   592→    </div>
   593→  </div>
   594→</div>
   595→```
   596→
   597→---
   598→
   599→## 4. CHAPTER DETECTION
   600→
   601→### FireCut Analysis
   602→
   603→**Source**: `/Users/imorgado/Desktop/Fireside_Deobstuficated/firecut_host_decoded.jsx`
   604→**Key Functions**:
   605→- `createChapterMarker()` (line 1567)
   606→- `addChapterClip()` (line 1612)
   607→- `getChapterMarkers()` (line 1589)
   608→
   609→**FireCut Approach**:
   610→1. Uses GPT-4 to analyze transcript for topic changes
   611→2. Creates timeline markers at chapter boundaries
   612→3. Generates YouTube-formatted timestamps
   613→4. Optional: adds motion graphics title cards
   614→
   615→### SPLICE Implementation Plan
   616→
   617→#### Backend Changes
   618→
   619→**New File**: `chapterDetection.js`
   620→```javascript
   621→/**
   622→ * Chapter Detection Service
   623→ * Analyzes transcript to identify topic/chapter boundaries
   624→ */
   625→
   626→const OpenAI = require('openai');
   627→const openai = new OpenAI();
   628→
   629→async function detectChapters(transcript, settings = {}) {
   630→  const { maxChapters = 10, minChapterLength = 60 } = settings;
   631→
   632→  const prompt = `Analyze this transcript and identify natural chapter breaks.
   633→For each chapter, provide:
   634→1. Start time (seconds)
   635→2. A short title (3-5 words)
   636→3. Brief description (1 sentence)
   637→
   638→Rules:
   639→- Maximum ${maxChapters} chapters
   640→- Minimum ${minChapterLength} seconds per chapter
   641→- First chapter starts at 0:00
   642→- Identify topic/subject changes
   643→
   644→Transcript:
   645→${transcript.text}
   646→
   647→Return as JSON array: [{ startTime, title, description }]`;
   648→
   649→  const response = await openai.chat.completions.create({
   650→    model: 'gpt-4o-mini',
   651→    messages: [
   652→      { role: 'system', content: 'You are a video chapter detection expert.' },
   653→      { role: 'user', content: prompt }
   654→    ],
   655→    temperature: 0.3,
   656→    response_format: { type: 'json_object' }
   657→  });
   658→
   659→  const chapters = JSON.parse(response.choices[0].message.content);
   660→
   661→  return {
   662→    chapters: chapters.chapters || chapters,
   663→    youtubeTimestamps: formatYouTubeTimestamps(chapters),
   664→    markers: chapters.map(ch => ({
   665→      time: ch.startTime,
   666→      name: ch.title,
   667→      comment: ch.description
   668→    }))
   669→  };
   670→}
   671→
   672→function formatYouTubeTimestamps(chapters) {
   673→  return chapters.map(ch => {
   674→    const mins = Math.floor(ch.startTime / 60);
   675→    const secs = Math.floor(ch.startTime % 60);
   676→    const time = `${mins}:${secs.toString().padStart(2, '0')}`;
   677→    return `${time} ${ch.title}`;
   678→  }).join('\n');
   679→}
   680→
   681→module.exports = { detectChapters };
   682→```
   683→
   684→**Modify**: `server.js` - Add chapters endpoint
   685→```javascript
   686→app.post('/chapters', requireCredits({ endpoint: 'chapters' }), async (req, res) => {
   687→  try {
   688→    const { transcript, settings } = req.body;
   689→    const chapters = await detectChapters(transcript, settings);
   690→
   691→    await req.deductUsage(5); // 5 seconds per chapter detection
   692→
   693→    res.json({ success: true, ...chapters });
   694→  } catch (err) {
   695→    res.status(500).json({ error: err.message });
   696→  }
   697→});
   698→```
   699→
   700→#### Plugin Changes
   701→
   702→**Modify**: `builder.js` - Add marker creation
   703→```javascript
   704→async function createChapterMarkers(sequence, chapters, project) {
   705→  const markers = await ppro.Markers.getMarkers(sequence);
   706→
   707→  await project.lockedAccess(async () => {
   708→    await project.executeTransaction((compoundAction) => {
   709→      for (const chapter of chapters) {
   710→        const startTime = ppro.TickTime.createWithSeconds(chapter.time);
   711→        const duration = ppro.TickTime.createWithSeconds(1); // 1 second marker
   712→
   713→        const action = markers.createAddMarkerAction(
   714→          `Chapter: ${chapter.name}`,
   715→          ppro.Marker.MARKER_TYPE_CHAPTER,
   716→          startTime,
   717→          duration,
   718→          chapter.comment || ''
   719→        );
   720→        compoundAction.addAction(action);
   721→      }
   722→    }, 'SPLICE: Add Chapter Markers');
   723→  });
   724→}
   725→```
   726→
   727→**Modify**: `index.html` - Add Chapter UI
   728→```html
   729→<div class="chapter-section">
   730→  <label>
   731→    <input type="checkbox" id="enableChapters"> Detect Chapters
   732→  </label>
   733→
   734→  <div id="chapterSettings" class="collapsed">
   735→    <div class="option-row">
   736→      <label>Max Chapters:</label>
   737→      <input type="number" id="maxChapters" min="3" max="20" value="10">
   738→    </div>
   739→    <div class="option-row">
   740→      <label>Min Length:</label>
   741→      <input type="number" id="minChapterLength" min="30" max="300" value="60">
   742→      <span>seconds</span>
   743→    </div>
   744→  </div>
   745→</div>
   746→
   747→<!-- Chapter Results -->
   748→<div id="chapterResults" class="hidden">
   749→  <h4>Detected Chapters</h4>
   750→  <div id="chapterList"></div>
   751→  <button id="copyYouTubeTimestamps">Copy YouTube Timestamps</button>
   752→  <button id="addChapterMarkers">Add to Timeline</button>
   753→</div>
   754→```
   755→
   756→---
   757→
   758→## 5. TAKES LABELING & COLOR CODING
   759→
   760→### Current SPLICE Status
   761→
   762→**Already Implemented** in `builder.js`:
   763→```javascript
   764→const SPLICE_COLORS = {
   765→  SPEECH: COLOR_LABELS.GREEN,        // Green for speech
   766→  SILENCE: COLOR_LABELS.VIOLET,      // Purple for silence
   767→  BEST_TAKE: COLOR_LABELS.CERULEAN   // Light blue for best take
   768→};
   769→```
   770→
   771→### Enhancements Needed
   772→
   773→#### Modify: `cutListGenerator.js`
   774→```javascript
   775→// Add take numbering and labels
   776→function generateCutList(options) {
   777→  let takeNumber = 1;
   778→
   779→  for (const segment of segments) {
   780→    if (segment.type === 'best_take' || segment.type === 'take') {
   781→      segment.takeLabel = `Take ${takeNumber}`;
   782→      segment.takeNumber = takeNumber;
   783→
   784→      // Add short label from take detection
   785→      if (segment.take?.shortLabel) {
   786→        segment.takeLabel += `: ${segment.take.shortLabel}`;
   787→      }
   788→
   789→      takeNumber++;
   790→    }
   791→  }
   792→
   793→  cutList.metadata.takesLabeled = true;
   794→  return cutList;
   795→}
   796→```
   797→
   798→#### Modify: `builder.js`
   799→```javascript
   800→// Apply take labels and colors
   801→async function applyTakeLabeling(sequence, segments, project) {
   802→  const videoTrack = await sequence.getVideoTrack(0);
   803→  const trackItems = await videoTrack.getTrackItems(
   804→    ppro.Constants.TrackItemType.CLIP, false
   805→  );
   806→
   807→  await project.lockedAccess(async () => {
   808→    await project.executeTransaction((compoundAction) => {
   809→      for (let i = 0; i < segments.length && i < trackItems.length; i++) {
   810→        const segment = segments[i];
   811→        const clip = trackItems[i];
   812→
   813→        // Set clip name/label
   814→        if (segment.takeLabel) {
   815→          const nameAction = clip.createSetNameAction(segment.takeLabel);
   816→          compoundAction.addAction(nameAction);
   817→        }
   818→
   819→        // Set color based on type
   820→        const colorIndex = getColorForSegmentType(segment.type);
   821→        const colorAction = clip.projectItem.createSetColorLabelAction(colorIndex);
   822→        compoundAction.addAction(colorAction);
   823→      }
   824→    }, 'SPLICE: Label Takes');
   825→  });
   826→}
   827→
   828→function getColorForSegmentType(type) {
   829→  switch (type) {
   830→    case 'best_take':
   831→      return COLOR_LABELS.CERULEAN;  // Light blue
   832→    case 'take':
   833→      return COLOR_LABELS.LAVENDER;  // Light purple
   834→    case 'speech':
   835→      return COLOR_LABELS.GREEN;     // Green
   836→    case 'silence':
   837→      return COLOR_LABELS.VIOLET;    // Purple
   838→    case 'wide_shot':
   839→      return COLOR_LABELS.YELLOW;    // Yellow
   840→    default:
   841→      return COLOR_LABELS.NONE;
   842→  }
   843→}
   844→```
   845→
   846→---
   847→
   848→## Implementation Phases
   849→
   850→### Phase 1: Foundation (Week 1)
   851→
   852→| Task | Effort | Dependencies |
   853→|------|--------|--------------|
   854→| Takes labeling & color coding | 3-4 hrs | None |
   855→| J-Cut basic support | 4-6 hrs | cutListGenerator.js |
   856→| UI infrastructure updates | 2-3 hrs | index.html, main.js |
   857→
   858→**Deliverables**:
   859→- Enhanced take detection with labels
   860→- Color-coded clips in timeline
   861→- Basic J-cut toggle in UI
   862→
   863→### Phase 2: Multitrack (Week 2)
   864→
   865→| Task | Effort | Dependencies |
   866→|------|--------|--------------|
   867→| Multitrack UI panel | 4-5 hrs | index.html |
   868→| Multitrack.js module | 3-4 hrs | multitrackAnalysis.js (done) |
   869→| Speaker distribution preview | 2-3 hrs | Chart rendering |
   870→
   871→**Deliverables**:
   872→- Complete multitrack UI
   873→- Speaker-to-track mapping
   874→- Visual distribution preview
   875→
   876→### Phase 3: Zoom & Chapters (Week 3)
   877→
   878→| Task | Effort | Dependencies |
   879→|------|--------|--------------|
   880→| Zoom generator service | 3-4 hrs | transcript data |
   881→| Zoom keyframe application | 4-5 hrs | builder.js, UXP APIs |
   882→| Chapter detection service | 3-4 hrs | OpenAI integration |
   883→| Chapter marker creation | 2-3 hrs | UXP Markers API |
   884→
   885→**Deliverables**:
   886→- Auto zoom with presets
   887→- Chapter detection & markers
   888→- YouTube timestamp export
   889→
   890→### Phase 4: Polish & Testing (Week 4)
   891→
   892→| Task | Effort | Dependencies |
   893→|------|--------|--------------|
   894→| E2E test suite for new features | 3-4 hrs | All features |
   895→| UI/UX polish | 2-3 hrs | Design feedback |
   896→| Documentation updates | 2 hrs | CLAUDE.md, README |
   897→| Performance optimization | 2-3 hrs | Profiling |
   898→
   899→**Deliverables**:
   900→- Complete test coverage
   901→- Production-ready features
   902→- Updated documentation
   903→
   904→---
   905→
   906→## File Modifications Summary
   907→
   908→### Backend (`splice-backend/`)
   909→
   910→| File | Action | Changes |
   911→|------|--------|---------|
   912→| `services/zoomGenerator.js` | NEW | Zoom point generation |
   913→| `services/chapterDetection.js` | NEW | Chapter analysis |
   914→| `services/cutListGenerator.js` | MODIFY | Add zoom, J-cut, chapter data |
   915→| `server.js` | MODIFY | Add /chapters endpoint |
   916→
   917→### Plugin (`splice-plugin/`)
   918→
   919→| File | Action | Changes |
   920→|------|--------|---------|
   921→| `js/multitrack.js` | NEW | Multitrack UI module |
   922→| `js/builder.js` | MODIFY | Zoom, J-cut, labeling |
   923→| `js/main.js` | MODIFY | New feature wiring |
   924→| `index.html` | MODIFY | UI panels for all features |
   925→
   926→---
   927→
   928→## Risk Assessment
   929→
   930→| Risk | Likelihood | Impact | Mitigation |
   931→|------|------------|--------|------------|
   932→| UXP API limitations | Medium | High | Test early, have fallbacks |
   933→| Performance with many zooms | Medium | Medium | Batch keyframe operations |
   934→| OpenAI chapter quality | Low | Medium | Allow manual override |
   935→| Multitrack complexity | Medium | Medium | Incremental UI release |
   936→
   937→---
   938→
   939→## Success Metrics
   940→
   941→| Metric | Target | Measurement |
   942→|--------|--------|-------------|
   943→| Auto Zoom accuracy | 90%+ appropriate | User feedback survey |
   944→| Multitrack speaker detection | 95%+ correct | Test with known speakers |
   945→| Chapter detection relevance | 85%+ useful | A/B test vs manual |
   946→| J-Cut smoothness | No audio glitches | QA testing |
   947→| Color coding consistency | 100% applied | Automated tests |
   948→
   949→---
   950→
   951→## Conclusion
   952→
   953→This integration plan provides a clear roadmap to achieve 95%+ FireCut parity while maintaining SPLICE's performance advantages. The phased approach ensures incremental value delivery and allows for user feedback incorporation.
   954→
   955→**Total Effort**: 26-35 hours across 4 weeks
   956→**Result**: SPLICE v4.0 with complete podcast/video editing workflow
   957→
   958→---
   959→
   960→*Generated: 2025-12-25 | Based on comprehensive FireCut deobfuscation analysis*
   961→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
