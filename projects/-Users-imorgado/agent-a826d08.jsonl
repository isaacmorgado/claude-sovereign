{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/imorgado","sessionId":"4f42a096-2511-44a1-ab75-1966aaf695aa","version":"2.0.70","gitBranch":"","agentId":"a826d08","slug":"streamed-tickling-tower","type":"user","message":{"role":"user","content":"You are a software architect. Update the existing BUILD.md file located at /Users/imorgado/Desktop/SPLICE/BUILD.md with the following new research findings. Integrate these findings into the appropriate sections of the existing architecture document, updating recommendations and adding new sections as needed.\n\n## New Research Findings to Integrate:\n\n### 1. Voice Isolation - Updated Recommendation\n**ElevenLabs is NOT the best value at ~$0.20/min**\n\nReplace ElevenLabs recommendation with:\n| Service | Cost per Minute | Best For |\n|---------|----------------|----------|\n| **Picovoice Koala** | ~$0.005/min | Best SDK, on-device processing |\n| **Auphonic** | ~$0.015-$0.02/min | Best value with full REST API |\n| **LALAL.AI** | ~$0.02-$0.04/min | Excellent for music separation |\n| **Dolby.io** | ~$0.03/min | Best for mixed conditions |\n| **Demucs v4** | Free | Open source, high quality |\n\n**Primary Recommendation:** Auphonic or LALAL.AI for cloud processing. These handle loud music, wind, and low dialogue volume well.\n\n### 2. API Pricing for 70% Profit Margin\n**Recommended API Stack:**\n- Transcription: **Deepgram Nova-3** ($0.0043/min)\n- LLM Analysis: **GPT-4o-mini Batch API** ($0.075/1M input tokens)\n- Voice Isolation: **LALAL.AI** or **Dolby.io** (~$0.03/min)\n\n**Cost Breakdown per Hour:**\n| Configuration | Cost/Hour | Price for 70% Margin |\n|--------------|-----------|---------------------|\n| WITHOUT Voice Isolation | $0.27 | **$0.90/hour** |\n| WITH Voice Isolation | $3.87 | **$12.90/hour** |\n\n**Subscription Tiers:**\n| Tier | Hours/Month | Price | Effective Rate |\n|------|-------------|-------|----------------|\n| Starter | 10 hours | $19/month | $1.90/hr |\n| Pro | 30 hours | $49/month | $1.63/hr |\n| Studio | 100 hours | $149/month | $1.49/hr |\n| Voice Isolation Add-on | — | +$59-$549/month | Varies |\n\n### 3. Rate Limiting Implementation\n**Architecture:**\n- **API-level limiting:** Token Bucket algorithm\n- **Monthly quotas:** Sliding Window Counter\n- **Storage:** Redis with `rate-limiter-flexible` library\n- **Job queue:** BullMQ for processing queue management\n\n**External API Limits:**\n- ElevenLabs: 2-15 concurrent requests (tier dependent)\n- Deepgram: 100 concurrent STT requests\n- OpenAI: Varies by tier (TPM/RPM limits)\n\n**Implementation:** Middleware-based rate limiting with Redis tracking, per-user usage counters, graceful degradation with retry-after headers.\n\n### 4. Frontend Stack for User Account Management\n**Recommended Stack:**\n- **Framework:** Next.js 15 (App Router)\n- **Authentication:** Clerk (10K free MAU, built-in billing portal)\n- **Database:** Supabase (PostgreSQL)\n- **Payments:** Stripe Billing with Meters API\n- **UI Components:** shadcn/ui + Tremor (for charts/dashboards)\n\n**Features:**\n- Usage dashboard with real-time charts\n- Subscription management via Stripe Customer Portal\n- API key generation and management\n- Processing history and logs\n- Billing history and invoices\n\n**Alternative:** Supastarter boilerplate ($349) - pre-built SaaS template\n\n### 5. User Interaction - Panel-Based Workflow\n**Critical Finding: Native right-click context menu customization is NOT possible in Adobe Premiere Pro.**\n\n**Solution - Panel with Selection Detection:**\nCreate a CEP panel that:\n1. Detects when user selects clips on timeline\n2. Displays action buttons (\"Process with Voice Isolation\", \"Process without Voice Isolation\")\n3. Uses `onActiveSequenceSelectionChanged` event\n\n```javascript\n// ExtendScript to get selected clips\nfunction getSelection() {\n    var seq = app.project.activeSequence;\n    if (!seq) return [];\n    var selection = seq.getSelection();\n    return selection;\n}\n\n// Listen for selection changes\napp.bind(\"onActiveSequenceSelectionChanged\", function() {\n    // Update panel UI with selection info\n});\n```\n\n**Updated Workflow:**\nSelect clips on timeline → Click button in SPLICE panel → Processing begins\n\n---\n\nRead the existing BUILD.md first, then update it comprehensively with all these new findings. Ensure the document remains well-structured and all sections are properly integrated."},"uuid":"3923ea4a-7330-4cbe-bda9-0f3135de55ee","timestamp":"2025-12-16T17:08:53.221Z"}
{"parentUuid":"3923ea4a-7330-4cbe-bda9-0f3135de55ee","isSidechain":true,"userType":"external","cwd":"/Users/imorgado","sessionId":"4f42a096-2511-44a1-ab75-1966aaf695aa","version":"2.0.70","gitBranch":"","agentId":"a826d08","slug":"streamed-tickling-tower","message":{"model":"claude-opus-4-5-20251101","id":"msg_01KoQnpYHyryQz5KFuYiXcoK","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016h6xTDqHyW3dbWSTX2SkHg","name":"Read","input":{"file_path":"/Users/imorgado/Desktop/SPLICE/BUILD.md"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":12454,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":12454,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWAja81AmG7qU9db5bPsu","type":"assistant","uuid":"78882172-b282-4644-885b-2f06b8151a00","timestamp":"2025-12-16T17:08:56.149Z"}
{"parentUuid":"78882172-b282-4644-885b-2f06b8151a00","isSidechain":true,"userType":"external","cwd":"/Users/imorgado","sessionId":"4f42a096-2511-44a1-ab75-1966aaf695aa","version":"2.0.70","gitBranch":"","agentId":"a826d08","slug":"streamed-tickling-tower","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016h6xTDqHyW3dbWSTX2SkHg","type":"tool_result","content":"     1→# SPLICE - Architecture Documentation\n     2→\n     3→**Version:** 1.0.0\n     4→**Date:** December 16, 2025\n     5→**Status:** Production-Ready Architecture\n     6→\n     7→---\n     8→\n     9→## Table of Contents\n    10→\n    11→1. [System Architecture Overview](#1-system-architecture-overview)\n    12→2. [Technical Stack](#2-technical-stack)\n    13→3. [Core Modules](#3-core-modules)\n    14→4. [Data Models](#4-data-models)\n    15→5. [Processing Pipeline](#5-processing-pipeline)\n    16→6. [API Integration Patterns](#6-api-integration-patterns)\n    17→7. [Premiere Pro Integration](#7-premiere-pro-integration)\n    18→8. [UI/UX Design](#8-uiux-design)\n    19→9. [Development Phases](#9-development-phases)\n    20→10. [File Structure](#10-file-structure)\n    21→11. [Build and Distribution](#11-build-and-distribution)\n    22→\n    23→---\n    24→\n    25→## 1. System Architecture Overview\n    26→\n    27→### High-Level Component Diagram\n    28→\n    29→```\n    30→┌─────────────────────────────────────────────────────────────────────────────┐\n    31→│                         SPLICE PLUGIN ARCHITECTURE                           │\n    32→├─────────────────────────────────────────────────────────────────────────────┤\n    33→│                                                                              │\n    34→│  ┌─────────────────────────────────────────────────────────────────────┐    │\n    35→│  │                      PREMIERE PRO HOST                               │    │\n    36→│  │  ┌─────────────────┐    ┌──────────────────────────────────────┐   │    │\n    37→│  │  │   Project       │    │          SPLICE CEP PANEL            │   │    │\n    38→│  │  │   - Bins        │◄──►│  ┌────────────────────────────────┐  │   │    │\n    39→│  │  │   - Clips       │    │  │       React + TypeScript       │  │   │    │\n    40→│  │  │   - Sequences   │    │  │  ┌──────────┐ ┌─────────────┐  │  │   │    │\n    41→│  │  └─────────────────┘    │  │  │ DropZone │ │ TakeGroups  │  │  │   │    │\n    42→│  │                         │  │  └──────────┘ └─────────────┘  │  │   │    │\n    43→│  │  ┌─────────────────┐    │  │  ┌──────────┐ ┌─────────────┐  │  │   │    │\n    44→│  │  │   Timeline      │    │  │  │ Progress │ │  Settings   │  │  │   │    │\n    45→│  │  │   - Tracks      │◄──►│  │  └──────────┘ └─────────────┘  │  │   │    │\n    46→│  │  │   - Markers     │    │  └────────────────────────────────┘  │   │    │\n    47→│  │  └─────────────────┘    │               │                       │   │    │\n    48→│  │                         │               │ evalTS()               │   │    │\n    49→│  │  ┌─────────────────┐    │               ▼                       │   │    │\n    50→│  │  │  ExtendScript   │◄───│  ┌────────────────────────────────┐  │   │    │\n    51→│  │  │  - timeline.ts  │    │  │      ExtendScript Bridge       │  │   │    │\n    52→│  │  │  - clips.ts     │    │  │  - Timeline manipulation       │  │   │    │\n    53→│  │  │  - markers.ts   │    │  │  - Color coding                │  │   │    │\n    54→│  │  └─────────────────┘    │  │  - Bin organization            │  │   │    │\n    55→│  │                         │  └────────────────────────────────┘  │   │    │\n    56→│  │                         └──────────────────────────────────────┘   │    │\n    57→│  └─────────────────────────────────────────────────────────────────────┘    │\n    58→│                                        │                                     │\n    59→│                                        │ HTTP/HTTPS                          │\n    60→│                                        ▼                                     │\n    61→│  ┌─────────────────────────────────────────────────────────────────────┐    │\n    62→│  │                        PROCESSING LAYER                              │    │\n    63→│  │                                                                      │    │\n    64→│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌────────────┐ │    │\n    65→│  │  │   FFmpeg    │  │ ElevenLabs  │  │  Deepgram   │  │ GPT-4o-mini│ │    │\n    66→│  │  │   (Local)   │  │    API      │  │    API      │  │    API     │ │    │\n    67→│  │  │             │  │             │  │             │  │            │ │    │\n    68→│  │  │ - Extract   │  │ - Voice     │  │ - Transcribe│  │ - Analyze  │ │    │\n    69→│  │  │   Audio     │  │   Isolation │  │ - Timestamps│  │   Takes    │ │    │\n    70→│  │  └─────────────┘  └─────────────┘  └─────────────┘  └────────────┘ │    │\n    71→│  │                                                                      │    │\n    72→│  │  ┌─────────────┐  ┌─────────────────────────────────────────────┐  │    │\n    73→│  │  │ Silero VAD  │  │           sentence-transformers              │  │    │\n    74→│  │  │   (Local)   │  │                 (Local)                      │  │    │\n    75→│  │  │             │  │                                              │  │    │\n    76→│  │  │ - Silence   │  │ - Generate embeddings (all-MiniLM-L6-v2)    │  │    │\n    77→│  │  │   Detection │  │ - Cosine similarity clustering              │  │    │\n    78→│  │  └─────────────┘  └─────────────────────────────────────────────┘  │    │\n    79→│  └─────────────────────────────────────────────────────────────────────┘    │\n    80→│                                                                              │\n    81→└─────────────────────────────────────────────────────────────────────────────┘\n    82→```\n    83→\n    84→### Data Flow\n    85→\n    86→```\n    87→┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐\n    88→│  User    │     │   Extract    │     │    Voice      │     │  Transcribe  │\n    89→│  Drops   │────►│   Audio      │────►│   Isolation   │────►│  + Timestamp │\n    90→│  Clips   │     │   (FFmpeg)   │     │  (ElevenLabs) │     │  (Deepgram)  │\n    91→└──────────┘     └──────────────┘     └───────────────┘     └──────────────┘\n    92→                                                                    │\n    93→                                                                    ▼\n    94→┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐\n    95→│ Timeline │     │   Organize   │     │    Detect     │     │   Detect     │\n    96→│  Import  │◄────│   & Color    │◄────│    Silence    │◄────│   Takes      │\n    97→│          │     │   Code       │     │  (Silero VAD) │     │  (Embed+LLM) │\n    98→└──────────┘     └──────────────┘     └───────────────┘     └──────────────┘\n    99→```\n   100→\n   101→### Technology Decisions\n   102→\n   103→| Component | Technology | Rationale |\n   104→|-----------|------------|-----------|\n   105→| Plugin Platform | CEP | Production-ready, full API access, UXP still in beta |\n   106→| UI Framework | React + TypeScript | Best Bolt CEP support, type safety |\n   107→| Build Tool | Vite | 10-100x faster than webpack |\n   108→| Voice Isolation | ElevenLabs | Best quality/price, 500MB limit |\n   109→| Transcription | Deepgram Nova-3 | Native word timestamps, diarization included |\n   110→| Take Detection | Embeddings + GPT-4o-mini | Hybrid approach: fast filtering + LLM analysis |\n   111→| Silence Detection | Silero VAD | ML-based, 95%+ accuracy, local processing |\n   112→| Premiere API | ExtendScript | Only option for timeline manipulation |\n   113→\n   114→---\n   115→\n   116→## 2. Technical Stack\n   117→\n   118→### Frontend (CEP Panel)\n   119→\n   120→```yaml\n   121→Framework: React 18.2+\n   122→Language: TypeScript 5.0+\n   123→Build Tool: Vite 5.0+ (via Bolt CEP)\n   124→UI Components: Adobe Spectrum Web Components\n   125→State Management: Zustand (or React Context)\n   126→Styling: Tailwind CSS + CSS Modules\n   127→```\n   128→\n   129→### Backend (Processing Services)\n   130→\n   131→```yaml\n   132→Audio Extraction: FFmpeg 6.0+ (bundled binary)\n   133→Voice Isolation: ElevenLabs API v1\n   134→Transcription: Deepgram Nova-3 API\n   135→Embeddings: sentence-transformers (all-MiniLM-L6-v2)\n   136→Clustering: scikit-learn AgglomerativeClustering\n   137→LLM Analysis: OpenAI GPT-4o-mini API\n   138→Silence Detection: Silero VAD 4.0+\n   139→```\n   140→\n   141→### Premiere Integration\n   142→\n   143→```yaml\n   144→Panel Platform: CEP 12\n   145→Scripting: ExtendScript (ECMAScript 3)\n   146→Communication: evalTS() via Bolt CEP\n   147→Supported Premiere: 22.0+ (2022 and later)\n   148→```\n   149→\n   150→### Development Tools\n   151→\n   152→```yaml\n   153→Package Manager: pnpm or yarn\n   154→Node.js: 18.0+ (LTS)\n   155→IDE: VS Code\n   156→Debugging: Chrome DevTools (localhost:8088)\n   157→ExtendScript Debug: Adobe ExtendScript Debugger extension\n   158→```\n   159→\n   160→---\n   161→\n   162→## 3. Core Modules\n   163→\n   164→### 3.1 DropZone/Import Module\n   165→\n   166→```typescript\n   167→// src/js/main/modules/import/DropZone.ts\n   168→\n   169→interface ImportModule {\n   170→  // Events\n   171→  onFilesDropped: (files: File[]) => void;\n   172→  onClipsSelected: (clips: ClipInfo[]) => void;\n   173→\n   174→  // Methods\n   175→  validateFiles(files: File[]): ValidationResult;\n   176→  extractClipInfo(files: File[]): Promise<ClipInfo[]>;\n   177→  getSelectedClips(): Promise<ClipInfo[]>;\n   178→}\n   179→\n   180→interface ValidationResult {\n   181→  valid: boolean;\n   182→  errors: string[];\n   183→  warnings: string[];\n   184→  supportedFormats: string[];\n   185→}\n   186→```\n   187→\n   188→### 3.2 Audio Extraction Module\n   189→\n   190→```typescript\n   191→// src/js/main/modules/audio/AudioExtractor.ts\n   192→\n   193→interface AudioExtractorModule {\n   194→  extractAudio(clipPath: string, options: ExtractionOptions): Promise<AudioBuffer>;\n   195→  getAudioInfo(clipPath: string): Promise<AudioInfo>;\n   196→  splitAudio(buffer: AudioBuffer, chunks: number): AudioBuffer[];\n   197→}\n   198→\n   199→interface ExtractionOptions {\n   200→  format: 'wav' | 'mp3' | 'pcm';\n   201→  sampleRate: 16000 | 44100 | 48000;\n   202→  channels: 1 | 2;\n   203→  bitDepth?: 16 | 24 | 32;\n   204→}\n   205→\n   206→interface AudioInfo {\n   207→  duration: number;\n   208→  sampleRate: number;\n   209→  channels: number;\n   210→  format: string;\n   211→  bitRate: number;\n   212→}\n   213→```\n   214→\n   215→### 3.3 Voice Isolation Module\n   216→\n   217→```typescript\n   218→// src/js/main/modules/isolation/VoiceIsolator.ts\n   219→\n   220→interface VoiceIsolationModule {\n   221→  isolate(audioBuffer: ArrayBuffer): Promise<ArrayBuffer>;\n   222→  isAvailable(): boolean;\n   223→  estimateCost(durationMinutes: number): number;\n   224→}\n   225→\n   226→interface IsolationConfig {\n   227→  apiKey: string;\n   228→  outputFormat: 'pcm_s16le_16' | 'mp3' | 'wav';\n   229→  timeout: number;\n   230→}\n   231→```\n   232→\n   233→### 3.4 Transcription Module\n   234→\n   235→```typescript\n   236→// src/js/main/modules/transcription/Transcriber.ts\n   237→\n   238→interface TranscriptionModule {\n   239→  transcribe(audioBuffer: ArrayBuffer): Promise<Transcript>;\n   240→  transcribeWithDiarization(audioBuffer: ArrayBuffer): Promise<DiarizedTranscript>;\n   241→  estimateCost(durationMinutes: number): number;\n   242→}\n   243→\n   244→interface TranscriptWord {\n   245→  text: string;\n   246→  start: number;      // seconds\n   247→  end: number;        // seconds\n   248→  confidence: number; // 0-1\n   249→  speaker?: string;   // \"Speaker 1\", \"Speaker 2\"\n   250→}\n   251→\n   252→interface Transcript {\n   253→  text: string;\n   254→  words: TranscriptWord[];\n   255→  duration: number;\n   256→  language: string;\n   257→}\n   258→```\n   259→\n   260→### 3.5 Take Detection Module\n   261→\n   262→```typescript\n   263→// src/js/main/modules/takes/TakeDetector.ts\n   264→\n   265→interface TakeDetectionModule {\n   266→  detectTakes(transcript: Transcript): Promise<TakeGroup[]>;\n   267→  generateEmbeddings(sentences: string[]): Promise<number[][]>;\n   268→  clusterBySimilarity(embeddings: number[][], threshold: number): number[];\n   269→  analyzeTakesWithLLM(candidates: TakeCandidate[]): Promise<TakeAnalysis>;\n   270→}\n   271→\n   272→interface TakeCandidate {\n   273→  sentences: string[];\n   274→  timestamps: TimeRange[];\n   275→  similarityScore: number;\n   276→}\n   277→\n   278→interface TakeAnalysis {\n   279→  groups: TakeGroup[];\n   280→  bestTakes: Record<string, number>; // groupId -> takeIndex\n   281→  confidence: number;\n   282→}\n   283→```\n   284→\n   285→### 3.6 Silence Detection Module\n   286→\n   287→```typescript\n   288→// src/js/main/modules/silence/SilenceDetector.ts\n   289→\n   290→interface SilenceDetectionModule {\n   291→  detectSilence(audioBuffer: ArrayBuffer): Promise<SilentSection[]>;\n   292→  detectFromTranscript(words: TranscriptWord[]): SilentSection[];\n   293→  filterNaturalPauses(sections: SilentSection[]): SilentSection[];\n   294→}\n   295→\n   296→interface SilenceConfig {\n   297→  minDuration: number;      // seconds (default: 1.5)\n   298→  threshold: number;        // dB (default: -40)\n   299→  preservePauseDuration: number; // seconds (default: 0.8)\n   300→}\n   301→```\n   302→\n   303→### 3.7 Timeline Integration Module\n   304→\n   305→```typescript\n   306→// src/js/main/modules/premiere/TimelineIntegrator.ts\n   307→\n   308→interface TimelineModule {\n   309→  // Bin operations\n   310→  createBin(name: string, parentPath?: string): Promise<string>;\n   311→  moveToBin(clipId: string, binPath: string): Promise<void>;\n   312→\n   313→  // Color coding\n   314→  setClipColor(clipId: string, colorIndex: number): Promise<void>;\n   315→\n   316→  // Timeline operations\n   317→  insertClip(clipId: string, trackIndex: number, time: number): Promise<void>;\n   318→  setInOutPoints(clipId: string, inPoint: number, outPoint: number): Promise<void>;\n   319→\n   320→  // Markers\n   321→  createMarker(time: number, name: string, color: number): Promise<void>;\n   322→}\n   323→\n   324→// Color mapping for takes\n   325→const TAKE_COLORS = {\n   326→  1: 5,   // Rose\n   327→  2: 3,   // Cerulean\n   328→  3: 6,   // Mango\n   329→  4: 7,   // Purple\n   330→  5: 4,   // Forest\n   331→  6: 9,   // Lavender\n   332→  7: 12,  // Orange\n   333→  8: 14,  // Blue\n   334→} as const;\n   335→```\n   336→\n   337→### 3.8 Settings Module\n   338→\n   339→```typescript\n   340→// src/js/main/modules/settings/SettingsManager.ts\n   341→\n   342→interface SettingsModule {\n   343→  get<T>(key: string): T | undefined;\n   344→  set<T>(key: string, value: T): void;\n   345→  getAll(): Settings;\n   346→  reset(): void;\n   347→  export(): string;\n   348→  import(json: string): void;\n   349→}\n   350→\n   351→interface Settings {\n   352→  // Voice Isolation\n   353→  useVoiceIsolation: boolean;\n   354→  audioOutputMode: 'original' | 'isolated' | 'none';\n   355→\n   356→  // Transcription\n   357→  transcriptionProvider: 'deepgram' | 'assemblyai' | 'whisper';\n   358→  language: string;\n   359→\n   360→  // Take Detection\n   361→  similarityThreshold: number;  // 0.70-0.95\n   362→  useLLMAnalysis: boolean;\n   363→\n   364→  // Silence Detection\n   365→  minSilenceDuration: number;   // seconds\n   366→  silenceThreshold: number;     // dB\n   367→  preserveNaturalPauses: boolean;\n   368→\n   369→  // Timeline\n   370→  autoColorCode: boolean;\n   371→  createBins: boolean;\n   372→  addMarkers: boolean;\n   373→\n   374→  // API Keys\n   375→  elevenLabsApiKey: string;\n   376→  deepgramApiKey: string;\n   377→  openaiApiKey: string;\n   378→}\n   379→```\n   380→\n   381→---\n   382→\n   383→## 4. Data Models\n   384→\n   385→### Core Interfaces\n   386→\n   387→```typescript\n   388→// src/js/main/types/models.ts\n   389→\n   390→// === CLIP MODELS ===\n   391→\n   392→interface ClipInfo {\n   393→  id: string;\n   394→  name: string;\n   395→  path: string;\n   396→  duration: number;        // seconds\n   397→  inPoint: number;         // source in point\n   398→  outPoint: number;        // source out point\n   399→  hasAudio: boolean;\n   400→  hasVideo: boolean;\n   401→  audioChannels: number;\n   402→  frameRate: number;\n   403→  resolution: { width: number; height: number };\n   404→}\n   405→\n   406→// === TAKE MODELS ===\n   407→\n   408→interface Take {\n   409→  id: string;\n   410→  clipId: string;\n   411→  takeNumber: number;\n   412→  text: string;\n   413→  startTime: number;       // seconds in source\n   414→  endTime: number;\n   415→  duration: number;\n   416→  confidence: number;      // 0-1\n   417→  isPartial: boolean;      // incomplete take\n   418→  isBest: boolean;         // suggested best take\n   419→  audioClarity: number;    // 0-1 (if analyzed)\n   420→}\n   421→\n   422→interface TakeGroup {\n   423→  id: string;\n   424→  canonicalText: string;   // reference text for this line\n   425→  takes: Take[];\n   426→  similarityType: 'exact' | 'minor_variation' | 'paraphrase';\n   427→  suggestedBestTakeId: string | null;\n   428→}\n   429→\n   430→// === TRANSCRIPT MODELS ===\n   431→\n   432→interface Word {\n   433→  text: string;\n   434→  start: number;           // seconds\n   435→  end: number;             // seconds\n   436→  confidence: number;\n   437→  speaker?: string;\n   438→  punctuatedText?: string;\n   439→}\n   440→\n   441→interface Sentence {\n   442→  text: string;\n   443→  words: Word[];\n   444→  start: number;\n   445→  end: number;\n   446→  speaker?: string;\n   447→}\n   448→\n   449→interface Transcript {\n   450→  id: string;\n   451→  clipId: string;\n   452→  fullText: string;\n   453→  words: Word[];\n   454→  sentences: Sentence[];\n   455→  speakers: string[];\n   456→  language: string;\n   457→  duration: number;\n   458→}\n   459→\n   460→// === SILENCE MODELS ===\n   461→\n   462→interface SilentSection {\n   463→  id: string;\n   464→  startTime: number;\n   465→  endTime: number;\n   466→  duration: number;\n   467→  type: 'dead_space' | 'natural_pause' | 'take_boundary';\n   468→  shouldRemove: boolean;\n   469→  afterWord?: string;\n   470→  beforeWord?: string;\n   471→}\n   472→\n   473→// === PROCESSING MODELS ===\n   474→\n   475→interface ProcessingJob {\n   476→  id: string;\n   477→  clipId: string;\n   478→  status: JobStatus;\n   479→  progress: number;        // 0-100\n   480→  currentStep: ProcessingStep;\n   481→  startedAt: Date;\n   482→  completedAt?: Date;\n   483→  error?: ProcessingError;\n   484→  result?: ProcessingResult;\n   485→}\n   486→\n   487→type JobStatus = 'pending' | 'processing' | 'completed' | 'failed' | 'cancelled';\n   488→\n   489→type ProcessingStep =\n   490→  | 'extracting_audio'\n   491→  | 'isolating_voice'\n   492→  | 'transcribing'\n   493→  | 'detecting_takes'\n   494→  | 'detecting_silence'\n   495→  | 'organizing'\n   496→  | 'complete';\n   497→\n   498→interface ProcessingError {\n   499→  code: string;\n   500→  message: string;\n   501→  step: ProcessingStep;\n   502→  retryable: boolean;\n   503→}\n   504→\n   505→interface ProcessingResult {\n   506→  clipId: string;\n   507→  transcript: Transcript;\n   508→  takeGroups: TakeGroup[];\n   509→  silentSections: SilentSection[];\n   510→  processingTime: number;  // ms\n   511→  apiCosts: {\n   512→    voiceIsolation: number;\n   513→    transcription: number;\n   514→    llmAnalysis: number;\n   515→    total: number;\n   516→  };\n   517→}\n   518→\n   519→// === PREMIERE MODELS ===\n   520→\n   521→interface PremiereClip {\n   522→  projectItemId: string;\n   523→  name: string;\n   524→  binPath: string;\n   525→  colorLabel: number;\n   526→  inPoint: number;\n   527→  outPoint: number;\n   528→}\n   529→\n   530→interface PremiereTimeline {\n   531→  sequenceId: string;\n   532→  name: string;\n   533→  duration: number;\n   534→  videoTracks: number;\n   535→  audioTracks: number;\n   536→}\n   537→```\n   538→\n   539→---\n   540→\n   541→## 5. Processing Pipeline\n   542→\n   543→### Main Processing Flow\n   544→\n   545→```typescript\n   546→// src/js/main/services/ProcessingPipeline.ts\n   547→\n   548→class ProcessingPipeline {\n   549→  private audioExtractor: AudioExtractorModule;\n   550→  private voiceIsolator: VoiceIsolationModule;\n   551→  private transcriber: TranscriptionModule;\n   552→  private takeDetector: TakeDetectionModule;\n   553→  private silenceDetector: SilenceDetectionModule;\n   554→  private timelineIntegrator: TimelineModule;\n   555→\n   556→  async processClip(clip: ClipInfo, options: ProcessingOptions): Promise<ProcessingResult> {\n   557→    const job = this.createJob(clip);\n   558→\n   559→    try {\n   560→      // Step 1: Extract audio from video\n   561→      this.updateProgress(job, 'extracting_audio', 10);\n   562→      const audioBuffer = await this.audioExtractor.extractAudio(clip.path, {\n   563→        format: 'wav',\n   564→        sampleRate: 16000,\n   565→        channels: 1\n   566→      });\n   567→\n   568→      // Step 2: Voice isolation (optional)\n   569→      let analysisAudio = audioBuffer;\n   570→      let outputAudio = audioBuffer;\n   571→\n   572→      if (options.useVoiceIsolation) {\n   573→        this.updateProgress(job, 'isolating_voice', 25);\n   574→        const isolatedAudio = await this.voiceIsolator.isolate(audioBuffer);\n   575→        analysisAudio = isolatedAudio;\n   576→\n   577→        if (options.audioOutputMode === 'isolated') {\n   578→          outputAudio = isolatedAudio;\n   579→        }\n   580→      }\n   581→\n   582→      // Step 3: Transcription with word-level timestamps\n   583→      this.updateProgress(job, 'transcribing', 40);\n   584→      const transcript = await this.transcriber.transcribe(analysisAudio);\n   585→\n   586→      // Step 4: Take detection\n   587→      this.updateProgress(job, 'detecting_takes', 60);\n   588→      const takeGroups = await this.takeDetector.detectTakes(transcript);\n   589→\n   590→      // Step 5: Silence detection\n   591→      this.updateProgress(job, 'detecting_silence', 75);\n   592→      const silentSections = await this.silenceDetector.detectFromTranscript(transcript.words);\n   593→      const filteredSilence = this.silenceDetector.filterNaturalPauses(silentSections);\n   594→\n   595→      // Step 6: Organize in Premiere\n   596→      this.updateProgress(job, 'organizing', 90);\n   597→      await this.organizeInPremiere(clip, takeGroups, filteredSilence, options);\n   598→\n   599→      this.updateProgress(job, 'complete', 100);\n   600→\n   601→      return {\n   602→        clipId: clip.id,\n   603→        transcript,\n   604→        takeGroups,\n   605→        silentSections: filteredSilence,\n   606→        processingTime: Date.now() - job.startedAt.getTime(),\n   607→        apiCosts: this.calculateCosts(job)\n   608→      };\n   609→\n   610→    } catch (error) {\n   611→      job.status = 'failed';\n   612→      job.error = this.createError(error);\n   613→      throw error;\n   614→    }\n   615→  }\n   616→\n   617→  private async organizeInPremiere(\n   618→    clip: ClipInfo,\n   619→    takeGroups: TakeGroup[],\n   620→    silentSections: SilentSection[],\n   621→    options: ProcessingOptions\n   622→  ): Promise<void> {\n   623→\n   624→    // Create bin structure\n   625→    if (options.createBins) {\n   626→      const mainBin = await this.timelineIntegrator.createBin('SPLICE Takes');\n   627→\n   628→      for (const group of takeGroups) {\n   629→        const groupBin = await this.timelineIntegrator.createBin(\n   630→          `Line ${group.id}`,\n   631→          mainBin\n   632→        );\n   633→\n   634→        for (const take of group.takes) {\n   635→          // Create subclip with in/out points\n   636→          // Move to appropriate bin\n   637→          // Set color based on take number\n   638→          await this.timelineIntegrator.setClipColor(\n   639→            take.clipId,\n   640→            TAKE_COLORS[take.takeNumber] || 0\n   641→          );\n   642→        }\n   643→      }\n   644→    }\n   645→\n   646→    // Add markers for take boundaries\n   647→    if (options.addMarkers) {\n   648→      for (const group of takeGroups) {\n   649→        for (const take of group.takes) {\n   650→          await this.timelineIntegrator.createMarker(\n   651→            take.startTime,\n   652→            `Take ${take.takeNumber}: ${group.canonicalText.substring(0, 30)}...`,\n   653→            take.isBest ? 1 : 0  // Red for best, green for others\n   654→          );\n   655→        }\n   656→      }\n   657→    }\n   658→  }\n   659→}\n   660→```\n   661→\n   662→### Take Detection Algorithm\n   663→\n   664→```typescript\n   665→// src/js/main/services/TakeDetectionService.ts\n   666→\n   667→class TakeDetectionService implements TakeDetectionModule {\n   668→  private embeddingModel: SentenceTransformer;\n   669→  private openaiClient: OpenAI;\n   670→\n   671→  async detectTakes(transcript: Transcript): Promise<TakeGroup[]> {\n   672→    // 1. Segment transcript into sentences\n   673→    const sentences = this.segmentIntoSentences(transcript);\n   674→\n   675→    // 2. Generate embeddings for all sentences\n   676→    const embeddings = await this.generateEmbeddings(\n   677→      sentences.map(s => s.text)\n   678→    );\n   679→\n   680→    // 3. Compute similarity matrix\n   681→    const similarityMatrix = this.computeCosineSimilarity(embeddings);\n   682→\n   683→    // 4. Find candidate pairs (similarity > threshold)\n   684→    const candidates = this.findCandidatePairs(\n   685→      sentences,\n   686→      similarityMatrix,\n   687→      0.80  // threshold\n   688→    );\n   689→\n   690→    // 5. Cluster into groups\n   691→    const clusters = this.clusterCandidates(candidates);\n   692→\n   693→    // 6. Analyze with LLM for confirmation and best-take selection\n   694→    const analysis = await this.analyzeTakesWithLLM(clusters);\n   695→\n   696→    // 7. Build TakeGroup objects\n   697→    return this.buildTakeGroups(sentences, clusters, analysis);\n   698→  }\n   699→\n   700→  private computeCosineSimilarity(embeddings: number[][]): number[][] {\n   701→    const n = embeddings.length;\n   702→    const matrix: number[][] = Array(n).fill(null).map(() => Array(n).fill(0));\n   703→\n   704→    for (let i = 0; i < n; i++) {\n   705→      for (let j = i; j < n; j++) {\n   706→        const sim = this.cosineSim(embeddings[i], embeddings[j]);\n   707→        matrix[i][j] = sim;\n   708→        matrix[j][i] = sim;\n   709→      }\n   710→    }\n   711→\n   712→    return matrix;\n   713→  }\n   714→\n   715→  private cosineSim(a: number[], b: number[]): number {\n   716→    let dotProduct = 0;\n   717→    let normA = 0;\n   718→    let normB = 0;\n   719→\n   720→    for (let i = 0; i < a.length; i++) {\n   721→      dotProduct += a[i] * b[i];\n   722→      normA += a[i] * a[i];\n   723→      normB += b[i] * b[i];\n   724→    }\n   725→\n   726→    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));\n   727→  }\n   728→\n   729→  private async analyzeTakesWithLLM(clusters: TakeCluster[]): Promise<LLMAnalysis> {\n   730→    const prompt = this.buildAnalysisPrompt(clusters);\n   731→\n   732→    const response = await this.openaiClient.chat.completions.create({\n   733→      model: 'gpt-4o-mini',\n   734→      messages: [\n   735→        { role: 'system', content: TAKE_ANALYSIS_SYSTEM_PROMPT },\n   736→        { role: 'user', content: prompt }\n   737→      ],\n   738→      response_format: { type: 'json_object' }\n   739→    });\n   740→\n   741→    return JSON.parse(response.choices[0].message.content);\n   742→  }\n   743→}\n   744→\n   745→const TAKE_ANALYSIS_SYSTEM_PROMPT = `\n   746→You are analyzing transcript segments to identify repeated takes.\n   747→\n   748→For each group of similar sentences, determine:\n   749→1. Whether they represent the same intended dialogue (different takes)\n   750→2. The similarity type: exact, minor_variation, or paraphrase\n   751→3. Which take appears to be the \"best\" (most complete, clearest)\n   752→\n   753→Return JSON with this structure:\n   754→{\n   755→  \"groups\": [\n   756→    {\n   757→      \"id\": \"string\",\n   758→      \"isTakeGroup\": boolean,\n   759→      \"similarityType\": \"exact\" | \"minor_variation\" | \"paraphrase\",\n   760→      \"bestTakeIndex\": number,\n   761→      \"confidence\": number\n   762→    }\n   763→  ]\n   764→}\n   765→`;\n   766→```\n   767→\n   768→---\n   769→\n   770→## 6. API Integration Patterns\n   771→\n   772→### Rate Limiting and Retry Logic\n   773→\n   774→```typescript\n   775→// src/js/main/utils/apiClient.ts\n   776→\n   777→interface RetryConfig {\n   778→  maxRetries: number;\n   779→  baseDelay: number;\n   780→  maxDelay: number;\n   781→  retryableStatuses: number[];\n   782→}\n   783→\n   784→const DEFAULT_RETRY_CONFIG: RetryConfig = {\n   785→  maxRetries: 3,\n   786→  baseDelay: 1000,\n   787→  maxDelay: 30000,\n   788→  retryableStatuses: [429, 500, 502, 503, 504]\n   789→};\n   790→\n   791→async function fetchWithRetry<T>(\n   792→  url: string,\n   793→  options: RequestInit,\n   794→  config: RetryConfig = DEFAULT_RETRY_CONFIG\n   795→): Promise<T> {\n   796→  let lastError: Error;\n   797→\n   798→  for (let attempt = 0; attempt <= config.maxRetries; attempt++) {\n   799→    try {\n   800→      const response = await fetch(url, options);\n   801→\n   802→      if (!response.ok) {\n   803→        if (config.retryableStatuses.includes(response.status)) {\n   804→          throw new RetryableError(response.status, await response.text());\n   805→        }\n   806→        throw new ApiError(response.status, await response.text());\n   807→      }\n   808→\n   809→      return await response.json();\n   810→\n   811→    } catch (error) {\n   812→      lastError = error;\n   813→\n   814→      if (error instanceof RetryableError && attempt < config.maxRetries) {\n   815→        const delay = Math.min(\n   816→          config.baseDelay * Math.pow(2, attempt),\n   817→          config.maxDelay\n   818→        );\n   819→        await sleep(delay);\n   820→        continue;\n   821→      }\n   822→\n   823→      throw error;\n   824→    }\n   825→  }\n   826→\n   827→  throw lastError;\n   828→}\n   829→```\n   830→\n   831→### API Key Management\n   832→\n   833→```typescript\n   834→// src/js/main/services/CredentialManager.ts\n   835→\n   836→import keytar from 'keytar';\n   837→\n   838→const SERVICE_NAME = 'SPLICE_Plugin';\n   839→\n   840→class CredentialManager {\n   841→  private static instance: CredentialManager;\n   842→  private cache: Map<string, string> = new Map();\n   843→\n   844→  static getInstance(): CredentialManager {\n   845→    if (!this.instance) {\n   846→      this.instance = new CredentialManager();\n   847→    }\n   848→    return this.instance;\n   849→  }\n   850→\n   851→  async setApiKey(provider: ApiProvider, key: string): Promise<void> {\n   852→    await keytar.setPassword(SERVICE_NAME, provider, key);\n   853→    this.cache.set(provider, key);\n   854→  }\n   855→\n   856→  async getApiKey(provider: ApiProvider): Promise<string | null> {\n   857→    if (this.cache.has(provider)) {\n   858→      return this.cache.get(provider)!;\n   859→    }\n   860→\n   861→    const key = await keytar.getPassword(SERVICE_NAME, provider);\n   862→    if (key) {\n   863→      this.cache.set(provider, key);\n   864→    }\n   865→    return key;\n   866→  }\n   867→\n   868→  async deleteApiKey(provider: ApiProvider): Promise<void> {\n   869→    await keytar.deletePassword(SERVICE_NAME, provider);\n   870→    this.cache.delete(provider);\n   871→  }\n   872→\n   873→  async validateApiKey(provider: ApiProvider, key: string): Promise<boolean> {\n   874→    switch (provider) {\n   875→      case 'elevenlabs':\n   876→        return this.validateElevenLabsKey(key);\n   877→      case 'deepgram':\n   878→        return this.validateDeepgramKey(key);\n   879→      case 'openai':\n   880→        return this.validateOpenAIKey(key);\n   881→      default:\n   882→        return false;\n   883→    }\n   884→  }\n   885→}\n   886→\n   887→type ApiProvider = 'elevenlabs' | 'deepgram' | 'openai';\n   888→```\n   889→\n   890→### Cost Tracking\n   891→\n   892→```typescript\n   893→// src/js/main/services/CostTracker.ts\n   894→\n   895→interface CostRates {\n   896→  elevenlabs: {\n   897→    perMinute: number;  // $0.30 at Creator tier\n   898→  };\n   899→  deepgram: {\n   900→    perMinute: number;  // $0.0043\n   901→  };\n   902→  openai: {\n   903→    inputPer1M: number;   // $0.15\n   904→    outputPer1M: number;  // $0.60\n   905→  };\n   906→}\n   907→\n   908→const DEFAULT_RATES: CostRates = {\n   909→  elevenlabs: { perMinute: 0.30 },\n   910→  deepgram: { perMinute: 0.0043 },\n   911→  openai: { inputPer1M: 0.15, outputPer1M: 0.60 }\n   912→};\n   913→\n   914→class CostTracker {\n   915→  private rates: CostRates;\n   916→  private usage: UsageRecord = {\n   917→    elevenlabs: { minutes: 0, cost: 0 },\n   918→    deepgram: { minutes: 0, cost: 0 },\n   919→    openai: { inputTokens: 0, outputTokens: 0, cost: 0 }\n   920→  };\n   921→\n   922→  estimateCost(durationMinutes: number, options: ProcessingOptions): CostEstimate {\n   923→    let total = 0;\n   924→\n   925→    // Transcription (always needed)\n   926→    const transcriptionCost = durationMinutes * this.rates.deepgram.perMinute;\n   927→    total += transcriptionCost;\n   928→\n   929→    // Voice isolation (optional)\n   930→    let isolationCost = 0;\n   931→    if (options.useVoiceIsolation) {\n   932→      isolationCost = durationMinutes * this.rates.elevenlabs.perMinute;\n   933→      total += isolationCost;\n   934→    }\n   935→\n   936→    // LLM analysis (estimated based on transcript length)\n   937→    const estimatedTokens = durationMinutes * 500; // ~500 tokens per minute\n   938→    const llmCost = (estimatedTokens / 1_000_000) * this.rates.openai.inputPer1M +\n   939→                    (estimatedTokens / 4 / 1_000_000) * this.rates.openai.outputPer1M;\n   940→    total += llmCost;\n   941→\n   942→    return {\n   943→      transcription: transcriptionCost,\n   944→      voiceIsolation: isolationCost,\n   945→      llmAnalysis: llmCost,\n   946→      total\n   947→    };\n   948→  }\n   949→}\n   950→```\n   951→\n   952→---\n   953→\n   954→## 7. Premiere Pro Integration\n   955→\n   956→### ExtendScript Functions\n   957→\n   958→```typescript\n   959→// src/jsx/index.ts\n   960→\n   961→// === CLIP OPERATIONS ===\n   962→\n   963→export function getSelectedClips(): string {\n   964→  const selection = app.project.activeSequence?.getSelection() || [];\n   965→  const clips = [];\n   966→\n   967→  for (let i = 0; i < selection.length; i++) {\n   968→    clips.push({\n   969→      id: selection[i].projectItem.nodeId,\n   970→      name: selection[i].name,\n   971→      duration: selection[i].duration.seconds\n   972→    });\n   973→  }\n   974→\n   975→  return JSON.stringify(clips);\n   976→}\n   977→\n   978→export function setClipColor(clipId: string, colorIndex: number): string {\n   979→  const item = findProjectItemById(clipId);\n   980→  if (item) {\n   981→    item.setColorLabel(colorIndex);\n   982→    return JSON.stringify({ success: true });\n   983→  }\n   984→  return JSON.stringify({ success: false, error: 'Clip not found' });\n   985→}\n   986→\n   987→export function setInOutPoints(clipId: string, inPoint: number, outPoint: number): string {\n   988→  const item = findProjectItemById(clipId);\n   989→  if (item) {\n   990→    item.setInPoint(inPoint, 4);  // 4 = update UI\n   991→    item.setOutPoint(outPoint, 4);\n   992→    return JSON.stringify({ success: true });\n   993→  }\n   994→  return JSON.stringify({ success: false, error: 'Clip not found' });\n   995→}\n   996→\n   997→// === BIN OPERATIONS ===\n   998→\n   999→export function createBin(name: string, parentPath?: string): string {\n  1000→  const parent = parentPath\n  1001→    ? findBinByPath(parentPath)\n  1002→    : app.project.rootItem;\n  1003→\n  1004→  if (parent) {\n  1005→    const newBin = parent.createBin(name);\n  1006→    return JSON.stringify({\n  1007→      success: true,\n  1008→      binId: newBin.nodeId,\n  1009→      path: getBinPath(newBin)\n  1010→    });\n  1011→  }\n  1012→  return JSON.stringify({ success: false, error: 'Parent bin not found' });\n  1013→}\n  1014→\n  1015→export function moveClipToBin(clipId: string, binPath: string): string {\n  1016→  const clip = findProjectItemById(clipId);\n  1017→  const bin = findBinByPath(binPath);\n  1018→\n  1019→  if (clip && bin) {\n  1020→    clip.moveBin(bin);\n  1021→    return JSON.stringify({ success: true });\n  1022→  }\n  1023→  return JSON.stringify({ success: false, error: 'Clip or bin not found' });\n  1024→}\n  1025→\n  1026→// === TIMELINE OPERATIONS ===\n  1027→\n  1028→export function insertClipToTimeline(\n  1029→  clipId: string,\n  1030→  trackIndex: number,\n  1031→  timeSeconds: number\n  1032→): string {\n  1033→  const seq = app.project.activeSequence;\n  1034→  const clip = findProjectItemById(clipId);\n  1035→\n  1036→  if (!seq || !clip) {\n  1037→    return JSON.stringify({ success: false, error: 'Sequence or clip not found' });\n  1038→  }\n  1039→\n  1040→  try {\n  1041→    seq.videoTracks[trackIndex].insertClip(clip, timeSeconds);\n  1042→    return JSON.stringify({ success: true });\n  1043→  } catch (e) {\n  1044→    return JSON.stringify({ success: false, error: e.toString() });\n  1045→  }\n  1046→}\n  1047→\n  1048→// === MARKER OPERATIONS ===\n  1049→\n  1050→export function createMarker(\n  1051→  timeSeconds: number,\n  1052→  name: string,\n  1053→  colorIndex: number\n  1054→): string {\n  1055→  const seq = app.project.activeSequence;\n  1056→  if (!seq) {\n  1057→    return JSON.stringify({ success: false, error: 'No active sequence' });\n  1058→  }\n  1059→\n  1060→  const marker = seq.markers.createMarker(timeSeconds);\n  1061→  marker.name = name;\n  1062→  marker.setTypeAsComment();\n  1063→  marker.setColorByIndex(colorIndex, 0);\n  1064→\n  1065→  return JSON.stringify({ success: true, markerId: marker.guid });\n  1066→}\n  1067→\n  1068→// === HELPER FUNCTIONS ===\n  1069→\n  1070→function findProjectItemById(nodeId: string): ProjectItem | null {\n  1071→  return searchInBin(app.project.rootItem, nodeId);\n  1072→}\n  1073→\n  1074→function searchInBin(bin: ProjectItem, nodeId: string): ProjectItem | null {\n  1075→  for (let i = 0; i < bin.children.numItems; i++) {\n  1076→    const item = bin.children[i];\n  1077→    if (item.nodeId === nodeId) {\n  1078→      return item;\n  1079→    }\n  1080→    if (item.type === ProjectItemType.BIN) {\n  1081→      const found = searchInBin(item, nodeId);\n  1082→      if (found) return found;\n  1083→    }\n  1084→  }\n  1085→  return null;\n  1086→}\n  1087→\n  1088→function findBinByPath(path: string): ProjectItem | null {\n  1089→  const parts = path.split('/').filter(p => p);\n  1090→  let current = app.project.rootItem;\n  1091→\n  1092→  for (const part of parts) {\n  1093→    let found = false;\n  1094→    for (let i = 0; i < current.children.numItems; i++) {\n  1095→      const child = current.children[i];\n  1096→      if (child.name === part && child.type === ProjectItemType.BIN) {\n  1097→        current = child;\n  1098→        found = true;\n  1099→        break;\n  1100→      }\n  1101→    }\n  1102→    if (!found) return null;\n  1103→  }\n  1104→\n  1105→  return current;\n  1106→}\n  1107→```\n  1108→\n  1109→### Color Coding System\n  1110→\n  1111→```typescript\n  1112→// src/js/main/constants/colors.ts\n  1113→\n  1114→export const TAKE_COLOR_MAP = {\n  1115→  1: { index: 5,  name: 'Rose',     hex: '#F77FBE' },\n  1116→  2: { index: 3,  name: 'Cerulean', hex: '#00A0DC' },\n  1117→  3: { index: 6,  name: 'Mango',    hex: '#F5A623' },\n  1118→  4: { index: 7,  name: 'Purple',   hex: '#9B59B6' },\n  1119→  5: { index: 4,  name: 'Forest',   hex: '#27AE60' },\n  1120→  6: { index: 9,  name: 'Lavender', hex: '#B39DDB' },\n  1121→  7: { index: 12, name: 'Orange',   hex: '#FF5722' },\n  1122→  8: { index: 14, name: 'Blue',     hex: '#2196F3' },\n  1123→} as const;\n  1124→\n  1125→export const MARKER_COLORS = {\n  1126→  BEST_TAKE: 1,    // Red\n  1127→  REGULAR_TAKE: 4, // Green\n  1128→  SILENCE: 3,      // Yellow\n  1129→} as const;\n  1130→```\n  1131→\n  1132→### Bin Organization Structure\n  1133→\n  1134→```\n  1135→Project Panel\n  1136→└── SPLICE Takes\n  1137→    ├── Line 1 - \"Hello everyone...\"\n  1138→    │   ├── Take 1 [Rose]\n  1139→    │   ├── Take 2 [Cerulean] ★ Best\n  1140→    │   └── Take 3 [Mango]\n  1141→    ├── Line 2 - \"Today we're going to...\"\n  1142→    │   ├── Take 1 [Rose] ★ Best\n  1143→    │   └── Take 2 [Cerulean]\n  1144→    └── Silence Removed\n  1145→        ├── Gap 1 (1.5s)\n  1146→        └── Gap 2 (2.3s)\n  1147→```\n  1148→\n  1149→---\n  1150→\n  1151→## 8. UI/UX Design\n  1152→\n  1153→### Panel Layout\n  1154→\n  1155→```tsx\n  1156→// src/js/main/components/Panel.tsx\n  1157→\n  1158→import React from 'react';\n  1159→import { DropZone } from './DropZone';\n  1160→import { ClipList } from './ClipList';\n  1161→import { TakeGroups } from './TakeGroups';\n  1162→import { ProgressBar } from './ProgressBar';\n  1163→import { SettingsPanel } from './SettingsPanel';\n  1164→\n  1165→export function SplicePanel() {\n  1166→  return (\n  1167→    <sp-theme color=\"dark\" scale=\"medium\">\n  1168→      <div className=\"splice-panel\">\n  1169→        {/* Header */}\n  1170→        <header className=\"panel-header\">\n  1171→          <sp-heading size=\"M\">SPLICE Auto-Cut</sp-heading>\n  1172→          <sp-action-button quiet onClick={openSettings}>\n  1173→            <sp-icon name=\"Settings\" />\n  1174→          </sp-action-button>\n  1175→        </header>\n  1176→\n  1177→        {/* Drop Zone / Clip Selection */}\n  1178→        <section className=\"import-section\">\n  1179→          <DropZone onFilesDropped={handleFiles}>\n  1180→            <sp-illustrated-message>\n  1181→              <sp-icon name=\"Upload\" />\n  1182→              <sp-heading>Drop clips here</sp-heading>\n  1183→              <sp-body>or click to select from project</sp-body>\n  1184→            </sp-illustrated-message>\n  1185→          </DropZone>\n  1186→        </section>\n  1187→\n  1188→        {/* Selected Clips */}\n  1189→        <section className=\"clips-section\">\n  1190→          <ClipList\n  1191→            clips={selectedClips}\n  1192→            onRemove={removeClip}\n  1193→          />\n  1194→        </section>\n  1195→\n  1196→        {/* Processing Options */}\n  1197→        <section className=\"options-section\">\n  1198→          <sp-field-group>\n  1199→            <sp-checkbox checked={useVoiceIsolation}>\n  1200→              Use Voice Isolation\n  1201→            </sp-checkbox>\n  1202→            <sp-checkbox checked={detectTakes}>\n  1203→              Detect Multiple Takes\n  1204→            </sp-checkbox>\n  1205→            <sp-checkbox checked={removeSilence}>\n  1206→              Remove Dead Space\n  1207→            </sp-checkbox>\n  1208→          </sp-field-group>\n  1209→\n  1210→          <sp-slider\n  1211→            label=\"Silence Threshold\"\n  1212→            min={0.5}\n  1213→            max={3.0}\n  1214→            value={silenceThreshold}\n  1215→            step={0.1}\n  1216→          />\n  1217→        </section>\n  1218→\n  1219→        {/* Process Button */}\n  1220→        <section className=\"action-section\">\n  1221→          <sp-button\n  1222→            variant=\"cta\"\n  1223→            onClick={processClips}\n  1224→            disabled={isProcessing || selectedClips.length === 0}\n  1225→          >\n  1226→            {isProcessing ? 'Processing...' : 'Process Clips'}\n  1227→          </sp-button>\n  1228→        </section>\n  1229→\n  1230→        {/* Progress */}\n  1231→        {isProcessing && (\n  1232→          <section className=\"progress-section\">\n  1233→            <ProgressBar\n  1234→              value={progress}\n  1235→              label={currentStep}\n  1236→            />\n  1237→          </section>\n  1238→        )}\n  1239→\n  1240→        {/* Results */}\n  1241→        {results && (\n  1242→          <section className=\"results-section\">\n  1243→            <sp-tabs>\n  1244→              <sp-tab label=\"Takes\" panel=\"takes-panel\" />\n  1245→              <sp-tab label=\"Silence\" panel=\"silence-panel\" />\n  1246→            </sp-tabs>\n  1247→\n  1248→            <sp-tab-panel id=\"takes-panel\">\n  1249→              <TakeGroups groups={results.takeGroups} />\n  1250→            </sp-tab-panel>\n  1251→\n  1252→            <sp-tab-panel id=\"silence-panel\">\n  1253→              <SilenceList sections={results.silentSections} />\n  1254→            </sp-tab-panel>\n  1255→          </section>\n  1256→        )}\n  1257→\n  1258→        {/* Apply Actions */}\n  1259→        {results && (\n  1260→          <section className=\"apply-section\">\n  1261→            <sp-button-group>\n  1262→              <sp-button onClick={applyColorCoding}>\n  1263→                Apply Colors\n  1264→              </sp-button>\n  1265→              <sp-button onClick={createBins}>\n  1266→                Create Bins\n  1267→              </sp-button>\n  1268→              <sp-button variant=\"cta\" onClick={applyAll}>\n  1269→                Apply All\n  1270→              </sp-button>\n  1271→            </sp-button-group>\n  1272→          </section>\n  1273→        )}\n  1274→\n  1275→        {/* Footer */}\n  1276→        <footer className=\"panel-footer\">\n  1277→          <sp-body size=\"XS\">\n  1278→            Est. cost: ${estimatedCost.toFixed(2)}\n  1279→          </sp-body>\n  1280→        </footer>\n  1281→      </div>\n  1282→    </sp-theme>\n  1283→  );\n  1284→}\n  1285→```\n  1286→\n  1287→### Settings Panel\n  1288→\n  1289→```tsx\n  1290→// src/js/main/components/SettingsPanel.tsx\n  1291→\n  1292→export function SettingsPanel() {\n  1293→  return (\n  1294→    <sp-dialog>\n  1295→      <sp-heading slot=\"heading\">Settings</sp-heading>\n  1296→\n  1297→      <div className=\"settings-content\">\n  1298→        {/* API Keys */}\n  1299→        <sp-field-group>\n  1300→          <sp-field-label>API Keys</sp-field-label>\n  1301→\n  1302→          <sp-textfield\n  1303→            label=\"ElevenLabs API Key\"\n  1304→            type=\"password\"\n  1305→            value={elevenLabsKey}\n  1306→            onChange={setElevenLabsKey}\n  1307→          />\n  1308→\n  1309→          <sp-textfield\n  1310→            label=\"Deepgram API Key\"\n  1311→            type=\"password\"\n  1312→            value={deepgramKey}\n  1313→            onChange={setDeepgramKey}\n  1314→          />\n  1315→\n  1316→          <sp-textfield\n  1317→            label=\"OpenAI API Key\"\n  1318→            type=\"password\"\n  1319→            value={openaiKey}\n  1320→            onChange={setOpenaiKey}\n  1321→          />\n  1322→        </sp-field-group>\n  1323→\n  1324→        {/* Voice Isolation Settings */}\n  1325→        <sp-field-group>\n  1326→          <sp-field-label>Voice Isolation</sp-field-label>\n  1327→\n  1328→          <sp-dropdown label=\"Audio Output Mode\">\n  1329→            <sp-menu-item value=\"original\" selected>\n  1330→              Keep Original Audio\n  1331→            </sp-menu-item>\n  1332→            <sp-menu-item value=\"isolated\">\n  1333→              Use Isolated Voice\n  1334→            </sp-menu-item>\n  1335→            <sp-menu-item value=\"none\">\n  1336→              Skip Isolation (faster)\n  1337→            </sp-menu-item>\n  1338→          </sp-dropdown>\n  1339→        </sp-field-group>\n  1340→\n  1341→        {/* Take Detection Settings */}\n  1342→        <sp-field-group>\n  1343→          <sp-field-label>Take Detection</sp-field-label>\n  1344→\n  1345→          <sp-slider\n  1346→            label=\"Similarity Threshold\"\n  1347→            min={0.70}\n  1348→            max={0.95}\n  1349→            value={similarityThreshold}\n  1350→            step={0.05}\n  1351→          />\n  1352→\n  1353→          <sp-checkbox checked={useLLMAnalysis}>\n  1354→            Use LLM for Better Accuracy\n  1355→          </sp-checkbox>\n  1356→        </sp-field-group>\n  1357→\n  1358→        {/* Silence Detection Settings */}\n  1359→        <sp-field-group>\n  1360→          <sp-field-label>Silence Detection</sp-field-label>\n  1361→\n  1362→          <sp-slider\n  1363→            label=\"Minimum Silence Duration (seconds)\"\n  1364→            min={0.5}\n  1365→            max={3.0}\n  1366→            value={minSilenceDuration}\n  1367→            step={0.1}\n  1368→          />\n  1369→\n  1370→          <sp-checkbox checked={preserveNaturalPauses}>\n  1371→            Preserve Natural Pauses\n  1372→          </sp-checkbox>\n  1373→        </sp-field-group>\n  1374→\n  1375→        {/* Timeline Settings */}\n  1376→        <sp-field-group>\n  1377→          <sp-field-label>Timeline Integration</sp-field-label>\n  1378→\n  1379→          <sp-checkbox checked={autoColorCode}>\n  1380→            Auto Color Code Takes\n  1381→          </sp-checkbox>\n  1382→\n  1383→          <sp-checkbox checked={createBins}>\n  1384→            Organize in Bins\n  1385→          </sp-checkbox>\n  1386→\n  1387→          <sp-checkbox checked={addMarkers}>\n  1388→            Add Timeline Markers\n  1389→          </sp-checkbox>\n  1390→        </sp-field-group>\n  1391→      </div>\n  1392→\n  1393→      <sp-button-group slot=\"button\">\n  1394→        <sp-button variant=\"secondary\" onClick={resetDefaults}>\n  1395→          Reset Defaults\n  1396→        </sp-button>\n  1397→        <sp-button variant=\"cta\" onClick={saveSettings}>\n  1398→          Save\n  1399→        </sp-button>\n  1400→      </sp-button-group>\n  1401→    </sp-dialog>\n  1402→  );\n  1403→}\n  1404→```\n  1405→\n  1406→---\n  1407→\n  1408→## 9. Development Phases\n  1409→\n  1410→### Phase 1: MVP (Weeks 1-6)\n  1411→\n  1412→**Goals:**\n  1413→- Working plugin with core functionality\n  1414→- Basic take detection\n  1415→- Silence removal\n  1416→- Timeline integration\n  1417→\n  1418→**Features:**\n  1419→- [ ] CEP panel setup with Bolt CEP\n  1420→- [ ] Drag-and-drop clip import\n  1421→- [ ] Audio extraction (FFmpeg)\n  1422→- [ ] Deepgram transcription integration\n  1423→- [ ] Basic take detection (embeddings only)\n  1424→- [ ] Silence detection (threshold-based)\n  1425→- [ ] Color coding in Premiere\n  1426→- [ ] Bin organization\n  1427→- [ ] Basic settings panel\n  1428→\n  1429→**Tech Debt:**\n  1430→- Skip voice isolation\n  1431→- Skip LLM analysis\n  1432→- Minimal error handling\n  1433→- No progress streaming\n  1434→\n  1435→### Phase 2: Enhanced (Weeks 7-10)\n  1436→\n  1437→**Goals:**\n  1438→- Production-ready quality\n  1439→- Full feature set\n  1440→- Robust error handling\n  1441→\n  1442→**Features:**\n  1443→- [ ] ElevenLabs voice isolation\n  1444→- [ ] GPT-4o-mini take analysis\n  1445→- [ ] Advanced similarity clustering\n  1446→- [ ] \"Best take\" suggestions\n  1447→- [ ] Batch processing\n  1448→- [ ] Progress streaming\n  1449→- [ ] Cost estimation UI\n  1450→- [ ] Full settings panel\n  1451→- [ ] Error recovery\n  1452→\n  1453→### Phase 3: Polish (Weeks 11-14)\n  1454→\n  1455→**Goals:**\n  1456→- Professional quality\n  1457→- User delight\n  1458→- Market readiness\n  1459→\n  1460→**Features:**\n  1461→- [ ] Local LLM option (Ollama)\n  1462→- [ ] Custom color schemes\n  1463→- [ ] Export/import settings\n  1464→- [ ] Keyboard shortcuts\n  1465→- [ ] Tutorial/onboarding\n  1466→- [ ] Usage analytics\n  1467→- [ ] Undo/redo for applied changes\n  1468→- [ ] Performance optimization\n  1469→- [ ] Comprehensive testing\n  1470→\n  1471→---\n  1472→\n  1473→## 10. File Structure\n  1474→\n  1475→```\n  1476→splice-plugin/\n  1477→├── .vscode/\n  1478→│   └── launch.json              # Debug configurations\n  1479→├── .github/\n  1480→│   └── workflows/\n  1481→│       └── release.yml          # GitHub Actions release\n  1482→├── bin/\n  1483→│   ├── mac/\n  1484→│   │   └── ffmpeg               # macOS FFmpeg binary\n  1485→│   └── win/\n  1486→│       └── ffmpeg.exe           # Windows FFmpeg binary\n  1487→├── src/\n  1488→│   ├── js/\n  1489→│   │   └── main/\n  1490→│   │       ├── components/\n  1491→│   │       │   ├── DropZone.tsx\n  1492→│   │       │   ├── ClipList.tsx\n  1493→│   │       │   ├── TakeGroups.tsx\n  1494→│   │       │   ├── SilenceList.tsx\n  1495→│   │       │   ├── ProgressBar.tsx\n  1496→│   │       │   ├── SettingsPanel.tsx\n  1497→│   │       │   └── Panel.tsx\n  1498→│   │       ├── hooks/\n  1499→│   │       │   ├── useProcessing.ts\n  1500→│   │       │   ├── usePremiere.ts\n  1501→│   │       │   ├── useSettings.ts\n  1502→│   │       │   └── useApi.ts\n  1503→│   │       ├── services/\n  1504→│   │       │   ├── ProcessingPipeline.ts\n  1505→│   │       │   ├── AudioExtractor.ts\n  1506→│   │       │   ├── VoiceIsolator.ts\n  1507→│   │       │   ├── Transcriber.ts\n  1508→│   │       │   ├── TakeDetector.ts\n  1509→│   │       │   ├── SilenceDetector.ts\n  1510→│   │       │   ├── CredentialManager.ts\n  1511→│   │       │   └── CostTracker.ts\n  1512→│   │       ├── modules/\n  1513→│   │       │   ├── audio/\n  1514→│   │       │   ├── takes/\n  1515→│   │       │   ├── silence/\n  1516→│   │       │   └── premiere/\n  1517→│   │       ├── utils/\n  1518→│   │       │   ├── apiClient.ts\n  1519→│   │       │   ├── embeddings.ts\n  1520→│   │       │   ├── clustering.ts\n  1521→│   │       │   └── helpers.ts\n  1522→│   │       ├── constants/\n  1523→│   │       │   ├── colors.ts\n  1524→│   │       │   └── config.ts\n  1525→│   │       ├── types/\n  1526→│   │       │   ├── models.ts\n  1527→│   │       │   ├── api.ts\n  1528→│   │       │   └── premiere.ts\n  1529→│   │       ├── store/\n  1530→│   │       │   └── index.ts     # Zustand store\n  1531→│   │       ├── main.tsx         # Entry point\n  1532→│   │       └── index.html\n  1533→│   └── jsx/\n  1534→│       ├── index.ts             # ExtendScript entry\n  1535→│       ├── timeline.ts          # Timeline operations\n  1536→│       ├── clips.ts             # Clip operations\n  1537→│       ├── bins.ts              # Bin operations\n  1538→│       ├── markers.ts           # Marker operations\n  1539→│       └── helpers.ts           # Utility functions\n  1540→├── public/\n  1541→│   └── icons/\n  1542→│       ├── icon-light.png\n  1543→│       └── icon-dark.png\n  1544→├── tests/\n  1545→│   ├── unit/\n  1546→│   │   ├── TakeDetector.test.ts\n  1547→│   │   └── SilenceDetector.test.ts\n  1548→│   └── integration/\n  1549→│       └── Pipeline.test.ts\n  1550→├── cep.config.ts                # Bolt CEP configuration\n  1551→├── vite.config.ts               # Vite configuration\n  1552→├── tsconfig.json                # TypeScript config\n  1553→├── tsconfig.jsx.json            # ExtendScript TypeScript config\n  1554→├── tailwind.config.js           # Tailwind CSS config\n  1555→├── package.json\n  1556→├── pnpm-lock.yaml\n  1557→├── .gitignore\n  1558→├── .env.example                 # Environment variables template\n  1559→├── README.md\n  1560→├── LICENSE\n  1561→└── CHANGELOG.md\n  1562→```\n  1563→\n  1564→---\n  1565→\n  1566→## 11. Build and Distribution\n  1567→\n  1568→### Development Workflow\n  1569→\n  1570→```bash\n  1571→# Install dependencies\n  1572→pnpm install\n  1573→\n  1574→# Start development server with HMR\n  1575→pnpm dev\n  1576→\n  1577→# Build for production\n  1578→pnpm build\n  1579→\n  1580→# Create signed ZXP package\n  1581→pnpm zxp\n  1582→\n  1583→# Run tests\n  1584→pnpm test\n  1585→\n  1586→# Lint and type check\n  1587→pnpm lint\n  1588→pnpm typecheck\n  1589→```\n  1590→\n  1591→### CEP Configuration\n  1592→\n  1593→```typescript\n  1594→// cep.config.ts\n  1595→\n  1596→import { CEPConfig } from 'bolt-cep';\n  1597→\n  1598→const config: CEPConfig = {\n  1599→  id: 'com.splice.autocut',\n  1600→  displayName: 'SPLICE Auto-Cut',\n  1601→  version: '1.0.0',\n  1602→\n  1603→  hosts: [\n  1604→    {\n  1605→      name: 'PPRO',\n  1606→      version: '[22.0,99.9]'\n  1607→    }\n  1608→  ],\n  1609→\n  1610→  panels: {\n  1611→    main: {\n  1612→      type: 'Panel',\n  1613→      name: 'SPLICE Auto-Cut',\n  1614→      root: './src/js/main/index.html',\n  1615→      script: './src/jsx/index.ts',\n  1616→      size: {\n  1617→        width: 350,\n  1618→        height: 600\n  1619→      },\n  1620→      minSize: {\n  1621→        width: 300,\n  1622→        height: 400\n  1623→      }\n  1624→    }\n  1625→  },\n  1626→\n  1627→  lifecycle: {\n  1628→    autoVisible: true,\n  1629→    startOnEvents: ['applicationActivate']\n  1630→  },\n  1631→\n  1632→  cepVersion: '11.0',\n  1633→  runtimeVersion: '9.0'\n  1634→};\n  1635→\n  1636→export default config;\n  1637→```\n  1638→\n  1639→### ZXP Signing\n  1640→\n  1641→```bash\n  1642→# Generate self-signed certificate (development)\n  1643→ZXPSignCmd -selfSignedCert US \"SPLICE\" \"SPLICE Auto-Cut\" password ./cert.p12\n  1644→\n  1645→# Sign the package\n  1646→pnpm zxp  # Uses Bolt CEP's built-in signing\n  1647→```\n  1648→\n  1649→### Distribution Strategy\n  1650→\n  1651→**Phase 1: Direct Distribution**\n  1652→- Host ZXP on website\n  1653→- Install via aescripts ZXP Installer or Anastasiy's Extension Manager\n  1654→- Free trial + subscription model\n  1655→\n  1656→**Phase 2: Adobe Exchange**\n  1657→- Submit after v1.0 stability\n  1658→- Requires Adobe review\n  1659→- Broader discovery\n  1660→\n  1661→**Phase 3: Enterprise**\n  1662→- Custom licensing\n  1663→- Volume discounts\n  1664→- Priority support\n  1665→\n  1666→### Environment Variables\n  1667→\n  1668→```bash\n  1669→# .env.example\n  1670→\n  1671→# API Keys (stored securely, not in env)\n  1672→# ELEVENLABS_API_KEY=\n  1673→# DEEPGRAM_API_KEY=\n  1674→# OPENAI_API_KEY=\n  1675→\n  1676→# Development\n  1677→DEBUG=true\n  1678→LOG_LEVEL=info\n  1679→\n  1680→# Feature Flags\n  1681→ENABLE_VOICE_ISOLATION=true\n  1682→ENABLE_LLM_ANALYSIS=true\n  1683→ENABLE_LOCAL_LLM=false\n  1684→\n  1685→# Pricing (for cost estimation)\n  1686→ELEVENLABS_RATE_PER_MIN=0.30\n  1687→DEEPGRAM_RATE_PER_MIN=0.0043\n  1688→OPENAI_INPUT_RATE_PER_1M=0.15\n  1689→OPENAI_OUTPUT_RATE_PER_1M=0.60\n  1690→```\n  1691→\n  1692→---\n  1693→\n  1694→## Quick Start for Developers\n  1695→\n  1696→```bash\n  1697→# 1. Clone repository\n  1698→git clone https://github.com/your-org/splice-plugin.git\n  1699→cd splice-plugin\n  1700→\n  1701→# 2. Install dependencies\n  1702→pnpm install\n  1703→\n  1704→# 3. Enable CEP debug mode\n  1705→# macOS:\n  1706→defaults write com.adobe.CSXS.12 PlayerDebugMode 1\n  1707→# Windows (PowerShell as Admin):\n  1708→# reg add \"HKCU\\Software\\Adobe\\CSXS.12\" /v PlayerDebugMode /t REG_SZ /d 1\n  1709→\n  1710→# 4. Start development\n  1711→pnpm dev\n  1712→\n  1713→# 5. Open Premiere Pro\n  1714→# Window > Extensions > SPLICE Auto-Cut\n  1715→\n  1716→# 6. Debug in Chrome\n  1717→# Navigate to http://localhost:8088\n  1718→```\n  1719→\n  1720→---\n  1721→\n  1722→**Architecture by:** Claude (Anthropic)\n  1723→**Version:** 1.0.0\n  1724→**Last Updated:** December 16, 2025\n  1725→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"a969739e-4203-4d6b-8dfc-88ab07872009","timestamp":"2025-12-16T17:08:56.487Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/Desktop/SPLICE/BUILD.md","content":"# SPLICE - Architecture Documentation\n\n**Version:** 1.0.0\n**Date:** December 16, 2025\n**Status:** Production-Ready Architecture\n\n---\n\n## Table of Contents\n\n1. [System Architecture Overview](#1-system-architecture-overview)\n2. [Technical Stack](#2-technical-stack)\n3. [Core Modules](#3-core-modules)\n4. [Data Models](#4-data-models)\n5. [Processing Pipeline](#5-processing-pipeline)\n6. [API Integration Patterns](#6-api-integration-patterns)\n7. [Premiere Pro Integration](#7-premiere-pro-integration)\n8. [UI/UX Design](#8-uiux-design)\n9. [Development Phases](#9-development-phases)\n10. [File Structure](#10-file-structure)\n11. [Build and Distribution](#11-build-and-distribution)\n\n---\n\n## 1. System Architecture Overview\n\n### High-Level Component Diagram\n\n```\n┌─────────────────────────────────────────────────────────────────────────────┐\n│                         SPLICE PLUGIN ARCHITECTURE                           │\n├─────────────────────────────────────────────────────────────────────────────┤\n│                                                                              │\n│  ┌─────────────────────────────────────────────────────────────────────┐    │\n│  │                      PREMIERE PRO HOST                               │    │\n│  │  ┌─────────────────┐    ┌──────────────────────────────────────┐   │    │\n│  │  │   Project       │    │          SPLICE CEP PANEL            │   │    │\n│  │  │   - Bins        │◄──►│  ┌────────────────────────────────┐  │   │    │\n│  │  │   - Clips       │    │  │       React + TypeScript       │  │   │    │\n│  │  │   - Sequences   │    │  │  ┌──────────┐ ┌─────────────┐  │  │   │    │\n│  │  └─────────────────┘    │  │  │ DropZone │ │ TakeGroups  │  │  │   │    │\n│  │                         │  │  └──────────┘ └─────────────┘  │  │   │    │\n│  │  ┌─────────────────┐    │  │  ┌──────────┐ ┌─────────────┐  │  │   │    │\n│  │  │   Timeline      │    │  │  │ Progress │ │  Settings   │  │  │   │    │\n│  │  │   - Tracks      │◄──►│  │  └──────────┘ └─────────────┘  │  │   │    │\n│  │  │   - Markers     │    │  └────────────────────────────────┘  │   │    │\n│  │  └─────────────────┘    │               │                       │   │    │\n│  │                         │               │ evalTS()               │   │    │\n│  │  ┌─────────────────┐    │               ▼                       │   │    │\n│  │  │  ExtendScript   │◄───│  ┌────────────────────────────────┐  │   │    │\n│  │  │  - timeline.ts  │    │  │      ExtendScript Bridge       │  │   │    │\n│  │  │  - clips.ts     │    │  │  - Timeline manipulation       │  │   │    │\n│  │  │  - markers.ts   │    │  │  - Color coding                │  │   │    │\n│  │  └─────────────────┘    │  │  - Bin organization            │  │   │    │\n│  │                         │  └────────────────────────────────┘  │   │    │\n│  │                         └──────────────────────────────────────┘   │    │\n│  └─────────────────────────────────────────────────────────────────────┘    │\n│                                        │                                     │\n│                                        │ HTTP/HTTPS                          │\n│                                        ▼                                     │\n│  ┌─────────────────────────────────────────────────────────────────────┐    │\n│  │                        PROCESSING LAYER                              │    │\n│  │                                                                      │    │\n│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────┐  ┌────────────┐ │    │\n│  │  │   FFmpeg    │  │ ElevenLabs  │  │  Deepgram   │  │ GPT-4o-mini│ │    │\n│  │  │   (Local)   │  │    API      │  │    API      │  │    API     │ │    │\n│  │  │             │  │             │  │             │  │            │ │    │\n│  │  │ - Extract   │  │ - Voice     │  │ - Transcribe│  │ - Analyze  │ │    │\n│  │  │   Audio     │  │   Isolation │  │ - Timestamps│  │   Takes    │ │    │\n│  │  └─────────────┘  └─────────────┘  └─────────────┘  └────────────┘ │    │\n│  │                                                                      │    │\n│  │  ┌─────────────┐  ┌─────────────────────────────────────────────┐  │    │\n│  │  │ Silero VAD  │  │           sentence-transformers              │  │    │\n│  │  │   (Local)   │  │                 (Local)                      │  │    │\n│  │  │             │  │                                              │  │    │\n│  │  │ - Silence   │  │ - Generate embeddings (all-MiniLM-L6-v2)    │  │    │\n│  │  │   Detection │  │ - Cosine similarity clustering              │  │    │\n│  │  └─────────────┘  └─────────────────────────────────────────────┘  │    │\n│  └─────────────────────────────────────────────────────────────────────┘    │\n│                                                                              │\n└─────────────────────────────────────────────────────────────────────────────┘\n```\n\n### Data Flow\n\n```\n┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐\n│  User    │     │   Extract    │     │    Voice      │     │  Transcribe  │\n│  Drops   │────►│   Audio      │────►│   Isolation   │────►│  + Timestamp │\n│  Clips   │     │   (FFmpeg)   │     │  (ElevenLabs) │     │  (Deepgram)  │\n└──────────┘     └──────────────┘     └───────────────┘     └──────────────┘\n                                                                    │\n                                                                    ▼\n┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐\n│ Timeline │     │   Organize   │     │    Detect     │     │   Detect     │\n│  Import  │◄────│   & Color    │◄────│    Silence    │◄────│   Takes      │\n│          │     │   Code       │     │  (Silero VAD) │     │  (Embed+LLM) │\n└──────────┘     └──────────────┘     └───────────────┘     └──────────────┘\n```\n\n### Technology Decisions\n\n| Component | Technology | Rationale |\n|-----------|------------|-----------|\n| Plugin Platform | CEP | Production-ready, full API access, UXP still in beta |\n| UI Framework | React + TypeScript | Best Bolt CEP support, type safety |\n| Build Tool | Vite | 10-100x faster than webpack |\n| Voice Isolation | ElevenLabs | Best quality/price, 500MB limit |\n| Transcription | Deepgram Nova-3 | Native word timestamps, diarization included |\n| Take Detection | Embeddings + GPT-4o-mini | Hybrid approach: fast filtering + LLM analysis |\n| Silence Detection | Silero VAD | ML-based, 95%+ accuracy, local processing |\n| Premiere API | ExtendScript | Only option for timeline manipulation |\n\n---\n\n## 2. Technical Stack\n\n### Frontend (CEP Panel)\n\n```yaml\nFramework: React 18.2+\nLanguage: TypeScript 5.0+\nBuild Tool: Vite 5.0+ (via Bolt CEP)\nUI Components: Adobe Spectrum Web Components\nState Management: Zustand (or React Context)\nStyling: Tailwind CSS + CSS Modules\n```\n\n### Backend (Processing Services)\n\n```yaml\nAudio Extraction: FFmpeg 6.0+ (bundled binary)\nVoice Isolation: ElevenLabs API v1\nTranscription: Deepgram Nova-3 API\nEmbeddings: sentence-transformers (all-MiniLM-L6-v2)\nClustering: scikit-learn AgglomerativeClustering\nLLM Analysis: OpenAI GPT-4o-mini API\nSilence Detection: Silero VAD 4.0+\n```\n\n### Premiere Integration\n\n```yaml\nPanel Platform: CEP 12\nScripting: ExtendScript (ECMAScript 3)\nCommunication: evalTS() via Bolt CEP\nSupported Premiere: 22.0+ (2022 and later)\n```\n\n### Development Tools\n\n```yaml\nPackage Manager: pnpm or yarn\nNode.js: 18.0+ (LTS)\nIDE: VS Code\nDebugging: Chrome DevTools (localhost:8088)\nExtendScript Debug: Adobe ExtendScript Debugger extension\n```\n\n---\n\n## 3. Core Modules\n\n### 3.1 DropZone/Import Module\n\n```typescript\n// src/js/main/modules/import/DropZone.ts\n\ninterface ImportModule {\n  // Events\n  onFilesDropped: (files: File[]) => void;\n  onClipsSelected: (clips: ClipInfo[]) => void;\n\n  // Methods\n  validateFiles(files: File[]): ValidationResult;\n  extractClipInfo(files: File[]): Promise<ClipInfo[]>;\n  getSelectedClips(): Promise<ClipInfo[]>;\n}\n\ninterface ValidationResult {\n  valid: boolean;\n  errors: string[];\n  warnings: string[];\n  supportedFormats: string[];\n}\n```\n\n### 3.2 Audio Extraction Module\n\n```typescript\n// src/js/main/modules/audio/AudioExtractor.ts\n\ninterface AudioExtractorModule {\n  extractAudio(clipPath: string, options: ExtractionOptions): Promise<AudioBuffer>;\n  getAudioInfo(clipPath: string): Promise<AudioInfo>;\n  splitAudio(buffer: AudioBuffer, chunks: number): AudioBuffer[];\n}\n\ninterface ExtractionOptions {\n  format: 'wav' | 'mp3' | 'pcm';\n  sampleRate: 16000 | 44100 | 48000;\n  channels: 1 | 2;\n  bitDepth?: 16 | 24 | 32;\n}\n\ninterface AudioInfo {\n  duration: number;\n  sampleRate: number;\n  channels: number;\n  format: string;\n  bitRate: number;\n}\n```\n\n### 3.3 Voice Isolation Module\n\n```typescript\n// src/js/main/modules/isolation/VoiceIsolator.ts\n\ninterface VoiceIsolationModule {\n  isolate(audioBuffer: ArrayBuffer): Promise<ArrayBuffer>;\n  isAvailable(): boolean;\n  estimateCost(durationMinutes: number): number;\n}\n\ninterface IsolationConfig {\n  apiKey: string;\n  outputFormat: 'pcm_s16le_16' | 'mp3' | 'wav';\n  timeout: number;\n}\n```\n\n### 3.4 Transcription Module\n\n```typescript\n// src/js/main/modules/transcription/Transcriber.ts\n\ninterface TranscriptionModule {\n  transcribe(audioBuffer: ArrayBuffer): Promise<Transcript>;\n  transcribeWithDiarization(audioBuffer: ArrayBuffer): Promise<DiarizedTranscript>;\n  estimateCost(durationMinutes: number): number;\n}\n\ninterface TranscriptWord {\n  text: string;\n  start: number;      // seconds\n  end: number;        // seconds\n  confidence: number; // 0-1\n  speaker?: string;   // \"Speaker 1\", \"Speaker 2\"\n}\n\ninterface Transcript {\n  text: string;\n  words: TranscriptWord[];\n  duration: number;\n  language: string;\n}\n```\n\n### 3.5 Take Detection Module\n\n```typescript\n// src/js/main/modules/takes/TakeDetector.ts\n\ninterface TakeDetectionModule {\n  detectTakes(transcript: Transcript): Promise<TakeGroup[]>;\n  generateEmbeddings(sentences: string[]): Promise<number[][]>;\n  clusterBySimilarity(embeddings: number[][], threshold: number): number[];\n  analyzeTakesWithLLM(candidates: TakeCandidate[]): Promise<TakeAnalysis>;\n}\n\ninterface TakeCandidate {\n  sentences: string[];\n  timestamps: TimeRange[];\n  similarityScore: number;\n}\n\ninterface TakeAnalysis {\n  groups: TakeGroup[];\n  bestTakes: Record<string, number>; // groupId -> takeIndex\n  confidence: number;\n}\n```\n\n### 3.6 Silence Detection Module\n\n```typescript\n// src/js/main/modules/silence/SilenceDetector.ts\n\ninterface SilenceDetectionModule {\n  detectSilence(audioBuffer: ArrayBuffer): Promise<SilentSection[]>;\n  detectFromTranscript(words: TranscriptWord[]): SilentSection[];\n  filterNaturalPauses(sections: SilentSection[]): SilentSection[];\n}\n\ninterface SilenceConfig {\n  minDuration: number;      // seconds (default: 1.5)\n  threshold: number;        // dB (default: -40)\n  preservePauseDuration: number; // seconds (default: 0.8)\n}\n```\n\n### 3.7 Timeline Integration Module\n\n```typescript\n// src/js/main/modules/premiere/TimelineIntegrator.ts\n\ninterface TimelineModule {\n  // Bin operations\n  createBin(name: string, parentPath?: string): Promise<string>;\n  moveToBin(clipId: string, binPath: string): Promise<void>;\n\n  // Color coding\n  setClipColor(clipId: string, colorIndex: number): Promise<void>;\n\n  // Timeline operations\n  insertClip(clipId: string, trackIndex: number, time: number): Promise<void>;\n  setInOutPoints(clipId: string, inPoint: number, outPoint: number): Promise<void>;\n\n  // Markers\n  createMarker(time: number, name: string, color: number): Promise<void>;\n}\n\n// Color mapping for takes\nconst TAKE_COLORS = {\n  1: 5,   // Rose\n  2: 3,   // Cerulean\n  3: 6,   // Mango\n  4: 7,   // Purple\n  5: 4,   // Forest\n  6: 9,   // Lavender\n  7: 12,  // Orange\n  8: 14,  // Blue\n} as const;\n```\n\n### 3.8 Settings Module\n\n```typescript\n// src/js/main/modules/settings/SettingsManager.ts\n\ninterface SettingsModule {\n  get<T>(key: string): T | undefined;\n  set<T>(key: string, value: T): void;\n  getAll(): Settings;\n  reset(): void;\n  export(): string;\n  import(json: string): void;\n}\n\ninterface Settings {\n  // Voice Isolation\n  useVoiceIsolation: boolean;\n  audioOutputMode: 'original' | 'isolated' | 'none';\n\n  // Transcription\n  transcriptionProvider: 'deepgram' | 'assemblyai' | 'whisper';\n  language: string;\n\n  // Take Detection\n  similarityThreshold: number;  // 0.70-0.95\n  useLLMAnalysis: boolean;\n\n  // Silence Detection\n  minSilenceDuration: number;   // seconds\n  silenceThreshold: number;     // dB\n  preserveNaturalPauses: boolean;\n\n  // Timeline\n  autoColorCode: boolean;\n  createBins: boolean;\n  addMarkers: boolean;\n\n  // API Keys\n  elevenLabsApiKey: string;\n  deepgramApiKey: string;\n  openaiApiKey: string;\n}\n```\n\n---\n\n## 4. Data Models\n\n### Core Interfaces\n\n```typescript\n// src/js/main/types/models.ts\n\n// === CLIP MODELS ===\n\ninterface ClipInfo {\n  id: string;\n  name: string;\n  path: string;\n  duration: number;        // seconds\n  inPoint: number;         // source in point\n  outPoint: number;        // source out point\n  hasAudio: boolean;\n  hasVideo: boolean;\n  audioChannels: number;\n  frameRate: number;\n  resolution: { width: number; height: number };\n}\n\n// === TAKE MODELS ===\n\ninterface Take {\n  id: string;\n  clipId: string;\n  takeNumber: number;\n  text: string;\n  startTime: number;       // seconds in source\n  endTime: number;\n  duration: number;\n  confidence: number;      // 0-1\n  isPartial: boolean;      // incomplete take\n  isBest: boolean;         // suggested best take\n  audioClarity: number;    // 0-1 (if analyzed)\n}\n\ninterface TakeGroup {\n  id: string;\n  canonicalText: string;   // reference text for this line\n  takes: Take[];\n  similarityType: 'exact' | 'minor_variation' | 'paraphrase';\n  suggestedBestTakeId: string | null;\n}\n\n// === TRANSCRIPT MODELS ===\n\ninterface Word {\n  text: string;\n  start: number;           // seconds\n  end: number;             // seconds\n  confidence: number;\n  speaker?: string;\n  punctuatedText?: string;\n}\n\ninterface Sentence {\n  text: string;\n  words: Word[];\n  start: number;\n  end: number;\n  speaker?: string;\n}\n\ninterface Transcript {\n  id: string;\n  clipId: string;\n  fullText: string;\n  words: Word[];\n  sentences: Sentence[];\n  speakers: string[];\n  language: string;\n  duration: number;\n}\n\n// === SILENCE MODELS ===\n\ninterface SilentSection {\n  id: string;\n  startTime: number;\n  endTime: number;\n  duration: number;\n  type: 'dead_space' | 'natural_pause' | 'take_boundary';\n  shouldRemove: boolean;\n  afterWord?: string;\n  beforeWord?: string;\n}\n\n// === PROCESSING MODELS ===\n\ninterface ProcessingJob {\n  id: string;\n  clipId: string;\n  status: JobStatus;\n  progress: number;        // 0-100\n  currentStep: ProcessingStep;\n  startedAt: Date;\n  completedAt?: Date;\n  error?: ProcessingError;\n  result?: ProcessingResult;\n}\n\ntype JobStatus = 'pending' | 'processing' | 'completed' | 'failed' | 'cancelled';\n\ntype ProcessingStep =\n  | 'extracting_audio'\n  | 'isolating_voice'\n  | 'transcribing'\n  | 'detecting_takes'\n  | 'detecting_silence'\n  | 'organizing'\n  | 'complete';\n\ninterface ProcessingError {\n  code: string;\n  message: string;\n  step: ProcessingStep;\n  retryable: boolean;\n}\n\ninterface ProcessingResult {\n  clipId: string;\n  transcript: Transcript;\n  takeGroups: TakeGroup[];\n  silentSections: SilentSection[];\n  processingTime: number;  // ms\n  apiCosts: {\n    voiceIsolation: number;\n    transcription: number;\n    llmAnalysis: number;\n    total: number;\n  };\n}\n\n// === PREMIERE MODELS ===\n\ninterface PremiereClip {\n  projectItemId: string;\n  name: string;\n  binPath: string;\n  colorLabel: number;\n  inPoint: number;\n  outPoint: number;\n}\n\ninterface PremiereTimeline {\n  sequenceId: string;\n  name: string;\n  duration: number;\n  videoTracks: number;\n  audioTracks: number;\n}\n```\n\n---\n\n## 5. Processing Pipeline\n\n### Main Processing Flow\n\n```typescript\n// src/js/main/services/ProcessingPipeline.ts\n\nclass ProcessingPipeline {\n  private audioExtractor: AudioExtractorModule;\n  private voiceIsolator: VoiceIsolationModule;\n  private transcriber: TranscriptionModule;\n  private takeDetector: TakeDetectionModule;\n  private silenceDetector: SilenceDetectionModule;\n  private timelineIntegrator: TimelineModule;\n\n  async processClip(clip: ClipInfo, options: ProcessingOptions): Promise<ProcessingResult> {\n    const job = this.createJob(clip);\n\n    try {\n      // Step 1: Extract audio from video\n      this.updateProgress(job, 'extracting_audio', 10);\n      const audioBuffer = await this.audioExtractor.extractAudio(clip.path, {\n        format: 'wav',\n        sampleRate: 16000,\n        channels: 1\n      });\n\n      // Step 2: Voice isolation (optional)\n      let analysisAudio = audioBuffer;\n      let outputAudio = audioBuffer;\n\n      if (options.useVoiceIsolation) {\n        this.updateProgress(job, 'isolating_voice', 25);\n        const isolatedAudio = await this.voiceIsolator.isolate(audioBuffer);\n        analysisAudio = isolatedAudio;\n\n        if (options.audioOutputMode === 'isolated') {\n          outputAudio = isolatedAudio;\n        }\n      }\n\n      // Step 3: Transcription with word-level timestamps\n      this.updateProgress(job, 'transcribing', 40);\n      const transcript = await this.transcriber.transcribe(analysisAudio);\n\n      // Step 4: Take detection\n      this.updateProgress(job, 'detecting_takes', 60);\n      const takeGroups = await this.takeDetector.detectTakes(transcript);\n\n      // Step 5: Silence detection\n      this.updateProgress(job, 'detecting_silence', 75);\n      const silentSections = await this.silenceDetector.detectFromTranscript(transcript.words);\n      const filteredSilence = this.silenceDetector.filterNaturalPauses(silentSections);\n\n      // Step 6: Organize in Premiere\n      this.updateProgress(job, 'organizing', 90);\n      await this.organizeInPremiere(clip, takeGroups, filteredSilence, options);\n\n      this.updateProgress(job, 'complete', 100);\n\n      return {\n        clipId: clip.id,\n        transcript,\n        takeGroups,\n        silentSections: filteredSilence,\n        processingTime: Date.now() - job.startedAt.getTime(),\n        apiCosts: this.calculateCosts(job)\n      };\n\n    } catch (error) {\n      job.status = 'failed';\n      job.error = this.createError(error);\n      throw error;\n    }\n  }\n\n  private async organizeInPremiere(\n    clip: ClipInfo,\n    takeGroups: TakeGroup[],\n    silentSections: SilentSection[],\n    options: ProcessingOptions\n  ): Promise<void> {\n\n    // Create bin structure\n    if (options.createBins) {\n      const mainBin = await this.timelineIntegrator.createBin('SPLICE Takes');\n\n      for (const group of takeGroups) {\n        const groupBin = await this.timelineIntegrator.createBin(\n          `Line ${group.id}`,\n          mainBin\n        );\n\n        for (const take of group.takes) {\n          // Create subclip with in/out points\n          // Move to appropriate bin\n          // Set color based on take number\n          await this.timelineIntegrator.setClipColor(\n            take.clipId,\n            TAKE_COLORS[take.takeNumber] || 0\n          );\n        }\n      }\n    }\n\n    // Add markers for take boundaries\n    if (options.addMarkers) {\n      for (const group of takeGroups) {\n        for (const take of group.takes) {\n          await this.timelineIntegrator.createMarker(\n            take.startTime,\n            `Take ${take.takeNumber}: ${group.canonicalText.substring(0, 30)}...`,\n            take.isBest ? 1 : 0  // Red for best, green for others\n          );\n        }\n      }\n    }\n  }\n}\n```\n\n### Take Detection Algorithm\n\n```typescript\n// src/js/main/services/TakeDetectionService.ts\n\nclass TakeDetectionService implements TakeDetectionModule {\n  private embeddingModel: SentenceTransformer;\n  private openaiClient: OpenAI;\n\n  async detectTakes(transcript: Transcript): Promise<TakeGroup[]> {\n    // 1. Segment transcript into sentences\n    const sentences = this.segmentIntoSentences(transcript);\n\n    // 2. Generate embeddings for all sentences\n    const embeddings = await this.generateEmbeddings(\n      sentences.map(s => s.text)\n    );\n\n    // 3. Compute similarity matrix\n    const similarityMatrix = this.computeCosineSimilarity(embeddings);\n\n    // 4. Find candidate pairs (similarity > threshold)\n    const candidates = this.findCandidatePairs(\n      sentences,\n      similarityMatrix,\n      0.80  // threshold\n    );\n\n    // 5. Cluster into groups\n    const clusters = this.clusterCandidates(candidates);\n\n    // 6. Analyze with LLM for confirmation and best-take selection\n    const analysis = await this.analyzeTakesWithLLM(clusters);\n\n    // 7. Build TakeGroup objects\n    return this.buildTakeGroups(sentences, clusters, analysis);\n  }\n\n  private computeCosineSimilarity(embeddings: number[][]): number[][] {\n    const n = embeddings.length;\n    const matrix: number[][] = Array(n).fill(null).map(() => Array(n).fill(0));\n\n    for (let i = 0; i < n; i++) {\n      for (let j = i; j < n; j++) {\n        const sim = this.cosineSim(embeddings[i], embeddings[j]);\n        matrix[i][j] = sim;\n        matrix[j][i] = sim;\n      }\n    }\n\n    return matrix;\n  }\n\n  private cosineSim(a: number[], b: number[]): number {\n    let dotProduct = 0;\n    let normA = 0;\n    let normB = 0;\n\n    for (let i = 0; i < a.length; i++) {\n      dotProduct += a[i] * b[i];\n      normA += a[i] * a[i];\n      normB += b[i] * b[i];\n    }\n\n    return dotProduct / (Math.sqrt(normA) * Math.sqrt(normB));\n  }\n\n  private async analyzeTakesWithLLM(clusters: TakeCluster[]): Promise<LLMAnalysis> {\n    const prompt = this.buildAnalysisPrompt(clusters);\n\n    const response = await this.openaiClient.chat.completions.create({\n      model: 'gpt-4o-mini',\n      messages: [\n        { role: 'system', content: TAKE_ANALYSIS_SYSTEM_PROMPT },\n        { role: 'user', content: prompt }\n      ],\n      response_format: { type: 'json_object' }\n    });\n\n    return JSON.parse(response.choices[0].message.content);\n  }\n}\n\nconst TAKE_ANALYSIS_SYSTEM_PROMPT = `\nYou are analyzing transcript segments to identify repeated takes.\n\nFor each group of similar sentences, determine:\n1. Whether they represent the same intended dialogue (different takes)\n2. The similarity type: exact, minor_variation, or paraphrase\n3. Which take appears to be the \"best\" (most complete, clearest)\n\nReturn JSON with this structure:\n{\n  \"groups\": [\n    {\n      \"id\": \"string\",\n      \"isTakeGroup\": boolean,\n      \"similarityType\": \"exact\" | \"minor_variation\" | \"paraphrase\",\n      \"bestTakeIndex\": number,\n      \"confidence\": number\n    }\n  ]\n}\n`;\n```\n\n---\n\n## 6. API Integration Patterns\n\n### Rate Limiting and Retry Logic\n\n```typescript\n// src/js/main/utils/apiClient.ts\n\ninterface RetryConfig {\n  maxRetries: number;\n  baseDelay: number;\n  maxDelay: number;\n  retryableStatuses: number[];\n}\n\nconst DEFAULT_RETRY_CONFIG: RetryConfig = {\n  maxRetries: 3,\n  baseDelay: 1000,\n  maxDelay: 30000,\n  retryableStatuses: [429, 500, 502, 503, 504]\n};\n\nasync function fetchWithRetry<T>(\n  url: string,\n  options: RequestInit,\n  config: RetryConfig = DEFAULT_RETRY_CONFIG\n): Promise<T> {\n  let lastError: Error;\n\n  for (let attempt = 0; attempt <= config.maxRetries; attempt++) {\n    try {\n      const response = await fetch(url, options);\n\n      if (!response.ok) {\n        if (config.retryableStatuses.includes(response.status)) {\n          throw new RetryableError(response.status, await response.text());\n        }\n        throw new ApiError(response.status, await response.text());\n      }\n\n      return await response.json();\n\n    } catch (error) {\n      lastError = error;\n\n      if (error instanceof RetryableError && attempt < config.maxRetries) {\n        const delay = Math.min(\n          config.baseDelay * Math.pow(2, attempt),\n          config.maxDelay\n        );\n        await sleep(delay);\n        continue;\n      }\n\n      throw error;\n    }\n  }\n\n  throw lastError;\n}\n```\n\n### API Key Management\n\n```typescript\n// src/js/main/services/CredentialManager.ts\n\nimport keytar from 'keytar';\n\nconst SERVICE_NAME = 'SPLICE_Plugin';\n\nclass CredentialManager {\n  private static instance: CredentialManager;\n  private cache: Map<string, string> = new Map();\n\n  static getInstance(): CredentialManager {\n    if (!this.instance) {\n      this.instance = new CredentialManager();\n    }\n    return this.instance;\n  }\n\n  async setApiKey(provider: ApiProvider, key: string): Promise<void> {\n    await keytar.setPassword(SERVICE_NAME, provider, key);\n    this.cache.set(provider, key);\n  }\n\n  async getApiKey(provider: ApiProvider): Promise<string | null> {\n    if (this.cache.has(provider)) {\n      return this.cache.get(provider)!;\n    }\n\n    const key = await keytar.getPassword(SERVICE_NAME, provider);\n    if (key) {\n      this.cache.set(provider, key);\n    }\n    return key;\n  }\n\n  async deleteApiKey(provider: ApiProvider): Promise<void> {\n    await keytar.deletePassword(SERVICE_NAME, provider);\n    this.cache.delete(provider);\n  }\n\n  async validateApiKey(provider: ApiProvider, key: string): Promise<boolean> {\n    switch (provider) {\n      case 'elevenlabs':\n        return this.validateElevenLabsKey(key);\n      case 'deepgram':\n        return this.validateDeepgramKey(key);\n      case 'openai':\n        return this.validateOpenAIKey(key);\n      default:\n        return false;\n    }\n  }\n}\n\ntype ApiProvider = 'elevenlabs' | 'deepgram' | 'openai';\n```\n\n### Cost Tracking\n\n```typescript\n// src/js/main/services/CostTracker.ts\n\ninterface CostRates {\n  elevenlabs: {\n    perMinute: number;  // $0.30 at Creator tier\n  };\n  deepgram: {\n    perMinute: number;  // $0.0043\n  };\n  openai: {\n    inputPer1M: number;   // $0.15\n    outputPer1M: number;  // $0.60\n  };\n}\n\nconst DEFAULT_RATES: CostRates = {\n  elevenlabs: { perMinute: 0.30 },\n  deepgram: { perMinute: 0.0043 },\n  openai: { inputPer1M: 0.15, outputPer1M: 0.60 }\n};\n\nclass CostTracker {\n  private rates: CostRates;\n  private usage: UsageRecord = {\n    elevenlabs: { minutes: 0, cost: 0 },\n    deepgram: { minutes: 0, cost: 0 },\n    openai: { inputTokens: 0, outputTokens: 0, cost: 0 }\n  };\n\n  estimateCost(durationMinutes: number, options: ProcessingOptions): CostEstimate {\n    let total = 0;\n\n    // Transcription (always needed)\n    const transcriptionCost = durationMinutes * this.rates.deepgram.perMinute;\n    total += transcriptionCost;\n\n    // Voice isolation (optional)\n    let isolationCost = 0;\n    if (options.useVoiceIsolation) {\n      isolationCost = durationMinutes * this.rates.elevenlabs.perMinute;\n      total += isolationCost;\n    }\n\n    // LLM analysis (estimated based on transcript length)\n    const estimatedTokens = durationMinutes * 500; // ~500 tokens per minute\n    const llmCost = (estimatedTokens / 1_000_000) * this.rates.openai.inputPer1M +\n                    (estimatedTokens / 4 / 1_000_000) * this.rates.openai.outputPer1M;\n    total += llmCost;\n\n    return {\n      transcription: transcriptionCost,\n      voiceIsolation: isolationCost,\n      llmAnalysis: llmCost,\n      total\n    };\n  }\n}\n```\n\n---\n\n## 7. Premiere Pro Integration\n\n### ExtendScript Functions\n\n```typescript\n// src/jsx/index.ts\n\n// === CLIP OPERATIONS ===\n\nexport function getSelectedClips(): string {\n  const selection = app.project.activeSequence?.getSelection() || [];\n  const clips = [];\n\n  for (let i = 0; i < selection.length; i++) {\n    clips.push({\n      id: selection[i].projectItem.nodeId,\n      name: selection[i].name,\n      duration: selection[i].duration.seconds\n    });\n  }\n\n  return JSON.stringify(clips);\n}\n\nexport function setClipColor(clipId: string, colorIndex: number): string {\n  const item = findProjectItemById(clipId);\n  if (item) {\n    item.setColorLabel(colorIndex);\n    return JSON.stringify({ success: true });\n  }\n  return JSON.stringify({ success: false, error: 'Clip not found' });\n}\n\nexport function setInOutPoints(clipId: string, inPoint: number, outPoint: number): string {\n  const item = findProjectItemById(clipId);\n  if (item) {\n    item.setInPoint(inPoint, 4);  // 4 = update UI\n    item.setOutPoint(outPoint, 4);\n    return JSON.stringify({ success: true });\n  }\n  return JSON.stringify({ success: false, error: 'Clip not found' });\n}\n\n// === BIN OPERATIONS ===\n\nexport function createBin(name: string, parentPath?: string): string {\n  const parent = parentPath\n    ? findBinByPath(parentPath)\n    : app.project.rootItem;\n\n  if (parent) {\n    const newBin = parent.createBin(name);\n    return JSON.stringify({\n      success: true,\n      binId: newBin.nodeId,\n      path: getBinPath(newBin)\n    });\n  }\n  return JSON.stringify({ success: false, error: 'Parent bin not found' });\n}\n\nexport function moveClipToBin(clipId: string, binPath: string): string {\n  const clip = findProjectItemById(clipId);\n  const bin = findBinByPath(binPath);\n\n  if (clip && bin) {\n    clip.moveBin(bin);\n    return JSON.stringify({ success: true });\n  }\n  return JSON.stringify({ success: false, error: 'Clip or bin not found' });\n}\n\n// === TIMELINE OPERATIONS ===\n\nexport function insertClipToTimeline(\n  clipId: string,\n  trackIndex: number,\n  timeSeconds: number\n): string {\n  const seq = app.project.activeSequence;\n  const clip = findProjectItemById(clipId);\n\n  if (!seq || !clip) {\n    return JSON.stringify({ success: false, error: 'Sequence or clip not found' });\n  }\n\n  try {\n    seq.videoTracks[trackIndex].insertClip(clip, timeSeconds);\n    return JSON.stringify({ success: true });\n  } catch (e) {\n    return JSON.stringify({ success: false, error: e.toString() });\n  }\n}\n\n// === MARKER OPERATIONS ===\n\nexport function createMarker(\n  timeSeconds: number,\n  name: string,\n  colorIndex: number\n): string {\n  const seq = app.project.activeSequence;\n  if (!seq) {\n    return JSON.stringify({ success: false, error: 'No active sequence' });\n  }\n\n  const marker = seq.markers.createMarker(timeSeconds);\n  marker.name = name;\n  marker.setTypeAsComment();\n  marker.setColorByIndex(colorIndex, 0);\n\n  return JSON.stringify({ success: true, markerId: marker.guid });\n}\n\n// === HELPER FUNCTIONS ===\n\nfunction findProjectItemById(nodeId: string): ProjectItem | null {\n  return searchInBin(app.project.rootItem, nodeId);\n}\n\nfunction searchInBin(bin: ProjectItem, nodeId: string): ProjectItem | null {\n  for (let i = 0; i < bin.children.numItems; i++) {\n    const item = bin.children[i];\n    if (item.nodeId === nodeId) {\n      return item;\n    }\n    if (item.type === ProjectItemType.BIN) {\n      const found = searchInBin(item, nodeId);\n      if (found) return found;\n    }\n  }\n  return null;\n}\n\nfunction findBinByPath(path: string): ProjectItem | null {\n  const parts = path.split('/').filter(p => p);\n  let current = app.project.rootItem;\n\n  for (const part of parts) {\n    let found = false;\n    for (let i = 0; i < current.children.numItems; i++) {\n      const child = current.children[i];\n      if (child.name === part && child.type === ProjectItemType.BIN) {\n        current = child;\n        found = true;\n        break;\n      }\n    }\n    if (!found) return null;\n  }\n\n  return current;\n}\n```\n\n### Color Coding System\n\n```typescript\n// src/js/main/constants/colors.ts\n\nexport const TAKE_COLOR_MAP = {\n  1: { index: 5,  name: 'Rose',     hex: '#F77FBE' },\n  2: { index: 3,  name: 'Cerulean', hex: '#00A0DC' },\n  3: { index: 6,  name: 'Mango',    hex: '#F5A623' },\n  4: { index: 7,  name: 'Purple',   hex: '#9B59B6' },\n  5: { index: 4,  name: 'Forest',   hex: '#27AE60' },\n  6: { index: 9,  name: 'Lavender', hex: '#B39DDB' },\n  7: { index: 12, name: 'Orange',   hex: '#FF5722' },\n  8: { index: 14, name: 'Blue',     hex: '#2196F3' },\n} as const;\n\nexport const MARKER_COLORS = {\n  BEST_TAKE: 1,    // Red\n  REGULAR_TAKE: 4, // Green\n  SILENCE: 3,      // Yellow\n} as const;\n```\n\n### Bin Organization Structure\n\n```\nProject Panel\n└── SPLICE Takes\n    ├── Line 1 - \"Hello everyone...\"\n    │   ├── Take 1 [Rose]\n    │   ├── Take 2 [Cerulean] ★ Best\n    │   └── Take 3 [Mango]\n    ├── Line 2 - \"Today we're going to...\"\n    │   ├── Take 1 [Rose] ★ Best\n    │   └── Take 2 [Cerulean]\n    └── Silence Removed\n        ├── Gap 1 (1.5s)\n        └── Gap 2 (2.3s)\n```\n\n---\n\n## 8. UI/UX Design\n\n### Panel Layout\n\n```tsx\n// src/js/main/components/Panel.tsx\n\nimport React from 'react';\nimport { DropZone } from './DropZone';\nimport { ClipList } from './ClipList';\nimport { TakeGroups } from './TakeGroups';\nimport { ProgressBar } from './ProgressBar';\nimport { SettingsPanel } from './SettingsPanel';\n\nexport function SplicePanel() {\n  return (\n    <sp-theme color=\"dark\" scale=\"medium\">\n      <div className=\"splice-panel\">\n        {/* Header */}\n        <header className=\"panel-header\">\n          <sp-heading size=\"M\">SPLICE Auto-Cut</sp-heading>\n          <sp-action-button quiet onClick={openSettings}>\n            <sp-icon name=\"Settings\" />\n          </sp-action-button>\n        </header>\n\n        {/* Drop Zone / Clip Selection */}\n        <section className=\"import-section\">\n          <DropZone onFilesDropped={handleFiles}>\n            <sp-illustrated-message>\n              <sp-icon name=\"Upload\" />\n              <sp-heading>Drop clips here</sp-heading>\n              <sp-body>or click to select from project</sp-body>\n            </sp-illustrated-message>\n          </DropZone>\n        </section>\n\n        {/* Selected Clips */}\n        <section className=\"clips-section\">\n          <ClipList\n            clips={selectedClips}\n            onRemove={removeClip}\n          />\n        </section>\n\n        {/* Processing Options */}\n        <section className=\"options-section\">\n          <sp-field-group>\n            <sp-checkbox checked={useVoiceIsolation}>\n              Use Voice Isolation\n            </sp-checkbox>\n            <sp-checkbox checked={detectTakes}>\n              Detect Multiple Takes\n            </sp-checkbox>\n            <sp-checkbox checked={removeSilence}>\n              Remove Dead Space\n            </sp-checkbox>\n          </sp-field-group>\n\n          <sp-slider\n            label=\"Silence Threshold\"\n            min={0.5}\n            max={3.0}\n            value={silenceThreshold}\n            step={0.1}\n          />\n        </section>\n\n        {/* Process Button */}\n        <section className=\"action-section\">\n          <sp-button\n            variant=\"cta\"\n            onClick={processClips}\n            disabled={isProcessing || selectedClips.length === 0}\n          >\n            {isProcessing ? 'Processing...' : 'Process Clips'}\n          </sp-button>\n        </section>\n\n        {/* Progress */}\n        {isProcessing && (\n          <section className=\"progress-section\">\n            <ProgressBar\n              value={progress}\n              label={currentStep}\n            />\n          </section>\n        )}\n\n        {/* Results */}\n        {results && (\n          <section className=\"results-section\">\n            <sp-tabs>\n              <sp-tab label=\"Takes\" panel=\"takes-panel\" />\n              <sp-tab label=\"Silence\" panel=\"silence-panel\" />\n            </sp-tabs>\n\n            <sp-tab-panel id=\"takes-panel\">\n              <TakeGroups groups={results.takeGroups} />\n            </sp-tab-panel>\n\n            <sp-tab-panel id=\"silence-panel\">\n              <SilenceList sections={results.silentSections} />\n            </sp-tab-panel>\n          </section>\n        )}\n\n        {/* Apply Actions */}\n        {results && (\n          <section className=\"apply-section\">\n            <sp-button-group>\n              <sp-button onClick={applyColorCoding}>\n                Apply Colors\n              </sp-button>\n              <sp-button onClick={createBins}>\n                Create Bins\n              </sp-button>\n              <sp-button variant=\"cta\" onClick={applyAll}>\n                Apply All\n              </sp-button>\n            </sp-button-group>\n          </section>\n        )}\n\n        {/* Footer */}\n        <footer className=\"panel-footer\">\n          <sp-body size=\"XS\">\n            Est. cost: ${estimatedCost.toFixed(2)}\n          </sp-body>\n        </footer>\n      </div>\n    </sp-theme>\n  );\n}\n```\n\n### Settings Panel\n\n```tsx\n// src/js/main/components/SettingsPanel.tsx\n\nexport function SettingsPanel() {\n  return (\n    <sp-dialog>\n      <sp-heading slot=\"heading\">Settings</sp-heading>\n\n      <div className=\"settings-content\">\n        {/* API Keys */}\n        <sp-field-group>\n          <sp-field-label>API Keys</sp-field-label>\n\n          <sp-textfield\n            label=\"ElevenLabs API Key\"\n            type=\"password\"\n            value={elevenLabsKey}\n            onChange={setElevenLabsKey}\n          />\n\n          <sp-textfield\n            label=\"Deepgram API Key\"\n            type=\"password\"\n            value={deepgramKey}\n            onChange={setDeepgramKey}\n          />\n\n          <sp-textfield\n            label=\"OpenAI API Key\"\n            type=\"password\"\n            value={openaiKey}\n            onChange={setOpenaiKey}\n          />\n        </sp-field-group>\n\n        {/* Voice Isolation Settings */}\n        <sp-field-group>\n          <sp-field-label>Voice Isolation</sp-field-label>\n\n          <sp-dropdown label=\"Audio Output Mode\">\n            <sp-menu-item value=\"original\" selected>\n              Keep Original Audio\n            </sp-menu-item>\n            <sp-menu-item value=\"isolated\">\n              Use Isolated Voice\n            </sp-menu-item>\n            <sp-menu-item value=\"none\">\n              Skip Isolation (faster)\n            </sp-menu-item>\n          </sp-dropdown>\n        </sp-field-group>\n\n        {/* Take Detection Settings */}\n        <sp-field-group>\n          <sp-field-label>Take Detection</sp-field-label>\n\n          <sp-slider\n            label=\"Similarity Threshold\"\n            min={0.70}\n            max={0.95}\n            value={similarityThreshold}\n            step={0.05}\n          />\n\n          <sp-checkbox checked={useLLMAnalysis}>\n            Use LLM for Better Accuracy\n          </sp-checkbox>\n        </sp-field-group>\n\n        {/* Silence Detection Settings */}\n        <sp-field-group>\n          <sp-field-label>Silence Detection</sp-field-label>\n\n          <sp-slider\n            label=\"Minimum Silence Duration (seconds)\"\n            min={0.5}\n            max={3.0}\n            value={minSilenceDuration}\n            step={0.1}\n          />\n\n          <sp-checkbox checked={preserveNaturalPauses}>\n            Preserve Natural Pauses\n          </sp-checkbox>\n        </sp-field-group>\n\n        {/* Timeline Settings */}\n        <sp-field-group>\n          <sp-field-label>Timeline Integration</sp-field-label>\n\n          <sp-checkbox checked={autoColorCode}>\n            Auto Color Code Takes\n          </sp-checkbox>\n\n          <sp-checkbox checked={createBins}>\n            Organize in Bins\n          </sp-checkbox>\n\n          <sp-checkbox checked={addMarkers}>\n            Add Timeline Markers\n          </sp-checkbox>\n        </sp-field-group>\n      </div>\n\n      <sp-button-group slot=\"button\">\n        <sp-button variant=\"secondary\" onClick={resetDefaults}>\n          Reset Defaults\n        </sp-button>\n        <sp-button variant=\"cta\" onClick={saveSettings}>\n          Save\n        </sp-button>\n      </sp-button-group>\n    </sp-dialog>\n  );\n}\n```\n\n---\n\n## 9. Development Phases\n\n### Phase 1: MVP (Weeks 1-6)\n\n**Goals:**\n- Working plugin with core functionality\n- Basic take detection\n- Silence removal\n- Timeline integration\n\n**Features:**\n- [ ] CEP panel setup with Bolt CEP\n- [ ] Drag-and-drop clip import\n- [ ] Audio extraction (FFmpeg)\n- [ ] Deepgram transcription integration\n- [ ] Basic take detection (embeddings only)\n- [ ] Silence detection (threshold-based)\n- [ ] Color coding in Premiere\n- [ ] Bin organization\n- [ ] Basic settings panel\n\n**Tech Debt:**\n- Skip voice isolation\n- Skip LLM analysis\n- Minimal error handling\n- No progress streaming\n\n### Phase 2: Enhanced (Weeks 7-10)\n\n**Goals:**\n- Production-ready quality\n- Full feature set\n- Robust error handling\n\n**Features:**\n- [ ] ElevenLabs voice isolation\n- [ ] GPT-4o-mini take analysis\n- [ ] Advanced similarity clustering\n- [ ] \"Best take\" suggestions\n- [ ] Batch processing\n- [ ] Progress streaming\n- [ ] Cost estimation UI\n- [ ] Full settings panel\n- [ ] Error recovery\n\n### Phase 3: Polish (Weeks 11-14)\n\n**Goals:**\n- Professional quality\n- User delight\n- Market readiness\n\n**Features:**\n- [ ] Local LLM option (Ollama)\n- [ ] Custom color schemes\n- [ ] Export/import settings\n- [ ] Keyboard shortcuts\n- [ ] Tutorial/onboarding\n- [ ] Usage analytics\n- [ ] Undo/redo for applied changes\n- [ ] Performance optimization\n- [ ] Comprehensive testing\n\n---\n\n## 10. File Structure\n\n```\nsplice-plugin/\n├── .vscode/\n│   └── launch.json              # Debug configurations\n├── .github/\n│   └── workflows/\n│       └── release.yml          # GitHub Actions release\n├── bin/\n│   ├── mac/\n│   │   └── ffmpeg               # macOS FFmpeg binary\n│   └── win/\n│       └── ffmpeg.exe           # Windows FFmpeg binary\n├── src/\n│   ├── js/\n│   │   └── main/\n│   │       ├── components/\n│   │       │   ├── DropZone.tsx\n│   │       │   ├── ClipList.tsx\n│   │       │   ├── TakeGroups.tsx\n│   │       │   ├── SilenceList.tsx\n│   │       │   ├── ProgressBar.tsx\n│   │       │   ├── SettingsPanel.tsx\n│   │       │   └── Panel.tsx\n│   │       ├── hooks/\n│   │       │   ├── useProcessing.ts\n│   │       │   ├── usePremiere.ts\n│   │       │   ├── useSettings.ts\n│   │       │   └── useApi.ts\n│   │       ├── services/\n│   │       │   ├── ProcessingPipeline.ts\n│   │       │   ├── AudioExtractor.ts\n│   │       │   ├── VoiceIsolator.ts\n│   │       │   ├── Transcriber.ts\n│   │       │   ├── TakeDetector.ts\n│   │       │   ├── SilenceDetector.ts\n│   │       │   ├── CredentialManager.ts\n│   │       │   └── CostTracker.ts\n│   │       ├── modules/\n│   │       │   ├── audio/\n│   │       │   ├── takes/\n│   │       │   ├── silence/\n│   │       │   └── premiere/\n│   │       ├── utils/\n│   │       │   ├── apiClient.ts\n│   │       │   ├── embeddings.ts\n│   │       │   ├── clustering.ts\n│   │       │   └── helpers.ts\n│   │       ├── constants/\n│   │       │   ├── colors.ts\n│   │       │   └── config.ts\n│   │       ├── types/\n│   │       │   ├── models.ts\n│   │       │   ├── api.ts\n│   │       │   └── premiere.ts\n│   │       ├── store/\n│   │       │   └── index.ts     # Zustand store\n│   │       ├── main.tsx         # Entry point\n│   │       └── index.html\n│   └── jsx/\n│       ├── index.ts             # ExtendScript entry\n│       ├── timeline.ts          # Timeline operations\n│       ├── clips.ts             # Clip operations\n│       ├── bins.ts              # Bin operations\n│       ├── markers.ts           # Marker operations\n│       └── helpers.ts           # Utility functions\n├── public/\n│   └── icons/\n│       ├── icon-light.png\n│       └── icon-dark.png\n├── tests/\n│   ├── unit/\n│   │   ├── TakeDetector.test.ts\n│   │   └── SilenceDetector.test.ts\n│   └── integration/\n│       └── Pipeline.test.ts\n├── cep.config.ts                # Bolt CEP configuration\n├── vite.config.ts               # Vite configuration\n├── tsconfig.json                # TypeScript config\n├── tsconfig.jsx.json            # ExtendScript TypeScript config\n├── tailwind.config.js           # Tailwind CSS config\n├── package.json\n├── pnpm-lock.yaml\n├── .gitignore\n├── .env.example                 # Environment variables template\n├── README.md\n├── LICENSE\n└── CHANGELOG.md\n```\n\n---\n\n## 11. Build and Distribution\n\n### Development Workflow\n\n```bash\n# Install dependencies\npnpm install\n\n# Start development server with HMR\npnpm dev\n\n# Build for production\npnpm build\n\n# Create signed ZXP package\npnpm zxp\n\n# Run tests\npnpm test\n\n# Lint and type check\npnpm lint\npnpm typecheck\n```\n\n### CEP Configuration\n\n```typescript\n// cep.config.ts\n\nimport { CEPConfig } from 'bolt-cep';\n\nconst config: CEPConfig = {\n  id: 'com.splice.autocut',\n  displayName: 'SPLICE Auto-Cut',\n  version: '1.0.0',\n\n  hosts: [\n    {\n      name: 'PPRO',\n      version: '[22.0,99.9]'\n    }\n  ],\n\n  panels: {\n    main: {\n      type: 'Panel',\n      name: 'SPLICE Auto-Cut',\n      root: './src/js/main/index.html',\n      script: './src/jsx/index.ts',\n      size: {\n        width: 350,\n        height: 600\n      },\n      minSize: {\n        width: 300,\n        height: 400\n      }\n    }\n  },\n\n  lifecycle: {\n    autoVisible: true,\n    startOnEvents: ['applicationActivate']\n  },\n\n  cepVersion: '11.0',\n  runtimeVersion: '9.0'\n};\n\nexport default config;\n```\n\n### ZXP Signing\n\n```bash\n# Generate self-signed certificate (development)\nZXPSignCmd -selfSignedCert US \"SPLICE\" \"SPLICE Auto-Cut\" password ./cert.p12\n\n# Sign the package\npnpm zxp  # Uses Bolt CEP's built-in signing\n```\n\n### Distribution Strategy\n\n**Phase 1: Direct Distribution**\n- Host ZXP on website\n- Install via aescripts ZXP Installer or Anastasiy's Extension Manager\n- Free trial + subscription model\n\n**Phase 2: Adobe Exchange**\n- Submit after v1.0 stability\n- Requires Adobe review\n- Broader discovery\n\n**Phase 3: Enterprise**\n- Custom licensing\n- Volume discounts\n- Priority support\n\n### Environment Variables\n\n```bash\n# .env.example\n\n# API Keys (stored securely, not in env)\n# ELEVENLABS_API_KEY=\n# DEEPGRAM_API_KEY=\n# OPENAI_API_KEY=\n\n# Development\nDEBUG=true\nLOG_LEVEL=info\n\n# Feature Flags\nENABLE_VOICE_ISOLATION=true\nENABLE_LLM_ANALYSIS=true\nENABLE_LOCAL_LLM=false\n\n# Pricing (for cost estimation)\nELEVENLABS_RATE_PER_MIN=0.30\nDEEPGRAM_RATE_PER_MIN=0.0043\nOPENAI_INPUT_RATE_PER_1M=0.15\nOPENAI_OUTPUT_RATE_PER_1M=0.60\n```\n\n---\n\n## Quick Start for Developers\n\n```bash\n# 1. Clone repository\ngit clone https://github.com/your-org/splice-plugin.git\ncd splice-plugin\n\n# 2. Install dependencies\npnpm install\n\n# 3. Enable CEP debug mode\n# macOS:\ndefaults write com.adobe.CSXS.12 PlayerDebugMode 1\n# Windows (PowerShell as Admin):\n# reg add \"HKCU\\Software\\Adobe\\CSXS.12\" /v PlayerDebugMode /t REG_SZ /d 1\n\n# 4. Start development\npnpm dev\n\n# 5. Open Premiere Pro\n# Window > Extensions > SPLICE Auto-Cut\n\n# 6. Debug in Chrome\n# Navigate to http://localhost:8088\n```\n\n---\n\n**Architecture by:** Claude (Anthropic)\n**Version:** 1.0.0\n**Last Updated:** December 16, 2025\n","numLines":1725,"startLine":1,"totalLines":1725}}}
{"parentUuid":"a969739e-4203-4d6b-8dfc-88ab07872009","isSidechain":true,"userType":"external","cwd":"/Users/imorgado","sessionId":"4f42a096-2511-44a1-ab75-1966aaf695aa","version":"2.0.70","gitBranch":"","agentId":"a826d08","slug":"streamed-tickling-tower","message":{"model":"claude-opus-4-5-20251101","id":"msg_01RWhQhrDWpKcLazmphs7nE3","type":"message","role":"assistant","content":[{"type":"text","text":"I have read the existing BUILD.md file. Now I understand the current architecture document structure and can design how to integrate the new research findings. Let me analyze the document structure and plan the updates.\n\n## Analysis of Current Document Structure\n\nThe existing BUILD.md is well-organized with these main sections:\n1. System Architecture Overview\n2. Technical Stack\n3. Core Modules\n4. Data Models\n5. Processing Pipeline\n6. API Integration Patterns\n7. Premiere Pro Integration\n8. UI/UX Design\n9. Development Phases\n10. File Structure\n11. Build and Distribution\n\n## Integration Plan for New Research Findings\n\nBased on my analysis, here is the comprehensive plan for updating BUILD.md with the new research findings:\n\n---\n\n### 1. Voice Isolation Updates (Section 1 & 2)\n\n**Location:** Section 1 \"Technology Decisions\" table (line 103-113) and Section 2 \"Backend (Processing Services)\" (line 129-139)\n\n**Changes Required:**\n\nIn the Technology Decisions table, update:\n```\n| Voice Isolation | ElevenLabs | Best quality/price, 500MB limit |\n```\nTo:\n```\n| Voice Isolation | LALAL.AI / Dolby.io / Auphonic | Best value cloud processing (~$0.02-$0.04/min), handles loud music and wind |\n| Voice Isolation (Local) | Demucs v4 | Free open source option, high quality |\n```\n\nIn the Backend Processing Services section, change:\n```yaml\nVoice Isolation: ElevenLabs API v1\n```\nTo:\n```yaml\nVoice Isolation: LALAL.AI API / Dolby.io / Auphonic (cloud) or Demucs v4 (local)\n```\n\n**Add New Section:** After line 113, add a new subsection titled \"Voice Isolation Options\" with the pricing comparison table:\n\n```markdown\n### Voice Isolation Cost Comparison\n\n| Service | Cost per Minute | Best For |\n|---------|----------------|----------|\n| **Picovoice Koala** | ~$0.005/min | Best SDK, on-device processing |\n| **Auphonic** | ~$0.015-$0.02/min | Best value with full REST API |\n| **LALAL.AI** | ~$0.02-$0.04/min | Excellent for music separation |\n| **Dolby.io** | ~$0.03/min | Best for mixed conditions |\n| **Demucs v4** | Free | Open source, high quality |\n\n**Primary Recommendation:** Auphonic or LALAL.AI for cloud processing. These handle loud music, wind, and low dialogue volume well.\n\n**Note:** ElevenLabs (~$0.20/min) is NOT recommended due to high cost relative to alternatives.\n```\n\n---\n\n### 2. API Pricing & Business Model (NEW SECTION)\n\n**Location:** Add as new Section 12 \"Pricing and Business Model\" before the final footer\n\n**Content to Add:**\n\n```markdown\n## 12. Pricing and Business Model\n\n### Recommended API Stack for 70% Profit Margin\n\n| Component | Service | Cost |\n|-----------|---------|------|\n| Transcription | Deepgram Nova-3 | $0.0043/min |\n| LLM Analysis | GPT-4o-mini Batch API | $0.075/1M input tokens |\n| Voice Isolation | LALAL.AI or Dolby.io | ~$0.03/min |\n\n### Cost Breakdown per Hour of Processed Content\n\n| Configuration | Cost/Hour | Price for 70% Margin |\n|--------------|-----------|---------------------|\n| WITHOUT Voice Isolation | $0.27 | **$0.90/hour** |\n| WITH Voice Isolation | $3.87 | **$12.90/hour** |\n\n### Subscription Tiers\n\n| Tier | Hours/Month | Price | Effective Rate |\n|------|-------------|-------|----------------|\n| Starter | 10 hours | $19/month | $1.90/hr |\n| Pro | 30 hours | $49/month | $1.63/hr |\n| Studio | 100 hours | $149/month | $1.49/hr |\n| Voice Isolation Add-on | - | +$59-$549/month | Varies |\n```\n\n---\n\n### 3. Rate Limiting Implementation (Section 6 Enhancement)\n\n**Location:** Section 6 \"API Integration Patterns\" (lines 770-949)\n\n**Changes Required:** Add new subsection after \"Rate Limiting and Retry Logic\" (after line 829):\n\n```markdown\n### Production Rate Limiting Architecture\n\n**Algorithm Selection:**\n- **API-level limiting:** Token Bucket algorithm\n- **Monthly quotas:** Sliding Window Counter\n- **Storage:** Redis with `rate-limiter-flexible` library\n- **Job queue:** BullMQ for processing queue management\n\n**External API Limits:**\n\n| Service | Concurrent Limit | Notes |\n|---------|-----------------|-------|\n| ElevenLabs | 2-15 requests | Tier dependent |\n| Deepgram | 100 requests | STT concurrent limit |\n| OpenAI | Varies | TPM/RPM limits by tier |\n\n**Implementation Pattern:**\n\n```typescript\n// src/js/main/services/RateLimiter.ts\n\nimport { RateLimiterRedis } from 'rate-limiter-flexible';\nimport Redis from 'ioredis';\n\nconst redis = new Redis();\n\n// Token bucket for API calls\nconst apiLimiter = new RateLimiterRedis({\n  storeClient: redis,\n  keyPrefix: 'splice_api',\n  points: 10,           // requests\n  duration: 1,          // per second\n  blockDuration: 60,    // block for 60s if exceeded\n});\n\n// Sliding window for monthly quotas\nconst monthlyLimiter = new RateLimiterRedis({\n  storeClient: redis,\n  keyPrefix: 'splice_monthly',\n  points: 600,          // 10 hours = 600 minutes\n  duration: 30 * 24 * 60 * 60, // 30 days\n});\n\n// Middleware implementation\nasync function rateLimitMiddleware(userId: string, minutes: number): Promise<RateLimitResult> {\n  try {\n    await apiLimiter.consume(userId, 1);\n    await monthlyLimiter.consume(userId, minutes);\n    return { allowed: true };\n  } catch (error) {\n    return {\n      allowed: false,\n      retryAfter: error.msBeforeNext / 1000,\n      reason: error.remainingPoints === 0 ? 'quota_exceeded' : 'rate_limited'\n    };\n  }\n}\n```\n\n**Graceful Degradation:**\n- Return `Retry-After` headers when rate limited\n- Queue jobs in BullMQ when at capacity\n- Per-user usage counters for quota tracking\n```\n\n---\n\n### 4. Frontend Stack for SaaS (NEW SECTION)\n\n**Location:** Add as new Section 13 \"SaaS Frontend Architecture\" (or integrate into existing Section 2)\n\n**Content to Add:**\n\n```markdown\n## 13. SaaS Web Application (User Account Management)\n\n### Recommended Tech Stack\n\n```yaml\nFramework: Next.js 15 (App Router)\nAuthentication: Clerk (10K free MAU, built-in billing portal)\nDatabase: Supabase (PostgreSQL)\nPayments: Stripe Billing with Meters API\nUI Components: shadcn/ui + Tremor (for charts/dashboards)\n```\n\n### Key Features\n\n- **Usage Dashboard:** Real-time charts showing processing hours consumed\n- **Subscription Management:** Stripe Customer Portal integration\n- **API Key Generation:** Self-service API key creation and management\n- **Processing History:** Logs of all processed files with timestamps\n- **Billing History:** Invoice access and payment method management\n\n### Architecture Diagram\n\n```\n┌─────────────────────────────────────────────────────────────────────┐\n│                        SPLICE SaaS Platform                          │\n├─────────────────────────────────────────────────────────────────────┤\n│                                                                      │\n│  ┌─────────────────┐     ┌─────────────────┐     ┌───────────────┐  │\n│  │   Next.js 15    │     │     Clerk       │     │   Supabase    │  │\n│  │   App Router    │◄───►│  Authentication │◄───►│  PostgreSQL   │  │\n│  │                 │     │  + User Mgmt    │     │               │  │\n│  └────────┬────────┘     └─────────────────┘     └───────────────┘  │\n│           │                                                          │\n│           ▼                                                          │\n│  ┌─────────────────┐     ┌─────────────────┐                        │\n│  │   shadcn/ui +   │     │ Stripe Billing  │                        │\n│  │   Tremor Charts │     │ + Meters API    │                        │\n│  └─────────────────┘     └─────────────────┘                        │\n│                                                                      │\n└─────────────────────────────────────────────────────────────────────┘\n```\n\n### Alternative: Pre-built Boilerplate\n\n**Supastarter** ($349 one-time) - Pre-built SaaS template with:\n- Authentication already configured\n- Stripe subscription management\n- User dashboard\n- Admin panel\n```\n\n---\n\n### 5. User Interaction - Panel-Based Workflow (Section 7 & 8)\n\n**Location:** Section 7 \"Premiere Pro Integration\" (lines 954-1107) and Section 8 \"UI/UX Design\" (lines 1151-1405)\n\n**Critical Finding to Add:** Add a warning box near the beginning of Section 7:\n\n```markdown\n> **Important Discovery:** Native right-click context menu customization is NOT possible in Adobe Premiere Pro. The solution is a CEP panel with selection detection.\n```\n\n**Add New Subsection in Section 7 after line 1067:**\n\n```markdown\n### Selection Detection and Panel Workflow\n\nSince native context menus cannot be customized, SPLICE uses a panel-based approach with automatic selection detection.\n\n**Workflow:**\n1. User selects clips on timeline\n2. SPLICE panel detects selection change\n3. Panel displays action buttons\n4. User clicks \"Process with Voice Isolation\" or \"Process without Voice Isolation\"\n\n**ExtendScript Selection Detection:**\n\n```javascript\n// src/jsx/selection.ts\n\nfunction getSelection(): ProjectItem[] {\n  var seq = app.project.activeSequence;\n  if (!seq) return [];\n  var selection = seq.getSelection();\n  return selection;\n}\n\n// Listen for selection changes in the panel\n// Note: Event binding must be done from CEP side\n```\n\n**CEP Panel Event Binding:**\n\n```typescript\n// src/js/main/hooks/useSelectionDetection.ts\n\nimport { useEffect, useState } from 'react';\nimport { evalTS } from '../utils/bolt';\n\nexport function useSelectionDetection() {\n  const [selectedClips, setSelectedClips] = useState<ClipInfo[]>([]);\n\n  useEffect(() => {\n    // Bind to Premiere's selection change event\n    const csInterface = new CSInterface();\n\n    csInterface.addEventListener(\n      'com.adobe.csxs.events.SelectionChanged',\n      async () => {\n        const result = await evalTS('getSelection');\n        const clips = JSON.parse(result);\n        setSelectedClips(clips);\n      }\n    );\n\n    // Cleanup\n    return () => {\n      csInterface.removeEventListener('com.adobe.csxs.events.SelectionChanged');\n    };\n  }, []);\n\n  return selectedClips;\n}\n```\n\n**Updated User Flow:**\n\n```\n┌──────────────────────────────────────────────────────────────────┐\n│                        UPDATED WORKFLOW                           │\n├──────────────────────────────────────────────────────────────────┤\n│                                                                   │\n│  1. Select clips on timeline                                      │\n│           │                                                       │\n│           ▼                                                       │\n│  2. SPLICE panel auto-detects selection                          │\n│           │                                                       │\n│           ▼                                                       │\n│  3. Panel shows: [Process with Voice Isolation]                   │\n│                  [Process without Voice Isolation]                │\n│           │                                                       │\n│           ▼                                                       │\n│  4. Click button → Processing begins                              │\n│                                                                   │\n└──────────────────────────────────────────────────────────────────┘\n```\n```\n\n**Update Panel Layout in Section 8:** Modify the Panel.tsx code (around line 1188) to reflect the new selection-based workflow:\n\n```tsx\n// Add after the header section in Panel.tsx\n\n{/* Selection Detection Section */}\n<section className=\"selection-section\">\n  {selectedClips.length === 0 ? (\n    <sp-illustrated-message>\n      <sp-icon name=\"SelectionOutline\" />\n      <sp-heading>No clips selected</sp-heading>\n      <sp-body>Select clips on the timeline to process</sp-body>\n    </sp-illustrated-message>\n  ) : (\n    <div className=\"selection-info\">\n      <sp-body>\n        {selectedClips.length} clip{selectedClips.length > 1 ? 's' : ''} selected\n      </sp-body>\n      <sp-body size=\"S\" class=\"secondary\">\n        Total duration: {formatDuration(totalDuration)}\n      </sp-body>\n    </div>\n  )}\n</section>\n\n{/* Quick Action Buttons */}\n{selectedClips.length > 0 && (\n  <section className=\"quick-actions\">\n    <sp-button-group vertical>\n      <sp-button\n        variant=\"cta\"\n        onClick={() => processClips({ useVoiceIsolation: true })}\n      >\n        Process with Voice Isolation\n      </sp-button>\n      <sp-button\n        variant=\"secondary\"\n        onClick={() => processClips({ useVoiceIsolation: false })}\n      >\n        Process without Voice Isolation\n      </sp-button>\n    </sp-button-group>\n    <sp-body size=\"XS\" class=\"cost-estimate\">\n      Est. cost: ${estimateCost(selectedClips, true).toFixed(2)} with / \n      ${estimateCost(selectedClips, false).toFixed(2)} without isolation\n    </sp-body>\n  </section>\n)}\n```\n\n---\n\n### 6. Update Cost Tracking Service (Section 6)\n\n**Location:** Update the CostTracker class (lines 898-949)\n\n**Changes Required:** Update the DEFAULT_RATES constant and add new voice isolation providers:\n\n```typescript\n// Updated cost rates based on research\n\nconst DEFAULT_RATES: CostRates = {\n  voiceIsolation: {\n    lalalai: { perMinute: 0.03 },\n    dolbyio: { perMinute: 0.03 },\n    auphonic: { perMinute: 0.02 },\n    picovoice: { perMinute: 0.005 },\n    demucs: { perMinute: 0 },  // Free, local processing\n    elevenlabs: { perMinute: 0.20 }  // Not recommended\n  },\n  deepgram: { perMinute: 0.0043 },\n  openai: {\n    inputPer1M: 0.075,  // GPT-4o-mini batch pricing\n    outputPer1M: 0.30\n  }\n};\n\ninterface VoiceIsolationProvider {\n  id: 'lalalai' | 'dolbyio' | 'auphonic' | 'picovoice' | 'demucs';\n  name: string;\n  costPerMinute: number;\n  isLocal: boolean;\n}\n\nconst RECOMMENDED_PROVIDERS: VoiceIsolationProvider[] = [\n  { id: 'auphonic', name: 'Auphonic', costPerMinute: 0.02, isLocal: false },\n  { id: 'lalalai', name: 'LALAL.AI', costPerMinute: 0.03, isLocal: false },\n  { id: 'dolbyio', name: 'Dolby.io', costPerMinute: 0.03, isLocal: false },\n  { id: 'demucs', name: 'Demucs v4', costPerMinute: 0, isLocal: true }\n];\n```\n\n---\n\n### 7. Update Settings Interface (Section 3.8)\n\n**Location:** Section 3.8 Settings Module (lines 339-379)\n\n**Changes Required:** Update the Settings interface to include new voice isolation provider options:\n\n```typescript\ninterface Settings {\n  // Voice Isolation - UPDATED\n  useVoiceIsolation: boolean;\n  voiceIsolationProvider: 'auphonic' | 'lalalai' | 'dolbyio' | 'picovoice' | 'demucs';\n  audioOutputMode: 'original' | 'isolated' | 'none';\n\n  // ... rest of settings remain the same\n  \n  // API Keys - UPDATED\n  auphonicApiKey?: string;\n  lalalaiApiKey?: string;\n  dolbyioApiKey?: string;\n  deepgramApiKey: string;\n  openaiApiKey: string;\n}\n```\n\n---\n\n### 8. Update Table of Contents\n\n**Location:** Lines 9-22\n\n**Changes Required:** Add new sections to ToC:\n\n```markdown\n## Table of Contents\n\n1. [System Architecture Overview](#1-system-architecture-overview)\n2. [Technical Stack](#2-technical-stack)\n3. [Core Modules](#3-core-modules)\n4. [Data Models](#4-data-models)\n5. [Processing Pipeline](#5-processing-pipeline)\n6. [API Integration Patterns](#6-api-integration-patterns)\n7. [Premiere Pro Integration](#7-premiere-pro-integration)\n8. [UI/UX Design](#8-uiux-design)\n9. [Development Phases](#9-development-phases)\n10. [File Structure](#10-file-structure)\n11. [Build and Distribution](#11-build-and-distribution)\n12. [Pricing and Business Model](#12-pricing-and-business-model)\n13. [SaaS Web Application](#13-saas-web-application)\n```\n\n---\n\n### 9. Update Processing Pipeline Diagram\n\n**Location:** Lines 86-99 (Data Flow diagram)\n\n**Changes Required:** Update to reflect new voice isolation providers:\n\n```\n┌──────────┐     ┌──────────────┐     ┌───────────────┐     ┌──────────────┐\n│  User    │     │   Extract    │     │    Voice      │     │  Transcribe  │\n│ Selects  │────►│   Audio      │────►│   Isolation   │────►│  + Timestamp │\n│  Clips   │     │   (FFmpeg)   │     │ (LALAL.AI/    │     │  (Deepgram)  │\n└──────────┘     └──────────────┘     │  Dolby/Demucs)│     └──────────────┘\n                                      └───────────────┘\n```\n\n---\n\n### 10. Update Version and Date\n\n**Location:** Lines 3-5\n\n**Changes Required:**\n```markdown\n**Version:** 1.1.0\n**Date:** December 16, 2025\n**Status:** Production-Ready Architecture (Updated with Research Findings)\n```\n\n---\n\n## Summary of All Changes\n\n| Section | Change Type | Description |\n|---------|-------------|-------------|\n| Header | Update | Version 1.1.0, updated status |\n| ToC | Add | New sections 12 and 13 |\n| Section 1 | Update | Voice isolation in Technology Decisions table |\n| Section 1 | Add | Voice Isolation Cost Comparison subsection |\n| Section 2 | Update | Backend stack voice isolation entry |\n| Section 3.8 | Update | Settings interface with new providers |\n| Section 6 | Update | Cost tracking rates |\n| Section 6 | Add | Production Rate Limiting Architecture subsection |\n| Section 7 | Add | Warning about context menu limitation |\n| Section 7 | Add | Selection Detection and Panel Workflow subsection |\n| Section 8 | Update | Panel.tsx with selection-based UI |\n| NEW Section 12 | Add | Pricing and Business Model |\n| NEW Section 13 | Add | SaaS Web Application architecture |\n\n---\n\n### Critical Files for Implementation\n\n- **/Users/imorgado/Desktop/SPLICE/BUILD.md** - Main architecture document to update with all new research findings\n- **/Users/imorgado/Desktop/SPLICE/src/js/main/services/CostTracker.ts** (to be created) - Will implement updated pricing logic with new voice isolation providers\n- **/Users/imorgado/Desktop/SPLICE/src/js/main/hooks/useSelectionDetection.ts** (to be created) - New hook for panel-based selection workflow\n- **/Users/imorgado/Desktop/SPLICE/src/js/main/services/RateLimiter.ts** (to be created) - New rate limiting service using Redis and Token Bucket algorithm\n- **/Users/imorgado/Desktop/SPLICE/src/jsx/selection.ts** (to be created) - ExtendScript selection detection functions"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":21334,"cache_read_input_tokens":12454,"cache_creation":{"ephemeral_5m_input_tokens":21334,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWAjaN6z7V7rXDPD1WPUY","type":"assistant","uuid":"de3071de-8dbf-4505-993f-77186bfa0962","timestamp":"2025-12-16T17:10:22.644Z"}
