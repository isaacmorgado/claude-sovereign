{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":"Test the memory system in Claude Sovereign thoroughly:\n\n1. Read ~/Desktop/claude-sovereign/hooks/memory-manager.sh\n2. Verify episodic memory works (record, search)\n3. Verify semantic memory works (facts, patterns)\n4. Verify working memory works (current context)\n5. Test 3-factor scoring (BM25 + Vector + RL)\n6. Test checkpoint/restore functionality\n7. Test file change detection (SHA-256)\n8. Test context budgeting\n9. Test git-aware channels\n10. Identify any bugs or missing features\n11. Report all findings with line numbers"},"uuid":"6caadaf2-65d3-4c1f-a071-0869d4f5098d","timestamp":"2026-01-12T21:32:05.341Z"}
{"parentUuid":"6caadaf2-65d3-4c1f-a071-0869d4f5098d","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PfMrt5rkN5Xz793Us6PaVy","type":"message","role":"assistant","content":[{"type":"text","text":"I'll thoroughly test the memory system in Claude Sovereign. Let me start by reading the memory-manager.sh file and then systematically test each component."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":20366,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":20366,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CX4CUyMiLWpsQA6ufnABH","type":"assistant","uuid":"7bb1a5b4-13ff-44c7-ada5-bddc8d7fae3b","timestamp":"2026-01-12T21:32:08.599Z"}
{"parentUuid":"7bb1a5b4-13ff-44c7-ada5-bddc8d7fae3b","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01PfMrt5rkN5Xz793Us6PaVy","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01VUfXiSLVFz6AuF4dEWmuVQ","name":"Read","input":{"file_path":"/Users/imorgado/.claude/hooks/memory-manager.sh"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":2,"cache_creation_input_tokens":20366,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":20366,"ephemeral_1h_input_tokens":0},"output_tokens":101,"service_tier":"standard"}},"requestId":"req_011CX4CUyMiLWpsQA6ufnABH","type":"assistant","uuid":"6c37c611-06cc-40ce-808a-200e642a3b72","timestamp":"2026-01-12T21:32:09.025Z"}
{"parentUuid":"6c37c611-06cc-40ce-808a-200e642a3b72","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01VUfXiSLVFz6AuF4dEWmuVQ","type":"tool_result","content":"     1→#!/bin/bash\n     2→# Memory Manager - Persistent memory for Claude Code sessions\n     3→# Based on patterns from: MemGPT/Letta, Generative Agents (Stanford), Mem0, LangChain, CrewAI\n     4→#\n     5→# Supports both global and project-scoped memory:\n     6→# - Global: ~/.claude/memory/ (shared across all projects)\n     7→# - Project: .claude/memory/ (project-specific, auto-detected)\n     8→#\n     9→# Set MEMORY_SCOPE=project to force project-local memory\n    10→# Set MEMORY_SCOPE=global to force global memory\n    11→\n    12→set -uo pipefail\n    13→\n    14→# Detect project root (look for .git, package.json, Cargo.toml, etc.)\n    15→find_project_root() {\n    16→    local dir=\"$PWD\"\n    17→    while [[ \"$dir\" != \"/\" ]]; do\n    18→        if [[ -d \"$dir/.git\" ]] || [[ -f \"$dir/package.json\" ]] || \\\n    19→           [[ -f \"$dir/Cargo.toml\" ]] || [[ -f \"$dir/go.mod\" ]] || \\\n    20→           [[ -f \"$dir/pyproject.toml\" ]] || [[ -f \"$dir/CLAUDE.md\" ]]; then\n    21→            echo \"$dir\"\n    22→            return 0\n    23→        fi\n    24→        dir=\"$(dirname \"$dir\")\"\n    25→    done\n    26→    return 1\n    27→}\n    28→\n    29→# Get git channel (branch name, sanitized)\n    30→# Based on patterns from GitHub: git rev-parse --abbrev-ref HEAD\n    31→get_git_channel() {\n    32→    local branch\n    33→\n    34→    # Check if we're in a git repo\n    35→    if git rev-parse --git-dir >/dev/null 2>&1; then\n    36→        branch=$(git rev-parse --abbrev-ref HEAD 2>/dev/null)\n    37→\n    38→        # Handle edge case: newly initialized repo with no commits returns \"HEAD\"\n    39→        if [[ \"$branch\" == \"HEAD\" ]]; then\n    40→            # Try to get default branch name from git config\n    41→            branch=$(git config --get init.defaultBranch 2>/dev/null || echo \"main\")\n    42→        fi\n    43→    else\n    44→        branch=\"main\"\n    45→    fi\n    46→\n    47→    # Sanitize branch name: replace non-alphanumeric with dashes\n    48→    echo \"$branch\" | sed 's/[^a-zA-Z0-9_-]/-/g'\n    49→}\n    50→\n    51→# Determine memory directory based on scope\n    52→get_memory_dir() {\n    53→    local scope=\"${MEMORY_SCOPE:-auto}\"\n    54→    local channel\n    55→    channel=$(get_git_channel)\n    56→\n    57→    if [[ \"$scope\" == \"project\" ]]; then\n    58→        local project_root\n    59→        if project_root=$(find_project_root); then\n    60→            echo \"$project_root/.claude/memory/$channel\"\n    61→            return 0\n    62→        fi\n    63→    elif [[ \"$scope\" == \"global\" ]]; then\n    64→        echo \"${HOME}/.claude/memory/$channel\"\n    65→        return 0\n    66→    fi\n    67→\n    68→    # Auto mode: use project memory if in a project, otherwise global\n    69→    local project_root\n    70→    if project_root=$(find_project_root); then\n    71→        echo \"$project_root/.claude/memory/$channel\"\n    72→    else\n    73→        echo \"${HOME}/.claude/memory/$channel\"\n    74→    fi\n    75→}\n    76→\n    77→MEMORY_DIR=\"$(get_memory_dir)\"\n    78→WORKING_MEMORY=\"$MEMORY_DIR/working.json\"\n    79→EPISODIC_MEMORY=\"$MEMORY_DIR/episodic.json\"\n    80→SEMANTIC_MEMORY=\"$MEMORY_DIR/semantic.json\"\n    81→ACTION_LOG=\"$MEMORY_DIR/actions.jsonl\"\n    82→REFLECTION_LOG=\"$MEMORY_DIR/reflections.json\"\n    83→LOG_FILE=\"${HOME}/.claude/memory-manager.log\"\n    84→\n    85→# Memory limits\n    86→MAX_WORKING_ITEMS=\"${MAX_WORKING_ITEMS:-50}\"\n    87→MAX_EPISODIC_ITEMS=\"${MAX_EPISODIC_ITEMS:-1000}\"\n    88→MAX_SEMANTIC_ITEMS=\"${MAX_SEMANTIC_ITEMS:-500}\"\n    89→\n    90→log() {\n    91→    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\" >> \"$LOG_FILE\"\n    92→}\n    93→\n    94→init_memory() {\n    95→    mkdir -p \"$MEMORY_DIR\"\n    96→\n    97→    # Initialize working memory (current session state)\n    98→    if [[ ! -f \"$WORKING_MEMORY\" ]]; then\n    99→        cat > \"$WORKING_MEMORY\" << 'EOF'\n   100→{\n   101→    \"currentTask\": null,\n   102→    \"currentContext\": [],\n   103→    \"recentActions\": [],\n   104→    \"pendingItems\": [],\n   105→    \"scratchpad\": \"\",\n   106→    \"lastUpdated\": null\n   107→}\n   108→EOF\n   109→    fi\n   110→\n   111→    # Initialize episodic memory (past experiences)\n   112→    if [[ ! -f \"$EPISODIC_MEMORY\" ]]; then\n   113→        echo '{\"episodes\":[]}' > \"$EPISODIC_MEMORY\"\n   114→    fi\n   115→\n   116→    # Initialize semantic memory (facts and knowledge)\n   117→    if [[ ! -f \"$SEMANTIC_MEMORY\" ]]; then\n   118→        echo '{\"facts\":[],\"patterns\":[],\"preferences\":[]}' > \"$SEMANTIC_MEMORY\"\n   119→    fi\n   120→\n   121→    # Initialize reflections\n   122→    if [[ ! -f \"$REFLECTION_LOG\" ]]; then\n   123→        echo '{\"reflections\":[]}' > \"$REFLECTION_LOG\"\n   124→    fi\n   125→\n   126→    # Initialize action log (JSONL)\n   127→    if [[ ! -f \"$ACTION_LOG\" ]]; then\n   128→        touch \"$ACTION_LOG\"\n   129→    fi\n   130→}\n   131→\n   132→# =============================================================================\n   133→# WORKING MEMORY (Short-term, current session)\n   134→# Based on: MemGPT core memory, Generative Agents scratch\n   135→# =============================================================================\n   136→\n   137→# Set current task\n   138→set_task() {\n   139→    local task=\"$1\"\n   140→    local context=\"${2:-}\"\n   141→\n   142→    init_memory\n   143→\n   144→    local temp_file\n   145→    temp_file=$(mktemp)\n   146→\n   147→    local timestamp\n   148→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   149→\n   150→    jq --arg task \"$task\" \\\n   151→       --arg context \"$context\" \\\n   152→       --arg ts \"$timestamp\" \\\n   153→       '\n   154→       .currentTask = $task |\n   155→       .currentContext = (if $context != \"\" then [{content: $context, importance: 5, addedAt: $ts}] else [] end) |\n   156→       .lastUpdated = $ts\n   157→       ' \"$WORKING_MEMORY\" > \"$temp_file\"\n   158→\n   159→    mv \"$temp_file\" \"$WORKING_MEMORY\"\n   160→    log \"Set task: $task\"\n   161→}\n   162→\n   163→# Add to current context\n   164→add_context() {\n   165→    local context=\"$1\"\n   166→    local importance=\"${2:-5}\"  # 1-10 scale\n   167→\n   168→    init_memory\n   169→\n   170→    local temp_file\n   171→    temp_file=$(mktemp)\n   172→\n   173→    local timestamp\n   174→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   175→\n   176→    jq --arg ctx \"$context\" \\\n   177→       --argjson imp \"$importance\" \\\n   178→       --arg ts \"$timestamp\" \\\n   179→       '\n   180→       .currentContext += [{\n   181→           content: $ctx,\n   182→           importance: $imp,\n   183→           addedAt: $ts\n   184→       }] |\n   185→       .currentContext = (.currentContext | sort_by(-.importance) | .[0:20]) |\n   186→       .lastUpdated = $ts\n   187→       ' \"$WORKING_MEMORY\" > \"$temp_file\"\n   188→\n   189→    mv \"$temp_file\" \"$WORKING_MEMORY\"\n   190→    log \"Added context (importance: $importance)\"\n   191→}\n   192→\n   193→# Update scratchpad (quick notes)\n   194→update_scratchpad() {\n   195→    local note=\"$1\"\n   196→    local append=\"${2:-true}\"\n   197→\n   198→    init_memory\n   199→\n   200→    local temp_file\n   201→    temp_file=$(mktemp)\n   202→\n   203→    local timestamp\n   204→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   205→\n   206→    if [[ \"$append\" == \"true\" ]]; then\n   207→        jq --arg note \"$note\" \\\n   208→           --arg ts \"$timestamp\" \\\n   209→           '\n   210→           .scratchpad = (.scratchpad + \"\\n\" + $ts + \": \" + $note) |\n   211→           .lastUpdated = $ts\n   212→           ' \"$WORKING_MEMORY\" > \"$temp_file\"\n   213→    else\n   214→        jq --arg note \"$note\" \\\n   215→           --arg ts \"$timestamp\" \\\n   216→           '\n   217→           .scratchpad = $note |\n   218→           .lastUpdated = $ts\n   219→           ' \"$WORKING_MEMORY\" > \"$temp_file\"\n   220→    fi\n   221→\n   222→    mv \"$temp_file\" \"$WORKING_MEMORY\"\n   223→}\n   224→\n   225→# Get working memory state\n   226→get_working() {\n   227→    init_memory\n   228→    jq '.' \"$WORKING_MEMORY\"\n   229→}\n   230→\n   231→# Clear working memory (new session)\n   232→clear_working() {\n   233→    init_memory\n   234→\n   235→    local timestamp\n   236→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   237→\n   238→    cat > \"$WORKING_MEMORY\" << EOF\n   239→{\n   240→    \"currentTask\": null,\n   241→    \"currentContext\": [],\n   242→    \"recentActions\": [],\n   243→    \"pendingItems\": [],\n   244→    \"scratchpad\": \"\",\n   245→    \"lastUpdated\": \"$timestamp\"\n   246→}\n   247→EOF\n   248→    log \"Cleared working memory\"\n   249→}\n   250→\n   251→# =============================================================================\n   252→# EPISODIC MEMORY (Past experiences/episodes)\n   253→# Based on: Generative Agents memory stream, MemGPT archival memory\n   254→# =============================================================================\n   255→\n   256→# Record an episode (completed task/action)\n   257→record_episode() {\n   258→    local type=\"$1\"        # task_complete, error_fixed, research_done, etc.\n   259→    local description=\"$2\"\n   260→    local outcome=\"${3:-success}\"\n   261→    local details=\"${4:-}\"\n   262→\n   263→    init_memory\n   264→\n   265→    local temp_file\n   266→    temp_file=$(mktemp)\n   267→\n   268→    local timestamp\n   269→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   270→\n   271→    local episode_id\n   272→    episode_id=\"ep_$(date +%s%N | cut -c1-13)\"\n   273→\n   274→    # Calculate importance based on type and outcome\n   275→    local importance=5\n   276→    case \"$type\" in\n   277→        error_fixed) importance=8 ;;\n   278→        task_complete) importance=7 ;;\n   279→        research_done) importance=6 ;;\n   280→        pattern_learned) importance=9 ;;\n   281→        failure) importance=7 ;;\n   282→    esac\n   283→\n   284→    jq --arg id \"$episode_id\" \\\n   285→       --arg type \"$type\" \\\n   286→       --arg desc \"$description\" \\\n   287→       --arg outcome \"$outcome\" \\\n   288→       --arg details \"$details\" \\\n   289→       --argjson imp \"$importance\" \\\n   290→       --arg ts \"$timestamp\" \\\n   291→       '\n   292→       .episodes = [{\n   293→           id: $id,\n   294→           type: $type,\n   295→           description: $desc,\n   296→           outcome: $outcome,\n   297→           details: $details,\n   298→           importance: $imp,\n   299→           timestamp: $ts,\n   300→           accessCount: 0,\n   301→           lastAccessed: null\n   302→       }] + .episodes |\n   303→       .episodes = .episodes[0:'\"$MAX_EPISODIC_ITEMS\"']\n   304→       ' \"$EPISODIC_MEMORY\" > \"$temp_file\"\n   305→\n   306→    mv \"$temp_file\" \"$EPISODIC_MEMORY\"\n   307→    log \"Recorded episode: $type - $description\"\n   308→    echo \"$episode_id\"\n   309→}\n   310→\n   311→# Search episodes (simple text match)\n   312→search_episodes() {\n   313→    local query=\"$1\"\n   314→    local limit=\"${2:-10}\"\n   315→\n   316→    init_memory\n   317→\n   318→    jq --arg q \"$query\" \\\n   319→       --argjson limit \"$limit\" \\\n   320→       '\n   321→       .episodes |\n   322→       map(select(\n   323→           (.description | ascii_downcase | contains($q | ascii_downcase)) or\n   324→           (.details | ascii_downcase | contains($q | ascii_downcase)) or\n   325→           (.type | ascii_downcase | contains($q | ascii_downcase))\n   326→       )) |\n   327→       sort_by(.importance | (- if type == \"number\" then . else 0 end)) |\n   328→       .[0:$limit]\n   329→       ' \"$EPISODIC_MEMORY\"\n   330→}\n   331→\n   332→# Get recent episodes\n   333→get_recent_episodes() {\n   334→    local limit=\"${1:-10}\"\n   335→    local type_filter=\"${2:-}\"\n   336→\n   337→    init_memory\n   338→\n   339→    if [[ -n \"$type_filter\" ]]; then\n   340→        jq --arg type \"$type_filter\" \\\n   341→           --argjson limit \"$limit\" \\\n   342→           '.episodes | map(select(.type == $type)) | .[0:$limit]' \"$EPISODIC_MEMORY\"\n   343→    else\n   344→        jq --argjson limit \"$limit\" \\\n   345→           '.episodes | .[0:$limit]' \"$EPISODIC_MEMORY\"\n   346→    fi\n   347→}\n   348→\n   349→# =============================================================================\n   350→# SEMANTIC MEMORY (Facts, patterns, preferences)\n   351→# Based on: MemGPT persona/human blocks, knowledge graphs\n   352→# =============================================================================\n   353→\n   354→# Add a fact\n   355→add_fact() {\n   356→    local category=\"$1\"    # project, user, tool, api, etc.\n   357→    local key=\"$2\"\n   358→    local value=\"$3\"\n   359→    local confidence=\"${4:-0.8}\"\n   360→\n   361→    init_memory\n   362→\n   363→    local temp_file\n   364→    temp_file=$(mktemp)\n   365→\n   366→    local timestamp\n   367→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   368→\n   369→    # Update or insert fact\n   370→    jq --arg cat \"$category\" \\\n   371→       --arg key \"$key\" \\\n   372→       --arg val \"$value\" \\\n   373→       --argjson conf \"$confidence\" \\\n   374→       --arg ts \"$timestamp\" \\\n   375→       '\n   376→       .facts = [.facts[] | select(.category != $cat or .key != $key)] + [{\n   377→           category: $cat,\n   378→           key: $key,\n   379→           value: $val,\n   380→           confidence: $conf,\n   381→           updatedAt: $ts\n   382→       }] |\n   383→       .facts = (.facts | sort_by(.category, .key) | .[0:'\"$MAX_SEMANTIC_ITEMS\"'])\n   384→       ' \"$SEMANTIC_MEMORY\" > \"$temp_file\"\n   385→\n   386→    mv \"$temp_file\" \"$SEMANTIC_MEMORY\"\n   387→    log \"Added fact: $category/$key\"\n   388→}\n   389→\n   390→# Get a fact\n   391→get_fact() {\n   392→    local category=\"$1\"\n   393→    local key=\"$2\"\n   394→\n   395→    init_memory\n   396→\n   397→    jq --arg cat \"$category\" \\\n   398→       --arg key \"$key\" \\\n   399→       '.facts[] | select(.category == $cat and .key == $key)' \"$SEMANTIC_MEMORY\"\n   400→}\n   401→\n   402→# Get all facts in category\n   403→get_facts_by_category() {\n   404→    local category=\"$1\"\n   405→\n   406→    init_memory\n   407→\n   408→    jq --arg cat \"$category\" \\\n   409→       '.facts | map(select(.category == $cat))' \"$SEMANTIC_MEMORY\"\n   410→}\n   411→\n   412→# Add a learned pattern\n   413→add_pattern() {\n   414→    local pattern_type=\"$1\"   # error_fix, optimization, workflow, etc.\n   415→    local trigger=\"$2\"        # What triggers this pattern\n   416→    local solution=\"$3\"       # The solution/action\n   417→    local success_rate=\"${4:-1.0}\"\n   418→\n   419→    init_memory\n   420→\n   421→    local temp_file\n   422→    temp_file=$(mktemp)\n   423→\n   424→    local timestamp\n   425→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   426→\n   427→    local pattern_id\n   428→    pattern_id=\"pat_$(date +%s%N | cut -c1-13)\"\n   429→\n   430→    jq --arg id \"$pattern_id\" \\\n   431→       --arg type \"$pattern_type\" \\\n   432→       --arg trigger \"$trigger\" \\\n   433→       --arg solution \"$solution\" \\\n   434→       --argjson rate \"$success_rate\" \\\n   435→       --arg ts \"$timestamp\" \\\n   436→       '\n   437→       .patterns = [{\n   438→           id: $id,\n   439→           type: $type,\n   440→           trigger: $trigger,\n   441→           solution: $solution,\n   442→           successRate: $rate,\n   443→           useCount: 0,\n   444→           createdAt: $ts\n   445→       }] + .patterns |\n   446→       .patterns = .patterns[0:200]\n   447→       ' \"$SEMANTIC_MEMORY\" > \"$temp_file\"\n   448→\n   449→    mv \"$temp_file\" \"$SEMANTIC_MEMORY\"\n   450→    log \"Added pattern: $pattern_type\"\n   451→    echo \"$pattern_id\"\n   452→}\n   453→\n   454→# Find matching patterns\n   455→find_patterns() {\n   456→    local query=\"$1\"\n   457→    local limit=\"${2:-5}\"\n   458→\n   459→    init_memory\n   460→\n   461→    jq --arg q \"$query\" \\\n   462→       --argjson limit \"$limit\" \\\n   463→       '\n   464→       .patterns |\n   465→       map(select(\n   466→           (.trigger | ascii_downcase | contains($q | ascii_downcase)) or\n   467→           (.type | ascii_downcase | contains($q | ascii_downcase))\n   468→       )) |\n   469→       sort_by(-.successRate, -.useCount) |\n   470→       .[0:$limit]\n   471→       ' \"$SEMANTIC_MEMORY\"\n   472→}\n   473→\n   474→# Add user preference\n   475→add_preference() {\n   476→    local key=\"$1\"\n   477→    local value=\"$2\"\n   478→\n   479→    init_memory\n   480→\n   481→    local temp_file\n   482→    temp_file=$(mktemp)\n   483→\n   484→    local timestamp\n   485→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   486→\n   487→    jq --arg key \"$key\" \\\n   488→       --arg val \"$value\" \\\n   489→       --arg ts \"$timestamp\" \\\n   490→       '\n   491→       .preferences = [.preferences[] | select(.key != $key)] + [{\n   492→           key: $key,\n   493→           value: $val,\n   494→           updatedAt: $ts\n   495→       }]\n   496→       ' \"$SEMANTIC_MEMORY\" > \"$temp_file\"\n   497→\n   498→    mv \"$temp_file\" \"$SEMANTIC_MEMORY\"\n   499→    log \"Set preference: $key\"\n   500→}\n   501→\n   502→# Get preference\n   503→get_preference() {\n   504→    local key=\"$1\"\n   505→    local default=\"${2:-}\"\n   506→\n   507→    init_memory\n   508→\n   509→    local value\n   510→    value=$(jq -r --arg key \"$key\" \\\n   511→       '.preferences[] | select(.key == $key) | .value' \"$SEMANTIC_MEMORY\")\n   512→\n   513→    if [[ -n \"$value\" && \"$value\" != \"null\" ]]; then\n   514→        echo \"$value\"\n   515→    else\n   516→        echo \"$default\"\n   517→    fi\n   518→}\n   519→\n   520→# =============================================================================\n   521→# ACTION LOG (Append-only log of all actions)\n   522→# Based on: Generative Agents memory stream\n   523→# =============================================================================\n   524→\n   525→# Log an action\n   526→log_action() {\n   527→    local action_type=\"$1\"   # tool_call, edit, search, etc.\n   528→    local description=\"$2\"\n   529→    local result=\"${3:-}\"\n   530→    local metadata=\"${4:-}\"\n   531→\n   532→    init_memory\n   533→\n   534→    local timestamp\n   535→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   536→\n   537→    local action_id\n   538→    action_id=\"act_$(date +%s%N | cut -c1-13)\"\n   539→\n   540→    # Validate or default metadata to empty object\n   541→    if [[ -z \"$metadata\" ]] || ! echo \"$metadata\" | jq -e . >/dev/null 2>&1; then\n   542→        metadata='{}'\n   543→    fi\n   544→\n   545→    # Append to JSONL file\n   546→    jq -n -c \\\n   547→       --arg id \"$action_id\" \\\n   548→       --arg type \"$action_type\" \\\n   549→       --arg desc \"$description\" \\\n   550→       --arg result \"$result\" \\\n   551→       --argjson meta \"$metadata\" \\\n   552→       --arg ts \"$timestamp\" \\\n   553→       '{\n   554→           id: $id,\n   555→           type: $type,\n   556→           description: $desc,\n   557→           result: $result,\n   558→           metadata: $meta,\n   559→           timestamp: $ts\n   560→       }' >> \"$ACTION_LOG\"\n   561→\n   562→    log \"Logged action: $action_type\"\n   563→    echo \"$action_id\"\n   564→}\n   565→\n   566→# Get recent actions\n   567→get_recent_actions() {\n   568→    local limit=\"${1:-20}\"\n   569→    local type_filter=\"${2:-}\"\n   570→\n   571→    init_memory\n   572→\n   573→    if [[ ! -f \"$ACTION_LOG\" ]]; then\n   574→        echo \"[]\"\n   575→        return\n   576→    fi\n   577→\n   578→    if [[ -n \"$type_filter\" ]]; then\n   579→        tail -n 1000 \"$ACTION_LOG\" | jq -s --arg type \"$type_filter\" \\\n   580→           'map(select(.type == $type)) | reverse | .[0:'\"$limit\"']'\n   581→    else\n   582→        tail -n \"$limit\" \"$ACTION_LOG\" | jq -s 'reverse'\n   583→    fi\n   584→}\n   585→\n   586→# Search action log\n   587→search_actions() {\n   588→    local query=\"$1\"\n   589→    local limit=\"${2:-20}\"\n   590→\n   591→    init_memory\n   592→\n   593→    if [[ ! -f \"$ACTION_LOG\" ]]; then\n   594→        echo \"[]\"\n   595→        return\n   596→    fi\n   597→\n   598→    grep -i \"$query\" \"$ACTION_LOG\" 2>/dev/null | tail -n \"$limit\" | jq -s '.'\n   599→}\n   600→\n   601→# =============================================================================\n   602→# REFLECTION (Memory consolidation)\n   603→# Based on: Generative Agents reflection, MemGPT summarization\n   604→# =============================================================================\n   605→\n   606→# Create a reflection (consolidate recent experiences)\n   607→create_reflection() {\n   608→    local focus=\"${1:-general}\"  # general, errors, patterns, progress\n   609→    local content=\"$2\"\n   610→    local insights=\"${3:-}\"\n   611→\n   612→    init_memory\n   613→\n   614→    local temp_file\n   615→    temp_file=$(mktemp)\n   616→\n   617→    local timestamp\n   618→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n   619→\n   620→    local reflection_id\n   621→    reflection_id=\"ref_$(date +%s)\"\n   622→\n   623→    jq --arg id \"$reflection_id\" \\\n   624→       --arg focus \"$focus\" \\\n   625→       --arg content \"$content\" \\\n   626→       --arg insights \"$insights\" \\\n   627→       --arg ts \"$timestamp\" \\\n   628→       '\n   629→       .reflections = [{\n   630→           id: $id,\n   631→           focus: $focus,\n   632→           content: $content,\n   633→           insights: $insights,\n   634→           timestamp: $ts\n   635→       }] + .reflections |\n   636→       .reflections = .reflections[0:100]\n   637→       ' \"$REFLECTION_LOG\" > \"$temp_file\"\n   638→\n   639→    mv \"$temp_file\" \"$REFLECTION_LOG\"\n   640→    log \"Created reflection: $focus\"\n   641→    echo \"$reflection_id\"\n   642→}\n   643→\n   644→# Get reflections\n   645→get_reflections() {\n   646→    local focus=\"${1:-}\"\n   647→    local limit=\"${2:-10}\"\n   648→\n   649→    init_memory\n   650→\n   651→    if [[ -n \"$focus\" ]]; then\n   652→        jq --arg focus \"$focus\" \\\n   653→           --argjson limit \"$limit\" \\\n   654→           '.reflections | map(select(.focus == $focus)) | .[0:$limit]' \"$REFLECTION_LOG\"\n   655→    else\n   656→        jq --argjson limit \"$limit\" \\\n   657→           '.reflections | .[0:$limit]' \"$REFLECTION_LOG\"\n   658→    fi\n   659→}\n   660→\n   661→# =============================================================================\n   662→# MEMORY RETRIEVAL (Combined search across all memory types)\n   663→# Based on: Generative Agents retrieval (recency + relevance + importance)\n   664→# =============================================================================\n   665→\n   666→# Calculate recency score with exponential decay\n   667→# Formula: decay_rate ^ hours_since_access\n   668→calculate_recency_score() {\n   669→    local timestamp=\"$1\"\n   670→    local decay_rate=\"${2:-0.995}\"\n   671→\n   672→    # Get hours since timestamp\n   673→    local now_epoch\n   674→    now_epoch=$(date +%s)\n   675→\n   676→    local ts_epoch\n   677→    ts_epoch=$(date -j -f \"%Y-%m-%dT%H:%M:%SZ\" \"$timestamp\" +%s 2>/dev/null || echo \"$now_epoch\")\n   678→\n   679→    local hours_ago\n   680→    hours_ago=$(( (now_epoch - ts_epoch) / 3600 ))\n   681→\n   682→    # Calculate decay (using bc for floating point)\n   683→    echo \"scale=4; e(l($decay_rate) * $hours_ago)\" | bc -l 2>/dev/null || echo \"0.5\"\n   684→}\n   685→\n   686→# Calculate relevance score (keyword overlap)\n   687→calculate_relevance_score() {\n   688→    local query=\"$1\"\n   689→    local content=\"$2\"\n   690→\n   691→    # Normalize to lowercase and split into words\n   692→    local query_words\n   693→    query_words=$(echo \"$query\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]' '\\n' | sort -u)\n   694→\n   695→    local content_words\n   696→    content_words=$(echo \"$content\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]' '\\n' | sort -u)\n   697→\n   698→    # Count matching words\n   699→    local matches=0\n   700→    local total=0\n   701→    for word in $query_words; do\n   702→        total=$((total + 1))\n   703→        if echo \"$content_words\" | grep -qw \"$word\"; then\n   704→            matches=$((matches + 1))\n   705→        fi\n   706→    done\n   707→\n   708→    # Return overlap ratio\n   709→    if [[ $total -gt 0 ]]; then\n   710→        echo \"scale=4; $matches / $total\" | bc -l 2>/dev/null || echo \"0\"\n   711→    else\n   712→        echo \"0\"\n   713→    fi\n   714→}\n   715→\n   716→# =============================================================================\n   717→# PHASE 2: HYBRID SEARCH (BM25 + Semantic)\n   718→# =============================================================================\n   719→\n   720→# Calculate BM25 score for a document given a query\n   721→# BM25 parameters: k1 (term frequency saturation), b (document length normalization)\n   722→# Formula: BM25(q,d) = Σ IDF(qi) * (f(qi,d) * (k1+1)) / (f(qi,d) + k1 * (1-b + b * (|d|/avgdl)))\n   723→calculate_bm25_score() {\n   724→    local query=\"$1\"\n   725→    local content=\"$2\"\n   726→    local avgdl=\"${3:-50}\"     # Average document length (words)\n   727→    local k1=\"${4:-1.5}\"       # Term frequency saturation (1.2-2.0)\n   728→    local b=\"${5:-0.75}\"       # Length normalization (0-1)\n   729→\n   730→    # Normalize and tokenize\n   731→    local query_words\n   732→    query_words=$(echo \"$query\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]' '\\n' | grep -v '^$')\n   733→\n   734→    local content_lower\n   735→    content_lower=$(echo \"$content\" | tr '[:upper:]' '[:lower:]')\n   736→\n   737→    # Get document length\n   738→    local doc_length\n   739→    doc_length=$(echo \"$content_lower\" | tr -cs '[:alnum:]' '\\n' | grep -v '^$' | wc -l | tr -d ' ')\n   740→\n   741→    # Calculate BM25 score\n   742→    local bm25_score=0\n   743→    local idf_score\n   744→    local tf\n   745→    local numerator\n   746→    local denominator\n   747→    local term_score\n   748→\n   749→    for term in $query_words; do\n   750→        # Term frequency in document\n   751→        tf=$(echo \"$content_lower\" | tr -cs '[:alnum:]' '\\n' | grep -wc \"$term\" || echo \"0\")\n   752→\n   753→        # Simple IDF approximation (log(1 + 1/df) where df=1 for simplicity)\n   754→        # In a full implementation, we'd calculate across all documents\n   755→        idf_score=1.0\n   756→\n   757→        # BM25 formula\n   758→        numerator=$(echo \"scale=4; $tf * ($k1 + 1)\" | bc -l 2>/dev/null || echo \"0\")\n   759→        denominator=$(echo \"scale=4; $tf + $k1 * (1 - $b + $b * ($doc_length / $avgdl))\" | bc -l 2>/dev/null || echo \"1\")\n   760→\n   761→        # Check if denominator is positive (avoid division by zero)\n   762→        if [[ -n \"$denominator\" ]] && [[ $(echo \"$denominator > 0\" | bc -l 2>/dev/null || echo \"0\") == \"1\" ]]; then\n   763→            term_score=$(echo \"scale=4; $idf_score * ($numerator / $denominator)\" | bc -l 2>/dev/null || echo \"0\")\n   764→            bm25_score=$(echo \"scale=4; $bm25_score + $term_score\" | bc -l 2>/dev/null || echo \"$bm25_score\")\n   765→        fi\n   766→    done\n   767→\n   768→    # Normalize to 0-1 range (approximate)\n   769→    echo \"scale=4; if ($bm25_score > 10) 1.0 else $bm25_score / 10\" | bc -l 2>/dev/null || echo \"0\"\n   770→}\n   771→\n   772→# Reciprocal Rank Fusion - combines rankings from multiple sources\n   773→# ENHANCED: Now combines BM25, word overlap, recency, and importance\n   774→# RRF(d) = Σ 1/(k + rank(d)) where k=60 is standard\n   775→# Research shows RRF achieves 95%+ accuracy when combining multiple signals\n   776→reciprocal_rank_fusion_enhanced() {\n   777→    local results_json=\"$1\"  # JSON array with items having multiple scores\n   778→    local k=\"${2:-60}\"       # Constant (typically 60, proven optimal)\n   779→\n   780→    # Calculate RRF score for each item across ALL ranking dimensions\n   781→    echo \"$results_json\" | jq --argjson k \"$k\" '\n   782→        # Rank by BM25 score (exact term matching)\n   783→        (. | sort_by(-.bm25_score) | to_entries | map({id: .key, bm25_rank: .key + 1})) as $bm25_ranks |\n   784→        # Rank by relevance score (semantic word overlap)\n   785→        (. | sort_by(-.relevance_score) | to_entries | map({id: .key, rel_rank: .key + 1})) as $rel_ranks |\n   786→        # Rank by recency (temporal relevance)\n   787→        (. | sort_by(-.recency_score) | to_entries | map({id: .key, rec_rank: .key + 1})) as $rec_ranks |\n   788→        # Rank by importance (user-defined priority)\n   789→        (. | sort_by(-.importance_score) | to_entries | map({id: .key, imp_rank: .key + 1})) as $imp_ranks |\n   790→        # Combine ALL rankings with RRF formula\n   791→        . | to_entries | map(\n   792→            . as $item |\n   793→            ($bm25_ranks[$item.key].bm25_rank // 999) as $bm25_r |\n   794→            ($rel_ranks[$item.key].rel_rank // 999) as $rel_r |\n   795→            ($rec_ranks[$item.key].rec_rank // 999) as $rec_r |\n   796→            ($imp_ranks[$item.key].imp_rank // 999) as $imp_r |\n   797→            $item.value + {\n   798→                rrf_score: (1.0 / ($k + $bm25_r)) + (1.0 / ($k + $rel_r)) +\n   799→                          (1.0 / ($k + $rec_r)) + (1.0 / ($k + $imp_r)),\n   800→                bm25_rank: $bm25_r,\n   801→                relevance_rank: $rel_r,\n   802→                recency_rank: $rec_r,\n   803→                importance_rank: $imp_r,\n   804→                combined_accuracy: \"95%+\"\n   805→            }\n   806→        ) | map(.value) |\n   807→        # Sort by RRF score for final ranking (proven 95%+ accuracy)\n   808→        sort_by(-.rrf_score)\n   809→    '\n   810→}\n   811→\n   812→# Legacy RRF function (kept for backward compatibility)\n   813→reciprocal_rank_fusion() {\n   814→    local results_json=\"$1\"\n   815→    local k=\"${2:-60}\"\n   816→    reciprocal_rank_fusion_enhanced \"$results_json\" \"$k\"\n   817→}\n   818→\n   819→# Hybrid retrieval: combines BM25 (keyword) + word overlap (semantic) scoring\n   820→retrieve_hybrid() {\n   821→    local query=\"$1\"\n   822→    local limit=\"${2:-10}\"\n   823→    local recency_weight=\"${3:-0.5}\"\n   824→    local bm25_weight=\"${4:-2.0}\"\n   825→    local relevance_weight=\"${5:-2.0}\"\n   826→    local importance_weight=\"${6:-2.0}\"\n   827→\n   828→    init_memory\n   829→\n   830→    local results=\"[]\"\n   831→\n   832→    # Calculate average document length for BM25\n   833→    local avgdl=50  # Default, could be calculated dynamically\n   834→\n   835→    # Score episodic memories with hybrid approach\n   836→    local episodes\n   837→    episodes=$(jq '.episodes' \"$EPISODIC_MEMORY\")\n   838→\n   839→    while IFS= read -r episode; do\n   840→        if [[ -z \"$episode\" || \"$episode\" == \"null\" ]]; then\n   841→            continue\n   842→        fi\n   843→\n   844→        local description\n   845→        description=$(echo \"$episode\" | jq -r '.description')\n   846→\n   847→        local timestamp\n   848→        timestamp=$(echo \"$episode\" | jq -r '.timestamp')\n   849→\n   850→        local importance\n   851→        importance=$(echo \"$episode\" | jq -r '.importance // 5')\n   852→\n   853→        # Calculate all scores\n   854→        local recency_score\n   855→        recency_score=$(calculate_recency_score \"$timestamp\")\n   856→\n   857→        local relevance_score\n   858→        relevance_score=$(calculate_relevance_score \"$query\" \"$description\")\n   859→\n   860→        local bm25_score\n   861→        bm25_score=$(calculate_bm25_score \"$query\" \"$description\" \"$avgdl\")\n   862→\n   863→        # Normalize importance to 0-1\n   864→        local importance_score\n   865→        importance_score=$(echo \"scale=4; $importance / 10\" | bc -l 2>/dev/null || echo \"0.5\")\n   866→\n   867→        # Combined score with all four factors\n   868→        local final_score\n   869→        final_score=$(echo \"scale=4; ($recency_weight * $recency_score) + ($bm25_weight * $bm25_score) + ($relevance_weight * $relevance_score) + ($importance_weight * $importance_score)\" | bc -l 2>/dev/null || echo \"0\")\n   870→\n   871→        # Add to results with all scores\n   872→        results=$(echo \"$results\" | jq --argjson ep \"$episode\" \\\n   873→            --arg final_score \"$final_score\" \\\n   874→            --arg bm25 \"$bm25_score\" \\\n   875→            --arg relevance \"$relevance_score\" \\\n   876→            --arg recency \"$recency_score\" \\\n   877→            --arg importance \"$importance_score\" \\\n   878→            '. + [($ep + {\n   879→                retrievalScore: ($final_score | tonumber),\n   880→                bm25_score: ($bm25 | tonumber),\n   881→                relevance_score: ($relevance | tonumber),\n   882→                recency_score: ($recency | tonumber),\n   883→                importance_score: ($importance | tonumber),\n   884→                source: \"episodic\"\n   885→            })]')\n   886→    done < <(echo \"$episodes\" | jq -c '.[]')\n   887→\n   888→    # Score patterns with hybrid approach\n   889→    local patterns\n   890→    patterns=$(jq '.patterns' \"$SEMANTIC_MEMORY\")\n   891→\n   892→    while IFS= read -r pattern; do\n   893→        if [[ -z \"$pattern\" || \"$pattern\" == \"null\" ]]; then\n   894→            continue\n   895→        fi\n   896→\n   897→        local trigger\n   898→        trigger=$(echo \"$pattern\" | jq -r '.trigger')\n   899→\n   900→        local timestamp\n   901→        timestamp=$(echo \"$pattern\" | jq -r '.createdAt')\n   902→\n   903→        local success_rate\n   904→        success_rate=$(echo \"$pattern\" | jq -r '.successRate // 1.0')\n   905→\n   906→        # Calculate scores\n   907→        local recency_score\n   908→        recency_score=$(calculate_recency_score \"$timestamp\")\n   909→\n   910→        local relevance_score\n   911→        relevance_score=$(calculate_relevance_score \"$query\" \"$trigger\")\n   912→\n   913→        local bm25_score\n   914→        bm25_score=$(calculate_bm25_score \"$query\" \"$trigger\" \"$avgdl\")\n   915→\n   916→        # Use success rate as importance\n   917→        local importance_score=\"$success_rate\"\n   918→\n   919→        # Combined score\n   920→        local final_score\n   921→        final_score=$(echo \"scale=4; ($recency_weight * $recency_score) + ($bm25_weight * $bm25_score) + ($relevance_weight * $relevance_score) + ($importance_weight * $importance_score)\" | bc -l 2>/dev/null || echo \"0\")\n   922→\n   923→        # Add to results\n   924→        results=$(echo \"$results\" | jq --argjson pat \"$pattern\" \\\n   925→            --arg final_score \"$final_score\" \\\n   926→            --arg bm25 \"$bm25_score\" \\\n   927→            --arg relevance \"$relevance_score\" \\\n   928→            --arg recency \"$recency_score\" \\\n   929→            --arg importance \"$importance_score\" \\\n   930→            '. + [($pat + {\n   931→                retrievalScore: ($final_score | tonumber),\n   932→                bm25_score: ($bm25 | tonumber),\n   933→                relevance_score: ($relevance | tonumber),\n   934→                recency_score: ($recency | tonumber),\n   935→                importance_score: ($importance | tonumber),\n   936→                source: \"pattern\"\n   937→            })]')\n   938→    done < <(echo \"$patterns\" | jq -c '.[]')\n   939→\n   940→    # Sort by score and return top results\n   941→    echo \"$results\" | jq --argjson limit \"$limit\" \\\n   942→        'sort_by(-.retrievalScore) | .[0:$limit]'\n   943→}\n   944→\n   945→# =============================================================================\n   946→# PHASE 3: AST-BASED CHUNKING (Semantic Code Splitting)\n   947→# =============================================================================\n   948→\n   949→# Detect language from file extension\n   950→detect_language() {\n   951→    local filepath=\"$1\"\n   952→    local ext=\"${filepath##*.}\"\n   953→\n   954→    case \"$ext\" in\n   955→        js|jsx) echo \"javascript\" ;;\n   956→        ts|tsx) echo \"typescript\" ;;\n   957→        py) echo \"python\" ;;\n   958→        rb) echo \"ruby\" ;;\n   959→        go) echo \"go\" ;;\n   960→        rs) echo \"rust\" ;;\n   961→        java) echo \"java\" ;;\n   962→        c|h) echo \"c\" ;;\n   963→        cpp|hpp|cc|cxx) echo \"cpp\" ;;\n   964→        sh|bash) echo \"bash\" ;;\n   965→        *) echo \"text\" ;;\n   966→    esac\n   967→}\n   968→\n   969→# Find semantic boundaries (functions, classes) in code\n   970→# Returns line numbers where new chunks should start\n   971→find_semantic_boundaries() {\n   972→    local filepath=\"$1\"\n   973→    local language=\"$2\"\n   974→\n   975→    if [[ ! -f \"$filepath\" ]]; then\n   976→        echo \"[]\"\n   977→        return\n   978→    fi\n   979→\n   980→    local boundaries=\"[]\"\n   981→    local line_num=1\n   982→\n   983→    case \"$language\" in\n   984→        javascript|typescript)\n   985→            # Match: function name(), const name = function(), const name = () =>\n   986→            while IFS= read -r line; do\n   987→                if echo \"$line\" | grep -qE '^\\s*(function\\s+\\w+|const\\s+\\w+\\s*=\\s*(function|\\([^)]*\\)\\s*=>)|class\\s+\\w+|export\\s+(default\\s+)?(function|class))'; then\n   988→                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n   989→                fi\n   990→                ((line_num++))\n   991→            done < \"$filepath\"\n   992→            ;;\n   993→\n   994→        python)\n   995→            # Match: def name, class name\n   996→            while IFS= read -r line; do\n   997→                if echo \"$line\" | grep -qE '^\\s*(def\\s+\\w+|class\\s+\\w+)'; then\n   998→                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n   999→                fi\n  1000→                ((line_num++))\n  1001→            done < \"$filepath\"\n  1002→            ;;\n  1003→\n  1004→        go)\n  1005→            # Match: func name\n  1006→            while IFS= read -r line; do\n  1007→                if echo \"$line\" | grep -qE '^\\s*func\\s+(\\([^)]+\\)\\s*)?\\w+'; then\n  1008→                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n  1009→                fi\n  1010→                ((line_num++))\n  1011→            done < \"$filepath\"\n  1012→            ;;\n  1013→\n  1014→        rust)\n  1015→            # Match: fn name, impl Type, struct/enum Type\n  1016→            while IFS= read -r line; do\n  1017→                if echo \"$line\" | grep -qE '^\\s*(pub\\s+)?(fn\\s+\\w+|impl\\s+|struct\\s+\\w+|enum\\s+\\w+)'; then\n  1018→                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n  1019→                fi\n  1020→                ((line_num++))\n  1021→            done < \"$filepath\"\n  1022→            ;;\n  1023→\n  1024→        java|c|cpp)\n  1025→            # Match: class Name, public/private type name(\n  1026→            while IFS= read -r line; do\n  1027→                if echo \"$line\" | grep -qE '^\\s*((public|private|protected)\\s+)?(class|interface|enum)\\s+\\w+|^\\s*((public|private|protected|static)\\s+)*\\w+\\s+\\w+\\s*\\('; then\n  1028→                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n  1029→                fi\n  1030→                ((line_num++))\n  1031→            done < \"$filepath\"\n  1032→            ;;\n  1033→\n  1034→        bash)\n  1035→            # Match: function_name(), function name\n  1036→            while IFS= read -r line; do\n  1037→                if echo \"$line\" | grep -qE '^\\s*(\\w+\\s*\\(\\)|function\\s+\\w+)'; then\n  1038→                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n  1039→                fi\n  1040→                ((line_num++))\n  1041→            done < \"$filepath\"\n  1042→            ;;\n  1043→    esac\n  1044→\n  1045→    echo \"$boundaries\"\n  1046→}\n  1047→\n  1048→# Chunk code file at semantic boundaries\n  1049→# Returns array of chunks with start/end lines and content\n  1050→chunk_code_file() {\n  1051→    local filepath=\"$1\"\n  1052→    local max_chunk_tokens=\"${2:-500}\"  # Target chunk size in tokens (approx 4 chars = 1 token)\n  1053→    local max_chunk_chars=$((max_chunk_tokens * 4))\n  1054→\n  1055→    if [[ ! -f \"$filepath\" ]]; then\n  1056→        echo \"[]\"\n  1057→        return\n  1058→    fi\n  1059→\n  1060→    local language\n  1061→    language=$(detect_language \"$filepath\")\n  1062→\n  1063→    local boundaries\n  1064→    boundaries=$(find_semantic_boundaries \"$filepath\" \"$language\")\n  1065→\n  1066→    # If no boundaries found, chunk by fixed size\n  1067→    if [[ \"$(echo \"$boundaries\" | jq 'length')\" -eq 0 ]]; then\n  1068→        chunk_file_fixed \"$filepath\" \"$max_chunk_chars\"\n  1069→        return\n  1070→    fi\n  1071→\n  1072→    local chunks=\"[]\"\n  1073→    local total_lines\n  1074→    total_lines=$(wc -l < \"$filepath\" | tr -d ' ')\n  1075→\n  1076→    # Add boundaries for start and end\n  1077→    boundaries=$(echo \"$boundaries\" | jq '. + [1] | sort | unique')\n  1078→    boundaries=$(echo \"$boundaries\" | jq --argjson end \"$((total_lines + 1))\" '. + [$end] | sort | unique')\n  1079→\n  1080→    # Create chunks between boundaries\n  1081→    local boundary_count\n  1082→    boundary_count=$(echo \"$boundaries\" | jq 'length')\n  1083→\n  1084→    for ((i=0; i<boundary_count-1; i++)); do\n  1085→        local start_line\n  1086→        start_line=$(echo \"$boundaries\" | jq -r \".[$i]\")\n  1087→\n  1088→        local end_line\n  1089→        end_line=$(echo \"$boundaries\" | jq -r \".[$((i+1))]\")\n  1090→        end_line=$((end_line - 1))\n  1091→\n  1092→        # Extract chunk content\n  1093→        local content\n  1094→        content=$(sed -n \"${start_line},${end_line}p\" \"$filepath\")\n  1095→\n  1096→        # Check if chunk exceeds max size\n  1097→        local chunk_size=${#content}\n  1098→\n  1099→        if [[ $chunk_size -le $max_chunk_chars ]]; then\n  1100→            # Chunk fits, add it\n  1101→            chunks=$(echo \"$chunks\" | jq --arg content \"$content\" \\\n  1102→                --argjson start \"$start_line\" \\\n  1103→                --argjson end \"$end_line\" \\\n  1104→                --argjson size \"$chunk_size\" \\\n  1105→                '. + [{\n  1106→                    start_line: $start,\n  1107→                    end_line: $end,\n  1108→                    content: $content,\n  1109→                    size_chars: $size,\n  1110→                    size_tokens: ($size / 4 | floor)\n  1111→                }]')\n  1112→        else\n  1113→            # Chunk too large, split further by fixed size\n  1114→            local sub_chunks\n  1115→            sub_chunks=$(echo \"$content\" | fold -w \"$max_chunk_chars\" | jq -R -s 'split(\"\\n\") | map(select(length > 0))')\n  1116→\n  1117→            local sub_chunk_count\n  1118→            sub_chunk_count=$(echo \"$sub_chunks\" | jq 'length')\n  1119→\n  1120→            for ((j=0; j<sub_chunk_count; j++)); do\n  1121→                local sub_content\n  1122→                sub_content=$(echo \"$sub_chunks\" | jq -r \".[$j]\")\n  1123→\n  1124→                local sub_size=${#sub_content}\n  1125→\n  1126→                chunks=$(echo \"$chunks\" | jq --arg content \"$sub_content\" \\\n  1127→                    --argjson start \"$start_line\" \\\n  1128→                    --argjson end \"$end_line\" \\\n  1129→                    --argjson size \"$sub_size\" \\\n  1130→                    '. + [{\n  1131→                        start_line: $start,\n  1132→                        end_line: $end,\n  1133→                        content: $content,\n  1134→                        size_chars: $size,\n  1135→                        size_tokens: ($size / 4 | floor),\n  1136→                        split: true\n  1137→                    }]')\n  1138→            done\n  1139→        fi\n  1140→    done\n  1141→\n  1142→    echo \"$chunks\"\n  1143→}\n  1144→\n  1145→# Chunk file by fixed size (fallback)\n  1146→chunk_file_fixed() {\n  1147→    local filepath=\"$1\"\n  1148→    local max_chars=\"${2:-2000}\"\n  1149→\n  1150→    if [[ ! -f \"$filepath\" ]]; then\n  1151→        echo \"[]\"\n  1152→        return\n  1153→    fi\n  1154→\n  1155→    local content\n  1156→    content=$(cat \"$filepath\")\n  1157→\n  1158→    local chunks=\"[]\"\n  1159→    local chunk_num=0\n  1160→\n  1161→    # Split content into chunks\n  1162→    while [[ -n \"$content\" ]]; do\n  1163→        local chunk=\"${content:0:$max_chars}\"\n  1164→        content=\"${content:$max_chars}\"\n  1165→\n  1166→        local chunk_size=${#chunk}\n  1167→\n  1168→        chunks=$(echo \"$chunks\" | jq --arg content \"$chunk\" \\\n  1169→            --argjson num \"$chunk_num\" \\\n  1170→            --argjson size \"$chunk_size\" \\\n  1171→            '. + [{\n  1172→                chunk_number: $num,\n  1173→                content: $content,\n  1174→                size_chars: $size,\n  1175→                size_tokens: ($size / 4 | floor)\n  1176→            }]')\n  1177→\n  1178→        ((chunk_num++))\n  1179→    done\n  1180→\n  1181→    echo \"$chunks\"\n  1182→}\n  1183→\n  1184→# =============================================================================\n  1185→# PHASE 4: CONTEXT TOKEN BUDGETING\n  1186→# =============================================================================\n  1187→\n  1188→# Configuration file for context budgeting\n  1189→CONTEXT_CONFIG=\"${HOME}/.claude/config/context-budget.json\"\n  1190→\n  1191→# Initialize context budget configuration\n  1192→init_context_budget() {\n  1193→    mkdir -p \"$(dirname \"$CONTEXT_CONFIG\")\"\n  1194→\n  1195→    if [[ ! -f \"$CONTEXT_CONFIG\" ]]; then\n  1196→        cat > \"$CONTEXT_CONFIG\" << 'EOF'\n  1197→{\n  1198→    \"limits\": {\n  1199→        \"total_tokens\": 200000,\n  1200→        \"working_memory\": 20000,\n  1201→        \"episodic_memory\": 50000,\n  1202→        \"semantic_memory\": 30000,\n  1203→        \"actions\": 20000,\n  1204→        \"reserve\": 10000\n  1205→    },\n  1206→    \"thresholds\": {\n  1207→        \"warning\": 0.80,\n  1208→        \"critical\": 0.90\n  1209→    },\n  1210→    \"auto_compact\": true,\n  1211→    \"auto_prune_threshold\": 0.95\n  1212→}\n  1213→EOF\n  1214→    fi\n  1215→}\n  1216→\n  1217→# Estimate token count from text (rough: 1 token ≈ 4 characters)\n  1218→estimate_tokens() {\n  1219→    local text=\"$1\"\n  1220→    local char_count=${#text}\n  1221→    echo $((char_count / 4))\n  1222→}\n  1223→\n  1224→# Calculate current context usage\n  1225→calculate_context_usage() {\n  1226→    init_memory\n  1227→    init_context_budget\n  1228→\n  1229→    local working_content\n  1230→    working_content=$(cat \"$WORKING_MEMORY\" 2>/dev/null || echo \"{}\")\n  1231→\n  1232→    local episodic_content\n  1233→    episodic_content=$(cat \"$EPISODIC_MEMORY\" 2>/dev/null || echo \"{}\")\n  1234→\n  1235→    local semantic_content\n  1236→    semantic_content=$(cat \"$SEMANTIC_MEMORY\" 2>/dev/null || echo \"{}\")\n  1237→\n  1238→    local actions_content\n  1239→    actions_content=$(cat \"$ACTION_LOG\" 2>/dev/null || echo \"\")\n  1240→\n  1241→    # Estimate tokens for each memory type\n  1242→    local working_tokens\n  1243→    working_tokens=$(estimate_tokens \"$working_content\")\n  1244→\n  1245→    local episodic_tokens\n  1246→    episodic_tokens=$(estimate_tokens \"$episodic_content\")\n  1247→\n  1248→    local semantic_tokens\n  1249→    semantic_tokens=$(estimate_tokens \"$semantic_content\")\n  1250→\n  1251→    local actions_tokens\n  1252→    actions_tokens=$(estimate_tokens \"$actions_content\")\n  1253→\n  1254→    local total_tokens=$((working_tokens + episodic_tokens + semantic_tokens + actions_tokens))\n  1255→\n  1256→    # Get limits from config\n  1257→    local total_limit\n  1258→    total_limit=$(jq -r '.limits.total_tokens' \"$CONTEXT_CONFIG\")\n  1259→\n  1260→    local warning_threshold\n  1261→    warning_threshold=$(jq -r '.thresholds.warning' \"$CONTEXT_CONFIG\")\n  1262→\n  1263→    local critical_threshold\n  1264→    critical_threshold=$(jq -r '.thresholds.critical' \"$CONTEXT_CONFIG\")\n  1265→\n  1266→    # Calculate percentages\n  1267→    local usage_pct\n  1268→    usage_pct=$(echo \"scale=2; ($total_tokens / $total_limit) * 100\" | bc -l 2>/dev/null || echo \"0\")\n  1269→\n  1270→    local warning_tokens\n  1271→    warning_tokens=$(echo \"$total_limit * $warning_threshold\" | bc 2>/dev/null || echo \"0\")\n  1272→\n  1273→    local critical_tokens\n  1274→    critical_tokens=$(echo \"$total_limit * $critical_threshold\" | bc 2>/dev/null || echo \"0\")\n  1275→\n  1276→    # Determine status\n  1277→    local status=\"ok\"\n  1278→    if [[ $(echo \"$total_tokens > $critical_tokens\" | bc 2>/dev/null) -eq 1 ]]; then\n  1279→        status=\"critical\"\n  1280→    elif [[ $(echo \"$total_tokens > $warning_tokens\" | bc 2>/dev/null) -eq 1 ]]; then\n  1281→        status=\"warning\"\n  1282→    fi\n  1283→\n  1284→    # Output JSON\n  1285→    jq -n \\\n  1286→        --argjson total \"$total_tokens\" \\\n  1287→        --argjson limit \"$total_limit\" \\\n  1288→        --arg pct \"$usage_pct\" \\\n  1289→        --argjson working \"$working_tokens\" \\\n  1290→        --argjson episodic \"$episodic_tokens\" \\\n  1291→        --argjson semantic \"$semantic_tokens\" \\\n  1292→        --argjson actions \"$actions_tokens\" \\\n  1293→        --arg status \"$status\" \\\n  1294→        '{\n  1295→            total_tokens: $total,\n  1296→            total_limit: $limit,\n  1297→            usage_percent: $pct,\n  1298→            status: $status,\n  1299→            remaining: ($limit - $total),\n  1300→            breakdown: {\n  1301→                working_memory: $working,\n  1302→                episodic_memory: $episodic,\n  1303→                semantic_memory: $semantic,\n  1304→                actions: $actions\n  1305→            }\n  1306→        }'\n  1307→}\n  1308→\n  1309→# Check if context budget is exceeded\n  1310→check_context_budget() {\n  1311→    local usage\n  1312→    usage=$(calculate_context_usage)\n  1313→\n  1314→    local status\n  1315→    status=$(echo \"$usage\" | jq -r '.status')\n  1316→\n  1317→    local usage_pct\n  1318→    usage_pct=$(echo \"$usage\" | jq -r '.usage_percent')\n  1319→\n  1320→    local total\n  1321→    total=$(echo \"$usage\" | jq -r '.total_tokens')\n  1322→\n  1323→    local limit\n  1324→    limit=$(echo \"$usage\" | jq -r '.total_limit')\n  1325→\n  1326→    case \"$status\" in\n  1327→        critical)\n  1328→            echo \"⚠️  CRITICAL: Context budget at ${usage_pct}% ($total/$limit tokens)\"\n  1329→            return 2\n  1330→            ;;\n  1331→        warning)\n  1332→            echo \"⚠️  WARNING: Context budget at ${usage_pct}% ($total/$limit tokens)\"\n  1333→            return 1\n  1334→            ;;\n  1335→        ok)\n  1336→            echo \"✅ OK: Context budget at ${usage_pct}% ($total/$limit tokens)\"\n  1337→            return 0\n  1338→            ;;\n  1339→    esac\n  1340→}\n  1341→\n  1342→# Get context budget remaining\n  1343→context_remaining() {\n  1344→    local usage\n  1345→    usage=$(calculate_context_usage)\n  1346→\n  1347→    echo \"$usage\" | jq -r '.remaining'\n  1348→}\n  1349→\n  1350→# Compact memory to reduce context usage\n  1351→compact_memory() {\n  1352→    init_memory\n  1353→\n  1354→    echo \"Compacting memory to reduce context usage...\"\n  1355→\n  1356→    # Prune old low-importance episodes\n  1357→    local temp_file\n  1358→    temp_file=$(mktemp)\n  1359→\n  1360→    jq '.episodes |= (\n  1361→        sort_by(-.timestamp) |\n  1362→        map(select(.importance >= 5 or (.timestamp | fromdateiso8601) > (now - 604800)))\n  1363→    )' \"$EPISODIC_MEMORY\" > \"$temp_file\"\n  1364→    mv \"$temp_file\" \"$EPISODIC_MEMORY\"\n  1365→\n  1366→    # Prune old low-performing patterns\n  1367→    jq '.patterns |= (\n  1368→        sort_by(-.successRate, -.useCount) |\n  1369→        .[0:50]\n  1370→    )' \"$SEMANTIC_MEMORY\" > \"$temp_file\"\n  1371→    mv \"$temp_file\" \"$SEMANTIC_MEMORY\"\n  1372→\n  1373→    # Truncate action log (keep recent 1000 lines)\n  1374→    tail -n 1000 \"$ACTION_LOG\" > \"$temp_file\"\n  1375→    mv \"$temp_file\" \"$ACTION_LOG\"\n  1376→\n  1377→    echo \"Memory compacted.\"\n  1378→    check_context_budget\n  1379→}\n  1380→\n  1381→# Auto-compact if threshold exceeded\n  1382→auto_compact_if_needed() {\n  1383→    local auto_compact\n  1384→    auto_compact=$(jq -r '.auto_compact' \"$CONTEXT_CONFIG\" 2>/dev/null || echo \"true\")\n  1385→\n  1386→    if [[ \"$auto_compact\" != \"true\" ]]; then\n  1387→        return 0\n  1388→    fi\n  1389→\n  1390→    local auto_prune_threshold\n  1391→    auto_prune_threshold=$(jq -r '.auto_prune_threshold' \"$CONTEXT_CONFIG\" 2>/dev/null || echo \"0.95\")\n  1392→\n  1393→    local usage\n  1394→    usage=$(calculate_context_usage)\n  1395→\n  1396→    local usage_pct\n  1397→    usage_pct=$(echo \"$usage\" | jq -r '.usage_percent')\n  1398→\n  1399→    local threshold_pct\n  1400→    threshold_pct=$(echo \"$auto_prune_threshold * 100\" | bc 2>/dev/null || echo \"95\")\n  1401→\n  1402→    if [[ $(echo \"$usage_pct > $threshold_pct\" | bc 2>/dev/null) -eq 1 ]]; then\n  1403→        log \"Auto-compacting memory (usage: ${usage_pct}%)\"\n  1404→        compact_memory\n  1405→    fi\n  1406→}\n  1407→\n  1408→# Set context budget limits\n  1409→set_context_limit() {\n  1410→    local limit_type=\"$1\"\n  1411→    local limit_value=\"$2\"\n  1412→\n  1413→    init_context_budget\n  1414→\n  1415→    local temp_file\n  1416→    temp_file=$(mktemp)\n  1417→\n  1418→    jq --arg type \"$limit_type\" \\\n  1419→       --argjson value \"$limit_value\" \\\n  1420→       '.limits[$type] = $value' \"$CONTEXT_CONFIG\" > \"$temp_file\"\n  1421→\n  1422→    mv \"$temp_file\" \"$CONTEXT_CONFIG\"\n  1423→\n  1424→    echo \"Updated $limit_type limit to $limit_value tokens\"\n  1425→}\n  1426→\n  1427→# Advanced retrieval with three-factor scoring\n  1428→# Based on Generative Agents: recency + relevance + importance\n  1429→retrieve_scored() {\n  1430→    local query=\"$1\"\n  1431→    local limit=\"${2:-10}\"\n  1432→    local recency_weight=\"${3:-0.5}\"\n  1433→    local relevance_weight=\"${4:-3.0}\"\n  1434→    local importance_weight=\"${5:-2.0}\"\n  1435→\n  1436→    init_memory\n  1437→\n  1438→    local results=\"[]\"\n  1439→\n  1440→    # Score episodic memories\n  1441→    local episodes\n  1442→    episodes=$(jq '.episodes' \"$EPISODIC_MEMORY\")\n  1443→\n  1444→    while IFS= read -r episode; do\n  1445→        if [[ -z \"$episode\" || \"$episode\" == \"null\" ]]; then\n  1446→            continue\n  1447→        fi\n  1448→\n  1449→        local description\n  1450→        description=$(echo \"$episode\" | jq -r '.description')\n  1451→\n  1452→        local timestamp\n  1453→        timestamp=$(echo \"$episode\" | jq -r '.timestamp')\n  1454→\n  1455→        local importance\n  1456→        importance=$(echo \"$episode\" | jq -r '.importance // 5')\n  1457→\n  1458→        # Calculate scores\n  1459→        local recency_score\n  1460→        recency_score=$(calculate_recency_score \"$timestamp\")\n  1461→\n  1462→        local relevance_score\n  1463→        relevance_score=$(calculate_relevance_score \"$query\" \"$description\")\n  1464→\n  1465→        # Normalize importance to 0-1\n  1466→        local importance_score\n  1467→        importance_score=$(echo \"scale=4; $importance / 10\" | bc -l 2>/dev/null || echo \"0.5\")\n  1468→\n  1469→        # Combined score with weights\n  1470→        local final_score\n  1471→        final_score=$(echo \"scale=4; ($recency_weight * $recency_score) + ($relevance_weight * $relevance_score) + ($importance_weight * $importance_score)\" | bc -l 2>/dev/null || echo \"0\")\n  1472→\n  1473→        # Add to results with score\n  1474→        results=$(echo \"$results\" | jq --argjson ep \"$episode\" --arg score \"$final_score\" \\\n  1475→            '. + [($ep + {retrievalScore: ($score | tonumber), source: \"episodic\"})]')\n  1476→    done < <(echo \"$episodes\" | jq -c '.[]')\n  1477→\n  1478→    # Score patterns\n  1479→    local patterns\n  1480→    patterns=$(jq '.patterns' \"$SEMANTIC_MEMORY\")\n  1481→\n  1482→    while IFS= read -r pattern; do\n  1483→        if [[ -z \"$pattern\" || \"$pattern\" == \"null\" ]]; then\n  1484→            continue\n  1485→        fi\n  1486→\n  1487→        local trigger\n  1488→        trigger=$(echo \"$pattern\" | jq -r '.trigger')\n  1489→\n  1490→        local timestamp\n  1491→        timestamp=$(echo \"$pattern\" | jq -r '.createdAt')\n  1492→\n  1493→        local success_rate\n  1494→        success_rate=$(echo \"$pattern\" | jq -r '.successRate // 1.0')\n  1495→\n  1496→        # Calculate scores\n  1497→        local recency_score\n  1498→        recency_score=$(calculate_recency_score \"$timestamp\")\n  1499→\n  1500→        local relevance_score\n  1501→        relevance_score=$(calculate_relevance_score \"$query\" \"$trigger\")\n  1502→\n  1503→        # Use success rate as importance\n  1504→        local importance_score=\"$success_rate\"\n  1505→\n  1506→        # Combined score\n  1507→        local final_score\n  1508→        final_score=$(echo \"scale=4; ($recency_weight * $recency_score) + ($relevance_weight * $relevance_score) + ($importance_weight * $importance_score)\" | bc -l 2>/dev/null || echo \"0\")\n  1509→\n  1510→        # Add to results\n  1511→        results=$(echo \"$results\" | jq --argjson pat \"$pattern\" --arg score \"$final_score\" \\\n  1512→            '. + [($pat + {retrievalScore: ($score | tonumber), source: \"pattern\"})]')\n  1513→    done < <(echo \"$patterns\" | jq -c '.[]')\n  1514→\n  1515→    # Sort by score and return top results\n  1516→    echo \"$results\" | jq --argjson limit \"$limit\" \\\n  1517→        'sort_by(-.retrievalScore) | .[0:$limit]'\n  1518→}\n  1519→\n  1520→# Unified memory search (simple)\n  1521→remember() {\n  1522→    local query=\"$1\"\n  1523→    local limit=\"${2:-10}\"\n  1524→\n  1525→    init_memory\n  1526→\n  1527→    echo \"{\"\n  1528→\n  1529→    # Search episodic memory\n  1530→    echo '\"episodes\":'\n  1531→    search_episodes \"$query\" \"$limit\"\n  1532→    echo \",\"\n  1533→\n  1534→    # Search patterns\n  1535→    echo '\"patterns\":'\n  1536→    find_patterns \"$query\" \"$limit\"\n  1537→    echo \",\"\n  1538→\n  1539→    # Search actions\n  1540→    echo '\"actions\":'\n  1541→    search_actions \"$query\" \"$limit\"\n  1542→\n  1543→    echo \"}\"\n  1544→}\n  1545→\n  1546→# Unified memory search (with scoring)\n  1547→remember_scored() {\n  1548→    local query=\"$1\"\n  1549→    local limit=\"${2:-10}\"\n  1550→\n  1551→    retrieve_scored \"$query\" \"$limit\"\n  1552→}\n  1553→\n  1554→# Get context for current task\n  1555→get_context() {\n  1556→    init_memory\n  1557→\n  1558→    echo \"{\"\n  1559→\n  1560→    # Working memory\n  1561→    echo '\"working\":'\n  1562→    get_working\n  1563→    echo \",\"\n  1564→\n  1565→    # Recent episodes\n  1566→    echo '\"recentEpisodes\":'\n  1567→    get_recent_episodes 5\n  1568→    echo \",\"\n  1569→\n  1570→    # Recent reflections\n  1571→    echo '\"reflections\":'\n  1572→    get_reflections \"\" 3\n  1573→\n  1574→    echo \"}\"\n  1575→}\n  1576→\n  1577→# =============================================================================\n  1578→# MEMORY STATISTICS\n  1579→# =============================================================================\n  1580→\n  1581→get_stats() {\n  1582→    init_memory\n  1583→\n  1584→    local episodic_count\n  1585→    episodic_count=$(jq '.episodes | length' \"$EPISODIC_MEMORY\")\n  1586→\n  1587→    local facts_count\n  1588→    facts_count=$(jq '.facts | length' \"$SEMANTIC_MEMORY\")\n  1589→\n  1590→    local patterns_count\n  1591→    patterns_count=$(jq '.patterns | length' \"$SEMANTIC_MEMORY\")\n  1592→\n  1593→    local actions_count\n  1594→    actions_count=$(wc -l < \"$ACTION_LOG\" 2>/dev/null || echo \"0\")\n  1595→\n  1596→    local reflections_count\n  1597→    reflections_count=$(jq '.reflections | length' \"$REFLECTION_LOG\")\n  1598→\n  1599→    jq -n \\\n  1600→       --argjson episodes \"$episodic_count\" \\\n  1601→       --argjson facts \"$facts_count\" \\\n  1602→       --argjson patterns \"$patterns_count\" \\\n  1603→       --argjson actions \"$actions_count\" \\\n  1604→       --argjson reflections \"$reflections_count\" \\\n  1605→       '{\n  1606→           episodicMemory: $episodes,\n  1607→           semanticFacts: $facts,\n  1608→           learnedPatterns: $patterns,\n  1609→           actionLog: $actions,\n  1610→           reflections: $reflections\n  1611→       }'\n  1612→}\n  1613→\n  1614→# =============================================================================\n  1615→# CHECKPOINT/RESTORE (Session state snapshots)\n  1616→# Based on patterns from GitHub: checkpoint/snapshot/restore implementations\n  1617→# =============================================================================\n  1618→\n  1619→# Create a checkpoint (snapshot memory + git metadata)\n  1620→checkpoint() {\n  1621→    local description=\"${1:-Auto checkpoint}\"\n  1622→\n  1623→    init_memory\n  1624→\n  1625→    local checkpoint_dir=\"$MEMORY_DIR/checkpoints\"\n  1626→    mkdir -p \"$checkpoint_dir\"\n  1627→\n  1628→    local timestamp\n  1629→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n  1630→\n  1631→    local checkpoint_id\n  1632→    checkpoint_id=\"ckpt_$(date +%s)\"\n  1633→\n  1634→    local checkpoint_path=\"$checkpoint_dir/$checkpoint_id.json\"\n  1635→\n  1636→    # Capture git metadata\n  1637→    local git_branch\n  1638→    git_branch=$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo \"unknown\")\n  1639→\n  1640→    local git_commit\n  1641→    git_commit=$(git rev-parse HEAD 2>/dev/null || echo \"unknown\")\n  1642→\n  1643→    local git_dirty\n  1644→    git_dirty=\"false\"\n  1645→    if [[ -d .git ]] && ! git diff-index --quiet HEAD -- 2>/dev/null; then\n  1646→        git_dirty=\"true\"\n  1647→    fi\n  1648→\n  1649→    # Create checkpoint with all memory state\n  1650→    jq -n \\\n  1651→        --arg id \"$checkpoint_id\" \\\n  1652→        --arg desc \"$description\" \\\n  1653→        --arg ts \"$timestamp\" \\\n  1654→        --arg branch \"$git_branch\" \\\n  1655→        --arg commit \"$git_commit\" \\\n  1656→        --arg dirty \"$git_dirty\" \\\n  1657→        --slurpfile working \"$WORKING_MEMORY\" \\\n  1658→        --slurpfile episodic \"$EPISODIC_MEMORY\" \\\n  1659→        --slurpfile semantic \"$SEMANTIC_MEMORY\" \\\n  1660→        --slurpfile reflections \"$REFLECTION_LOG\" \\\n  1661→        '{\n  1662→            id: $id,\n  1663→            description: $desc,\n  1664→            timestamp: $ts,\n  1665→            git: {\n  1666→                branch: $branch,\n  1667→                commit: $commit,\n  1668→                dirty: ($dirty == \"true\")\n  1669→            },\n  1670→            memory: {\n  1671→                working: $working[0],\n  1672→                episodic: $episodic[0],\n  1673→                semantic: $semantic[0],\n  1674→                reflections: $reflections[0]\n  1675→            }\n  1676→        }' > \"$checkpoint_path\"\n  1677→\n  1678→    # Copy action log (JSONL, just copy the file)\n  1679→    if [[ -f \"$ACTION_LOG\" ]]; then\n  1680→        cp \"$ACTION_LOG\" \"$checkpoint_dir/$checkpoint_id.actions.jsonl\"\n  1681→    fi\n  1682→\n  1683→    log \"Created checkpoint: $checkpoint_id - $description\"\n  1684→    echo \"$checkpoint_id\"\n  1685→}\n  1686→\n  1687→# Restore from a checkpoint\n  1688→restore_checkpoint() {\n  1689→    local checkpoint_id=\"$1\"\n  1690→\n  1691→    init_memory\n  1692→\n  1693→    local checkpoint_dir=\"$MEMORY_DIR/checkpoints\"\n  1694→    local checkpoint_path=\"$checkpoint_dir/$checkpoint_id.json\"\n  1695→\n  1696→    if [[ ! -f \"$checkpoint_path\" ]]; then\n  1697→        echo \"Error: Checkpoint $checkpoint_id not found\" >&2\n  1698→        return 1\n  1699→    fi\n  1700→\n  1701→    # Extract and restore memory files\n  1702→    jq -r '.memory.working' \"$checkpoint_path\" > \"$WORKING_MEMORY\"\n  1703→    jq -r '.memory.episodic' \"$checkpoint_path\" > \"$EPISODIC_MEMORY\"\n  1704→    jq -r '.memory.semantic' \"$checkpoint_path\" > \"$SEMANTIC_MEMORY\"\n  1705→    jq -r '.memory.reflections' \"$checkpoint_path\" > \"$REFLECTION_LOG\"\n  1706→\n  1707→    # Restore action log if it exists\n  1708→    local action_log_backup=\"$checkpoint_dir/$checkpoint_id.actions.jsonl\"\n  1709→    if [[ -f \"$action_log_backup\" ]]; then\n  1710→        cp \"$action_log_backup\" \"$ACTION_LOG\"\n  1711→    fi\n  1712→\n  1713→    log \"Restored checkpoint: $checkpoint_id\"\n  1714→\n  1715→    # Return checkpoint info\n  1716→    jq '{id, description, timestamp, git}' \"$checkpoint_path\"\n  1717→}\n  1718→\n  1719→# List available checkpoints\n  1720→list_checkpoints() {\n  1721→    local limit=\"${1:-10}\"\n  1722→\n  1723→    init_memory\n  1724→\n  1725→    local checkpoint_dir=\"$MEMORY_DIR/checkpoints\"\n  1726→\n  1727→    if [[ ! -d \"$checkpoint_dir\" ]]; then\n  1728→        echo \"[]\"\n  1729→        return 0\n  1730→    fi\n  1731→\n  1732→    # Find all checkpoint files, extract metadata, sort by timestamp\n  1733→    local results=\"[]\"\n  1734→    for ckpt_file in \"$checkpoint_dir\"/ckpt_*.json; do\n  1735→        if [[ -f \"$ckpt_file\" ]]; then\n  1736→            local ckpt_data\n  1737→            ckpt_data=$(jq '{id, description, timestamp, git}' \"$ckpt_file\" 2>/dev/null)\n  1738→            if [[ -n \"$ckpt_data\" ]]; then\n  1739→                results=$(echo \"$results\" | jq --argjson ckpt \"$ckpt_data\" '. + [$ckpt]')\n  1740→            fi\n  1741→        fi\n  1742→    done\n  1743→\n  1744→    echo \"$results\" | jq --argjson limit \"$limit\" 'sort_by(.timestamp) | reverse | .[0:$limit]'\n  1745→}\n  1746→\n  1747→# Delete old checkpoints (keep N most recent)\n  1748→prune_checkpoints() {\n  1749→    local keep=\"${1:-5}\"\n  1750→\n  1751→    init_memory\n  1752→\n  1753→    local checkpoint_dir=\"$MEMORY_DIR/checkpoints\"\n  1754→\n  1755→    if [[ ! -d \"$checkpoint_dir\" ]]; then\n  1756→        return 0\n  1757→    fi\n  1758→\n  1759→    # Get list of checkpoints sorted by timestamp (oldest first)\n  1760→    local checkpoints\n  1761→    checkpoints=$(list_checkpoints 1000 | jq -r 'reverse | .[].id')\n  1762→\n  1763→    local count=0\n  1764→    local deleted=0\n  1765→\n  1766→    for ckpt_id in $checkpoints; do\n  1767→        count=$((count + 1))\n  1768→        if [[ $count -le $keep ]]; then\n  1769→            continue\n  1770→        fi\n  1771→\n  1772→        # Delete checkpoint files\n  1773→        rm -f \"$checkpoint_dir/$ckpt_id.json\"\n  1774→        rm -f \"$checkpoint_dir/$ckpt_id.actions.jsonl\"\n  1775→        deleted=$((deleted + 1))\n  1776→    done\n  1777→\n  1778→    log \"Pruned $deleted old checkpoints (kept $keep most recent)\"\n  1779→    echo \"$deleted\"\n  1780→}\n  1781→\n  1782→# =============================================================================\n  1783→# FILE CHANGE DETECTION (SHA-256 hash tracking)\n  1784→# Based on patterns from GitHub: shasum -a 256 / sha256sum implementations\n  1785→# =============================================================================\n  1786→\n  1787→FILE_CACHE=\"$MEMORY_DIR/file-cache.json\"\n  1788→\n  1789→# Initialize file cache\n  1790→init_file_cache() {\n  1791→    if [[ ! -f \"$FILE_CACHE\" ]]; then\n  1792→        echo '{\"files\":{}}' > \"$FILE_CACHE\"\n  1793→    fi\n  1794→}\n  1795→\n  1796→# Get SHA-256 hash of a file (portable: macOS and Linux)\n  1797→get_file_hash() {\n  1798→    local file_path=\"$1\"\n  1799→\n  1800→    if [[ ! -f \"$file_path\" ]]; then\n  1801→        echo \"error: file not found\" >&2\n  1802→        return 1\n  1803→    fi\n  1804→\n  1805→    # Try shasum (macOS) first, fall back to sha256sum (Linux)\n  1806→    if command -v shasum >/dev/null 2>&1; then\n  1807→        shasum -a 256 \"$file_path\" | awk '{print $1}'\n  1808→    elif command -v sha256sum >/dev/null 2>&1; then\n  1809→        sha256sum \"$file_path\" | awk '{print $1}'\n  1810→    else\n  1811→        echo \"error: no SHA-256 tool available\" >&2\n  1812→        return 1\n  1813→    fi\n  1814→}\n  1815→\n  1816→# Cache a file's hash\n  1817→cache_file() {\n  1818→    local file_path=\"$1\"\n  1819→\n  1820→    init_memory\n  1821→    init_file_cache\n  1822→\n  1823→    local hash\n  1824→    hash=$(get_file_hash \"$file_path\")\n  1825→\n  1826→    if [[ $? -ne 0 ]]; then\n  1827→        return 1\n  1828→    fi\n  1829→\n  1830→    local timestamp\n  1831→    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n  1832→\n  1833→    # Update cache\n  1834→    local temp_file\n  1835→    temp_file=$(mktemp)\n  1836→\n  1837→    jq --arg path \"$file_path\" \\\n  1838→       --arg hash \"$hash\" \\\n  1839→       --arg ts \"$timestamp\" \\\n  1840→       '.files[$path] = {hash: $hash, cachedAt: $ts}' \\\n  1841→       \"$FILE_CACHE\" > \"$temp_file\"\n  1842→\n  1843→    mv \"$temp_file\" \"$FILE_CACHE\"\n  1844→\n  1845→    log \"Cached file: $file_path (hash: ${hash:0:8}...)\"\n  1846→    echo \"$hash\"\n  1847→}\n  1848→\n  1849→# Check if a file has changed since last cache\n  1850→file_changed() {\n  1851→    local file_path=\"$1\"\n  1852→\n  1853→    init_memory\n  1854→    init_file_cache\n  1855→\n  1856→    # Get current hash\n  1857→    local current_hash\n  1858→    current_hash=$(get_file_hash \"$file_path\")\n  1859→\n  1860→    if [[ $? -ne 0 ]]; then\n  1861→        return 1\n  1862→    fi\n  1863→\n  1864→    # Get cached hash\n  1865→    local cached_hash\n  1866→    cached_hash=$(jq -r --arg path \"$file_path\" \\\n  1867→        '.files[$path].hash // \"none\"' \"$FILE_CACHE\")\n  1868→\n  1869→    # Compare\n  1870→    if [[ \"$cached_hash\" == \"none\" ]]; then\n  1871→        # File not cached yet\n  1872→        echo \"true\"\n  1873→        return 0\n  1874→    elif [[ \"$current_hash\" != \"$cached_hash\" ]]; then\n  1875→        # File changed\n  1876→        echo \"true\"\n  1877→        return 0\n  1878→    else\n  1879→        # File unchanged\n  1880→        echo \"false\"\n  1881→        return 0\n  1882→    fi\n  1883→}\n  1884→\n  1885→# Get file cache info\n  1886→get_file_cache_info() {\n  1887→    local file_path=\"$1\"\n  1888→\n  1889→    init_memory\n  1890→    init_file_cache\n  1891→\n  1892→    jq --arg path \"$file_path\" \\\n  1893→        '.files[$path] // {error: \"not cached\"}' \\\n  1894→        \"$FILE_CACHE\"\n  1895→}\n  1896→\n  1897→# List all cached files\n  1898→list_cached_files() {\n  1899→    init_memory\n  1900→    init_file_cache\n  1901→\n  1902→    jq -r '.files | keys[]' \"$FILE_CACHE\"\n  1903→}\n  1904→\n  1905→# Clear file cache\n  1906→clear_file_cache() {\n  1907→    init_memory\n  1908→\n  1909→    echo '{\"files\":{}}' > \"$FILE_CACHE\"\n  1910→    log \"Cleared file cache\"\n  1911→}\n  1912→\n  1913→# Prune file cache (remove files that don't exist)\n  1914→prune_file_cache() {\n  1915→    init_memory\n  1916→    init_file_cache\n  1917→\n  1918→    local temp_file\n  1919→    temp_file=$(mktemp)\n  1920→\n  1921→    local pruned=0\n  1922→\n  1923→    # Build new cache with only existing files\n  1924→    local cached_files\n  1925→    cached_files=$(jq -r '.files | keys[]' \"$FILE_CACHE\")\n  1926→\n  1927→    local new_cache='{\"files\":{}}'\n  1928→\n  1929→    for file_path in $cached_files; do\n  1930→        if [[ -f \"$file_path\" ]]; then\n  1931→            # File still exists, keep it\n  1932→            local file_data\n  1933→            file_data=$(jq --arg path \"$file_path\" '.files[$path]' \"$FILE_CACHE\")\n  1934→            new_cache=$(echo \"$new_cache\" | jq --arg path \"$file_path\" \\\n  1935→                --argjson data \"$file_data\" \\\n  1936→                '.files[$path] = $data')\n  1937→        else\n  1938→            # File deleted, don't keep\n  1939→            pruned=$((pruned + 1))\n  1940→        fi\n  1941→    done\n  1942→\n  1943→    echo \"$new_cache\" > \"$FILE_CACHE\"\n  1944→\n  1945→    log \"Pruned $pruned deleted files from cache\"\n  1946→    echo \"$pruned\"\n  1947→}\n  1948→\n  1949→# =============================================================================\n  1950→# COMMAND INTERFACE\n  1951→# =============================================================================\n  1952→\n  1953→case \"${1:-help}\" in\n  1954→    # Working memory\n  1955→    set-task)\n  1956→        set_task \"${2:-}\" \"${3:-}\"\n  1957→        ;;\n  1958→    add-context)\n  1959→        add_context \"${2:-}\" \"${3:-5}\"\n  1960→        ;;\n  1961→    scratchpad)\n  1962→        update_scratchpad \"${2:-}\" \"${3:-true}\"\n  1963→        ;;\n  1964→    get-working)\n  1965→        get_working\n  1966→        ;;\n  1967→    clear-working)\n  1968→        clear_working\n  1969→        ;;\n  1970→\n  1971→    # Episodic memory\n  1972→    record)\n  1973→        record_episode \"${2:-task}\" \"${3:-}\" \"${4:-success}\" \"${5:-}\"\n  1974→        ;;\n  1975→    search-episodes)\n  1976→        search_episodes \"${2:-}\" \"${3:-10}\"\n  1977→        ;;\n  1978→    recent-episodes)\n  1979→        get_recent_episodes \"${2:-10}\" \"${3:-}\"\n  1980→        ;;\n  1981→\n  1982→    # Semantic memory\n  1983→    add-fact)\n  1984→        add_fact \"${2:-}\" \"${3:-}\" \"${4:-}\" \"${5:-0.8}\"\n  1985→        ;;\n  1986→    get-fact)\n  1987→        get_fact \"${2:-}\" \"${3:-}\"\n  1988→        ;;\n  1989→    facts)\n  1990→        get_facts_by_category \"${2:-}\"\n  1991→        ;;\n  1992→    add-pattern)\n  1993→        add_pattern \"${2:-}\" \"${3:-}\" \"${4:-}\" \"${5:-1.0}\"\n  1994→        ;;\n  1995→    find-patterns)\n  1996→        find_patterns \"${2:-}\" \"${3:-5}\"\n  1997→        ;;\n  1998→    set-pref)\n  1999→        add_preference \"${2:-}\" \"${3:-}\"\n  2000→        ;;\n  2001→    get-pref)\n  2002→        get_preference \"${2:-}\" \"${3:-}\"\n  2003→        ;;\n  2004→\n  2005→    # Action log\n  2006→    log-action)\n  2007→        log_action \"${2:-}\" \"${3:-}\" \"${4:-}\" \"${5:-{}}\"\n  2008→        ;;\n  2009→    recent-actions)\n  2010→        get_recent_actions \"${2:-20}\" \"${3:-}\"\n  2011→        ;;\n  2012→    search-actions)\n  2013→        search_actions \"${2:-}\" \"${3:-20}\"\n  2014→        ;;\n  2015→\n  2016→    # Reflection\n  2017→    reflect)\n  2018→        create_reflection \"${2:-general}\" \"${3:-}\" \"${4:-}\"\n  2019→        ;;\n  2020→    reflections)\n  2021→        get_reflections \"${2:-}\" \"${3:-10}\"\n  2022→        ;;\n  2023→\n  2024→    # Retrieval\n  2025→    remember)\n  2026→        remember \"${2:-}\" \"${3:-10}\"\n  2027→        ;;\n  2028→    remember-scored)\n  2029→        remember_scored \"${2:-}\" \"${3:-10}\"\n  2030→        ;;\n  2031→    remember-hybrid|search-hybrid)\n  2032→        retrieve_hybrid \"${2:-}\" \"${3:-10}\"\n  2033→        ;;\n  2034→    context)\n  2035→        get_context\n  2036→        ;;\n  2037→\n  2038→    # Stats\n  2039→    stats)\n  2040→        get_stats\n  2041→        ;;\n  2042→\n  2043→    # Checkpoint/Restore\n  2044→    checkpoint)\n  2045→        checkpoint \"${2:-Auto checkpoint}\"\n  2046→        ;;\n  2047→    restore)\n  2048→        restore_checkpoint \"${2:-}\"\n  2049→        ;;\n  2050→    list-checkpoints)\n  2051→        list_checkpoints \"${2:-10}\"\n  2052→        ;;\n  2053→    prune-checkpoints)\n  2054→        prune_checkpoints \"${2:-5}\"\n  2055→        ;;\n  2056→\n  2057→    # File Change Detection\n  2058→    cache-file)\n  2059→        cache_file \"${2:-}\"\n  2060→        ;;\n  2061→    file-changed)\n  2062→        file_changed \"${2:-}\"\n  2063→        ;;\n  2064→    file-info)\n  2065→        get_file_cache_info \"${2:-}\"\n  2066→        ;;\n  2067→    list-cached)\n  2068→        list_cached_files\n  2069→        ;;\n  2070→    clear-cache)\n  2071→        clear_file_cache\n  2072→        ;;\n  2073→    prune-cache)\n  2074→        prune_file_cache\n  2075→        ;;\n  2076→\n  2077→    # Code Chunking (Phase 3)\n  2078→    chunk-file)\n  2079→        chunk_code_file \"${2:-}\" \"${3:-500}\"\n  2080→        ;;\n  2081→    detect-language)\n  2082→        detect_language \"${2:-}\"\n  2083→        ;;\n  2084→    find-boundaries)\n  2085→        language=\"${3:-$(detect_language \"${2:-}\")}\"\n  2086→        find_semantic_boundaries \"${2:-}\" \"$language\"\n  2087→        ;;\n  2088→\n  2089→    # Context Budgeting (Phase 4)\n  2090→    context-usage)\n  2091→        calculate_context_usage\n  2092→        ;;\n  2093→    context-check)\n  2094→        check_context_budget\n  2095→        ;;\n  2096→    context-remaining)\n  2097→        context_remaining\n  2098→        ;;\n  2099→    context-compact)\n  2100→        compact_memory\n  2101→        ;;\n  2102→    set-context-limit)\n  2103→        set_context_limit \"${2:-total_tokens}\" \"${3:-200000}\"\n  2104→        ;;\n  2105→\n  2106→    init)\n  2107→        init_memory\n  2108→        echo \"Memory initialized at $MEMORY_DIR\"\n  2109→        ;;\n  2110→\n  2111→    scope)\n  2112→        echo \"Memory Scope Configuration\"\n  2113→        echo \"\"\n  2114→        echo \"Current settings:\"\n  2115→        echo \"  MEMORY_DIR: $MEMORY_DIR\"\n  2116→        echo \"  MEMORY_SCOPE: ${MEMORY_SCOPE:-auto}\"\n  2117→        echo \"  Git Channel: $(get_git_channel)\"\n  2118→        echo \"\"\n  2119→        project_root=$(find_project_root 2>/dev/null) && \\\n  2120→            echo \"  Project root: $project_root\" || \\\n  2121→            echo \"  Project root: (none detected)\"\n  2122→        echo \"\"\n  2123→        echo \"To change scope, set MEMORY_SCOPE:\"\n  2124→        echo \"  export MEMORY_SCOPE=project  # Use project-local memory\"\n  2125→        echo \"  export MEMORY_SCOPE=global   # Use global memory\"\n  2126→        echo \"  export MEMORY_SCOPE=auto     # Auto-detect (default)\"\n  2127→        ;;\n  2128→\n  2129→    help|*)\n  2130→        echo \"Memory Manager - Persistent Agent Memory\"\n  2131→        echo \"\"\n  2132→        echo \"Usage: $0 <command> [args]\"\n  2133→        echo \"\"\n  2134→        echo \"Working Memory (current session):\"\n  2135→        echo \"  set-task <task> [context]        - Set current task\"\n  2136→        echo \"  add-context <text> [importance]  - Add context (1-10)\"\n  2137→        echo \"  scratchpad <note> [append]       - Update scratchpad\"\n  2138→        echo \"  get-working                      - Get working memory\"\n  2139→        echo \"  clear-working                    - Clear for new session\"\n  2140→        echo \"\"\n  2141→        echo \"Episodic Memory (past experiences):\"\n  2142→        echo \"  record <type> <desc> [outcome] [details] - Record episode\"\n  2143→        echo \"    Types: task_complete, error_fixed, research_done, pattern_learned\"\n  2144→        echo \"  search-episodes <query> [limit]  - Search episodes\"\n  2145→        echo \"  recent-episodes [limit] [type]   - Get recent episodes\"\n  2146→        echo \"\"\n  2147→        echo \"Semantic Memory (facts & patterns):\"\n  2148→        echo \"  add-fact <category> <key> <value> [confidence]\"\n  2149→        echo \"  get-fact <category> <key>        - Get a fact\"\n  2150→        echo \"  facts <category>                 - Get category facts\"\n  2151→        echo \"  add-pattern <type> <trigger> <solution> [success_rate]\"\n  2152→        echo \"  find-patterns <query> [limit]    - Find matching patterns\"\n  2153→        echo \"  set-pref <key> <value>           - Set preference\"\n  2154→        echo \"  get-pref <key> [default]         - Get preference\"\n  2155→        echo \"\"\n  2156→        echo \"Action Log:\"\n  2157→        echo \"  log-action <type> <desc> [result] [metadata]\"\n  2158→        echo \"  recent-actions [limit] [type]    - Get recent actions\"\n  2159→        echo \"  search-actions <query> [limit]   - Search actions\"\n  2160→        echo \"\"\n  2161→        echo \"Reflection:\"\n  2162→        echo \"  reflect <focus> <content> [insights]\"\n  2163→        echo \"  reflections [focus] [limit]      - Get reflections\"\n  2164→        echo \"\"\n  2165→        echo \"Retrieval:\"\n  2166→        echo \"  remember <query> [limit]         - Search all memory (simple)\"\n  2167→        echo \"  remember-scored <query> [limit]  - Search with 3-factor scoring\"\n  2168→        echo \"                                     (recency + relevance + importance)\"\n  2169→        echo \"  remember-hybrid <query> [limit]  - Hybrid search (BM25 + semantic)\"\n  2170→        echo \"                                     (recency + BM25 + relevance + importance)\"\n  2171→        echo \"  context                          - Get current context\"\n  2172→        echo \"\"\n  2173→        echo \"Checkpoint/Restore:\"\n  2174→        echo \"  checkpoint [description]         - Create memory snapshot\"\n  2175→        echo \"  restore <checkpoint_id>          - Restore from checkpoint\"\n  2176→        echo \"  list-checkpoints [limit]         - List available checkpoints\"\n  2177→        echo \"  prune-checkpoints [keep]         - Delete old checkpoints (keep N)\"\n  2178→        echo \"\"\n  2179→        echo \"File Change Detection:\"\n  2180→        echo \"  cache-file <path>                - Cache file hash\"\n  2181→        echo \"  file-changed <path>              - Check if file changed\"\n  2182→        echo \"  file-info <path>                 - Get cache info for file\"\n  2183→        echo \"  list-cached                      - List all cached files\"\n  2184→        echo \"  clear-cache                      - Clear file cache\"\n  2185→        echo \"  prune-cache                      - Remove deleted files from cache\"\n  2186→        echo \"\"\n  2187→        echo \"Code Chunking (Phase 3):\"\n  2188→        echo \"  chunk-file <path> [tokens]       - Chunk code at semantic boundaries\"\n  2189→        echo \"                                     (default: 500 tokens per chunk)\"\n  2190→        echo \"  detect-language <path>           - Detect programming language\"\n  2191→        echo \"  find-boundaries <path> [lang]    - Find function/class boundaries\"\n  2192→        echo \"\"\n  2193→        echo \"Context Budgeting (Phase 4):\"\n  2194→        echo \"  context-usage                    - Show current context token usage\"\n  2195→        echo \"  context-check                    - Check if budget exceeded\"\n  2196→        echo \"  context-remaining                - Show remaining token budget\"\n  2197→        echo \"  context-compact                  - Compact memory to reduce usage\"\n  2198→        echo \"  set-context-limit <type> <value> - Set context limit\"\n  2199→        echo \"                                     (types: total_tokens, working_memory, etc.)\"\n  2200→        echo \"\"\n  2201→        echo \"Management:\"\n  2202→        echo \"  stats                            - Memory statistics\"\n  2203→        echo \"  init                             - Initialize memory\"\n  2204→        echo \"  scope                            - Show memory scope info\"\n  2205→        echo \"\"\n  2206→        echo \"Environment:\"\n  2207→        echo \"  MEMORY_SCOPE=auto|project|global - Control memory location\"\n  2208→        echo \"  Current: $MEMORY_DIR\"\n  2209→        ;;\n  2210→esac\n  2211→\n\n<system-reminder>\nWhenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.\n</system-reminder>\n"}]},"uuid":"d4c2dc16-17bc-4647-96f3-98979f02dbb4","timestamp":"2026-01-12T21:32:09.400Z","toolUseResult":{"type":"text","file":{"filePath":"/Users/imorgado/.claude/hooks/memory-manager.sh","content":"#!/bin/bash\n# Memory Manager - Persistent memory for Claude Code sessions\n# Based on patterns from: MemGPT/Letta, Generative Agents (Stanford), Mem0, LangChain, CrewAI\n#\n# Supports both global and project-scoped memory:\n# - Global: ~/.claude/memory/ (shared across all projects)\n# - Project: .claude/memory/ (project-specific, auto-detected)\n#\n# Set MEMORY_SCOPE=project to force project-local memory\n# Set MEMORY_SCOPE=global to force global memory\n\nset -uo pipefail\n\n# Detect project root (look for .git, package.json, Cargo.toml, etc.)\nfind_project_root() {\n    local dir=\"$PWD\"\n    while [[ \"$dir\" != \"/\" ]]; do\n        if [[ -d \"$dir/.git\" ]] || [[ -f \"$dir/package.json\" ]] || \\\n           [[ -f \"$dir/Cargo.toml\" ]] || [[ -f \"$dir/go.mod\" ]] || \\\n           [[ -f \"$dir/pyproject.toml\" ]] || [[ -f \"$dir/CLAUDE.md\" ]]; then\n            echo \"$dir\"\n            return 0\n        fi\n        dir=\"$(dirname \"$dir\")\"\n    done\n    return 1\n}\n\n# Get git channel (branch name, sanitized)\n# Based on patterns from GitHub: git rev-parse --abbrev-ref HEAD\nget_git_channel() {\n    local branch\n\n    # Check if we're in a git repo\n    if git rev-parse --git-dir >/dev/null 2>&1; then\n        branch=$(git rev-parse --abbrev-ref HEAD 2>/dev/null)\n\n        # Handle edge case: newly initialized repo with no commits returns \"HEAD\"\n        if [[ \"$branch\" == \"HEAD\" ]]; then\n            # Try to get default branch name from git config\n            branch=$(git config --get init.defaultBranch 2>/dev/null || echo \"main\")\n        fi\n    else\n        branch=\"main\"\n    fi\n\n    # Sanitize branch name: replace non-alphanumeric with dashes\n    echo \"$branch\" | sed 's/[^a-zA-Z0-9_-]/-/g'\n}\n\n# Determine memory directory based on scope\nget_memory_dir() {\n    local scope=\"${MEMORY_SCOPE:-auto}\"\n    local channel\n    channel=$(get_git_channel)\n\n    if [[ \"$scope\" == \"project\" ]]; then\n        local project_root\n        if project_root=$(find_project_root); then\n            echo \"$project_root/.claude/memory/$channel\"\n            return 0\n        fi\n    elif [[ \"$scope\" == \"global\" ]]; then\n        echo \"${HOME}/.claude/memory/$channel\"\n        return 0\n    fi\n\n    # Auto mode: use project memory if in a project, otherwise global\n    local project_root\n    if project_root=$(find_project_root); then\n        echo \"$project_root/.claude/memory/$channel\"\n    else\n        echo \"${HOME}/.claude/memory/$channel\"\n    fi\n}\n\nMEMORY_DIR=\"$(get_memory_dir)\"\nWORKING_MEMORY=\"$MEMORY_DIR/working.json\"\nEPISODIC_MEMORY=\"$MEMORY_DIR/episodic.json\"\nSEMANTIC_MEMORY=\"$MEMORY_DIR/semantic.json\"\nACTION_LOG=\"$MEMORY_DIR/actions.jsonl\"\nREFLECTION_LOG=\"$MEMORY_DIR/reflections.json\"\nLOG_FILE=\"${HOME}/.claude/memory-manager.log\"\n\n# Memory limits\nMAX_WORKING_ITEMS=\"${MAX_WORKING_ITEMS:-50}\"\nMAX_EPISODIC_ITEMS=\"${MAX_EPISODIC_ITEMS:-1000}\"\nMAX_SEMANTIC_ITEMS=\"${MAX_SEMANTIC_ITEMS:-500}\"\n\nlog() {\n    echo \"[$(date '+%Y-%m-%d %H:%M:%S')] $1\" >> \"$LOG_FILE\"\n}\n\ninit_memory() {\n    mkdir -p \"$MEMORY_DIR\"\n\n    # Initialize working memory (current session state)\n    if [[ ! -f \"$WORKING_MEMORY\" ]]; then\n        cat > \"$WORKING_MEMORY\" << 'EOF'\n{\n    \"currentTask\": null,\n    \"currentContext\": [],\n    \"recentActions\": [],\n    \"pendingItems\": [],\n    \"scratchpad\": \"\",\n    \"lastUpdated\": null\n}\nEOF\n    fi\n\n    # Initialize episodic memory (past experiences)\n    if [[ ! -f \"$EPISODIC_MEMORY\" ]]; then\n        echo '{\"episodes\":[]}' > \"$EPISODIC_MEMORY\"\n    fi\n\n    # Initialize semantic memory (facts and knowledge)\n    if [[ ! -f \"$SEMANTIC_MEMORY\" ]]; then\n        echo '{\"facts\":[],\"patterns\":[],\"preferences\":[]}' > \"$SEMANTIC_MEMORY\"\n    fi\n\n    # Initialize reflections\n    if [[ ! -f \"$REFLECTION_LOG\" ]]; then\n        echo '{\"reflections\":[]}' > \"$REFLECTION_LOG\"\n    fi\n\n    # Initialize action log (JSONL)\n    if [[ ! -f \"$ACTION_LOG\" ]]; then\n        touch \"$ACTION_LOG\"\n    fi\n}\n\n# =============================================================================\n# WORKING MEMORY (Short-term, current session)\n# Based on: MemGPT core memory, Generative Agents scratch\n# =============================================================================\n\n# Set current task\nset_task() {\n    local task=\"$1\"\n    local context=\"${2:-}\"\n\n    init_memory\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    jq --arg task \"$task\" \\\n       --arg context \"$context\" \\\n       --arg ts \"$timestamp\" \\\n       '\n       .currentTask = $task |\n       .currentContext = (if $context != \"\" then [{content: $context, importance: 5, addedAt: $ts}] else [] end) |\n       .lastUpdated = $ts\n       ' \"$WORKING_MEMORY\" > \"$temp_file\"\n\n    mv \"$temp_file\" \"$WORKING_MEMORY\"\n    log \"Set task: $task\"\n}\n\n# Add to current context\nadd_context() {\n    local context=\"$1\"\n    local importance=\"${2:-5}\"  # 1-10 scale\n\n    init_memory\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    jq --arg ctx \"$context\" \\\n       --argjson imp \"$importance\" \\\n       --arg ts \"$timestamp\" \\\n       '\n       .currentContext += [{\n           content: $ctx,\n           importance: $imp,\n           addedAt: $ts\n       }] |\n       .currentContext = (.currentContext | sort_by(-.importance) | .[0:20]) |\n       .lastUpdated = $ts\n       ' \"$WORKING_MEMORY\" > \"$temp_file\"\n\n    mv \"$temp_file\" \"$WORKING_MEMORY\"\n    log \"Added context (importance: $importance)\"\n}\n\n# Update scratchpad (quick notes)\nupdate_scratchpad() {\n    local note=\"$1\"\n    local append=\"${2:-true}\"\n\n    init_memory\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    if [[ \"$append\" == \"true\" ]]; then\n        jq --arg note \"$note\" \\\n           --arg ts \"$timestamp\" \\\n           '\n           .scratchpad = (.scratchpad + \"\\n\" + $ts + \": \" + $note) |\n           .lastUpdated = $ts\n           ' \"$WORKING_MEMORY\" > \"$temp_file\"\n    else\n        jq --arg note \"$note\" \\\n           --arg ts \"$timestamp\" \\\n           '\n           .scratchpad = $note |\n           .lastUpdated = $ts\n           ' \"$WORKING_MEMORY\" > \"$temp_file\"\n    fi\n\n    mv \"$temp_file\" \"$WORKING_MEMORY\"\n}\n\n# Get working memory state\nget_working() {\n    init_memory\n    jq '.' \"$WORKING_MEMORY\"\n}\n\n# Clear working memory (new session)\nclear_working() {\n    init_memory\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    cat > \"$WORKING_MEMORY\" << EOF\n{\n    \"currentTask\": null,\n    \"currentContext\": [],\n    \"recentActions\": [],\n    \"pendingItems\": [],\n    \"scratchpad\": \"\",\n    \"lastUpdated\": \"$timestamp\"\n}\nEOF\n    log \"Cleared working memory\"\n}\n\n# =============================================================================\n# EPISODIC MEMORY (Past experiences/episodes)\n# Based on: Generative Agents memory stream, MemGPT archival memory\n# =============================================================================\n\n# Record an episode (completed task/action)\nrecord_episode() {\n    local type=\"$1\"        # task_complete, error_fixed, research_done, etc.\n    local description=\"$2\"\n    local outcome=\"${3:-success}\"\n    local details=\"${4:-}\"\n\n    init_memory\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    local episode_id\n    episode_id=\"ep_$(date +%s%N | cut -c1-13)\"\n\n    # Calculate importance based on type and outcome\n    local importance=5\n    case \"$type\" in\n        error_fixed) importance=8 ;;\n        task_complete) importance=7 ;;\n        research_done) importance=6 ;;\n        pattern_learned) importance=9 ;;\n        failure) importance=7 ;;\n    esac\n\n    jq --arg id \"$episode_id\" \\\n       --arg type \"$type\" \\\n       --arg desc \"$description\" \\\n       --arg outcome \"$outcome\" \\\n       --arg details \"$details\" \\\n       --argjson imp \"$importance\" \\\n       --arg ts \"$timestamp\" \\\n       '\n       .episodes = [{\n           id: $id,\n           type: $type,\n           description: $desc,\n           outcome: $outcome,\n           details: $details,\n           importance: $imp,\n           timestamp: $ts,\n           accessCount: 0,\n           lastAccessed: null\n       }] + .episodes |\n       .episodes = .episodes[0:'\"$MAX_EPISODIC_ITEMS\"']\n       ' \"$EPISODIC_MEMORY\" > \"$temp_file\"\n\n    mv \"$temp_file\" \"$EPISODIC_MEMORY\"\n    log \"Recorded episode: $type - $description\"\n    echo \"$episode_id\"\n}\n\n# Search episodes (simple text match)\nsearch_episodes() {\n    local query=\"$1\"\n    local limit=\"${2:-10}\"\n\n    init_memory\n\n    jq --arg q \"$query\" \\\n       --argjson limit \"$limit\" \\\n       '\n       .episodes |\n       map(select(\n           (.description | ascii_downcase | contains($q | ascii_downcase)) or\n           (.details | ascii_downcase | contains($q | ascii_downcase)) or\n           (.type | ascii_downcase | contains($q | ascii_downcase))\n       )) |\n       sort_by(.importance | (- if type == \"number\" then . else 0 end)) |\n       .[0:$limit]\n       ' \"$EPISODIC_MEMORY\"\n}\n\n# Get recent episodes\nget_recent_episodes() {\n    local limit=\"${1:-10}\"\n    local type_filter=\"${2:-}\"\n\n    init_memory\n\n    if [[ -n \"$type_filter\" ]]; then\n        jq --arg type \"$type_filter\" \\\n           --argjson limit \"$limit\" \\\n           '.episodes | map(select(.type == $type)) | .[0:$limit]' \"$EPISODIC_MEMORY\"\n    else\n        jq --argjson limit \"$limit\" \\\n           '.episodes | .[0:$limit]' \"$EPISODIC_MEMORY\"\n    fi\n}\n\n# =============================================================================\n# SEMANTIC MEMORY (Facts, patterns, preferences)\n# Based on: MemGPT persona/human blocks, knowledge graphs\n# =============================================================================\n\n# Add a fact\nadd_fact() {\n    local category=\"$1\"    # project, user, tool, api, etc.\n    local key=\"$2\"\n    local value=\"$3\"\n    local confidence=\"${4:-0.8}\"\n\n    init_memory\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    # Update or insert fact\n    jq --arg cat \"$category\" \\\n       --arg key \"$key\" \\\n       --arg val \"$value\" \\\n       --argjson conf \"$confidence\" \\\n       --arg ts \"$timestamp\" \\\n       '\n       .facts = [.facts[] | select(.category != $cat or .key != $key)] + [{\n           category: $cat,\n           key: $key,\n           value: $val,\n           confidence: $conf,\n           updatedAt: $ts\n       }] |\n       .facts = (.facts | sort_by(.category, .key) | .[0:'\"$MAX_SEMANTIC_ITEMS\"'])\n       ' \"$SEMANTIC_MEMORY\" > \"$temp_file\"\n\n    mv \"$temp_file\" \"$SEMANTIC_MEMORY\"\n    log \"Added fact: $category/$key\"\n}\n\n# Get a fact\nget_fact() {\n    local category=\"$1\"\n    local key=\"$2\"\n\n    init_memory\n\n    jq --arg cat \"$category\" \\\n       --arg key \"$key\" \\\n       '.facts[] | select(.category == $cat and .key == $key)' \"$SEMANTIC_MEMORY\"\n}\n\n# Get all facts in category\nget_facts_by_category() {\n    local category=\"$1\"\n\n    init_memory\n\n    jq --arg cat \"$category\" \\\n       '.facts | map(select(.category == $cat))' \"$SEMANTIC_MEMORY\"\n}\n\n# Add a learned pattern\nadd_pattern() {\n    local pattern_type=\"$1\"   # error_fix, optimization, workflow, etc.\n    local trigger=\"$2\"        # What triggers this pattern\n    local solution=\"$3\"       # The solution/action\n    local success_rate=\"${4:-1.0}\"\n\n    init_memory\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    local pattern_id\n    pattern_id=\"pat_$(date +%s%N | cut -c1-13)\"\n\n    jq --arg id \"$pattern_id\" \\\n       --arg type \"$pattern_type\" \\\n       --arg trigger \"$trigger\" \\\n       --arg solution \"$solution\" \\\n       --argjson rate \"$success_rate\" \\\n       --arg ts \"$timestamp\" \\\n       '\n       .patterns = [{\n           id: $id,\n           type: $type,\n           trigger: $trigger,\n           solution: $solution,\n           successRate: $rate,\n           useCount: 0,\n           createdAt: $ts\n       }] + .patterns |\n       .patterns = .patterns[0:200]\n       ' \"$SEMANTIC_MEMORY\" > \"$temp_file\"\n\n    mv \"$temp_file\" \"$SEMANTIC_MEMORY\"\n    log \"Added pattern: $pattern_type\"\n    echo \"$pattern_id\"\n}\n\n# Find matching patterns\nfind_patterns() {\n    local query=\"$1\"\n    local limit=\"${2:-5}\"\n\n    init_memory\n\n    jq --arg q \"$query\" \\\n       --argjson limit \"$limit\" \\\n       '\n       .patterns |\n       map(select(\n           (.trigger | ascii_downcase | contains($q | ascii_downcase)) or\n           (.type | ascii_downcase | contains($q | ascii_downcase))\n       )) |\n       sort_by(-.successRate, -.useCount) |\n       .[0:$limit]\n       ' \"$SEMANTIC_MEMORY\"\n}\n\n# Add user preference\nadd_preference() {\n    local key=\"$1\"\n    local value=\"$2\"\n\n    init_memory\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    jq --arg key \"$key\" \\\n       --arg val \"$value\" \\\n       --arg ts \"$timestamp\" \\\n       '\n       .preferences = [.preferences[] | select(.key != $key)] + [{\n           key: $key,\n           value: $val,\n           updatedAt: $ts\n       }]\n       ' \"$SEMANTIC_MEMORY\" > \"$temp_file\"\n\n    mv \"$temp_file\" \"$SEMANTIC_MEMORY\"\n    log \"Set preference: $key\"\n}\n\n# Get preference\nget_preference() {\n    local key=\"$1\"\n    local default=\"${2:-}\"\n\n    init_memory\n\n    local value\n    value=$(jq -r --arg key \"$key\" \\\n       '.preferences[] | select(.key == $key) | .value' \"$SEMANTIC_MEMORY\")\n\n    if [[ -n \"$value\" && \"$value\" != \"null\" ]]; then\n        echo \"$value\"\n    else\n        echo \"$default\"\n    fi\n}\n\n# =============================================================================\n# ACTION LOG (Append-only log of all actions)\n# Based on: Generative Agents memory stream\n# =============================================================================\n\n# Log an action\nlog_action() {\n    local action_type=\"$1\"   # tool_call, edit, search, etc.\n    local description=\"$2\"\n    local result=\"${3:-}\"\n    local metadata=\"${4:-}\"\n\n    init_memory\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    local action_id\n    action_id=\"act_$(date +%s%N | cut -c1-13)\"\n\n    # Validate or default metadata to empty object\n    if [[ -z \"$metadata\" ]] || ! echo \"$metadata\" | jq -e . >/dev/null 2>&1; then\n        metadata='{}'\n    fi\n\n    # Append to JSONL file\n    jq -n -c \\\n       --arg id \"$action_id\" \\\n       --arg type \"$action_type\" \\\n       --arg desc \"$description\" \\\n       --arg result \"$result\" \\\n       --argjson meta \"$metadata\" \\\n       --arg ts \"$timestamp\" \\\n       '{\n           id: $id,\n           type: $type,\n           description: $desc,\n           result: $result,\n           metadata: $meta,\n           timestamp: $ts\n       }' >> \"$ACTION_LOG\"\n\n    log \"Logged action: $action_type\"\n    echo \"$action_id\"\n}\n\n# Get recent actions\nget_recent_actions() {\n    local limit=\"${1:-20}\"\n    local type_filter=\"${2:-}\"\n\n    init_memory\n\n    if [[ ! -f \"$ACTION_LOG\" ]]; then\n        echo \"[]\"\n        return\n    fi\n\n    if [[ -n \"$type_filter\" ]]; then\n        tail -n 1000 \"$ACTION_LOG\" | jq -s --arg type \"$type_filter\" \\\n           'map(select(.type == $type)) | reverse | .[0:'\"$limit\"']'\n    else\n        tail -n \"$limit\" \"$ACTION_LOG\" | jq -s 'reverse'\n    fi\n}\n\n# Search action log\nsearch_actions() {\n    local query=\"$1\"\n    local limit=\"${2:-20}\"\n\n    init_memory\n\n    if [[ ! -f \"$ACTION_LOG\" ]]; then\n        echo \"[]\"\n        return\n    fi\n\n    grep -i \"$query\" \"$ACTION_LOG\" 2>/dev/null | tail -n \"$limit\" | jq -s '.'\n}\n\n# =============================================================================\n# REFLECTION (Memory consolidation)\n# Based on: Generative Agents reflection, MemGPT summarization\n# =============================================================================\n\n# Create a reflection (consolidate recent experiences)\ncreate_reflection() {\n    local focus=\"${1:-general}\"  # general, errors, patterns, progress\n    local content=\"$2\"\n    local insights=\"${3:-}\"\n\n    init_memory\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    local reflection_id\n    reflection_id=\"ref_$(date +%s)\"\n\n    jq --arg id \"$reflection_id\" \\\n       --arg focus \"$focus\" \\\n       --arg content \"$content\" \\\n       --arg insights \"$insights\" \\\n       --arg ts \"$timestamp\" \\\n       '\n       .reflections = [{\n           id: $id,\n           focus: $focus,\n           content: $content,\n           insights: $insights,\n           timestamp: $ts\n       }] + .reflections |\n       .reflections = .reflections[0:100]\n       ' \"$REFLECTION_LOG\" > \"$temp_file\"\n\n    mv \"$temp_file\" \"$REFLECTION_LOG\"\n    log \"Created reflection: $focus\"\n    echo \"$reflection_id\"\n}\n\n# Get reflections\nget_reflections() {\n    local focus=\"${1:-}\"\n    local limit=\"${2:-10}\"\n\n    init_memory\n\n    if [[ -n \"$focus\" ]]; then\n        jq --arg focus \"$focus\" \\\n           --argjson limit \"$limit\" \\\n           '.reflections | map(select(.focus == $focus)) | .[0:$limit]' \"$REFLECTION_LOG\"\n    else\n        jq --argjson limit \"$limit\" \\\n           '.reflections | .[0:$limit]' \"$REFLECTION_LOG\"\n    fi\n}\n\n# =============================================================================\n# MEMORY RETRIEVAL (Combined search across all memory types)\n# Based on: Generative Agents retrieval (recency + relevance + importance)\n# =============================================================================\n\n# Calculate recency score with exponential decay\n# Formula: decay_rate ^ hours_since_access\ncalculate_recency_score() {\n    local timestamp=\"$1\"\n    local decay_rate=\"${2:-0.995}\"\n\n    # Get hours since timestamp\n    local now_epoch\n    now_epoch=$(date +%s)\n\n    local ts_epoch\n    ts_epoch=$(date -j -f \"%Y-%m-%dT%H:%M:%SZ\" \"$timestamp\" +%s 2>/dev/null || echo \"$now_epoch\")\n\n    local hours_ago\n    hours_ago=$(( (now_epoch - ts_epoch) / 3600 ))\n\n    # Calculate decay (using bc for floating point)\n    echo \"scale=4; e(l($decay_rate) * $hours_ago)\" | bc -l 2>/dev/null || echo \"0.5\"\n}\n\n# Calculate relevance score (keyword overlap)\ncalculate_relevance_score() {\n    local query=\"$1\"\n    local content=\"$2\"\n\n    # Normalize to lowercase and split into words\n    local query_words\n    query_words=$(echo \"$query\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]' '\\n' | sort -u)\n\n    local content_words\n    content_words=$(echo \"$content\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]' '\\n' | sort -u)\n\n    # Count matching words\n    local matches=0\n    local total=0\n    for word in $query_words; do\n        total=$((total + 1))\n        if echo \"$content_words\" | grep -qw \"$word\"; then\n            matches=$((matches + 1))\n        fi\n    done\n\n    # Return overlap ratio\n    if [[ $total -gt 0 ]]; then\n        echo \"scale=4; $matches / $total\" | bc -l 2>/dev/null || echo \"0\"\n    else\n        echo \"0\"\n    fi\n}\n\n# =============================================================================\n# PHASE 2: HYBRID SEARCH (BM25 + Semantic)\n# =============================================================================\n\n# Calculate BM25 score for a document given a query\n# BM25 parameters: k1 (term frequency saturation), b (document length normalization)\n# Formula: BM25(q,d) = Σ IDF(qi) * (f(qi,d) * (k1+1)) / (f(qi,d) + k1 * (1-b + b * (|d|/avgdl)))\ncalculate_bm25_score() {\n    local query=\"$1\"\n    local content=\"$2\"\n    local avgdl=\"${3:-50}\"     # Average document length (words)\n    local k1=\"${4:-1.5}\"       # Term frequency saturation (1.2-2.0)\n    local b=\"${5:-0.75}\"       # Length normalization (0-1)\n\n    # Normalize and tokenize\n    local query_words\n    query_words=$(echo \"$query\" | tr '[:upper:]' '[:lower:]' | tr -cs '[:alnum:]' '\\n' | grep -v '^$')\n\n    local content_lower\n    content_lower=$(echo \"$content\" | tr '[:upper:]' '[:lower:]')\n\n    # Get document length\n    local doc_length\n    doc_length=$(echo \"$content_lower\" | tr -cs '[:alnum:]' '\\n' | grep -v '^$' | wc -l | tr -d ' ')\n\n    # Calculate BM25 score\n    local bm25_score=0\n    local idf_score\n    local tf\n    local numerator\n    local denominator\n    local term_score\n\n    for term in $query_words; do\n        # Term frequency in document\n        tf=$(echo \"$content_lower\" | tr -cs '[:alnum:]' '\\n' | grep -wc \"$term\" || echo \"0\")\n\n        # Simple IDF approximation (log(1 + 1/df) where df=1 for simplicity)\n        # In a full implementation, we'd calculate across all documents\n        idf_score=1.0\n\n        # BM25 formula\n        numerator=$(echo \"scale=4; $tf * ($k1 + 1)\" | bc -l 2>/dev/null || echo \"0\")\n        denominator=$(echo \"scale=4; $tf + $k1 * (1 - $b + $b * ($doc_length / $avgdl))\" | bc -l 2>/dev/null || echo \"1\")\n\n        # Check if denominator is positive (avoid division by zero)\n        if [[ -n \"$denominator\" ]] && [[ $(echo \"$denominator > 0\" | bc -l 2>/dev/null || echo \"0\") == \"1\" ]]; then\n            term_score=$(echo \"scale=4; $idf_score * ($numerator / $denominator)\" | bc -l 2>/dev/null || echo \"0\")\n            bm25_score=$(echo \"scale=4; $bm25_score + $term_score\" | bc -l 2>/dev/null || echo \"$bm25_score\")\n        fi\n    done\n\n    # Normalize to 0-1 range (approximate)\n    echo \"scale=4; if ($bm25_score > 10) 1.0 else $bm25_score / 10\" | bc -l 2>/dev/null || echo \"0\"\n}\n\n# Reciprocal Rank Fusion - combines rankings from multiple sources\n# ENHANCED: Now combines BM25, word overlap, recency, and importance\n# RRF(d) = Σ 1/(k + rank(d)) where k=60 is standard\n# Research shows RRF achieves 95%+ accuracy when combining multiple signals\nreciprocal_rank_fusion_enhanced() {\n    local results_json=\"$1\"  # JSON array with items having multiple scores\n    local k=\"${2:-60}\"       # Constant (typically 60, proven optimal)\n\n    # Calculate RRF score for each item across ALL ranking dimensions\n    echo \"$results_json\" | jq --argjson k \"$k\" '\n        # Rank by BM25 score (exact term matching)\n        (. | sort_by(-.bm25_score) | to_entries | map({id: .key, bm25_rank: .key + 1})) as $bm25_ranks |\n        # Rank by relevance score (semantic word overlap)\n        (. | sort_by(-.relevance_score) | to_entries | map({id: .key, rel_rank: .key + 1})) as $rel_ranks |\n        # Rank by recency (temporal relevance)\n        (. | sort_by(-.recency_score) | to_entries | map({id: .key, rec_rank: .key + 1})) as $rec_ranks |\n        # Rank by importance (user-defined priority)\n        (. | sort_by(-.importance_score) | to_entries | map({id: .key, imp_rank: .key + 1})) as $imp_ranks |\n        # Combine ALL rankings with RRF formula\n        . | to_entries | map(\n            . as $item |\n            ($bm25_ranks[$item.key].bm25_rank // 999) as $bm25_r |\n            ($rel_ranks[$item.key].rel_rank // 999) as $rel_r |\n            ($rec_ranks[$item.key].rec_rank // 999) as $rec_r |\n            ($imp_ranks[$item.key].imp_rank // 999) as $imp_r |\n            $item.value + {\n                rrf_score: (1.0 / ($k + $bm25_r)) + (1.0 / ($k + $rel_r)) +\n                          (1.0 / ($k + $rec_r)) + (1.0 / ($k + $imp_r)),\n                bm25_rank: $bm25_r,\n                relevance_rank: $rel_r,\n                recency_rank: $rec_r,\n                importance_rank: $imp_r,\n                combined_accuracy: \"95%+\"\n            }\n        ) | map(.value) |\n        # Sort by RRF score for final ranking (proven 95%+ accuracy)\n        sort_by(-.rrf_score)\n    '\n}\n\n# Legacy RRF function (kept for backward compatibility)\nreciprocal_rank_fusion() {\n    local results_json=\"$1\"\n    local k=\"${2:-60}\"\n    reciprocal_rank_fusion_enhanced \"$results_json\" \"$k\"\n}\n\n# Hybrid retrieval: combines BM25 (keyword) + word overlap (semantic) scoring\nretrieve_hybrid() {\n    local query=\"$1\"\n    local limit=\"${2:-10}\"\n    local recency_weight=\"${3:-0.5}\"\n    local bm25_weight=\"${4:-2.0}\"\n    local relevance_weight=\"${5:-2.0}\"\n    local importance_weight=\"${6:-2.0}\"\n\n    init_memory\n\n    local results=\"[]\"\n\n    # Calculate average document length for BM25\n    local avgdl=50  # Default, could be calculated dynamically\n\n    # Score episodic memories with hybrid approach\n    local episodes\n    episodes=$(jq '.episodes' \"$EPISODIC_MEMORY\")\n\n    while IFS= read -r episode; do\n        if [[ -z \"$episode\" || \"$episode\" == \"null\" ]]; then\n            continue\n        fi\n\n        local description\n        description=$(echo \"$episode\" | jq -r '.description')\n\n        local timestamp\n        timestamp=$(echo \"$episode\" | jq -r '.timestamp')\n\n        local importance\n        importance=$(echo \"$episode\" | jq -r '.importance // 5')\n\n        # Calculate all scores\n        local recency_score\n        recency_score=$(calculate_recency_score \"$timestamp\")\n\n        local relevance_score\n        relevance_score=$(calculate_relevance_score \"$query\" \"$description\")\n\n        local bm25_score\n        bm25_score=$(calculate_bm25_score \"$query\" \"$description\" \"$avgdl\")\n\n        # Normalize importance to 0-1\n        local importance_score\n        importance_score=$(echo \"scale=4; $importance / 10\" | bc -l 2>/dev/null || echo \"0.5\")\n\n        # Combined score with all four factors\n        local final_score\n        final_score=$(echo \"scale=4; ($recency_weight * $recency_score) + ($bm25_weight * $bm25_score) + ($relevance_weight * $relevance_score) + ($importance_weight * $importance_score)\" | bc -l 2>/dev/null || echo \"0\")\n\n        # Add to results with all scores\n        results=$(echo \"$results\" | jq --argjson ep \"$episode\" \\\n            --arg final_score \"$final_score\" \\\n            --arg bm25 \"$bm25_score\" \\\n            --arg relevance \"$relevance_score\" \\\n            --arg recency \"$recency_score\" \\\n            --arg importance \"$importance_score\" \\\n            '. + [($ep + {\n                retrievalScore: ($final_score | tonumber),\n                bm25_score: ($bm25 | tonumber),\n                relevance_score: ($relevance | tonumber),\n                recency_score: ($recency | tonumber),\n                importance_score: ($importance | tonumber),\n                source: \"episodic\"\n            })]')\n    done < <(echo \"$episodes\" | jq -c '.[]')\n\n    # Score patterns with hybrid approach\n    local patterns\n    patterns=$(jq '.patterns' \"$SEMANTIC_MEMORY\")\n\n    while IFS= read -r pattern; do\n        if [[ -z \"$pattern\" || \"$pattern\" == \"null\" ]]; then\n            continue\n        fi\n\n        local trigger\n        trigger=$(echo \"$pattern\" | jq -r '.trigger')\n\n        local timestamp\n        timestamp=$(echo \"$pattern\" | jq -r '.createdAt')\n\n        local success_rate\n        success_rate=$(echo \"$pattern\" | jq -r '.successRate // 1.0')\n\n        # Calculate scores\n        local recency_score\n        recency_score=$(calculate_recency_score \"$timestamp\")\n\n        local relevance_score\n        relevance_score=$(calculate_relevance_score \"$query\" \"$trigger\")\n\n        local bm25_score\n        bm25_score=$(calculate_bm25_score \"$query\" \"$trigger\" \"$avgdl\")\n\n        # Use success rate as importance\n        local importance_score=\"$success_rate\"\n\n        # Combined score\n        local final_score\n        final_score=$(echo \"scale=4; ($recency_weight * $recency_score) + ($bm25_weight * $bm25_score) + ($relevance_weight * $relevance_score) + ($importance_weight * $importance_score)\" | bc -l 2>/dev/null || echo \"0\")\n\n        # Add to results\n        results=$(echo \"$results\" | jq --argjson pat \"$pattern\" \\\n            --arg final_score \"$final_score\" \\\n            --arg bm25 \"$bm25_score\" \\\n            --arg relevance \"$relevance_score\" \\\n            --arg recency \"$recency_score\" \\\n            --arg importance \"$importance_score\" \\\n            '. + [($pat + {\n                retrievalScore: ($final_score | tonumber),\n                bm25_score: ($bm25 | tonumber),\n                relevance_score: ($relevance | tonumber),\n                recency_score: ($recency | tonumber),\n                importance_score: ($importance | tonumber),\n                source: \"pattern\"\n            })]')\n    done < <(echo \"$patterns\" | jq -c '.[]')\n\n    # Sort by score and return top results\n    echo \"$results\" | jq --argjson limit \"$limit\" \\\n        'sort_by(-.retrievalScore) | .[0:$limit]'\n}\n\n# =============================================================================\n# PHASE 3: AST-BASED CHUNKING (Semantic Code Splitting)\n# =============================================================================\n\n# Detect language from file extension\ndetect_language() {\n    local filepath=\"$1\"\n    local ext=\"${filepath##*.}\"\n\n    case \"$ext\" in\n        js|jsx) echo \"javascript\" ;;\n        ts|tsx) echo \"typescript\" ;;\n        py) echo \"python\" ;;\n        rb) echo \"ruby\" ;;\n        go) echo \"go\" ;;\n        rs) echo \"rust\" ;;\n        java) echo \"java\" ;;\n        c|h) echo \"c\" ;;\n        cpp|hpp|cc|cxx) echo \"cpp\" ;;\n        sh|bash) echo \"bash\" ;;\n        *) echo \"text\" ;;\n    esac\n}\n\n# Find semantic boundaries (functions, classes) in code\n# Returns line numbers where new chunks should start\nfind_semantic_boundaries() {\n    local filepath=\"$1\"\n    local language=\"$2\"\n\n    if [[ ! -f \"$filepath\" ]]; then\n        echo \"[]\"\n        return\n    fi\n\n    local boundaries=\"[]\"\n    local line_num=1\n\n    case \"$language\" in\n        javascript|typescript)\n            # Match: function name(), const name = function(), const name = () =>\n            while IFS= read -r line; do\n                if echo \"$line\" | grep -qE '^\\s*(function\\s+\\w+|const\\s+\\w+\\s*=\\s*(function|\\([^)]*\\)\\s*=>)|class\\s+\\w+|export\\s+(default\\s+)?(function|class))'; then\n                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n                fi\n                ((line_num++))\n            done < \"$filepath\"\n            ;;\n\n        python)\n            # Match: def name, class name\n            while IFS= read -r line; do\n                if echo \"$line\" | grep -qE '^\\s*(def\\s+\\w+|class\\s+\\w+)'; then\n                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n                fi\n                ((line_num++))\n            done < \"$filepath\"\n            ;;\n\n        go)\n            # Match: func name\n            while IFS= read -r line; do\n                if echo \"$line\" | grep -qE '^\\s*func\\s+(\\([^)]+\\)\\s*)?\\w+'; then\n                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n                fi\n                ((line_num++))\n            done < \"$filepath\"\n            ;;\n\n        rust)\n            # Match: fn name, impl Type, struct/enum Type\n            while IFS= read -r line; do\n                if echo \"$line\" | grep -qE '^\\s*(pub\\s+)?(fn\\s+\\w+|impl\\s+|struct\\s+\\w+|enum\\s+\\w+)'; then\n                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n                fi\n                ((line_num++))\n            done < \"$filepath\"\n            ;;\n\n        java|c|cpp)\n            # Match: class Name, public/private type name(\n            while IFS= read -r line; do\n                if echo \"$line\" | grep -qE '^\\s*((public|private|protected)\\s+)?(class|interface|enum)\\s+\\w+|^\\s*((public|private|protected|static)\\s+)*\\w+\\s+\\w+\\s*\\('; then\n                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n                fi\n                ((line_num++))\n            done < \"$filepath\"\n            ;;\n\n        bash)\n            # Match: function_name(), function name\n            while IFS= read -r line; do\n                if echo \"$line\" | grep -qE '^\\s*(\\w+\\s*\\(\\)|function\\s+\\w+)'; then\n                    boundaries=$(echo \"$boundaries\" | jq --argjson num \"$line_num\" '. + [$num]')\n                fi\n                ((line_num++))\n            done < \"$filepath\"\n            ;;\n    esac\n\n    echo \"$boundaries\"\n}\n\n# Chunk code file at semantic boundaries\n# Returns array of chunks with start/end lines and content\nchunk_code_file() {\n    local filepath=\"$1\"\n    local max_chunk_tokens=\"${2:-500}\"  # Target chunk size in tokens (approx 4 chars = 1 token)\n    local max_chunk_chars=$((max_chunk_tokens * 4))\n\n    if [[ ! -f \"$filepath\" ]]; then\n        echo \"[]\"\n        return\n    fi\n\n    local language\n    language=$(detect_language \"$filepath\")\n\n    local boundaries\n    boundaries=$(find_semantic_boundaries \"$filepath\" \"$language\")\n\n    # If no boundaries found, chunk by fixed size\n    if [[ \"$(echo \"$boundaries\" | jq 'length')\" -eq 0 ]]; then\n        chunk_file_fixed \"$filepath\" \"$max_chunk_chars\"\n        return\n    fi\n\n    local chunks=\"[]\"\n    local total_lines\n    total_lines=$(wc -l < \"$filepath\" | tr -d ' ')\n\n    # Add boundaries for start and end\n    boundaries=$(echo \"$boundaries\" | jq '. + [1] | sort | unique')\n    boundaries=$(echo \"$boundaries\" | jq --argjson end \"$((total_lines + 1))\" '. + [$end] | sort | unique')\n\n    # Create chunks between boundaries\n    local boundary_count\n    boundary_count=$(echo \"$boundaries\" | jq 'length')\n\n    for ((i=0; i<boundary_count-1; i++)); do\n        local start_line\n        start_line=$(echo \"$boundaries\" | jq -r \".[$i]\")\n\n        local end_line\n        end_line=$(echo \"$boundaries\" | jq -r \".[$((i+1))]\")\n        end_line=$((end_line - 1))\n\n        # Extract chunk content\n        local content\n        content=$(sed -n \"${start_line},${end_line}p\" \"$filepath\")\n\n        # Check if chunk exceeds max size\n        local chunk_size=${#content}\n\n        if [[ $chunk_size -le $max_chunk_chars ]]; then\n            # Chunk fits, add it\n            chunks=$(echo \"$chunks\" | jq --arg content \"$content\" \\\n                --argjson start \"$start_line\" \\\n                --argjson end \"$end_line\" \\\n                --argjson size \"$chunk_size\" \\\n                '. + [{\n                    start_line: $start,\n                    end_line: $end,\n                    content: $content,\n                    size_chars: $size,\n                    size_tokens: ($size / 4 | floor)\n                }]')\n        else\n            # Chunk too large, split further by fixed size\n            local sub_chunks\n            sub_chunks=$(echo \"$content\" | fold -w \"$max_chunk_chars\" | jq -R -s 'split(\"\\n\") | map(select(length > 0))')\n\n            local sub_chunk_count\n            sub_chunk_count=$(echo \"$sub_chunks\" | jq 'length')\n\n            for ((j=0; j<sub_chunk_count; j++)); do\n                local sub_content\n                sub_content=$(echo \"$sub_chunks\" | jq -r \".[$j]\")\n\n                local sub_size=${#sub_content}\n\n                chunks=$(echo \"$chunks\" | jq --arg content \"$sub_content\" \\\n                    --argjson start \"$start_line\" \\\n                    --argjson end \"$end_line\" \\\n                    --argjson size \"$sub_size\" \\\n                    '. + [{\n                        start_line: $start,\n                        end_line: $end,\n                        content: $content,\n                        size_chars: $size,\n                        size_tokens: ($size / 4 | floor),\n                        split: true\n                    }]')\n            done\n        fi\n    done\n\n    echo \"$chunks\"\n}\n\n# Chunk file by fixed size (fallback)\nchunk_file_fixed() {\n    local filepath=\"$1\"\n    local max_chars=\"${2:-2000}\"\n\n    if [[ ! -f \"$filepath\" ]]; then\n        echo \"[]\"\n        return\n    fi\n\n    local content\n    content=$(cat \"$filepath\")\n\n    local chunks=\"[]\"\n    local chunk_num=0\n\n    # Split content into chunks\n    while [[ -n \"$content\" ]]; do\n        local chunk=\"${content:0:$max_chars}\"\n        content=\"${content:$max_chars}\"\n\n        local chunk_size=${#chunk}\n\n        chunks=$(echo \"$chunks\" | jq --arg content \"$chunk\" \\\n            --argjson num \"$chunk_num\" \\\n            --argjson size \"$chunk_size\" \\\n            '. + [{\n                chunk_number: $num,\n                content: $content,\n                size_chars: $size,\n                size_tokens: ($size / 4 | floor)\n            }]')\n\n        ((chunk_num++))\n    done\n\n    echo \"$chunks\"\n}\n\n# =============================================================================\n# PHASE 4: CONTEXT TOKEN BUDGETING\n# =============================================================================\n\n# Configuration file for context budgeting\nCONTEXT_CONFIG=\"${HOME}/.claude/config/context-budget.json\"\n\n# Initialize context budget configuration\ninit_context_budget() {\n    mkdir -p \"$(dirname \"$CONTEXT_CONFIG\")\"\n\n    if [[ ! -f \"$CONTEXT_CONFIG\" ]]; then\n        cat > \"$CONTEXT_CONFIG\" << 'EOF'\n{\n    \"limits\": {\n        \"total_tokens\": 200000,\n        \"working_memory\": 20000,\n        \"episodic_memory\": 50000,\n        \"semantic_memory\": 30000,\n        \"actions\": 20000,\n        \"reserve\": 10000\n    },\n    \"thresholds\": {\n        \"warning\": 0.80,\n        \"critical\": 0.90\n    },\n    \"auto_compact\": true,\n    \"auto_prune_threshold\": 0.95\n}\nEOF\n    fi\n}\n\n# Estimate token count from text (rough: 1 token ≈ 4 characters)\nestimate_tokens() {\n    local text=\"$1\"\n    local char_count=${#text}\n    echo $((char_count / 4))\n}\n\n# Calculate current context usage\ncalculate_context_usage() {\n    init_memory\n    init_context_budget\n\n    local working_content\n    working_content=$(cat \"$WORKING_MEMORY\" 2>/dev/null || echo \"{}\")\n\n    local episodic_content\n    episodic_content=$(cat \"$EPISODIC_MEMORY\" 2>/dev/null || echo \"{}\")\n\n    local semantic_content\n    semantic_content=$(cat \"$SEMANTIC_MEMORY\" 2>/dev/null || echo \"{}\")\n\n    local actions_content\n    actions_content=$(cat \"$ACTION_LOG\" 2>/dev/null || echo \"\")\n\n    # Estimate tokens for each memory type\n    local working_tokens\n    working_tokens=$(estimate_tokens \"$working_content\")\n\n    local episodic_tokens\n    episodic_tokens=$(estimate_tokens \"$episodic_content\")\n\n    local semantic_tokens\n    semantic_tokens=$(estimate_tokens \"$semantic_content\")\n\n    local actions_tokens\n    actions_tokens=$(estimate_tokens \"$actions_content\")\n\n    local total_tokens=$((working_tokens + episodic_tokens + semantic_tokens + actions_tokens))\n\n    # Get limits from config\n    local total_limit\n    total_limit=$(jq -r '.limits.total_tokens' \"$CONTEXT_CONFIG\")\n\n    local warning_threshold\n    warning_threshold=$(jq -r '.thresholds.warning' \"$CONTEXT_CONFIG\")\n\n    local critical_threshold\n    critical_threshold=$(jq -r '.thresholds.critical' \"$CONTEXT_CONFIG\")\n\n    # Calculate percentages\n    local usage_pct\n    usage_pct=$(echo \"scale=2; ($total_tokens / $total_limit) * 100\" | bc -l 2>/dev/null || echo \"0\")\n\n    local warning_tokens\n    warning_tokens=$(echo \"$total_limit * $warning_threshold\" | bc 2>/dev/null || echo \"0\")\n\n    local critical_tokens\n    critical_tokens=$(echo \"$total_limit * $critical_threshold\" | bc 2>/dev/null || echo \"0\")\n\n    # Determine status\n    local status=\"ok\"\n    if [[ $(echo \"$total_tokens > $critical_tokens\" | bc 2>/dev/null) -eq 1 ]]; then\n        status=\"critical\"\n    elif [[ $(echo \"$total_tokens > $warning_tokens\" | bc 2>/dev/null) -eq 1 ]]; then\n        status=\"warning\"\n    fi\n\n    # Output JSON\n    jq -n \\\n        --argjson total \"$total_tokens\" \\\n        --argjson limit \"$total_limit\" \\\n        --arg pct \"$usage_pct\" \\\n        --argjson working \"$working_tokens\" \\\n        --argjson episodic \"$episodic_tokens\" \\\n        --argjson semantic \"$semantic_tokens\" \\\n        --argjson actions \"$actions_tokens\" \\\n        --arg status \"$status\" \\\n        '{\n            total_tokens: $total,\n            total_limit: $limit,\n            usage_percent: $pct,\n            status: $status,\n            remaining: ($limit - $total),\n            breakdown: {\n                working_memory: $working,\n                episodic_memory: $episodic,\n                semantic_memory: $semantic,\n                actions: $actions\n            }\n        }'\n}\n\n# Check if context budget is exceeded\ncheck_context_budget() {\n    local usage\n    usage=$(calculate_context_usage)\n\n    local status\n    status=$(echo \"$usage\" | jq -r '.status')\n\n    local usage_pct\n    usage_pct=$(echo \"$usage\" | jq -r '.usage_percent')\n\n    local total\n    total=$(echo \"$usage\" | jq -r '.total_tokens')\n\n    local limit\n    limit=$(echo \"$usage\" | jq -r '.total_limit')\n\n    case \"$status\" in\n        critical)\n            echo \"⚠️  CRITICAL: Context budget at ${usage_pct}% ($total/$limit tokens)\"\n            return 2\n            ;;\n        warning)\n            echo \"⚠️  WARNING: Context budget at ${usage_pct}% ($total/$limit tokens)\"\n            return 1\n            ;;\n        ok)\n            echo \"✅ OK: Context budget at ${usage_pct}% ($total/$limit tokens)\"\n            return 0\n            ;;\n    esac\n}\n\n# Get context budget remaining\ncontext_remaining() {\n    local usage\n    usage=$(calculate_context_usage)\n\n    echo \"$usage\" | jq -r '.remaining'\n}\n\n# Compact memory to reduce context usage\ncompact_memory() {\n    init_memory\n\n    echo \"Compacting memory to reduce context usage...\"\n\n    # Prune old low-importance episodes\n    local temp_file\n    temp_file=$(mktemp)\n\n    jq '.episodes |= (\n        sort_by(-.timestamp) |\n        map(select(.importance >= 5 or (.timestamp | fromdateiso8601) > (now - 604800)))\n    )' \"$EPISODIC_MEMORY\" > \"$temp_file\"\n    mv \"$temp_file\" \"$EPISODIC_MEMORY\"\n\n    # Prune old low-performing patterns\n    jq '.patterns |= (\n        sort_by(-.successRate, -.useCount) |\n        .[0:50]\n    )' \"$SEMANTIC_MEMORY\" > \"$temp_file\"\n    mv \"$temp_file\" \"$SEMANTIC_MEMORY\"\n\n    # Truncate action log (keep recent 1000 lines)\n    tail -n 1000 \"$ACTION_LOG\" > \"$temp_file\"\n    mv \"$temp_file\" \"$ACTION_LOG\"\n\n    echo \"Memory compacted.\"\n    check_context_budget\n}\n\n# Auto-compact if threshold exceeded\nauto_compact_if_needed() {\n    local auto_compact\n    auto_compact=$(jq -r '.auto_compact' \"$CONTEXT_CONFIG\" 2>/dev/null || echo \"true\")\n\n    if [[ \"$auto_compact\" != \"true\" ]]; then\n        return 0\n    fi\n\n    local auto_prune_threshold\n    auto_prune_threshold=$(jq -r '.auto_prune_threshold' \"$CONTEXT_CONFIG\" 2>/dev/null || echo \"0.95\")\n\n    local usage\n    usage=$(calculate_context_usage)\n\n    local usage_pct\n    usage_pct=$(echo \"$usage\" | jq -r '.usage_percent')\n\n    local threshold_pct\n    threshold_pct=$(echo \"$auto_prune_threshold * 100\" | bc 2>/dev/null || echo \"95\")\n\n    if [[ $(echo \"$usage_pct > $threshold_pct\" | bc 2>/dev/null) -eq 1 ]]; then\n        log \"Auto-compacting memory (usage: ${usage_pct}%)\"\n        compact_memory\n    fi\n}\n\n# Set context budget limits\nset_context_limit() {\n    local limit_type=\"$1\"\n    local limit_value=\"$2\"\n\n    init_context_budget\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    jq --arg type \"$limit_type\" \\\n       --argjson value \"$limit_value\" \\\n       '.limits[$type] = $value' \"$CONTEXT_CONFIG\" > \"$temp_file\"\n\n    mv \"$temp_file\" \"$CONTEXT_CONFIG\"\n\n    echo \"Updated $limit_type limit to $limit_value tokens\"\n}\n\n# Advanced retrieval with three-factor scoring\n# Based on Generative Agents: recency + relevance + importance\nretrieve_scored() {\n    local query=\"$1\"\n    local limit=\"${2:-10}\"\n    local recency_weight=\"${3:-0.5}\"\n    local relevance_weight=\"${4:-3.0}\"\n    local importance_weight=\"${5:-2.0}\"\n\n    init_memory\n\n    local results=\"[]\"\n\n    # Score episodic memories\n    local episodes\n    episodes=$(jq '.episodes' \"$EPISODIC_MEMORY\")\n\n    while IFS= read -r episode; do\n        if [[ -z \"$episode\" || \"$episode\" == \"null\" ]]; then\n            continue\n        fi\n\n        local description\n        description=$(echo \"$episode\" | jq -r '.description')\n\n        local timestamp\n        timestamp=$(echo \"$episode\" | jq -r '.timestamp')\n\n        local importance\n        importance=$(echo \"$episode\" | jq -r '.importance // 5')\n\n        # Calculate scores\n        local recency_score\n        recency_score=$(calculate_recency_score \"$timestamp\")\n\n        local relevance_score\n        relevance_score=$(calculate_relevance_score \"$query\" \"$description\")\n\n        # Normalize importance to 0-1\n        local importance_score\n        importance_score=$(echo \"scale=4; $importance / 10\" | bc -l 2>/dev/null || echo \"0.5\")\n\n        # Combined score with weights\n        local final_score\n        final_score=$(echo \"scale=4; ($recency_weight * $recency_score) + ($relevance_weight * $relevance_score) + ($importance_weight * $importance_score)\" | bc -l 2>/dev/null || echo \"0\")\n\n        # Add to results with score\n        results=$(echo \"$results\" | jq --argjson ep \"$episode\" --arg score \"$final_score\" \\\n            '. + [($ep + {retrievalScore: ($score | tonumber), source: \"episodic\"})]')\n    done < <(echo \"$episodes\" | jq -c '.[]')\n\n    # Score patterns\n    local patterns\n    patterns=$(jq '.patterns' \"$SEMANTIC_MEMORY\")\n\n    while IFS= read -r pattern; do\n        if [[ -z \"$pattern\" || \"$pattern\" == \"null\" ]]; then\n            continue\n        fi\n\n        local trigger\n        trigger=$(echo \"$pattern\" | jq -r '.trigger')\n\n        local timestamp\n        timestamp=$(echo \"$pattern\" | jq -r '.createdAt')\n\n        local success_rate\n        success_rate=$(echo \"$pattern\" | jq -r '.successRate // 1.0')\n\n        # Calculate scores\n        local recency_score\n        recency_score=$(calculate_recency_score \"$timestamp\")\n\n        local relevance_score\n        relevance_score=$(calculate_relevance_score \"$query\" \"$trigger\")\n\n        # Use success rate as importance\n        local importance_score=\"$success_rate\"\n\n        # Combined score\n        local final_score\n        final_score=$(echo \"scale=4; ($recency_weight * $recency_score) + ($relevance_weight * $relevance_score) + ($importance_weight * $importance_score)\" | bc -l 2>/dev/null || echo \"0\")\n\n        # Add to results\n        results=$(echo \"$results\" | jq --argjson pat \"$pattern\" --arg score \"$final_score\" \\\n            '. + [($pat + {retrievalScore: ($score | tonumber), source: \"pattern\"})]')\n    done < <(echo \"$patterns\" | jq -c '.[]')\n\n    # Sort by score and return top results\n    echo \"$results\" | jq --argjson limit \"$limit\" \\\n        'sort_by(-.retrievalScore) | .[0:$limit]'\n}\n\n# Unified memory search (simple)\nremember() {\n    local query=\"$1\"\n    local limit=\"${2:-10}\"\n\n    init_memory\n\n    echo \"{\"\n\n    # Search episodic memory\n    echo '\"episodes\":'\n    search_episodes \"$query\" \"$limit\"\n    echo \",\"\n\n    # Search patterns\n    echo '\"patterns\":'\n    find_patterns \"$query\" \"$limit\"\n    echo \",\"\n\n    # Search actions\n    echo '\"actions\":'\n    search_actions \"$query\" \"$limit\"\n\n    echo \"}\"\n}\n\n# Unified memory search (with scoring)\nremember_scored() {\n    local query=\"$1\"\n    local limit=\"${2:-10}\"\n\n    retrieve_scored \"$query\" \"$limit\"\n}\n\n# Get context for current task\nget_context() {\n    init_memory\n\n    echo \"{\"\n\n    # Working memory\n    echo '\"working\":'\n    get_working\n    echo \",\"\n\n    # Recent episodes\n    echo '\"recentEpisodes\":'\n    get_recent_episodes 5\n    echo \",\"\n\n    # Recent reflections\n    echo '\"reflections\":'\n    get_reflections \"\" 3\n\n    echo \"}\"\n}\n\n# =============================================================================\n# MEMORY STATISTICS\n# =============================================================================\n\nget_stats() {\n    init_memory\n\n    local episodic_count\n    episodic_count=$(jq '.episodes | length' \"$EPISODIC_MEMORY\")\n\n    local facts_count\n    facts_count=$(jq '.facts | length' \"$SEMANTIC_MEMORY\")\n\n    local patterns_count\n    patterns_count=$(jq '.patterns | length' \"$SEMANTIC_MEMORY\")\n\n    local actions_count\n    actions_count=$(wc -l < \"$ACTION_LOG\" 2>/dev/null || echo \"0\")\n\n    local reflections_count\n    reflections_count=$(jq '.reflections | length' \"$REFLECTION_LOG\")\n\n    jq -n \\\n       --argjson episodes \"$episodic_count\" \\\n       --argjson facts \"$facts_count\" \\\n       --argjson patterns \"$patterns_count\" \\\n       --argjson actions \"$actions_count\" \\\n       --argjson reflections \"$reflections_count\" \\\n       '{\n           episodicMemory: $episodes,\n           semanticFacts: $facts,\n           learnedPatterns: $patterns,\n           actionLog: $actions,\n           reflections: $reflections\n       }'\n}\n\n# =============================================================================\n# CHECKPOINT/RESTORE (Session state snapshots)\n# Based on patterns from GitHub: checkpoint/snapshot/restore implementations\n# =============================================================================\n\n# Create a checkpoint (snapshot memory + git metadata)\ncheckpoint() {\n    local description=\"${1:-Auto checkpoint}\"\n\n    init_memory\n\n    local checkpoint_dir=\"$MEMORY_DIR/checkpoints\"\n    mkdir -p \"$checkpoint_dir\"\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    local checkpoint_id\n    checkpoint_id=\"ckpt_$(date +%s)\"\n\n    local checkpoint_path=\"$checkpoint_dir/$checkpoint_id.json\"\n\n    # Capture git metadata\n    local git_branch\n    git_branch=$(git rev-parse --abbrev-ref HEAD 2>/dev/null || echo \"unknown\")\n\n    local git_commit\n    git_commit=$(git rev-parse HEAD 2>/dev/null || echo \"unknown\")\n\n    local git_dirty\n    git_dirty=\"false\"\n    if [[ -d .git ]] && ! git diff-index --quiet HEAD -- 2>/dev/null; then\n        git_dirty=\"true\"\n    fi\n\n    # Create checkpoint with all memory state\n    jq -n \\\n        --arg id \"$checkpoint_id\" \\\n        --arg desc \"$description\" \\\n        --arg ts \"$timestamp\" \\\n        --arg branch \"$git_branch\" \\\n        --arg commit \"$git_commit\" \\\n        --arg dirty \"$git_dirty\" \\\n        --slurpfile working \"$WORKING_MEMORY\" \\\n        --slurpfile episodic \"$EPISODIC_MEMORY\" \\\n        --slurpfile semantic \"$SEMANTIC_MEMORY\" \\\n        --slurpfile reflections \"$REFLECTION_LOG\" \\\n        '{\n            id: $id,\n            description: $desc,\n            timestamp: $ts,\n            git: {\n                branch: $branch,\n                commit: $commit,\n                dirty: ($dirty == \"true\")\n            },\n            memory: {\n                working: $working[0],\n                episodic: $episodic[0],\n                semantic: $semantic[0],\n                reflections: $reflections[0]\n            }\n        }' > \"$checkpoint_path\"\n\n    # Copy action log (JSONL, just copy the file)\n    if [[ -f \"$ACTION_LOG\" ]]; then\n        cp \"$ACTION_LOG\" \"$checkpoint_dir/$checkpoint_id.actions.jsonl\"\n    fi\n\n    log \"Created checkpoint: $checkpoint_id - $description\"\n    echo \"$checkpoint_id\"\n}\n\n# Restore from a checkpoint\nrestore_checkpoint() {\n    local checkpoint_id=\"$1\"\n\n    init_memory\n\n    local checkpoint_dir=\"$MEMORY_DIR/checkpoints\"\n    local checkpoint_path=\"$checkpoint_dir/$checkpoint_id.json\"\n\n    if [[ ! -f \"$checkpoint_path\" ]]; then\n        echo \"Error: Checkpoint $checkpoint_id not found\" >&2\n        return 1\n    fi\n\n    # Extract and restore memory files\n    jq -r '.memory.working' \"$checkpoint_path\" > \"$WORKING_MEMORY\"\n    jq -r '.memory.episodic' \"$checkpoint_path\" > \"$EPISODIC_MEMORY\"\n    jq -r '.memory.semantic' \"$checkpoint_path\" > \"$SEMANTIC_MEMORY\"\n    jq -r '.memory.reflections' \"$checkpoint_path\" > \"$REFLECTION_LOG\"\n\n    # Restore action log if it exists\n    local action_log_backup=\"$checkpoint_dir/$checkpoint_id.actions.jsonl\"\n    if [[ -f \"$action_log_backup\" ]]; then\n        cp \"$action_log_backup\" \"$ACTION_LOG\"\n    fi\n\n    log \"Restored checkpoint: $checkpoint_id\"\n\n    # Return checkpoint info\n    jq '{id, description, timestamp, git}' \"$checkpoint_path\"\n}\n\n# List available checkpoints\nlist_checkpoints() {\n    local limit=\"${1:-10}\"\n\n    init_memory\n\n    local checkpoint_dir=\"$MEMORY_DIR/checkpoints\"\n\n    if [[ ! -d \"$checkpoint_dir\" ]]; then\n        echo \"[]\"\n        return 0\n    fi\n\n    # Find all checkpoint files, extract metadata, sort by timestamp\n    local results=\"[]\"\n    for ckpt_file in \"$checkpoint_dir\"/ckpt_*.json; do\n        if [[ -f \"$ckpt_file\" ]]; then\n            local ckpt_data\n            ckpt_data=$(jq '{id, description, timestamp, git}' \"$ckpt_file\" 2>/dev/null)\n            if [[ -n \"$ckpt_data\" ]]; then\n                results=$(echo \"$results\" | jq --argjson ckpt \"$ckpt_data\" '. + [$ckpt]')\n            fi\n        fi\n    done\n\n    echo \"$results\" | jq --argjson limit \"$limit\" 'sort_by(.timestamp) | reverse | .[0:$limit]'\n}\n\n# Delete old checkpoints (keep N most recent)\nprune_checkpoints() {\n    local keep=\"${1:-5}\"\n\n    init_memory\n\n    local checkpoint_dir=\"$MEMORY_DIR/checkpoints\"\n\n    if [[ ! -d \"$checkpoint_dir\" ]]; then\n        return 0\n    fi\n\n    # Get list of checkpoints sorted by timestamp (oldest first)\n    local checkpoints\n    checkpoints=$(list_checkpoints 1000 | jq -r 'reverse | .[].id')\n\n    local count=0\n    local deleted=0\n\n    for ckpt_id in $checkpoints; do\n        count=$((count + 1))\n        if [[ $count -le $keep ]]; then\n            continue\n        fi\n\n        # Delete checkpoint files\n        rm -f \"$checkpoint_dir/$ckpt_id.json\"\n        rm -f \"$checkpoint_dir/$ckpt_id.actions.jsonl\"\n        deleted=$((deleted + 1))\n    done\n\n    log \"Pruned $deleted old checkpoints (kept $keep most recent)\"\n    echo \"$deleted\"\n}\n\n# =============================================================================\n# FILE CHANGE DETECTION (SHA-256 hash tracking)\n# Based on patterns from GitHub: shasum -a 256 / sha256sum implementations\n# =============================================================================\n\nFILE_CACHE=\"$MEMORY_DIR/file-cache.json\"\n\n# Initialize file cache\ninit_file_cache() {\n    if [[ ! -f \"$FILE_CACHE\" ]]; then\n        echo '{\"files\":{}}' > \"$FILE_CACHE\"\n    fi\n}\n\n# Get SHA-256 hash of a file (portable: macOS and Linux)\nget_file_hash() {\n    local file_path=\"$1\"\n\n    if [[ ! -f \"$file_path\" ]]; then\n        echo \"error: file not found\" >&2\n        return 1\n    fi\n\n    # Try shasum (macOS) first, fall back to sha256sum (Linux)\n    if command -v shasum >/dev/null 2>&1; then\n        shasum -a 256 \"$file_path\" | awk '{print $1}'\n    elif command -v sha256sum >/dev/null 2>&1; then\n        sha256sum \"$file_path\" | awk '{print $1}'\n    else\n        echo \"error: no SHA-256 tool available\" >&2\n        return 1\n    fi\n}\n\n# Cache a file's hash\ncache_file() {\n    local file_path=\"$1\"\n\n    init_memory\n    init_file_cache\n\n    local hash\n    hash=$(get_file_hash \"$file_path\")\n\n    if [[ $? -ne 0 ]]; then\n        return 1\n    fi\n\n    local timestamp\n    timestamp=$(date -u +%Y-%m-%dT%H:%M:%SZ)\n\n    # Update cache\n    local temp_file\n    temp_file=$(mktemp)\n\n    jq --arg path \"$file_path\" \\\n       --arg hash \"$hash\" \\\n       --arg ts \"$timestamp\" \\\n       '.files[$path] = {hash: $hash, cachedAt: $ts}' \\\n       \"$FILE_CACHE\" > \"$temp_file\"\n\n    mv \"$temp_file\" \"$FILE_CACHE\"\n\n    log \"Cached file: $file_path (hash: ${hash:0:8}...)\"\n    echo \"$hash\"\n}\n\n# Check if a file has changed since last cache\nfile_changed() {\n    local file_path=\"$1\"\n\n    init_memory\n    init_file_cache\n\n    # Get current hash\n    local current_hash\n    current_hash=$(get_file_hash \"$file_path\")\n\n    if [[ $? -ne 0 ]]; then\n        return 1\n    fi\n\n    # Get cached hash\n    local cached_hash\n    cached_hash=$(jq -r --arg path \"$file_path\" \\\n        '.files[$path].hash // \"none\"' \"$FILE_CACHE\")\n\n    # Compare\n    if [[ \"$cached_hash\" == \"none\" ]]; then\n        # File not cached yet\n        echo \"true\"\n        return 0\n    elif [[ \"$current_hash\" != \"$cached_hash\" ]]; then\n        # File changed\n        echo \"true\"\n        return 0\n    else\n        # File unchanged\n        echo \"false\"\n        return 0\n    fi\n}\n\n# Get file cache info\nget_file_cache_info() {\n    local file_path=\"$1\"\n\n    init_memory\n    init_file_cache\n\n    jq --arg path \"$file_path\" \\\n        '.files[$path] // {error: \"not cached\"}' \\\n        \"$FILE_CACHE\"\n}\n\n# List all cached files\nlist_cached_files() {\n    init_memory\n    init_file_cache\n\n    jq -r '.files | keys[]' \"$FILE_CACHE\"\n}\n\n# Clear file cache\nclear_file_cache() {\n    init_memory\n\n    echo '{\"files\":{}}' > \"$FILE_CACHE\"\n    log \"Cleared file cache\"\n}\n\n# Prune file cache (remove files that don't exist)\nprune_file_cache() {\n    init_memory\n    init_file_cache\n\n    local temp_file\n    temp_file=$(mktemp)\n\n    local pruned=0\n\n    # Build new cache with only existing files\n    local cached_files\n    cached_files=$(jq -r '.files | keys[]' \"$FILE_CACHE\")\n\n    local new_cache='{\"files\":{}}'\n\n    for file_path in $cached_files; do\n        if [[ -f \"$file_path\" ]]; then\n            # File still exists, keep it\n            local file_data\n            file_data=$(jq --arg path \"$file_path\" '.files[$path]' \"$FILE_CACHE\")\n            new_cache=$(echo \"$new_cache\" | jq --arg path \"$file_path\" \\\n                --argjson data \"$file_data\" \\\n                '.files[$path] = $data')\n        else\n            # File deleted, don't keep\n            pruned=$((pruned + 1))\n        fi\n    done\n\n    echo \"$new_cache\" > \"$FILE_CACHE\"\n\n    log \"Pruned $pruned deleted files from cache\"\n    echo \"$pruned\"\n}\n\n# =============================================================================\n# COMMAND INTERFACE\n# =============================================================================\n\ncase \"${1:-help}\" in\n    # Working memory\n    set-task)\n        set_task \"${2:-}\" \"${3:-}\"\n        ;;\n    add-context)\n        add_context \"${2:-}\" \"${3:-5}\"\n        ;;\n    scratchpad)\n        update_scratchpad \"${2:-}\" \"${3:-true}\"\n        ;;\n    get-working)\n        get_working\n        ;;\n    clear-working)\n        clear_working\n        ;;\n\n    # Episodic memory\n    record)\n        record_episode \"${2:-task}\" \"${3:-}\" \"${4:-success}\" \"${5:-}\"\n        ;;\n    search-episodes)\n        search_episodes \"${2:-}\" \"${3:-10}\"\n        ;;\n    recent-episodes)\n        get_recent_episodes \"${2:-10}\" \"${3:-}\"\n        ;;\n\n    # Semantic memory\n    add-fact)\n        add_fact \"${2:-}\" \"${3:-}\" \"${4:-}\" \"${5:-0.8}\"\n        ;;\n    get-fact)\n        get_fact \"${2:-}\" \"${3:-}\"\n        ;;\n    facts)\n        get_facts_by_category \"${2:-}\"\n        ;;\n    add-pattern)\n        add_pattern \"${2:-}\" \"${3:-}\" \"${4:-}\" \"${5:-1.0}\"\n        ;;\n    find-patterns)\n        find_patterns \"${2:-}\" \"${3:-5}\"\n        ;;\n    set-pref)\n        add_preference \"${2:-}\" \"${3:-}\"\n        ;;\n    get-pref)\n        get_preference \"${2:-}\" \"${3:-}\"\n        ;;\n\n    # Action log\n    log-action)\n        log_action \"${2:-}\" \"${3:-}\" \"${4:-}\" \"${5:-{}}\"\n        ;;\n    recent-actions)\n        get_recent_actions \"${2:-20}\" \"${3:-}\"\n        ;;\n    search-actions)\n        search_actions \"${2:-}\" \"${3:-20}\"\n        ;;\n\n    # Reflection\n    reflect)\n        create_reflection \"${2:-general}\" \"${3:-}\" \"${4:-}\"\n        ;;\n    reflections)\n        get_reflections \"${2:-}\" \"${3:-10}\"\n        ;;\n\n    # Retrieval\n    remember)\n        remember \"${2:-}\" \"${3:-10}\"\n        ;;\n    remember-scored)\n        remember_scored \"${2:-}\" \"${3:-10}\"\n        ;;\n    remember-hybrid|search-hybrid)\n        retrieve_hybrid \"${2:-}\" \"${3:-10}\"\n        ;;\n    context)\n        get_context\n        ;;\n\n    # Stats\n    stats)\n        get_stats\n        ;;\n\n    # Checkpoint/Restore\n    checkpoint)\n        checkpoint \"${2:-Auto checkpoint}\"\n        ;;\n    restore)\n        restore_checkpoint \"${2:-}\"\n        ;;\n    list-checkpoints)\n        list_checkpoints \"${2:-10}\"\n        ;;\n    prune-checkpoints)\n        prune_checkpoints \"${2:-5}\"\n        ;;\n\n    # File Change Detection\n    cache-file)\n        cache_file \"${2:-}\"\n        ;;\n    file-changed)\n        file_changed \"${2:-}\"\n        ;;\n    file-info)\n        get_file_cache_info \"${2:-}\"\n        ;;\n    list-cached)\n        list_cached_files\n        ;;\n    clear-cache)\n        clear_file_cache\n        ;;\n    prune-cache)\n        prune_file_cache\n        ;;\n\n    # Code Chunking (Phase 3)\n    chunk-file)\n        chunk_code_file \"${2:-}\" \"${3:-500}\"\n        ;;\n    detect-language)\n        detect_language \"${2:-}\"\n        ;;\n    find-boundaries)\n        language=\"${3:-$(detect_language \"${2:-}\")}\"\n        find_semantic_boundaries \"${2:-}\" \"$language\"\n        ;;\n\n    # Context Budgeting (Phase 4)\n    context-usage)\n        calculate_context_usage\n        ;;\n    context-check)\n        check_context_budget\n        ;;\n    context-remaining)\n        context_remaining\n        ;;\n    context-compact)\n        compact_memory\n        ;;\n    set-context-limit)\n        set_context_limit \"${2:-total_tokens}\" \"${3:-200000}\"\n        ;;\n\n    init)\n        init_memory\n        echo \"Memory initialized at $MEMORY_DIR\"\n        ;;\n\n    scope)\n        echo \"Memory Scope Configuration\"\n        echo \"\"\n        echo \"Current settings:\"\n        echo \"  MEMORY_DIR: $MEMORY_DIR\"\n        echo \"  MEMORY_SCOPE: ${MEMORY_SCOPE:-auto}\"\n        echo \"  Git Channel: $(get_git_channel)\"\n        echo \"\"\n        project_root=$(find_project_root 2>/dev/null) && \\\n            echo \"  Project root: $project_root\" || \\\n            echo \"  Project root: (none detected)\"\n        echo \"\"\n        echo \"To change scope, set MEMORY_SCOPE:\"\n        echo \"  export MEMORY_SCOPE=project  # Use project-local memory\"\n        echo \"  export MEMORY_SCOPE=global   # Use global memory\"\n        echo \"  export MEMORY_SCOPE=auto     # Auto-detect (default)\"\n        ;;\n\n    help|*)\n        echo \"Memory Manager - Persistent Agent Memory\"\n        echo \"\"\n        echo \"Usage: $0 <command> [args]\"\n        echo \"\"\n        echo \"Working Memory (current session):\"\n        echo \"  set-task <task> [context]        - Set current task\"\n        echo \"  add-context <text> [importance]  - Add context (1-10)\"\n        echo \"  scratchpad <note> [append]       - Update scratchpad\"\n        echo \"  get-working                      - Get working memory\"\n        echo \"  clear-working                    - Clear for new session\"\n        echo \"\"\n        echo \"Episodic Memory (past experiences):\"\n        echo \"  record <type> <desc> [outcome] [details] - Record episode\"\n        echo \"    Types: task_complete, error_fixed, research_done, pattern_learned\"\n        echo \"  search-episodes <query> [limit]  - Search episodes\"\n        echo \"  recent-episodes [limit] [type]   - Get recent episodes\"\n        echo \"\"\n        echo \"Semantic Memory (facts & patterns):\"\n        echo \"  add-fact <category> <key> <value> [confidence]\"\n        echo \"  get-fact <category> <key>        - Get a fact\"\n        echo \"  facts <category>                 - Get category facts\"\n        echo \"  add-pattern <type> <trigger> <solution> [success_rate]\"\n        echo \"  find-patterns <query> [limit]    - Find matching patterns\"\n        echo \"  set-pref <key> <value>           - Set preference\"\n        echo \"  get-pref <key> [default]         - Get preference\"\n        echo \"\"\n        echo \"Action Log:\"\n        echo \"  log-action <type> <desc> [result] [metadata]\"\n        echo \"  recent-actions [limit] [type]    - Get recent actions\"\n        echo \"  search-actions <query> [limit]   - Search actions\"\n        echo \"\"\n        echo \"Reflection:\"\n        echo \"  reflect <focus> <content> [insights]\"\n        echo \"  reflections [focus] [limit]      - Get reflections\"\n        echo \"\"\n        echo \"Retrieval:\"\n        echo \"  remember <query> [limit]         - Search all memory (simple)\"\n        echo \"  remember-scored <query> [limit]  - Search with 3-factor scoring\"\n        echo \"                                     (recency + relevance + importance)\"\n        echo \"  remember-hybrid <query> [limit]  - Hybrid search (BM25 + semantic)\"\n        echo \"                                     (recency + BM25 + relevance + importance)\"\n        echo \"  context                          - Get current context\"\n        echo \"\"\n        echo \"Checkpoint/Restore:\"\n        echo \"  checkpoint [description]         - Create memory snapshot\"\n        echo \"  restore <checkpoint_id>          - Restore from checkpoint\"\n        echo \"  list-checkpoints [limit]         - List available checkpoints\"\n        echo \"  prune-checkpoints [keep]         - Delete old checkpoints (keep N)\"\n        echo \"\"\n        echo \"File Change Detection:\"\n        echo \"  cache-file <path>                - Cache file hash\"\n        echo \"  file-changed <path>              - Check if file changed\"\n        echo \"  file-info <path>                 - Get cache info for file\"\n        echo \"  list-cached                      - List all cached files\"\n        echo \"  clear-cache                      - Clear file cache\"\n        echo \"  prune-cache                      - Remove deleted files from cache\"\n        echo \"\"\n        echo \"Code Chunking (Phase 3):\"\n        echo \"  chunk-file <path> [tokens]       - Chunk code at semantic boundaries\"\n        echo \"                                     (default: 500 tokens per chunk)\"\n        echo \"  detect-language <path>           - Detect programming language\"\n        echo \"  find-boundaries <path> [lang]    - Find function/class boundaries\"\n        echo \"\"\n        echo \"Context Budgeting (Phase 4):\"\n        echo \"  context-usage                    - Show current context token usage\"\n        echo \"  context-check                    - Check if budget exceeded\"\n        echo \"  context-remaining                - Show remaining token budget\"\n        echo \"  context-compact                  - Compact memory to reduce usage\"\n        echo \"  set-context-limit <type> <value> - Set context limit\"\n        echo \"                                     (types: total_tokens, working_memory, etc.)\"\n        echo \"\"\n        echo \"Management:\"\n        echo \"  stats                            - Memory statistics\"\n        echo \"  init                             - Initialize memory\"\n        echo \"  scope                            - Show memory scope info\"\n        echo \"\"\n        echo \"Environment:\"\n        echo \"  MEMORY_SCOPE=auto|project|global - Control memory location\"\n        echo \"  Current: $MEMORY_DIR\"\n        ;;\nesac\n","numLines":2211,"startLine":1,"totalLines":2211}},"sourceToolAssistantUUID":"6c37c611-06cc-40ce-808a-200e642a3b72"}
{"parentUuid":"d4c2dc16-17bc-4647-96f3-98979f02dbb4","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Q4o2te8jDmDB7eXGB7mNXQ","type":"message","role":"assistant","content":[{"type":"text","text":"Now let me create a comprehensive test plan and execute it systematically."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":30353,"cache_read_input_tokens":20366,"cache_creation":{"ephemeral_5m_input_tokens":30353,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CX4CVG15XJ1jGdt6oovd8","type":"assistant","uuid":"da7f28f8-11ae-49e1-9dd5-1430aaa52201","timestamp":"2026-01-12T21:32:12.914Z"}
{"parentUuid":"da7f28f8-11ae-49e1-9dd5-1430aaa52201","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01Q4o2te8jDmDB7eXGB7mNXQ","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01BN9BaE4WyWEdFFS4UFDFgn","name":"TodoWrite","input":{"todos":[{"content":"Test working memory (set-task, add-context, get-working, clear-working)","status":"in_progress","activeForm":"Testing working memory operations"},{"content":"Test episodic memory (record, search-episodes, recent-episodes)","status":"pending","activeForm":"Testing episodic memory operations"},{"content":"Test semantic memory (add-fact, get-fact, add-pattern, find-patterns)","status":"pending","activeForm":"Testing semantic memory operations"},{"content":"Test 3-factor scoring (remember-scored with recency + relevance + importance)","status":"pending","activeForm":"Testing 3-factor scoring algorithm"},{"content":"Test checkpoint/restore functionality","status":"pending","activeForm":"Testing checkpoint and restore operations"},{"content":"Test file change detection (cache-file, file-changed, SHA-256)","status":"pending","activeForm":"Testing file change detection system"},{"content":"Test context budgeting (usage calculation, limits, warnings)","status":"pending","activeForm":"Testing context token budgeting"},{"content":"Test git-aware channels (branch organization)","status":"pending","activeForm":"Testing git channel organization"},{"content":"Test hybrid search (BM25 + semantic + RRF)","status":"pending","activeForm":"Testing hybrid search with BM25 and RRF"},{"content":"Compile findings and report bugs with line numbers","status":"pending","activeForm":"Compiling findings and bug report"}]}}],"stop_reason":"tool_use","stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":30353,"cache_read_input_tokens":20366,"cache_creation":{"ephemeral_5m_input_tokens":30353,"ephemeral_1h_input_tokens":0},"output_tokens":556,"service_tier":"standard"}},"requestId":"req_011CX4CVG15XJ1jGdt6oovd8","type":"assistant","uuid":"9cbaaecd-30b7-4a9e-9bc2-4c64d1dceadb","timestamp":"2026-01-12T21:32:18.971Z"}
{"parentUuid":"9cbaaecd-30b7-4a9e-9bc2-4c64d1dceadb","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01BN9BaE4WyWEdFFS4UFDFgn","type":"tool_result","content":"Todos have been modified successfully. Ensure that you continue to use the todo list to track your progress. Please proceed with the current tasks if applicable"}]},"uuid":"1be25455-1dbf-43e3-ab3c-9c3b75577f5f","timestamp":"2026-01-12T21:32:19.556Z","toolUseResult":{"oldTodos":[],"newTodos":[{"content":"Test working memory (set-task, add-context, get-working, clear-working)","status":"in_progress","activeForm":"Testing working memory operations"},{"content":"Test episodic memory (record, search-episodes, recent-episodes)","status":"pending","activeForm":"Testing episodic memory operations"},{"content":"Test semantic memory (add-fact, get-fact, add-pattern, find-patterns)","status":"pending","activeForm":"Testing semantic memory operations"},{"content":"Test 3-factor scoring (remember-scored with recency + relevance + importance)","status":"pending","activeForm":"Testing 3-factor scoring algorithm"},{"content":"Test checkpoint/restore functionality","status":"pending","activeForm":"Testing checkpoint and restore operations"},{"content":"Test file change detection (cache-file, file-changed, SHA-256)","status":"pending","activeForm":"Testing file change detection system"},{"content":"Test context budgeting (usage calculation, limits, warnings)","status":"pending","activeForm":"Testing context token budgeting"},{"content":"Test git-aware channels (branch organization)","status":"pending","activeForm":"Testing git channel organization"},{"content":"Test hybrid search (BM25 + semantic + RRF)","status":"pending","activeForm":"Testing hybrid search with BM25 and RRF"},{"content":"Compile findings and report bugs with line numbers","status":"pending","activeForm":"Compiling findings and bug report"}]},"sourceToolAssistantUUID":"9cbaaecd-30b7-4a9e-9bc2-4c64d1dceadb"}
{"parentUuid":"1be25455-1dbf-43e3-ab3c-9c3b75577f5f","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_011n53gTBsLDr5qx1fT8fwoD","type":"message","role":"assistant","content":[{"type":"text","text":"Let me start testing the memory system systematically."}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":486,"cache_read_input_tokens":50719,"cache_creation":{"ephemeral_5m_input_tokens":486,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CX4CW21VCZMC4WkP8cqoS","type":"assistant","uuid":"4d285075-d47e-4260-9ee6-2c1be73c7439","timestamp":"2026-01-12T21:32:22.737Z"}
{"parentUuid":"4d285075-d47e-4260-9ee6-2c1be73c7439","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_011n53gTBsLDr5qx1fT8fwoD","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01GhQVUeKm2gPEGJ4oyoK9Qm","name":"Bash","input":{"command":"~/.claude/hooks/memory-manager.sh init","description":"Initialize memory system"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":486,"cache_read_input_tokens":50719,"cache_creation":{"ephemeral_5m_input_tokens":486,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CX4CW21VCZMC4WkP8cqoS","type":"assistant","uuid":"af57de21-9a7e-415b-aadf-fab0e0908562","timestamp":"2026-01-12T21:32:23.158Z"}
{"parentUuid":"af57de21-9a7e-415b-aadf-fab0e0908562","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_011n53gTBsLDr5qx1fT8fwoD","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01Way6YW6wRKYzFqFf1u6cbq","name":"Bash","input":{"command":"~/.claude/hooks/memory-manager.sh scope","description":"Check memory scope and git channel"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":4,"cache_creation_input_tokens":486,"cache_read_input_tokens":50719,"cache_creation":{"ephemeral_5m_input_tokens":486,"ephemeral_1h_input_tokens":0},"output_tokens":3,"service_tier":"standard"}},"requestId":"req_011CX4CW21VCZMC4WkP8cqoS","type":"assistant","uuid":"d0075856-18c8-48bc-bcb0-4d939f8bb8e4","timestamp":"2026-01-12T21:32:23.674Z"}
{"parentUuid":"af57de21-9a7e-415b-aadf-fab0e0908562","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01GhQVUeKm2gPEGJ4oyoK9Qm","type":"tool_result","content":"Memory initialized at /Users/imorgado/Desktop/claude-sovereign/.claude/memory/master","is_error":false}]},"uuid":"2a116c94-c6ef-4a17-8968-31877b52857c","timestamp":"2026-01-12T21:32:24.547Z","toolUseResult":{"stdout":"Memory initialized at /Users/imorgado/Desktop/claude-sovereign/.claude/memory/master","stderr":"","interrupted":false,"isImage":false},"sourceToolAssistantUUID":"af57de21-9a7e-415b-aadf-fab0e0908562"}
{"parentUuid":"d0075856-18c8-48bc-bcb0-4d939f8bb8e4","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01Way6YW6wRKYzFqFf1u6cbq","type":"tool_result","content":"Memory Scope Configuration\n\nCurrent settings:\n  MEMORY_DIR: /Users/imorgado/Desktop/claude-sovereign/.claude/memory/master\n  MEMORY_SCOPE: auto\n  Git Channel: master\n\n  Project root: /Users/imorgado/Desktop/claude-sovereign\n\nTo change scope, set MEMORY_SCOPE:\n  export MEMORY_SCOPE=project  # Use project-local memory\n  export MEMORY_SCOPE=global   # Use global memory\n  export MEMORY_SCOPE=auto     # Auto-detect (default)","is_error":false}]},"uuid":"616c50e1-5d68-4512-b4e8-aeff6ce75cc5","timestamp":"2026-01-12T21:32:25.953Z","toolUseResult":{"stdout":"Memory Scope Configuration\n\nCurrent settings:\n  MEMORY_DIR: /Users/imorgado/Desktop/claude-sovereign/.claude/memory/master\n  MEMORY_SCOPE: auto\n  Git Channel: master\n\n  Project root: /Users/imorgado/Desktop/claude-sovereign\n\nTo change scope, set MEMORY_SCOPE:\n  export MEMORY_SCOPE=project  # Use project-local memory\n  export MEMORY_SCOPE=global   # Use global memory\n  export MEMORY_SCOPE=auto     # Auto-detect (default)","stderr":"","interrupted":false,"isImage":false},"sourceToolAssistantUUID":"d0075856-18c8-48bc-bcb0-4d939f8bb8e4"}
{"parentUuid":"e1193efc-3c7c-49fc-bec2-73a19a1010c3","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TQwN7enERziYvVazBkEHyw","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01RvrWvZUVoTN867hnnNXgbJ","name":"Bash","input":{"command":"~/.claude/hooks/memory-manager.sh set-task \"Test memory system\" \"Comprehensive testing of all memory features\"","description":"Test working memory set-task"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":368,"cache_read_input_tokens":51205,"cache_creation":{"ephemeral_5m_input_tokens":368,"ephemeral_1h_input_tokens":0},"output_tokens":24,"service_tier":"standard"}},"requestId":"req_011CX4CWV3g5wa3NmkQSuDPK","type":"assistant","uuid":"dc720e02-bb42-4e7a-9078-acd0d2065205","timestamp":"2026-01-12T21:32:29.595Z"}
{"parentUuid":"dc720e02-bb42-4e7a-9078-acd0d2065205","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TQwN7enERziYvVazBkEHyw","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01AjP1pg3pYC4DBJ3JYSXNkz","name":"Bash","input":{"command":"~/.claude/hooks/memory-manager.sh add-context \"Testing working memory operations\" 8","description":"Test add-context with importance"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":368,"cache_read_input_tokens":51205,"cache_creation":{"ephemeral_5m_input_tokens":368,"ephemeral_1h_input_tokens":0},"output_tokens":24,"service_tier":"standard"}},"requestId":"req_011CX4CWV3g5wa3NmkQSuDPK","type":"assistant","uuid":"92aef31b-4ffd-49aa-af18-fe2920796727","timestamp":"2026-01-12T21:32:30.357Z"}
{"parentUuid":"92aef31b-4ffd-49aa-af18-fe2920796727","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TQwN7enERziYvVazBkEHyw","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01TfN4f8WwyBAiDWtg656hkw","name":"Bash","input":{"command":"~/.claude/hooks/memory-manager.sh add-context \"Verifying episodic memory\" 7","description":"Add another context item"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":368,"cache_read_input_tokens":51205,"cache_creation":{"ephemeral_5m_input_tokens":368,"ephemeral_1h_input_tokens":0},"output_tokens":24,"service_tier":"standard"}},"requestId":"req_011CX4CWV3g5wa3NmkQSuDPK","type":"assistant","uuid":"a7052080-252a-4cc5-beb4-86863a3f9170","timestamp":"2026-01-12T21:32:31.487Z"}
{"parentUuid":"a7052080-252a-4cc5-beb4-86863a3f9170","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TQwN7enERziYvVazBkEHyw","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01NpZGfpAZGi3woR13HBk9s3","name":"Bash","input":{"command":"~/.claude/hooks/memory-manager.sh scratchpad \"Started testing at $(date)\"","description":"Test scratchpad functionality"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":368,"cache_read_input_tokens":51205,"cache_creation":{"ephemeral_5m_input_tokens":368,"ephemeral_1h_input_tokens":0},"output_tokens":24,"service_tier":"standard"}},"requestId":"req_011CX4CWV3g5wa3NmkQSuDPK","type":"assistant","uuid":"7fd12649-6912-4fa2-be45-f546f5fe0e33","timestamp":"2026-01-12T21:32:32.464Z"}
{"parentUuid":"7fd12649-6912-4fa2-be45-f546f5fe0e33","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","message":{"model":"claude-sonnet-4-5-20250929","id":"msg_01TQwN7enERziYvVazBkEHyw","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_018kuemWKYG2sR536eZuhqYQ","name":"Bash","input":{"command":"~/.claude/hooks/memory-manager.sh get-working","description":"Retrieve working memory state"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":5,"cache_creation_input_tokens":368,"cache_read_input_tokens":51205,"cache_creation":{"ephemeral_5m_input_tokens":368,"ephemeral_1h_input_tokens":0},"output_tokens":24,"service_tier":"standard"}},"requestId":"req_011CX4CWV3g5wa3NmkQSuDPK","type":"assistant","uuid":"1ca7674a-5acf-432e-b61c-41d816a8bdfa","timestamp":"2026-01-12T21:32:33.291Z"}
{"parentUuid":"dc720e02-bb42-4e7a-9078-acd0d2065205","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01RvrWvZUVoTN867hnnNXgbJ","type":"tool_result","content":"","is_error":false}]},"uuid":"fab917d4-bfac-4435-b3f9-45a511a43de0","timestamp":"2026-01-12T21:32:34.330Z","toolUseResult":{"stdout":"","stderr":"","interrupted":false,"isImage":false},"sourceToolAssistantUUID":"dc720e02-bb42-4e7a-9078-acd0d2065205"}
{"parentUuid":"92aef31b-4ffd-49aa-af18-fe2920796727","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01AjP1pg3pYC4DBJ3JYSXNkz","type":"tool_result","content":"","is_error":false}]},"uuid":"59dd0f35-29f4-44fa-9ece-3adeda11acda","timestamp":"2026-01-12T21:32:35.970Z","toolUseResult":{"stdout":"","stderr":"","interrupted":false,"isImage":false},"sourceToolAssistantUUID":"92aef31b-4ffd-49aa-af18-fe2920796727"}
{"parentUuid":"a7052080-252a-4cc5-beb4-86863a3f9170","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"Exit code 145\n[Request interrupted by user for tool use]\nCommand aborted before execution","is_error":true,"tool_use_id":"toolu_01TfN4f8WwyBAiDWtg656hkw"}]},"uuid":"2d7a759a-7e4f-435f-8fd7-e0cb04ea8ea8","timestamp":"2026-01-12T21:32:36.324Z","toolUseResult":"Error: Exit code 145\n[Request interrupted by user for tool use]\nCommand aborted before execution","sourceToolAssistantUUID":"a7052080-252a-4cc5-beb4-86863a3f9170"}
{"parentUuid":"7fd12649-6912-4fa2-be45-f546f5fe0e33","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"The user doesn't want to take this action right now. STOP what you are doing and wait for the user to tell you how to proceed.","is_error":true,"tool_use_id":"toolu_01NpZGfpAZGi3woR13HBk9s3"}]},"uuid":"5e7ab775-1ae7-4ba4-9ce5-1eb0f5fc0f86","timestamp":"2026-01-12T21:32:36.407Z","toolUseResult":"The user doesn't want to take this action right now. STOP what you are doing and wait for the user to tell you how to proceed.","sourceToolAssistantUUID":"7fd12649-6912-4fa2-be45-f546f5fe0e33"}
{"parentUuid":"1ca7674a-5acf-432e-b61c-41d816a8bdfa","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"type":"tool_result","content":"The user doesn't want to take this action right now. STOP what you are doing and wait for the user to tell you how to proceed.","is_error":true,"tool_use_id":"toolu_018kuemWKYG2sR536eZuhqYQ"}]},"uuid":"52356aa7-ecda-481a-b053-96dbe325522a","timestamp":"2026-01-12T21:32:36.483Z","toolUseResult":"The user doesn't want to take this action right now. STOP what you are doing and wait for the user to tell you how to proceed.","sourceToolAssistantUUID":"1ca7674a-5acf-432e-b61c-41d816a8bdfa"}
{"parentUuid":"52356aa7-ecda-481a-b053-96dbe325522a","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/Desktop/claude-sovereign","sessionId":"aafe69e8-4972-4b94-b4c9-b8b83fa963af","version":"2.1.5","gitBranch":"","agentId":"a9b185b","slug":"zazzy-jingling-bee","type":"user","message":{"role":"user","content":[{"type":"text","text":"[Request interrupted by user for tool use]"}]},"uuid":"69f401e2-2cca-4d92-b377-d1df228bc910","timestamp":"2026-01-12T21:32:36.521Z"}
