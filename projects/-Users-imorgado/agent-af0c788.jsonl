{"parentUuid":null,"isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","type":"user","message":{"role":"user","content":"Research Meta's Llama 4 Maverick model:\n\n1. What is Llama 4 Maverick? Is it officially released?\n2. What is its context window size?\n3. What are the model sizes and VRAM requirements?\n4. Is it available on HuggingFace?\n5. Is there an abliterated/uncensored version?\n6. Is it compatible with vLLM?\n7. What GPUs are needed to run it?\n\nSearch for the latest information about this model."},"uuid":"911e4036-e998-4d23-af05-50e700f083eb","timestamp":"2026-01-05T21:46:01.074Z"}
{"parentUuid":"911e4036-e998-4d23-af05-50e700f083eb","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","message":{"model":"claude-opus-4-5-20251101","id":"msg_01X5s7FVYDFWwGVjwJcQ3Guf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016xujSjpoNGLZqBH3GWW6gc","name":"WebSearch","input":{"query":"Meta Llama 4 Maverick model official release 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":18557,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":18557,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWpxt9dZ5F5Dny3RXQzAx","type":"assistant","uuid":"fcd30399-01f4-4c62-a9cd-68aaf078eabb","timestamp":"2026-01-05T21:46:04.353Z"}
{"parentUuid":"fcd30399-01f4-4c62-a9cd-68aaf078eabb","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","message":{"model":"claude-opus-4-5-20251101","id":"msg_01X5s7FVYDFWwGVjwJcQ3Guf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01YALdVoqSPvU7MozXLwP4rV","name":"WebSearch","input":{"query":"Llama 4 Maverick context window size parameters specs"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":18557,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":18557,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWpxt9dZ5F5Dny3RXQzAx","type":"assistant","uuid":"b68adeb6-7716-4857-9677-24ad074f787b","timestamp":"2026-01-05T21:46:04.948Z"}
{"parentUuid":"b68adeb6-7716-4857-9677-24ad074f787b","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","message":{"model":"claude-opus-4-5-20251101","id":"msg_01X5s7FVYDFWwGVjwJcQ3Guf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01UWnWU3Z5WsaxWFCBic8yRm","name":"WebSearch","input":{"query":"Llama 4 Maverick VRAM requirements GPU memory"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":18557,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":18557,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWpxt9dZ5F5Dny3RXQzAx","type":"assistant","uuid":"69cb542e-a1ad-4f6d-b23c-05dfd17b3a3d","timestamp":"2026-01-05T21:46:05.442Z"}
{"parentUuid":"69cb542e-a1ad-4f6d-b23c-05dfd17b3a3d","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","message":{"model":"claude-opus-4-5-20251101","id":"msg_01X5s7FVYDFWwGVjwJcQ3Guf","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01429mBGGgqA8saVPPjSoCpp","name":"WebSearch","input":{"query":"Llama 4 Maverick HuggingFace vLLM compatible"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1,"cache_creation_input_tokens":18557,"cache_read_input_tokens":0,"cache_creation":{"ephemeral_5m_input_tokens":18557,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWpxt9dZ5F5Dny3RXQzAx","type":"assistant","uuid":"539e8b5e-3852-4f3d-badb-42bee5d239dd","timestamp":"2026-01-05T21:46:06.090Z"}
{"parentUuid":"a77abbd9-cd5c-46ae-be7a-c82122b80c45","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01YALdVoqSPvU7MozXLwP4rV","type":"tool_result","content":"Web search results for query: \"Llama 4 Maverick context window size parameters specs\"\n\nLinks: [{\"title\":\"Meta Llama 4 Maverick (New)\",\"url\":\"https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm\"},{\"title\":\"The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation\",\"url\":\"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\"},{\"title\":\"meta-llama/Llama-4-Maverick-17B-128E-Instruct Â· Hugging Face\",\"url\":\"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct\"},{\"title\":\"Llama 4 Maverick - GroqDocs\",\"url\":\"https://console.groq.com/docs/model/meta-llama/llama-4-maverick-17b-128e-instruct\"},{\"title\":\"Llama-4-Maverick-17B-128E-Instruct context window is now 64K! - Announcements - SambaNova Developer Community\",\"url\":\"https://community.sambanova.ai/t/llama-4-maverick-17b-128e-instruct-context-window-is-now-64k/1120\"},{\"title\":\"Initial impressions of Llama 4\",\"url\":\"https://simonwillison.net/2025/Apr/5/llama-4-notes/\"},{\"title\":\"llama4\",\"url\":\"https://ollama.com/library/llama4\"},{\"title\":\"Llama 4 Maverick: Pricing, Context Window, Benchmarks, and More\",\"url\":\"https://llm-stats.com/models/llama-4-maverick\"},{\"title\":\"What to know about Metaâ€™s Llama 4 model family - TechTalks\",\"url\":\"https://bdtechtalks.com/2025/04/06/meta-llama-4/\"},{\"title\":\"Llama 4 in vLLM | vLLM Blog\",\"url\":\"https://blog.vllm.ai/2025/04/05/llama4.html\"}]\n\nBased on the search results, here are the specifications for **Llama 4 Maverick**:\n\n## Key Parameters\n\n| Specification | Value |\n|--------------|-------|\n| **Active Parameters** | 17 billion |\n| **Total Parameters** | 400 billion |\n| **Number of Experts** | 128 experts (Mixture-of-Experts architecture) |\n| **Context Window** | Up to 1 million tokens (varies by provider) |\n\n## Context Window Details\n\nThe context window for Llama 4 Maverick varies depending on the deployment:\n\n- The model includes a 1 million token context window\n- Oracle Cloud supports a context length of 512,000 tokens\n- Groq offers a 128K token context window\n- SambaNova increased their context window to 64K for their preview model\n\n## Architecture\n\nMaverick uses 128 experts with 17B activated parameters. Only one expert is activated per token, maintaining efficiency.(no content)Llama 4 uses Interleaved RoPE (iRoPE), which interleaves global attention (without RoPE) with chunked local attention (with RoPE) in a 1:3 ratio. The local attention layer attends to tokens in non-overlapping chunks, significantly reducing the quadratic complexity of attention as context length scales.\n\n## Training Data\n\nLlama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta's products and services.(no content)The pretraining data has a cutoff of August 2024.\n\n## Performance\n\nLlama 4 Maverick is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and codingâ€”at less than half the active parameters.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"2f05ceb3-a923-48c5-a421-724f1a2fe450","timestamp":"2026-01-05T21:46:22.162Z","toolUseResult":{"query":"Llama 4 Maverick context window size parameters specs","results":[{"tool_use_id":"srvtoolu_01NicfhsVcGMgi5NJY7qgtRg","content":[{"title":"Meta Llama 4 Maverick (New)","url":"https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm"},{"title":"The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation","url":"https://ai.meta.com/blog/llama-4-multimodal-intelligence/"},{"title":"meta-llama/Llama-4-Maverick-17B-128E-Instruct Â· Hugging Face","url":"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"},{"title":"Llama 4 Maverick - GroqDocs","url":"https://console.groq.com/docs/model/meta-llama/llama-4-maverick-17b-128e-instruct"},{"title":"Llama-4-Maverick-17B-128E-Instruct context window is now 64K! - Announcements - SambaNova Developer Community","url":"https://community.sambanova.ai/t/llama-4-maverick-17b-128e-instruct-context-window-is-now-64k/1120"},{"title":"Initial impressions of Llama 4","url":"https://simonwillison.net/2025/Apr/5/llama-4-notes/"},{"title":"llama4","url":"https://ollama.com/library/llama4"},{"title":"Llama 4 Maverick: Pricing, Context Window, Benchmarks, and More","url":"https://llm-stats.com/models/llama-4-maverick"},{"title":"What to know about Metaâ€™s Llama 4 model family - TechTalks","url":"https://bdtechtalks.com/2025/04/06/meta-llama-4/"},{"title":"Llama 4 in vLLM | vLLM Blog","url":"https://blog.vllm.ai/2025/04/05/llama4.html"}]},"Based on the search results, here are the specifications for **Llama 4 Maverick**:\n\n## Key Parameters\n\n| Specification | Value |\n|--------------|-------|\n| **Active Parameters** | 17 billion |\n| **Total Parameters** | 400 billion |\n| **Number of Experts** | 128 experts (Mixture-of-Experts architecture) |\n| **Context Window** | Up to 1 million tokens (varies by provider) |\n\n## Context Window Details\n\nThe context window for Llama 4 Maverick varies depending on the deployment:\n\n- The model includes a 1 million token context window\n- Oracle Cloud supports a context length of 512,000 tokens\n- Groq offers a 128K token context window\n- SambaNova increased their context window to 64K for their preview model\n\n## Architecture\n\nMaverick uses 128 experts with 17B activated parameters. Only one expert is activated per token, maintaining efficiency.(no content)Llama 4 uses Interleaved RoPE (iRoPE), which interleaves global attention (without RoPE) with chunked local attention (with RoPE) in a 1:3 ratio. The local attention layer attends to tokens in non-overlapping chunks, significantly reducing the quadratic complexity of attention as context length scales.\n\n## Training Data\n\nLlama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta's products and services.(no content)The pretraining data has a cutoff of August 2024.\n\n## Performance\n\nLlama 4 Maverick is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to the new DeepSeek v3 on reasoning and codingâ€”at less than half the active parameters."],"durationSeconds":16.031363874997943}}
{"parentUuid":"2f05ceb3-a923-48c5-a421-724f1a2fe450","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016xujSjpoNGLZqBH3GWW6gc","type":"tool_result","content":"Web search results for query: \"Meta Llama 4 Maverick model official release 2025\"\n\nLinks: [{\"title\":\"Meta Llama 4 Maverick (New)\",\"url\":\"https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm\"},{\"title\":\"LLaMA 4 Explained - Everything You Need to Know\",\"url\":\"https://www.labellerr.com/blog/llama-4/\"},{\"title\":\"Metaâ€™s Llama 4 Family: The Complete Guide to Scout, Maverick, and Behemoth AI Models in 2025 | by Divyansh Bhatia | Medium\",\"url\":\"https://medium.com/@divyanshbhatiajm19/metas-llama-4-family-the-complete-guide-to-scout-maverick-and-behemoth-ai-models-in-2025-21a90c882e8a\"},{\"title\":\"Llama 4â€™s 2T Behemoth: The Reality for Open-Source AI in 2025\",\"url\":\"https://skywork.ai/blog/llama-4-behemoth-open-source-2025/\"},{\"title\":\"Llama 4 Maverick - API, Providers, Stats | OpenRouter\",\"url\":\"https://openrouter.ai/meta-llama/llama-4-maverick\"},{\"title\":\"The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation\",\"url\":\"https://ai.meta.com/blog/llama-4-multimodal-intelligence/\"},{\"title\":\"Meta releases Llama 4, a new crop of flagship AI models | TechCrunch\",\"url\":\"https://techcrunch.com/2025/04/05/meta-releases-llama-4-a-new-crop-of-flagship-ai-models/\"},{\"title\":\"Meta Llama 4 Maverick and Llama 4 Scout now available in watsonx.ai | IBM\",\"url\":\"https://www.ibm.com/new/announcements/meta-llama-4-maverick-and-llama-4-scout-now-available-in-watsonx-ai\"},{\"title\":\"Meta Unveils Llama 4 Scout and Maverick | by Justin Downes | Medium\",\"url\":\"https://medium.com/@justin.edgewoods/meta-unveils-llama-4-scout-and-maverick-97e7e4d02bac\"},{\"title\":\"Meta Unleashes New Llama 4 AI Models\",\"url\":\"https://www.aiwire.net/2025/04/08/meta-unleashes-new-llama-4-ai-models/\"}]\n\nBased on the search results, here's information about the Meta Llama 4 Maverick model:\n\n## Meta Llama 4 Maverick - Official Release (2025)\n\nLlama 4 Maverick was released on April 5, 2025 under the Llama 4 Community License, and is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.\n\n### Key Specifications\n\nThe model features 17 billion active parameters within a larger framework of about 400 billion parameters, using 128 experts.(no content)The Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities.(no content)It supports a context length of 512,000 tokens, though coverage the week of launch described Maverick's context window as reaching 1 million tokens.\n\n### Performance & Capabilities\n\nLlama 4 Maverick is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to DeepSeek v3 on reasoning and codingâ€”at less than half the active parameters.(no content)These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems.(no content)Maverick has multilingual support, trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\n\n### Availability\n\nScout and Maverick are openly available on Llama.com and from Meta's partners, including the AI dev platform Hugging Face.(no content)Maverick is the main workhorse - the one Meta itself uses in apps like Facebook, Instagram, and WhatsApp.(no content)Maverick requires an Nvidia H100 DGX system or equivalent, according to Meta's calculations.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"fbaa91d1-98f6-41ec-98e8-961058ad0738","timestamp":"2026-01-05T21:46:24.909Z","toolUseResult":{"query":"Meta Llama 4 Maverick model official release 2025","results":[{"tool_use_id":"srvtoolu_012YXez9eJrqdBTShWWV6X8P","content":[{"title":"Meta Llama 4 Maverick (New)","url":"https://docs.oracle.com/en-us/iaas/Content/generative-ai/meta-llama-4-maverick.htm"},{"title":"LLaMA 4 Explained - Everything You Need to Know","url":"https://www.labellerr.com/blog/llama-4/"},{"title":"Metaâ€™s Llama 4 Family: The Complete Guide to Scout, Maverick, and Behemoth AI Models in 2025 | by Divyansh Bhatia | Medium","url":"https://medium.com/@divyanshbhatiajm19/metas-llama-4-family-the-complete-guide-to-scout-maverick-and-behemoth-ai-models-in-2025-21a90c882e8a"},{"title":"Llama 4â€™s 2T Behemoth: The Reality for Open-Source AI in 2025","url":"https://skywork.ai/blog/llama-4-behemoth-open-source-2025/"},{"title":"Llama 4 Maverick - API, Providers, Stats | OpenRouter","url":"https://openrouter.ai/meta-llama/llama-4-maverick"},{"title":"The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation","url":"https://ai.meta.com/blog/llama-4-multimodal-intelligence/"},{"title":"Meta releases Llama 4, a new crop of flagship AI models | TechCrunch","url":"https://techcrunch.com/2025/04/05/meta-releases-llama-4-a-new-crop-of-flagship-ai-models/"},{"title":"Meta Llama 4 Maverick and Llama 4 Scout now available in watsonx.ai | IBM","url":"https://www.ibm.com/new/announcements/meta-llama-4-maverick-and-llama-4-scout-now-available-in-watsonx-ai"},{"title":"Meta Unveils Llama 4 Scout and Maverick | by Justin Downes | Medium","url":"https://medium.com/@justin.edgewoods/meta-unveils-llama-4-scout-and-maverick-97e7e4d02bac"},{"title":"Meta Unleashes New Llama 4 AI Models","url":"https://www.aiwire.net/2025/04/08/meta-unleashes-new-llama-4-ai-models/"}]},"Based on the search results, here's information about the Meta Llama 4 Maverick model:\n\n## Meta Llama 4 Maverick - Official Release (2025)\n\nLlama 4 Maverick was released on April 5, 2025 under the Llama 4 Community License, and is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.\n\n### Key Specifications\n\nThe model features 17 billion active parameters within a larger framework of about 400 billion parameters, using 128 experts.(no content)The Llama 4 models leverage a Mixture of Experts (MoE) architecture, enabling efficient and powerful processing capabilities.(no content)It supports a context length of 512,000 tokens, though coverage the week of launch described Maverick's context window as reaching 1 million tokens.\n\n### Performance & Capabilities\n\nLlama 4 Maverick is the best multimodal model in its class, beating GPT-4o and Gemini 2.0 Flash across a broad range of widely reported benchmarks, while achieving comparable results to DeepSeek v3 on reasoning and codingâ€”at less than half the active parameters.(no content)These models are optimized for multimodal understanding, multilingual tasks, coding, tool-calling, and powering agentic systems.(no content)Maverick has multilingual support, trained on data encompassing 200 languages, with fine-tuning support for 12 languages including Arabic, English, French, German, Hindi, Indonesian, Italian, Portuguese, Spanish, Tagalog, Thai, and Vietnamese.\n\n### Availability\n\nScout and Maverick are openly available on Llama.com and from Meta's partners, including the AI dev platform Hugging Face.(no content)Maverick is the main workhorse - the one Meta itself uses in apps like Facebook, Instagram, and WhatsApp.(no content)Maverick requires an Nvidia H100 DGX system or equivalent, according to Meta's calculations."],"durationSeconds":18.77559329199791}}
{"parentUuid":"fbaa91d1-98f6-41ec-98e8-961058ad0738","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01429mBGGgqA8saVPPjSoCpp","type":"tool_result","content":"Web search results for query: \"Llama 4 Maverick HuggingFace vLLM compatible\"\n\nLinks: [{\"title\":\"Welcome Llama 4 Maverick & Scout on Hugging Face\",\"url\":\"https://huggingface.co/blog/llama4-release\"},{\"title\":\"Llama 4 in vLLM | vLLM Blog\",\"url\":\"https://blog.vllm.ai/2025/04/05/llama4.html\"},{\"title\":\"RedHatAI/Llama-4-Maverick-17B-128E-Instruct Â· Hugging Face\",\"url\":\"https://huggingface.co/RedHatAI/Llama-4-Maverick-17B-128E-Instruct\"},{\"title\":\"Llama 4 herd is here with Day 0 inference support in vLLM | Red Hat Developer\",\"url\":\"https://developers.redhat.com/articles/2025/04/05/llama-4-herd-here-day-zero-inference-support-vllm\"},{\"title\":\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 Â· Support for FP8 + Fused MoE layers in vLLM?\",\"url\":\"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8/discussions/23\"},{\"title\":\"unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF Â· Hugging Face\",\"url\":\"https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF\"},{\"title\":\"Llama 4 With vLLM: A Guide With Demo Project | DataCamp\",\"url\":\"https://www.datacamp.com/tutorial/llama-4-vllm\"},{\"title\":\"meta-llama/Llama-4-Maverick-17B-128E-Instruct Â· Deploying production ready Llama-4 [Maverick] on your AWS with vLLM\",\"url\":\"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct/discussions/21\"},{\"title\":\"Llama 4 Herd is here and already works with Red Hat OpenShift AI\",\"url\":\"https://www.redhat.com/en/blog/llama-4-herd-here-and-already-available-openshift-ai\"},{\"title\":\"Power Up Llama 4 with AMD Instinct: A Developerâ€™s Day 0 Quickstart\",\"url\":\"https://rocm.blogs.amd.com/artificial-intelligence/llama4-day-0-support/README.html\"}]\n\nBased on my search, here's what I found about Llama 4 Maverick on HuggingFace with vLLM compatibility:\n\n## Llama 4 Maverick - vLLM Compatibility\n\nvLLM now supports the Llama 4 herd of models: Scout (17B-16E) and Maverick (17B-128E).(no content)You can run these powerful long-context, natively multi-modal mixture-of-experts models in vLLM by updating to version v0.8.3 or later.\n\n### Key Details\n\n**Architecture:**\n- Llama 4 Maverick has 400 billion total parameters in the model, but the inference internally directs to an \"expert\", where only 17B parameters need to be processed per token.\n- Maverick uses 128 experts (17B activated parameters). Only one expert is activated per token, maintaining efficiency.\n\n**Available Models on HuggingFace:**\n- Both Scout and Maverick come with BF16 weights. Additionally, Maverick also comes with the FP8-quantized version on Day 0.\n- The model can be deployed efficiently on vLLM, Red Hat Enterprise Linux AI, and OpenShift AI.\n\n**Hardware Requirements:**\n- Using 8xH100, vLLM can serve Scout with 1M context and Maverick with about 430K.\n- The FP8-quantized version of Llama 4 Maverick 128E enables the 128 expert model to fit on a single NVIDIA 8xH100 node, resulting in more performance with lower costs.\n\n**Sample vLLM Command:**\n```bash\nvllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 \\\n  --tensor-parallel-size 8 \\\n  --max-model-len 430000\n```\n\nSetting `--kv-cache-dtype fp8` can potentially double the usable context window and provide a performance boost.\n\n**Multimodal Support:**\n- The Llama 4 models excel at image understanding up to 8-10 images. By default, vLLM server accepts 1 image per request. Pass `--limit-mm-per-prompt image=10` to serve up to 10 images per request with OpenAI-compatible API.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"fa3f58dc-cc59-418d-89b5-d43b66a55762","timestamp":"2026-01-05T21:46:25.239Z","toolUseResult":{"query":"Llama 4 Maverick HuggingFace vLLM compatible","results":[{"tool_use_id":"srvtoolu_01TnW3crTjEw1MFSNpRTGBKK","content":[{"title":"Welcome Llama 4 Maverick & Scout on Hugging Face","url":"https://huggingface.co/blog/llama4-release"},{"title":"Llama 4 in vLLM | vLLM Blog","url":"https://blog.vllm.ai/2025/04/05/llama4.html"},{"title":"RedHatAI/Llama-4-Maverick-17B-128E-Instruct Â· Hugging Face","url":"https://huggingface.co/RedHatAI/Llama-4-Maverick-17B-128E-Instruct"},{"title":"Llama 4 herd is here with Day 0 inference support in vLLM | Red Hat Developer","url":"https://developers.redhat.com/articles/2025/04/05/llama-4-herd-here-day-zero-inference-support-vllm"},{"title":"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 Â· Support for FP8 + Fused MoE layers in vLLM?","url":"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8/discussions/23"},{"title":"unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF Â· Hugging Face","url":"https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF"},{"title":"Llama 4 With vLLM: A Guide With Demo Project | DataCamp","url":"https://www.datacamp.com/tutorial/llama-4-vllm"},{"title":"meta-llama/Llama-4-Maverick-17B-128E-Instruct Â· Deploying production ready Llama-4 [Maverick] on your AWS with vLLM","url":"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct/discussions/21"},{"title":"Llama 4 Herd is here and already works with Red Hat OpenShift AI","url":"https://www.redhat.com/en/blog/llama-4-herd-here-and-already-available-openshift-ai"},{"title":"Power Up Llama 4 with AMD Instinct: A Developerâ€™s Day 0 Quickstart","url":"https://rocm.blogs.amd.com/artificial-intelligence/llama4-day-0-support/README.html"}]},"Based on my search, here's what I found about Llama 4 Maverick on HuggingFace with vLLM compatibility:\n\n## Llama 4 Maverick - vLLM Compatibility\n\nvLLM now supports the Llama 4 herd of models: Scout (17B-16E) and Maverick (17B-128E).(no content)You can run these powerful long-context, natively multi-modal mixture-of-experts models in vLLM by updating to version v0.8.3 or later.\n\n### Key Details\n\n**Architecture:**\n- Llama 4 Maverick has 400 billion total parameters in the model, but the inference internally directs to an \"expert\", where only 17B parameters need to be processed per token.\n- Maverick uses 128 experts (17B activated parameters). Only one expert is activated per token, maintaining efficiency.\n\n**Available Models on HuggingFace:**\n- Both Scout and Maverick come with BF16 weights. Additionally, Maverick also comes with the FP8-quantized version on Day 0.\n- The model can be deployed efficiently on vLLM, Red Hat Enterprise Linux AI, and OpenShift AI.\n\n**Hardware Requirements:**\n- Using 8xH100, vLLM can serve Scout with 1M context and Maverick with about 430K.\n- The FP8-quantized version of Llama 4 Maverick 128E enables the 128 expert model to fit on a single NVIDIA 8xH100 node, resulting in more performance with lower costs.\n\n**Sample vLLM Command:**\n```bash\nvllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 \\\n  --tensor-parallel-size 8 \\\n  --max-model-len 430000\n```\n\nSetting `--kv-cache-dtype fp8` can potentially double the usable context window and provide a performance boost.\n\n**Multimodal Support:**\n- The Llama 4 models excel at image understanding up to 8-10 images. By default, vLLM server accepts 1 image per request. Pass `--limit-mm-per-prompt image=10` to serve up to 10 images per request with OpenAI-compatible API."],"durationSeconds":19.107024333000183}}
{"parentUuid":"fa3f58dc-cc59-418d-89b5-d43b66a55762","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01UWnWU3Z5WsaxWFCBic8yRm","type":"tool_result","content":"Web search results for query: \"Llama 4 Maverick VRAM requirements GPU memory\"\n\nLinks: [{\"title\":\"Llama 4 GPU System Requirements (Scout, Maverick, Behemoth)\",\"url\":\"https://apxml.com/posts/llama-4-system-requirements\"},{\"title\":\"Can Your Hardware or VRAM Handle Llama 4 Maverick? | by Novita AI | Medium\",\"url\":\"https://medium.com/@marketing_novita.ai/can-your-hardware-or-vram-handle-llama-4-maverick-a024b63f8489\"},{\"title\":\"Llama 4 GPU System/GPU Requirements. Running LLaMA Locally | BIZON\",\"url\":\"https://bizon-tech.com/blog/llama-4-system-gpu-requirements-running-locally\"},{\"title\":\"Llama 4 Maverick: Specifications and GPU VRAM Requirements\",\"url\":\"https://apxml.com/models/llama-4-maverick\"},{\"title\":\"Llama 4: How to Run & Fine-tune | Unsloth Documentation\",\"url\":\"https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms/llama-4-how-to-run-and-fine-tune\"},{\"title\":\"Llama 4: What You Need to Know - Gradient Flow\",\"url\":\"https://gradientflow.com/llama-4-what-you-need-to-know/\"},{\"title\":\"Llama 4 Scout and Maverick Are Hereâ€”How Do They Shape Up? | Runpod Blog\",\"url\":\"https://www.runpod.io/blog/llama4-scout-maverick\"},{\"title\":\"Meta Releases Llama 4: Hereâ€™s the Hardware Youâ€™ll Need to Run It Yourself | Hardware Corner\",\"url\":\"https://www.hardware-corner.net/meta-releases-llama-4-what-hardware/\"},{\"title\":\"Llama 4 - Finetune & Run with Unsloth\",\"url\":\"https://unsloth.ai/blog/llama4\"},{\"title\":\"Meta Llama AI Requirements [What you Need to Use It] ðŸ’»\",\"url\":\"https://llamaimodel.com/requirements/\"}]\n\nHere are the results for Llama 4 Maverick VRAM requirements:\n\n## Llama 4 Maverick GPU Memory Requirements\n\n### Model Overview\nLlama 4 Maverick leverages a Mixture-of-Experts (MoE) design. It comprises 400 billion total parameters, with only 17 billion parameters actively engaged per token during inference. This efficiency is achieved through the use of 128 experts.\n\n### VRAM Requirements\n\n**Full Precision (FP16/BF16):**\nLlama 4 Maverick requires up to 145,016GB VRAM for FP16/128K configurations, far exceeding consumer GPUs (e.g., H100's 80GB).(no content)At full weights, Maverick requires approximately 800 GB + 16 GB KV cache for an 8k context.\n\n**Key Memory Consideration:**\nThe entire model (e.g., all 400B parameters of Maverick) must be loaded into memory, even though only a fraction is used per token.(no content)These MoE models will still need 2GB of VRAM for each 1B parameters overall, but they will infer much faster than a dense model as only the active parameters will be needed.\n\n### Quantized Options\n\nMaverick (402B) went from 422GB to just 122GB with quantization (-70% reduction).(no content)INT4 quantization reduces VRAM needs by 96%, though with slight performance degradation.\n\n### Practical Recommendations\n\nFor Llama 4 Maverick, it's best to have 2 RTX 4090s (2 x 24GB) for running quantized versions.(no content)While only 17B active parameters are used per token, loading the full 400B parameter MoE architecture pushes memory limits hard â€“ especially at higher precision.\n\n### Hardware Costs\nDeploying FP16 models demands thousands of H100 GPUs, costing $120Kâ€“$480K, excluding operational expenses.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"c62109b3-b477-4ff4-93ae-f4daa433be83","timestamp":"2026-01-05T21:46:26.164Z","toolUseResult":{"query":"Llama 4 Maverick VRAM requirements GPU memory","results":[{"tool_use_id":"srvtoolu_0124FP4EGkSS9epryMk5QDKG","content":[{"title":"Llama 4 GPU System Requirements (Scout, Maverick, Behemoth)","url":"https://apxml.com/posts/llama-4-system-requirements"},{"title":"Can Your Hardware or VRAM Handle Llama 4 Maverick? | by Novita AI | Medium","url":"https://medium.com/@marketing_novita.ai/can-your-hardware-or-vram-handle-llama-4-maverick-a024b63f8489"},{"title":"Llama 4 GPU System/GPU Requirements. Running LLaMA Locally | BIZON","url":"https://bizon-tech.com/blog/llama-4-system-gpu-requirements-running-locally"},{"title":"Llama 4 Maverick: Specifications and GPU VRAM Requirements","url":"https://apxml.com/models/llama-4-maverick"},{"title":"Llama 4: How to Run & Fine-tune | Unsloth Documentation","url":"https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms/llama-4-how-to-run-and-fine-tune"},{"title":"Llama 4: What You Need to Know - Gradient Flow","url":"https://gradientflow.com/llama-4-what-you-need-to-know/"},{"title":"Llama 4 Scout and Maverick Are Hereâ€”How Do They Shape Up? | Runpod Blog","url":"https://www.runpod.io/blog/llama4-scout-maverick"},{"title":"Meta Releases Llama 4: Hereâ€™s the Hardware Youâ€™ll Need to Run It Yourself | Hardware Corner","url":"https://www.hardware-corner.net/meta-releases-llama-4-what-hardware/"},{"title":"Llama 4 - Finetune & Run with Unsloth","url":"https://unsloth.ai/blog/llama4"},{"title":"Meta Llama AI Requirements [What you Need to Use It] ðŸ’»","url":"https://llamaimodel.com/requirements/"}]},"Here are the results for Llama 4 Maverick VRAM requirements:\n\n## Llama 4 Maverick GPU Memory Requirements\n\n### Model Overview\nLlama 4 Maverick leverages a Mixture-of-Experts (MoE) design. It comprises 400 billion total parameters, with only 17 billion parameters actively engaged per token during inference. This efficiency is achieved through the use of 128 experts.\n\n### VRAM Requirements\n\n**Full Precision (FP16/BF16):**\nLlama 4 Maverick requires up to 145,016GB VRAM for FP16/128K configurations, far exceeding consumer GPUs (e.g., H100's 80GB).(no content)At full weights, Maverick requires approximately 800 GB + 16 GB KV cache for an 8k context.\n\n**Key Memory Consideration:**\nThe entire model (e.g., all 400B parameters of Maverick) must be loaded into memory, even though only a fraction is used per token.(no content)These MoE models will still need 2GB of VRAM for each 1B parameters overall, but they will infer much faster than a dense model as only the active parameters will be needed.\n\n### Quantized Options\n\nMaverick (402B) went from 422GB to just 122GB with quantization (-70% reduction).(no content)INT4 quantization reduces VRAM needs by 96%, though with slight performance degradation.\n\n### Practical Recommendations\n\nFor Llama 4 Maverick, it's best to have 2 RTX 4090s (2 x 24GB) for running quantized versions.(no content)While only 17B active parameters are used per token, loading the full 400B parameter MoE architecture pushes memory limits hard â€“ especially at higher precision.\n\n### Hardware Costs\nDeploying FP16 models demands thousands of H100 GPUs, costing $120Kâ€“$480K, excluding operational expenses."],"durationSeconds":20.032984042000024}}
{"parentUuid":"c62109b3-b477-4ff4-93ae-f4daa433be83","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Fa9MtRrxCJEfjAGawCq922","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_016dXFNVgL8dMERuJHb5nsxS","name":"WebSearch","input":{"query":"Llama 4 Maverick abliterated uncensored version HuggingFace 2025"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1305,"cache_creation_input_tokens":3479,"cache_read_input_tokens":18557,"cache_creation":{"ephemeral_5m_input_tokens":3479,"ephemeral_1h_input_tokens":0},"output_tokens":25,"service_tier":"standard"}},"requestId":"req_011CWpxuzrGqLcNW6yWG6ah5","type":"assistant","uuid":"2646a367-6a49-4450-8788-267e79fa70f2","timestamp":"2026-01-05T21:46:36.156Z"}
{"parentUuid":"2646a367-6a49-4450-8788-267e79fa70f2","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","message":{"model":"claude-opus-4-5-20251101","id":"msg_01Fa9MtRrxCJEfjAGawCq922","type":"message","role":"assistant","content":[{"type":"tool_use","id":"toolu_01S7p2oJWVrYWdTAqohQ8oqj","name":"WebSearch","input":{"query":"Llama 4 Maverick quantized GGUF version local deployment"}}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":1305,"cache_creation_input_tokens":3479,"cache_read_input_tokens":18557,"cache_creation":{"ephemeral_5m_input_tokens":3479,"ephemeral_1h_input_tokens":0},"output_tokens":128,"service_tier":"standard"}},"requestId":"req_011CWpxuzrGqLcNW6yWG6ah5","type":"assistant","uuid":"9f19a68d-e3d6-49f7-92f3-b5e874d26c18","timestamp":"2026-01-05T21:46:37.937Z"}
{"parentUuid":"f5e4e1bb-1b84-47f0-a619-926f8259e890","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_016dXFNVgL8dMERuJHb5nsxS","type":"tool_result","content":"Web search results for query: \"Llama 4 Maverick abliterated uncensored version HuggingFace 2025\"\n\nLinks: [{\"title\":\"meta-llama/Llama-4-Maverick-17B-128E-Original Â· Hugging Face\",\"url\":\"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Original\"},{\"title\":\"Welcome Llama 4 Maverick & Scout on Hugging Face\",\"url\":\"https://huggingface.co/blog/llama4-release\"},{\"title\":\"meta-llama/Llama-4-Maverick-17B-128E Â· Hugging Face\",\"url\":\"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E\"},{\"title\":\"meta-llama/Llama-4-Maverick-17B-128E-Instruct Â· Hugging Face\",\"url\":\"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct\"},{\"title\":\"unsloth/Llama-4-Maverick-17B-128E Â· Hugging Face\",\"url\":\"https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E\"},{\"title\":\"unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF Â· Hugging Face\",\"url\":\"https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF\"},{\"title\":\"Top 10 LLMs with No Restrictions in 2026\",\"url\":\"https://apidog.com/blog/llms-no-restrictions/\"},{\"title\":\"nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8 Â· Hugging Face\",\"url\":\"https://huggingface.co/nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8\"},{\"title\":\"Llama 4 - a meta-llama Collection\",\"url\":\"https://huggingface.co/collections/meta-llama/llama-4\"},{\"title\":\"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 Â· Hugging Face\",\"url\":\"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8\"}]\n\nBased on my search results, here's what I found about Llama 4 Maverick and related topics:\n\n## Llama 4 Maverick on HuggingFace\n\nLlama 4 Maverick (~400B total parameters) and Llama 4 Scout (~109B) are the next generation of large language models from Meta on the Hugging Face Hub. Both are Mixture of Experts (MoE) models with 17B active parameters.(no content)The Llama 4 Community License Agreement became effective on April 5, 2025.\n\n### Model Details\n- Meta launched two efficient models in the Llama 4 series: Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\n- The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.\n- Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta's products and services.\n\n### Abliterated/Uncensored Models\n\nRegarding \"abliterated\" uncensored versions specifically, the search results don't show a specific Llama 4 Maverick abliterated version. However:\n\nOpen-source communities contribute by sharing fine-tuned versions on platforms like Hugging Face, where models achieve uncensorship through \"abliteration\"â€”a method that erodes safety alignments via targeted fine-tuning.(no content)There are examples of abliterated models available, such as 'DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B'.\n\n### Available Variants on HuggingFace\n- Unsloth has uploaded Meta's Llama 4 multimodal models (Scout & Maverick), including Dynamic GGUFs, 16-bit & Dynamic 4-bit uploads that can be run and fine-tuned with Unsloth.\n- NVIDIA also has an FP8 version (Llama-4-Maverick-17B-128E-Instruct-FP8) that supports multilingual text and up to 5 images, with context length up to 1M.\n\nThe search didn't return a specific \"abliterated\" or \"uncensored\" version of Llama 4 Maverick on HuggingFace. You may want to search directly on HuggingFace's model hub for community-created uncensored variants, as these may have been uploaded more recently or under different naming conventions.\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"4e557ad7-ee21-4924-8614-b7fcf0909226","timestamp":"2026-01-05T21:46:59.770Z","toolUseResult":{"query":"Llama 4 Maverick abliterated uncensored version HuggingFace 2025","results":[{"tool_use_id":"srvtoolu_01EQ6B1RJKHZ2qRosszbTBDj","content":[{"title":"meta-llama/Llama-4-Maverick-17B-128E-Original Â· Hugging Face","url":"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Original"},{"title":"Welcome Llama 4 Maverick & Scout on Hugging Face","url":"https://huggingface.co/blog/llama4-release"},{"title":"meta-llama/Llama-4-Maverick-17B-128E Â· Hugging Face","url":"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E"},{"title":"meta-llama/Llama-4-Maverick-17B-128E-Instruct Â· Hugging Face","url":"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"},{"title":"unsloth/Llama-4-Maverick-17B-128E Â· Hugging Face","url":"https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E"},{"title":"unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF Â· Hugging Face","url":"https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF"},{"title":"Top 10 LLMs with No Restrictions in 2026","url":"https://apidog.com/blog/llms-no-restrictions/"},{"title":"nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8 Â· Hugging Face","url":"https://huggingface.co/nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8"},{"title":"Llama 4 - a meta-llama Collection","url":"https://huggingface.co/collections/meta-llama/llama-4"},{"title":"meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 Â· Hugging Face","url":"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8"}]},"Based on my search results, here's what I found about Llama 4 Maverick and related topics:\n\n## Llama 4 Maverick on HuggingFace\n\nLlama 4 Maverick (~400B total parameters) and Llama 4 Scout (~109B) are the next generation of large language models from Meta on the Hugging Face Hub. Both are Mixture of Experts (MoE) models with 17B active parameters.(no content)The Llama 4 Community License Agreement became effective on April 5, 2025.\n\n### Model Details\n- Meta launched two efficient models in the Llama 4 series: Llama 4 Scout, a 17 billion parameter model with 16 experts, and Llama 4 Maverick, a 17 billion parameter model with 128 experts.\n- The Llama 4 models are auto-regressive language models that use a mixture-of-experts (MoE) architecture and incorporate early fusion for native multimodality.\n- Llama 4 Maverick was pretrained on ~22 trillion tokens of multimodal data from a mix of publicly available, licensed data and information from Meta's products and services.\n\n### Abliterated/Uncensored Models\n\nRegarding \"abliterated\" uncensored versions specifically, the search results don't show a specific Llama 4 Maverick abliterated version. However:\n\nOpen-source communities contribute by sharing fine-tuned versions on platforms like Hugging Face, where models achieve uncensorship through \"abliteration\"â€”a method that erodes safety alignments via targeted fine-tuning.(no content)There are examples of abliterated models available, such as 'DavidAU/Llama-3.2-8X3B-MOE-Dark-Champion-Instruct-uncensored-abliterated-18.4B'.\n\n### Available Variants on HuggingFace\n- Unsloth has uploaded Meta's Llama 4 multimodal models (Scout & Maverick), including Dynamic GGUFs, 16-bit & Dynamic 4-bit uploads that can be run and fine-tuned with Unsloth.\n- NVIDIA also has an FP8 version (Llama-4-Maverick-17B-128E-Instruct-FP8) that supports multilingual text and up to 5 images, with context length up to 1M.\n\nThe search didn't return a specific \"abliterated\" or \"uncensored\" version of Llama 4 Maverick on HuggingFace. You may want to search directly on HuggingFace's model hub for community-created uncensored variants, as these may have been uploaded more recently or under different naming conventions."],"durationSeconds":21.795628541000188}}
{"parentUuid":"4e557ad7-ee21-4924-8614-b7fcf0909226","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","type":"user","message":{"role":"user","content":[{"tool_use_id":"toolu_01S7p2oJWVrYWdTAqohQ8oqj","type":"tool_result","content":"Web search results for query: \"Llama 4 Maverick quantized GGUF version local deployment\"\n\nLinks: [{\"title\":\"unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF Â· Hugging Face\",\"url\":\"https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF\"},{\"title\":\"Llama 4: How to Run & Fine-tune | Unsloth Documentation\",\"url\":\"https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms/llama-4-how-to-run-and-fine-tune\"},{\"title\":\"meta-llama/Llama-4-Maverick-17B-128E-Instruct Â· Hugging Face\",\"url\":\"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct\"},{\"title\":\"GeorgyGUF/Llama-4-Maverick-17B-128E-Instruct-q8-with-bf16-embedding-and-bf16-output.gguf Â· Hugging Face\",\"url\":\"https://huggingface.co/GeorgyGUF/Llama-4-Maverick-17B-128E-Instruct-q8-with-bf16-embedding-and-bf16-output.gguf\"},{\"title\":\"Welcome Llama 4 Maverick & Scout on Hugging Face\",\"url\":\"https://huggingface.co/blog/llama4-release\"},{\"title\":\"DevQuasar/meta-llama.Llama-4-Maverick-17B-128E-Instruct-GGUF Â· Hugging Face\",\"url\":\"https://huggingface.co/DevQuasar/meta-llama.Llama-4-Maverick-17B-128E-Instruct-GGUF\"},{\"title\":\"AaronimusPrime/llama-4-maverick-17b-128e-instruct-f16-gguf Â· Hugging Face\",\"url\":\"https://huggingface.co/AaronimusPrime/llama-4-maverick-17b-128e-instruct-f16-gguf\"},{\"title\":\"glogwa68/Llama-4-scout-GGUF Â· Hugging Face\",\"url\":\"https://huggingface.co/glogwa68/Llama-4-scout-GGUF\"},{\"title\":\"unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF Â· Hugging Face\",\"url\":\"https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF\"},{\"title\":\"llama-4-maverick-17b-128e-instruct Model by Meta | NVIDIA NIM\",\"url\":\"https://build.nvidia.com/meta/llama-4-maverick-17b-128e-instruct/modelcard\"}]\n\nHere's information about Llama 4 Maverick quantized GGUF versions for local deployment:\n\n## Model Overview\n\nLlama 4 Maverick is a 17 billion parameter model with 128 experts, and the Maverick model has 402B total parameters.(no content)The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences, leveraging a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n\n## GGUF Quantized Versions Available\n\nSeveral GGUF quantized versions are available on Hugging Face:\n\n1. **Unsloth Dynamic GGUFs** (unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF):\n   - Unsloth offers dynamic GGUFs which recover accuracy compared to standard quantization.\n   - You can choose Q4_K_M, or other quantized versions (like BF16 full precision).\n\n2. **DevQuasar Quantizations**: Available in 2-bit (Q2_K at 146 GB), 3-bit (Q3_K_S at 173 GB), 4-bit (Q4_K_M at 243 GB), and 5-bit (Q5_K_S at 276 GB).\n\n3. **Q8 Version**: A Q8_0 version was created with llama.cpp b5074, using the command to quantize with BF16 embedding and output tensors.\n\n4. **Full Precision GGUF**: The unquantized GGUF file size is approximately 801 GB.\n\n## Local Deployment with llama.cpp\n\nThe GGUF format is compatible with inference libraries such as llama.cpp, llamacpp-python, llamafile, and Ollama.\n\nExample command for running with llama.cpp:\nEdit `--threads 32` for the number of CPU threads, `--ctx-size 16384` for context length, and `--n-gpu-layers 99` for GPU offloading on how many layers.\n\n## Hardware Requirements\n\nFor Llama 4 Maverick, it's best to have 2 RTX 4090s (2 x 24GB).(no content)Running this model requires high-end hardware, preferably with multiple GPUs or extremely large VRAM.\n\n## Official Meta Release\n\nThe Llama 4 Maverick model is released as both BF16 and FP8 quantized weights.(no content)The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality.\n\n## Context Length\n\nThe Llama 4 models were pre-trained with a context length of 256K, and the Instruct models were fine-tuned to support much larger context lengths: 1M for Maverick (the large 128 experts version).\n\n\nREMINDER: You MUST include the sources above in your response to the user using markdown hyperlinks."}]},"uuid":"2b2a91a8-bddf-4327-a622-3e8fd85075f4","timestamp":"2026-01-05T21:47:02.379Z","toolUseResult":{"query":"Llama 4 Maverick quantized GGUF version local deployment","results":[{"tool_use_id":"srvtoolu_01SGj3xpUWdBZcRDsTQ1YyNm","content":[{"title":"unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF Â· Hugging Face","url":"https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF"},{"title":"Llama 4: How to Run & Fine-tune | Unsloth Documentation","url":"https://docs.unsloth.ai/models/tutorials-how-to-fine-tune-and-run-llms/llama-4-how-to-run-and-fine-tune"},{"title":"meta-llama/Llama-4-Maverick-17B-128E-Instruct Â· Hugging Face","url":"https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct"},{"title":"GeorgyGUF/Llama-4-Maverick-17B-128E-Instruct-q8-with-bf16-embedding-and-bf16-output.gguf Â· Hugging Face","url":"https://huggingface.co/GeorgyGUF/Llama-4-Maverick-17B-128E-Instruct-q8-with-bf16-embedding-and-bf16-output.gguf"},{"title":"Welcome Llama 4 Maverick & Scout on Hugging Face","url":"https://huggingface.co/blog/llama4-release"},{"title":"DevQuasar/meta-llama.Llama-4-Maverick-17B-128E-Instruct-GGUF Â· Hugging Face","url":"https://huggingface.co/DevQuasar/meta-llama.Llama-4-Maverick-17B-128E-Instruct-GGUF"},{"title":"AaronimusPrime/llama-4-maverick-17b-128e-instruct-f16-gguf Â· Hugging Face","url":"https://huggingface.co/AaronimusPrime/llama-4-maverick-17b-128e-instruct-f16-gguf"},{"title":"glogwa68/Llama-4-scout-GGUF Â· Hugging Face","url":"https://huggingface.co/glogwa68/Llama-4-scout-GGUF"},{"title":"unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF Â· Hugging Face","url":"https://huggingface.co/unsloth/Llama-4-Scout-17B-16E-Instruct-GGUF"},{"title":"llama-4-maverick-17b-128e-instruct Model by Meta | NVIDIA NIM","url":"https://build.nvidia.com/meta/llama-4-maverick-17b-128e-instruct/modelcard"}]},"Here's information about Llama 4 Maverick quantized GGUF versions for local deployment:\n\n## Model Overview\n\nLlama 4 Maverick is a 17 billion parameter model with 128 experts, and the Maverick model has 402B total parameters.(no content)The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences, leveraging a mixture-of-experts architecture to offer industry-leading performance in text and image understanding.\n\n## GGUF Quantized Versions Available\n\nSeveral GGUF quantized versions are available on Hugging Face:\n\n1. **Unsloth Dynamic GGUFs** (unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF):\n   - Unsloth offers dynamic GGUFs which recover accuracy compared to standard quantization.\n   - You can choose Q4_K_M, or other quantized versions (like BF16 full precision).\n\n2. **DevQuasar Quantizations**: Available in 2-bit (Q2_K at 146 GB), 3-bit (Q3_K_S at 173 GB), 4-bit (Q4_K_M at 243 GB), and 5-bit (Q5_K_S at 276 GB).\n\n3. **Q8 Version**: A Q8_0 version was created with llama.cpp b5074, using the command to quantize with BF16 embedding and output tensors.\n\n4. **Full Precision GGUF**: The unquantized GGUF file size is approximately 801 GB.\n\n## Local Deployment with llama.cpp\n\nThe GGUF format is compatible with inference libraries such as llama.cpp, llamacpp-python, llamafile, and Ollama.\n\nExample command for running with llama.cpp:\nEdit `--threads 32` for the number of CPU threads, `--ctx-size 16384` for context length, and `--n-gpu-layers 99` for GPU offloading on how many layers.\n\n## Hardware Requirements\n\nFor Llama 4 Maverick, it's best to have 2 RTX 4090s (2 x 24GB).(no content)Running this model requires high-end hardware, preferably with multiple GPUs or extremely large VRAM.\n\n## Official Meta Release\n\nThe Llama 4 Maverick model is released as both BF16 and FP8 quantized weights.(no content)The FP8 quantized weights fit on a single H100 DGX host while still maintaining quality.\n\n## Context Length\n\nThe Llama 4 models were pre-trained with a context length of 256K, and the Instruct models were fine-tuned to support much larger context lengths: 1M for Maverick (the large 128 experts version)."],"durationSeconds":24.40394316700101}}
{"parentUuid":"2b2a91a8-bddf-4327-a622-3e8fd85075f4","isSidechain":true,"userType":"external","cwd":"/Users/imorgado/vscode-runpod-gpu","sessionId":"26070b4e-12bc-4e26-a664-84f935029102","version":"2.0.76","gitBranch":"","agentId":"af0c788","slug":"jazzy-cooking-sutherland","message":{"model":"claude-opus-4-5-20251101","id":"msg_01C8vmMjJAkjt3KSogM5Fj11","type":"message","role":"assistant","content":[{"type":"text","text":"Here is my research on Meta's Llama 4 Maverick model:\n\n---\n\n## 1. What is Llama 4 Maverick? Is it officially released?\n\n**Yes, Llama 4 Maverick is officially released.** It was launched by Meta on **April 5, 2025** under the Llama 4 Community License Agreement.\n\nLlama 4 Maverick is a natively multimodal AI model that uses a **Mixture-of-Experts (MoE) architecture**. It is part of the Llama 4 family alongside Scout (smaller) and the upcoming Behemoth (larger). Maverick is described as Meta's main workhorse model, used in their apps like Facebook, Instagram, and WhatsApp.\n\n---\n\n## 2. Context Window Size\n\n- **Maximum context length: Up to 1 million tokens** (for the Instruct version)\n- Pre-training context length: 256K tokens\n- Actual available context varies by provider:\n  - Oracle Cloud: 512K tokens\n  - Groq: 128K tokens\n  - SambaNova: 64K tokens\n  - vLLM on 8xH100: ~430K tokens\n\n---\n\n## 3. Model Sizes and VRAM Requirements\n\n| Specification | Value |\n|--------------|-------|\n| **Active Parameters** | 17 billion per token |\n| **Total Parameters** | ~400 billion (402B) |\n| **Number of Experts** | 128 |\n\n**VRAM Requirements:**\n\n| Precision | VRAM Required |\n|-----------|---------------|\n| Full BF16/FP16 | ~800 GB+ |\n| FP8 Quantized | Fits on single 8xH100 node (~640 GB) |\n| INT4/Q4_K_M Quantized | ~122-243 GB |\n\nThe entire 400B parameter model must be loaded into memory even though only 17B active parameters are used per token.\n\n---\n\n## 4. Available on HuggingFace?\n\n**Yes.** Multiple versions are available:\n\n- [meta-llama/Llama-4-Maverick-17B-128E-Instruct](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct) - Official BF16\n- [meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8) - Official FP8 quantized\n- [unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF](https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF) - GGUF format for llama.cpp\n- [nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8](https://huggingface.co/nvidia/Llama-4-Maverick-17B-128E-Instruct-FP8) - NVIDIA FP8 version\n\n---\n\n## 5. Abliterated/Uncensored Version?\n\n**No specific abliterated version found for Llama 4 Maverick.** My search did not reveal a publicly available uncensored/abliterated variant. The base model (non-Instruct) is available at [meta-llama/Llama-4-Maverick-17B-128E](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E) which has fewer safety alignments than the Instruct version, but is not specifically abliterated. Community-created uncensored versions may appear over time on HuggingFace.\n\n---\n\n## 6. Compatible with vLLM?\n\n**Yes, fully compatible with vLLM v0.8.3+**\n\nExample deployment command:\n```bash\nvllm serve meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8 \\\n  --tensor-parallel-size 8 \\\n  --max-model-len 430000\n```\n\nAdding `--kv-cache-dtype fp8` can potentially double the usable context window.\n\nFor multimodal (image) support, add `--limit-mm-per-prompt image=10` to handle up to 10 images per request.\n\n---\n\n## 7. GPUs Needed to Run It\n\n| Configuration | Hardware Required |\n|--------------|-------------------|\n| **FP8 (Recommended)** | Single 8xH100 DGX node (640 GB total VRAM) |\n| **BF16 Full Precision** | Multiple 8xH100 nodes |\n| **Quantized (Q4/INT4)** | 2x RTX 4090 (48 GB) or equivalent |\n| **Consumer Local** | Very challenging; requires heavy quantization |\n\nMeta's official recommendation is an **NVIDIA H100 DGX system or equivalent** for production deployment.\n\n---\n\n## Sources:\n- [Meta AI Blog - Llama 4 Announcement](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)\n- [HuggingFace - Llama 4 Release Blog](https://huggingface.co/blog/llama4-release)\n- [vLLM Blog - Llama 4 Support](https://blog.vllm.ai/2025/04/05/llama4.html)\n- [HuggingFace - Llama-4-Maverick-17B-128E-Instruct](https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct)\n- [Unsloth - Llama 4 GGUF](https://huggingface.co/unsloth/Llama-4-Maverick-17B-128E-Instruct-GGUF)\n- [RunPod Blog - Llama 4 Analysis](https://www.runpod.io/blog/llama4-scout-maverick)\n- [APXML - Llama 4 GPU Requirements](https://apxml.com/posts/llama-4-system-requirements)"}],"stop_reason":null,"stop_sequence":null,"usage":{"input_tokens":0,"cache_creation_input_tokens":4268,"cache_read_input_tokens":22036,"cache_creation":{"ephemeral_5m_input_tokens":4268,"ephemeral_1h_input_tokens":0},"output_tokens":1,"service_tier":"standard"}},"requestId":"req_011CWpxxfhg2TdaBB6i162NV","type":"assistant","uuid":"465b94d4-e8d1-45cd-99bb-563275ab67d8","timestamp":"2026-01-05T21:47:28.164Z"}
