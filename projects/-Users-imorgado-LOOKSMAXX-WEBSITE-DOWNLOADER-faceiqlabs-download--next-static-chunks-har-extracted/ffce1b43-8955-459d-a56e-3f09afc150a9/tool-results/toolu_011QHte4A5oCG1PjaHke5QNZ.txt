     1â†’# ğŸŒ Website Downloader CLI  
     2â†’[![CI â€“ Website Downloader](https://github.com/PKHarsimran/website-downloader/actions/workflows/python-app.yml/badge.svg)](https://github.com/PKHarsimran/website-downloader/actions/workflows/python-app.yml)
     3â†’[![Lint & Style](https://github.com/PKHarsimran/website-downloader/actions/workflows/lint.yml/badge.svg)](https://github.com/PKHarsimran/website-downloader/actions/workflows/lint.yml)
     4â†’[![Automatic Dependency Submission](https://github.com/PKHarsimran/website-downloader/actions/workflows/dependency-graph/auto-submission/badge.svg)](https://github.com/PKHarsimran/website-downloader/actions/workflows/dependency-graph/auto-submission)
     5â†’[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
     6â†’[![Python](https://img.shields.io/badge/Python-3.10%2B-blue.svg)](https://www.python.org/)
     7â†’[![Code style: Black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
     8â†’
     9â†’Website Downloader CLI is a **tiny, pure-Python** site-mirroring tool that lets you grab a complete, browsable offline copy of any publicly reachable website:
    10â†’
    11â†’* Recursively crawls every same-origin link (including â€œprettyâ€ `/about/` URLs)
    12â†’* Downloads **all** assets (images, CSS, JS, â€¦)
    13â†’* Rewrites internal links so pages open flawlessly from your local disk
    14â†’* Streams files concurrently with automatic retry / back-off
    15â†’* Generates a clean, flat directory tree (`example_com/index.html`, `example_com/about/index.html`, â€¦)
    16â†’* Handles extremely long filenames safely via hashing and graceful fallbacks
    17â†’
    18â†’> Perfect for web archiving, pentesting labs, long flights, or just poking around a site without an internet connection.
    19â†’
    20â†’---
    21â†’
    22â†’## ğŸš€ Quick Start
    23â†’
    24â†’```bash
    25â†’# 1. Grab the code
    26â†’git clone https://github.com/PKHarsimran/website-downloader.git
    27â†’cd website-downloader
    28â†’
    29â†’# 2. Install dependencies (only two runtime libs!)
    30â†’pip install -r requirements.txt
    31â†’
    32â†’# 3. Mirror a site â€“ no prompts needed
    33â†’python website-downloader.py \
    34â†’    --url https://harsim.ca \
    35â†’    --destination harsim_ca_backup \
    36â†’    --max-pages 100 \
    37â†’    --threads 8
    38â†’```
    39â†’
    40â†’---
    41â†’
    42â†’## ğŸ› ï¸ Libraries Used
    43â†’
    44â†’| Library | Emoji | Purpose in this project |
    45â†’|---------|-------|-------------------------|
    46â†’| **requests** + **urllib3.Retry** | ğŸŒ | High-level HTTP client with automatic retry / back-off for flaky hosts |
    47â†’| **BeautifulSoup (bs4)** | ğŸœ | Parses downloaded HTML and extracts every `<a>`, `<img>`, `<script>`, and `<link>` |
    48â†’| **argparse** | ğŸ› ï¸ | Powers the modern CLI (`--url`, `--destination`, `--max-pages`, `--threads`, â€¦) |
    49â†’| **logging** | ğŸ“ | Dual console / file logging with colour + crawl-time stats |
    50â†’| **threading** & **queue** | âš™ï¸ | Lightweight thread-pool that streams images/CSS/JS concurrently |
    51â†’| **pathlib** & **os** | ğŸ“‚ | Cross-platform file-system helpers (`Path` magic, directory creation, etc.) |
    52â†’| **time** | â±ï¸ | Measures per-page latency and total crawl duration |
    53â†’| **urllib.parse** | ğŸ”— | Safely joins / analyses URLs and rewrites them to local relative paths |
    54â†’| **sys** | ğŸ–¥ï¸ | Directs log output to `stdout` and handles graceful interrupts (`Ctrl-C`) |
    55â†’## ğŸ—‚ï¸ Project Structure
    56â†’
    57â†’| Path | What it is | Key features |
    58â†’|------|------------|--------------|
    59â†’| `website_downloader.py` | **Single-entry CLI** that performs the entire crawl *and* link-rewriting pipeline. | â€¢ Persistent `requests.Session` with automatic retries<br>â€¢ Breadth-first crawl capped by `--max-pages` (default = 50)<br>â€¢ Thread-pool (configurable via `--threads`, default = 6) to fetch images/CSS/JS in parallel<br>â€¢ Robust link rewriting so every internal URL works offline (pretty-URL folders âœ `index.html`, plain paths âœ `.html`)<br>â€¢ Smart output folder naming (`example.com` â†’ `example_com`)<br>â€¢ Colourised console + file logging with per-page latency and crawl summary |
    60â†’| `requirements.txt` | Minimal dependency pin-list. Only **`requests`** and **`beautifulsoup4`** are third-party; everything else is Python â‰¥ 3.10 std-lib. |
    61â†’| `web_scraper.log` | Auto-generated run log (rotates/overwrites on each invocation). Useful for troubleshooting or audit trails. |
    62â†’| `README.md` | The document youâ€™re reading â€“ quick-start, flags, and architecture notes. |
    63â†’| *(output folder)* | Created at runtime (`example_com/ â€¦`) â€“ mirrors the remote directory tree with `index.html` stubs and all static assets. |
    64â†’
    65â†’> **Removed:** The old `check_download.py` verifier is no longer required because the new downloader performs integrity checks (missing files, broken internal links) during the crawl and reports any issues directly in the log summary.
    66â†’
    67â†’## âœ¨ Recent Improvements
    68â†’
    69â†’âœ… Type Conversion Fix
    70â†’Fixed a TypeError caused by int(..., 10) when non-string arguments were passed.
    71â†’
    72â†’âœ… Safer Path Handling
    73â†’Added intelligent path shortening and hashing for long filenames to prevent
    74â†’OSError: [Errno 36] File name too long errors.
    75â†’
    76â†’âœ… Improved CLI Experience
    77â†’Rebuilt argument parsing with argparse for cleaner syntax and validation.
    78â†’
    79â†’âœ… Code Quality & Linting
    80â†’Applied Black + Flake8 formatting; the project now passes all CI lint checks.
    81â†’
    82â†’âœ… Logging & Stability
    83â†’Improved error handling, logging, and fallback mechanisms for failed writes.
    84â†’
    85â†’âœ… Skip Non-Fetchable Schemes  
    86â†’The crawler now safely skips `mailto:`, `tel:`, `javascript:`, and `data:` links instead of trying to download them.  
    87â†’This prevents `requests.exceptions.InvalidSchema: No connection adapters were found` errors and keeps those links intact in saved HTML.
    88â†’
    89â†’
    90â†’## ğŸ¤ Contributing
    91â†’
    92â†’Contributions are welcome! Please open an issue or submit a pull request for any improvements or bug fixes.
    93â†’
    94â†’## ğŸ“œ License
    95â†’
    96â†’This project is licensed under the MIT License.
    97â†’
    98â†’## â¤ï¸ Support This Project
    99â†’
   100â†’[![Donate](https://img.shields.io/badge/Donate-PayPal-blue)](https://www.paypal.com/donate/?business=MVEWG3QAX6UBC&no_recurring=1&item_name=Github+Project+-+Website+downloader&currency_code=CAD)
   101â†’
   102â†’

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
