     1→/**
     2→ * SPLICE Backend Server
     3→ *
     4→ * Main entry point for the SPLICE backend API.
     5→ * Orchestrates the audio analysis pipeline.
     6→ *
     7→ * Slices:
     8→ * - Slice 4: Transcription (services/transcription.js)
     9→ * - Slice 5: Take Detection (services/takeDetection.js)
    10→ */
    11→
    12→require('dotenv').config();
    13→
    14→const express = require('express');
    15→const cors = require('cors');
    16→const fs = require('fs');
    17→const https = require('https');
    18→const http = require('http');
    19→const path = require('path');
    20→
    21→// Check if running in production (Railway injects RAILWAY_ENVIRONMENT)
    22→const isProduction = process.env.NODE_ENV === 'production' || process.env.RAILWAY_ENVIRONMENT;
    23→
    24→// Import slice services
    25→const { transcribeAudio, transcribeWithWords } = require('./services/transcription');
    26→const { detectTakes } = require('./services/takeDetection');
    27→const { detectSilences } = require('./services/silenceDetection');
    28→const { detectAudioSilences, isFFprobeInstalled, getAudioDuration } = require('./services/ffprobeSilence');
    29→const { detectSilencesRMS, sensitivityToParams } = require('./services/rmsSilenceDetection');
    30→const {
    31→  detectProfanity,
    32→  getProfanityList,
    33→  getSupportedLanguages,
    34→  getAvailableBleepSounds,
    35→  parseWordList
    36→} = require('./services/profanityDetection');
    37→const {
    38→  detectRepetitionsBasic,
    39→  detectRepetitionsAdvanced,
    40→  detectStutters,
    41→  detectAllRepetitions
    42→} = require('./services/repetitionDetection');
    43→const {
    44→  analyzeMultitrack,
    45→  autoBalanceMultitrack
    46→} = require('./services/multitrackAnalysis');
    47→const {
    48→  toSRT,
    49→  toVTT,
    50→  toPlainText,
    51→  toJSON,
    52→  exportToFile,
    53→  getSupportedFormats
    54→} = require('./services/captionExporter');
    55→const { processXMLFile } = require('./services/xmlProcessor');
    56→const { isolateVocals, isReplicateConfigured } = require('./services/vocalIsolation');
    57→const { generateCutList, generateTakesCutList, validateCutList } = require('./services/cutListGenerator');
    58→
    59→// Usage tracking and billing
    60→const usageTracking = require('./services/usageTracking');
    61→// Rate limiter available for future use
    62→// const { requireCredits } = require('./middleware/rateLimiter');
    63→
    64→// Stripe for webhooks
    65→const Stripe = require('stripe');
    66→const stripe = new Stripe(process.env.STRIPE_SECRET_KEY);
    67→
    68→// =============================================================================
    69→// Server Configuration
    70→// =============================================================================
    71→
    72→const app = express();
    73→const PORT = process.env.PORT || 3847;
    74→
    75→// HTTPS certificates (generated by mkcert) - only for local development
    76→let httpsOptions = null;
    77→if (!isProduction) {
    78→  const keyPath = path.join(__dirname, 'localhost+1-key.pem');
    79→  const certPath = path.join(__dirname, 'localhost+1.pem');
    80→  if (fs.existsSync(keyPath) && fs.existsSync(certPath)) {
    81→    httpsOptions = {
    82→      key: fs.readFileSync(keyPath),
    83→      cert: fs.readFileSync(certPath)
    84→    };
    85→  }
    86→}
    87→
    88→app.use(cors());
    89→
    90→// Helper to determine tier from price ID with logging
    91→function getTierFromPriceId(priceId) {
    92→  if (priceId === process.env.STRIPE_PRICE_STARTER) return 'starter';
    93→  if (priceId === process.env.STRIPE_PRICE_PRO) return 'pro';
    94→  if (priceId === process.env.STRIPE_PRICE_TEAM) return 'team';
    95→
    96→  // Log unknown price ID for debugging
    97→  console.warn(`[SPLICE] Unknown price ID: ${priceId} - defaulting to starter tier`);
    98→  return 'starter';
    99→}
   100→
   101→// Stripe webhook needs raw body - must be before express.json()
   102→app.post('/webhooks/stripe', express.raw({ type: 'application/json' }), async (req, res) => {
   103→  const sig = req.headers['stripe-signature'];
   104→  const webhookSecret = process.env.STRIPE_WEBHOOK_SECRET;
   105→
   106→  let event;
   107→
   108→  try {
   109→    if (webhookSecret) {
   110→      event = stripe.webhooks.constructEvent(req.body, sig, webhookSecret);
   111→    } else {
   112→      // For testing without webhook secret
   113→      event = JSON.parse(req.body);
   114→      console.warn('[SPLICE] Warning: Processing webhook without signature verification');
   115→    }
   116→  } catch (err) {
   117→    console.error('[SPLICE] Webhook signature verification failed:', err.message);
   118→    return res.status(400).json({ error: 'Webhook signature verification failed' });
   119→  }
   120→
   121→  console.log(`[SPLICE] Webhook received: ${event.type} (${event.id})`);
   122→
   123→  // Idempotency check - skip if already processed
   124→  if (await usageTracking.isEventProcessed(event.id)) {
   125→    console.log(`[SPLICE] Event ${event.id} already processed, skipping`);
   126→    return res.json({ received: true, skipped: true });
   127→  }
   128→
   129→  try {
   130→    switch (event.type) {
   131→      case 'customer.subscription.created':
   132→      case 'customer.subscription.updated': {
   133→        const subscription = event.data.object;
   134→        const customerId = subscription.customer;
   135→
   136→        // Validate customerId
   137→        if (!customerId) {
   138→          console.error('[SPLICE] Missing customer ID in subscription event');
   139→          return res.status(400).json({ error: 'Missing customer ID' });
   140→        }
   141→
   142→        // Get tier from price ID
   143→        const priceId = subscription.items?.data?.[0]?.price?.id;
   144→        const tier = getTierFromPriceId(priceId);
   145→
   146→        // Update user tier and reset hours
   147→        await usageTracking.updateTier(customerId, tier);
   148→        console.log(`[SPLICE] Updated customer ${customerId} to tier: ${tier}`);
   149→        break;
   150→      }
   151→
   152→      case 'customer.subscription.deleted': {
   153→        const subscription = event.data.object;
   154→        const customerId = subscription.customer;
   155→
   156→        // Validate customerId
   157→        if (!customerId) {
   158→          console.error('[SPLICE] Missing customer ID in subscription.deleted event');
   159→          return res.status(400).json({ error: 'Missing customer ID' });
   160→        }
   161→
   162→        // Downgrade to cancelled (0 hours)
   163→        await usageTracking.updateTier(customerId, 'cancelled');
   164→        console.log(`[SPLICE] Subscription cancelled for customer ${customerId}`);
   165→        break;
   166→      }
   167→
   168→      case 'invoice.payment_succeeded': {
   169→        const invoice = event.data.object;
   170→        const customerId = invoice.customer;
   171→        const subscriptionId = invoice.subscription;
   172→
   173→        // Validate customerId
   174→        if (!customerId) {
   175→          console.error('[SPLICE] Missing customer ID in invoice event');
   176→          return res.status(400).json({ error: 'Missing customer ID' });
   177→        }
   178→
   179→        // Reset hours on successful payment (new billing period)
   180→        if (subscriptionId) {
   181→          const subscription = await stripe.subscriptions.retrieve(subscriptionId);
   182→          const priceId = subscription.items?.data?.[0]?.price?.id;
   183→          const tier = getTierFromPriceId(priceId);
   184→
   185→          await usageTracking.resetHours(customerId, tier);
   186→          console.log(`[SPLICE] Reset hours for customer ${customerId} (tier: ${tier})`);
   187→        }
   188→        break;
   189→      }
   190→
   191→      default:
   192→        console.log(`[SPLICE] Unhandled event type: ${event.type}`);
   193→    }
   194→
   195→    // Record event as processed (idempotency)
   196→    await usageTracking.recordWebhookEvent(event.id, event.type);
   197→
   198→    res.json({ received: true });
   199→  } catch (err) {
   200→    console.error('[SPLICE] Webhook handler error:', err);
   201→    res.status(500).json({ error: err.message });
   202→  }
   203→});
   204→
   205→app.use(express.json());
   206→
   207→// =============================================================================
   208→// Routes
   209→// =============================================================================
   210→
   211→/**
   212→ * GET / - API information
   213→ */
   214→app.get('/', (req, res) => {
   215→  res.json({
   216→    service: 'splice-backend',
   217→    version: '0.3.0',
   218→    endpoints: {
   219→      'GET /': 'This info',
   220→      'GET /health': 'Health check',
   221→      'GET /ffprobe-check': 'Check if FFprobe is installed',
   222→      'GET /replicate-check': 'Check if Replicate API is configured',
   223→      'POST /analyze': 'Analyze WAV file { wavPath }',
   224→      'POST /silences': 'Detect silences via Whisper gaps { wavPath, threshold: 0.5 }',
   225→      'POST /silences-audio': 'Detect silences via FFprobe { wavPath, threshold: -30, minDuration: 0.5, padding: 0.1 }',
   226→      'POST /silences-rms': 'Detect silences via RMS analysis { wavPath, threshold: -30, minSilenceLength: 0.5, paddingStart: 0.1, paddingEnd: 0.05, autoThreshold: false, sensitivity: 50 }',
   227→      'POST /profanity': 'Detect profanity in transcript { wavPath, language: "en", customBlocklist: [], customAllowlist: [] }',
   228→      'GET /profanity/languages': 'Get supported languages for profanity detection',
   229→      'GET /profanity/bleeps': 'Get available bleep sounds',
   230→      'POST /repetitions': 'Detect phrase repetitions and stutters { wavPath, phraseSize: 5, tolerance: 0.7, useOpenAI: false }',
   231→      'POST /fillers': 'Detect filler words (um, uh, like, etc.) { wavPath, customFillers: [] }',
   232→      'POST /stutters': 'Detect single-word stutters only { wavPath, minRepeats: 2 }',
   233→      'POST /export/captions': 'Export transcript to caption format { wavPath, format: srt|vtt|txt|json, outputPath? }',
   234→      'GET /export/formats': 'Get supported caption export formats',
   235→      'POST /multitrack': 'Analyze multiple audio tracks for multicam { audioPaths: [], speakerNames: [], videoTrackMapping: {} }',
   236→      'POST /multitrack/auto-balance': 'Auto-balance speaker screentime { audioPaths: [], speakerNames: [] }',
   237→      'POST /process-xml': 'Process FCP XML { xmlPath, silences, removeGaps: true }',
   238→      'POST /cut-list': 'Generate JSON cut list for DOM building (v3.5) { sourceName, sourcePath, duration, silences, takes?, settings? }',
   239→      'POST /cut-list/takes': 'Generate cut list keeping only takes { sourceName, sourcePath, duration, takes, settings? }',
   240→      'POST /isolate-vocals': 'Isolate vocals from audio { audioPath }',
   241→      'POST /batch/silences': 'Batch process multiple files for silence detection { files: [], options: {} }',
   242→      'GET /batch/status/:jobId': 'Get batch job status',
   243→      'GET /batch/results/:jobId': 'Get full batch job results',
   244→      'GET /batch/jobs': 'List all batch jobs',
   245→      'DELETE /batch/:jobId': 'Delete a batch job',
   246→      'GET /credits': 'Get user credit balance (requires x-stripe-customer-id header)',
   247→      'GET /usage-history': 'Get usage history (requires x-stripe-customer-id header)',
   248→      'POST /webhooks/stripe': 'Stripe webhook endpoint'
   249→    }
   250→  });
   251→});
   252→
   253→/**
   254→ * GET /health - Health check
   255→ */
   256→app.get('/health', (req, res) => {
   257→  res.json({ status: 'ok', service: 'splice-backend' });
   258→});
   259→
   260→/**
   261→ * POST /analyze - Main analysis endpoint
   262→ *
   263→ * Pipeline:
   264→ * 1. Validate input (wavPath)
   265→ * 2. Slice 4: Transcribe audio with Groq Whisper
   266→ * 3. Slice 5: Detect takes with GPT-4o-mini
   267→ * 4. Return combined results
   268→ */
   269→app.post('/analyze', async (req, res) => {
   270→  const { wavPath } = req.body;
   271→
   272→  // Validate input
   273→  if (!wavPath) {
   274→    return res.status(400).json({ error: 'wavPath is required' });
   275→  }
   276→
   277→  if (!fs.existsSync(wavPath)) {
   278→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   279→  }
   280→
   281→  console.log(`[SPLICE] Analyzing: ${wavPath}`);
   282→
   283→  try {
   284→    // Slice 4 - GPT-4o-mini transcription
   285→    const transcript = await transcribeAudio(wavPath);
   286→
   287→    // Slice 5 - GPT-4o-mini take detection
   288→    const takes = await detectTakes(transcript);
   289→
   290→    res.json({
   291→      success: true,
   292→      wavPath,
   293→      transcript,
   294→      takes
   295→    });
   296→  } catch (err) {
   297→    console.error('[SPLICE] Error:', err);
   298→    res.status(500).json({ error: err.message });
   299→  }
   300→});
   301→
   302→/**
   303→ * POST /silences - Detect silent gaps in audio
   304→ *
   305→ * Pipeline:
   306→ * 1. Transcribe audio with Whisper (reuses transcription)
   307→ * 2. Analyze gaps between segments
   308→ * 3. Return silence regions
   309→ */
   310→app.post('/silences', async (req, res) => {
   311→  const { wavPath, threshold = 0.5 } = req.body;
   312→
   313→  if (!wavPath) {
   314→    return res.status(400).json({ error: 'wavPath is required' });
   315→  }
   316→
   317→  if (!fs.existsSync(wavPath)) {
   318→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   319→  }
   320→
   321→  console.log(`[SPLICE] Detecting silences: ${wavPath} (threshold: ${threshold}s)`);
   322→
   323→  try {
   324→    const transcript = await transcribeAudio(wavPath);
   325→    const silences = detectSilences(transcript.segments, threshold);
   326→
   327→    res.json({
   328→      success: true,
   329→      wavPath,
   330→      threshold,
   331→      silences,
   332→      count: silences.length,
   333→      totalSilenceDuration: silences.reduce((sum, s) => sum + s.duration, 0).toFixed(2)
   334→    });
   335→  } catch (err) {
   336→    console.error('[SPLICE] Silence detection error:', err);
   337→    res.status(500).json({ error: err.message });
   338→  }
   339→});
   340→
   341→/**
   342→ * POST /silences-audio - Detect silences using FFprobe audio analysis
   343→ *
   344→ * Uses actual audio levels (dB threshold) instead of transcript gaps.
   345→ * More accurate for detecting silence vs background noise.
   346→ */
   347→app.post('/silences-audio', async (req, res) => {
   348→  const {
   349→    wavPath,
   350→    threshold = -30,
   351→    minDuration = 0.5,
   352→    padding = 0.1
   353→  } = req.body;
   354→
   355→  if (!wavPath) {
   356→    return res.status(400).json({ error: 'wavPath is required' });
   357→  }
   358→
   359→  if (!fs.existsSync(wavPath)) {
   360→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   361→  }
   362→
   363→  // Check FFprobe availability
   364→  const ffprobeAvailable = await isFFprobeInstalled();
   365→  if (!ffprobeAvailable) {
   366→    return res.status(500).json({
   367→      error: 'FFprobe not installed. Run: brew install ffmpeg'
   368→    });
   369→  }
   370→
   371→  console.log(`[SPLICE] FFprobe silence detection: ${wavPath} (threshold: ${threshold}dB, min: ${minDuration}s)`);
   372→
   373→  try {
   374→    const silences = await detectAudioSilences(wavPath, {
   375→      threshold,
   376→      minDuration,
   377→      padding
   378→    });
   379→
   380→    const totalDuration = silences.reduce((sum, s) => sum + s.duration, 0);
   381→
   382→    res.json({
   383→      success: true,
   384→      wavPath,
   385→      threshold,
   386→      minDuration,
   387→      padding,
   388→      silences,
   389→      count: silences.length,
   390→      totalSilenceDuration: totalDuration.toFixed(2)
   391→    });
   392→  } catch (err) {
   393→    console.error('[SPLICE] FFprobe silence detection error:', err);
   394→    res.status(500).json({ error: err.message });
   395→  }
   396→});
   397→
   398→/**
   399→ * POST /silences-rms - Detect silences using RMS audio analysis
   400→ *
   401→ * Advanced silence detection with:
   402→ * - RMS (Root Mean Square) audio level analysis
   403→ * - Auto-threshold detection from audio histogram
   404→ * - Configurable padding (before/after cuts)
   405→ * - Sensitivity slider mapping (0-100)
   406→ *
   407→ * Options:
   408→ * - threshold: dBFS threshold (-60 to -20, default: -30)
   409→ * - minSilenceLength: Minimum silence duration in seconds (default: 0.5)
   410→ * - seekStep: Analysis window step in seconds (default: 0.05)
   411→ * - paddingStart: Buffer before silence in seconds (default: 0.1)
   412→ * - paddingEnd: Buffer after silence in seconds (default: 0.05)
   413→ * - autoThreshold: Auto-detect optimal threshold (default: false)
   414→ * - sensitivity: UI sensitivity 0-100 (overrides other params if provided)
   415→ */
   416→app.post('/silences-rms', async (req, res) => {
   417→  const { wavPath, sensitivity, ...manualOptions } = req.body;
   418→
   419→  if (!wavPath) {
   420→    return res.status(400).json({ error: 'wavPath is required' });
   421→  }
   422→
   423→  if (!fs.existsSync(wavPath)) {
   424→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   425→  }
   426→
   427→  // Check FFprobe availability (needed for audio extraction)
   428→  const ffprobeAvailable = await isFFprobeInstalled();
   429→  if (!ffprobeAvailable) {
   430→    return res.status(500).json({
   431→      error: 'FFprobe not installed. Run: brew install ffmpeg'
   432→    });
   433→  }
   434→
   435→  // Build options - use sensitivity if provided, otherwise use manual options
   436→  let options = {};
   437→  if (typeof sensitivity === 'number') {
   438→    options = sensitivityToParams(sensitivity);
   439→    console.log(`[SPLICE] RMS detection with sensitivity ${sensitivity}`);
   440→  } else {
   441→    options = {
   442→      threshold: manualOptions.threshold ?? -30,
   443→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,
   444→      seekStep: manualOptions.seekStep ?? 0.05,
   445→      paddingStart: manualOptions.paddingStart ?? 0.1,
   446→      paddingEnd: manualOptions.paddingEnd ?? 0.05,
   447→      autoThreshold: manualOptions.autoThreshold ?? false,
   448→      mergeDistance: manualOptions.mergeDistance ?? 0.2
   449→    };
   450→  }
   451→
   452→  console.log(`[SPLICE] RMS silence detection: ${wavPath}`);
   453→
   454→  try {
   455→    const result = await detectSilencesRMS(wavPath, options);
   456→
   457→    res.json({
   458→      success: true,
   459→      wavPath,
   460→      ...result,
   461→      count: result.silences.length,
   462→      totalSilenceDuration: result.metadata.totalSilenceDuration.toFixed(2)
   463→    });
   464→  } catch (err) {
   465→    console.error('[SPLICE] RMS silence detection error:', err);
   466→    res.status(500).json({ error: err.message });
   467→  }
   468→});
   469→
   470→// =============================================================================
   471→// Profanity Detection Routes
   472→// =============================================================================
   473→
   474→/**
   475→ * POST /profanity - Detect profanity in audio/transcript
   476→ *
   477→ * Transcribes audio (if needed) and detects profanity words.
   478→ * Returns word-level and segment-level results for muting/bleeping.
   479→ *
   480→ * Options:
   481→ * - wavPath: Path to audio file (required)
   482→ * - transcript: Pre-existing transcript (optional, skips transcription)
   483→ * - language: Language code (en, es, fr, de) - default: en
   484→ * - customBlocklist: Array or comma-separated string of additional words to censor
   485→ * - customAllowlist: Array or comma-separated string of words to allow
   486→ * - frameRate: Frame rate for boundary alignment (default: 30)
   487→ */
   488→app.post('/profanity', async (req, res) => {
   489→  const {
   490→    wavPath,
   491→    transcript: providedTranscript,
   492→    language = 'en',
   493→    customBlocklist = [],
   494→    customAllowlist = [],
   495→    frameRate = 30
   496→  } = req.body;
   497→
   498→  if (!wavPath && !providedTranscript) {
   499→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   500→  }
   501→
   502→  if (wavPath && !fs.existsSync(wavPath)) {
   503→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   504→  }
   505→
   506→  console.log(`[SPLICE] Profanity detection: ${wavPath || 'provided transcript'} (language: ${language})`);
   507→
   508→  try {
   509→    // Get or create transcript with word-level timestamps
   510→    let transcript = providedTranscript;
   511→    if (!transcript && wavPath) {
   512→      // Use transcribeWithWords for word-level timestamps required by profanity detection
   513→      transcript = await transcribeWithWords(wavPath);
   514→    }
   515→
   516→    // Validate transcript has words
   517→    if (!transcript || !transcript.words || transcript.words.length === 0) {
   518→      return res.status(400).json({
   519→        error: 'Transcript must contain word-level timing data',
   520→        hint: 'Ensure transcription returns words array with start/end times'
   521→      });
   522→    }
   523→
   524→    // Parse custom lists
   525→    const blocklist = parseWordList(customBlocklist);
   526→    const allowlist = parseWordList(customAllowlist);
   527→
   528→    // Detect profanity
   529→    const result = detectProfanity(transcript, {
   530→      language,
   531→      customBlocklist: blocklist,
   532→      customAllowlist: allowlist,
   533→      frameRate
   534→    });
   535→
   536→    res.json({
   537→      success: true,
   538→      wavPath,
   539→      ...result
   540→    });
   541→  } catch (err) {
   542→    console.error('[SPLICE] Profanity detection error:', err);
   543→    res.status(500).json({ error: err.message });
   544→  }
   545→});
   546→
   547→/**
   548→ * GET /profanity/languages - Get supported languages
   549→ */
   550→app.get('/profanity/languages', (req, res) => {
   551→  res.json({
   552→    success: true,
   553→    languages: getSupportedLanguages()
   554→  });
   555→});
   556→
   557→/**
   558→ * GET /profanity/bleeps - Get available bleep sounds
   559→ */
   560→app.get('/profanity/bleeps', (req, res) => {
   561→  res.json({
   562→    success: true,
   563→    sounds: getAvailableBleepSounds()
   564→  });
   565→});
   566→
   567→/**
   568→ * GET /profanity/list/:language - Get default profanity list for a language
   569→ */
   570→app.get('/profanity/list/:language', (req, res) => {
   571→  const { language } = req.params;
   572→  const list = getProfanityList(language);
   573→
   574→  res.json({
   575→    success: true,
   576→    language,
   577→    wordCount: list.length,
   578→    // Return first 50 words as sample (full list is large)
   579→    sample: list.slice(0, 50),
   580→    note: 'Full list available but truncated for response size'
   581→  });
   582→});
   583→
   584→// =============================================================================
   585→// Repetition/Stutter Detection Routes
   586→// =============================================================================
   587→
   588→/**
   589→ * POST /repetitions - Detect phrase repetitions and stutters
   590→ *
   591→ * Analyzes transcript for repeated phrases and stutters.
   592→ * Returns segments that can be removed to clean up the edit.
   593→ *
   594→ * Options:
   595→ * - wavPath: Path to audio file (required unless transcript provided)
   596→ * - transcript: Pre-existing transcript (optional)
   597→ * - phraseSize: Words per comparison window (default: 5)
   598→ * - tolerance: Similarity threshold 0-1 (default: 0.7)
   599→ * - searchRadius: Words to search ahead (default: 100)
   600→ * - useOpenAI: Use OpenAI for boundary refinement (default: false)
   601→ * - includeStutters: Also detect single-word stutters (default: true)
   602→ */
   603→app.post('/repetitions', async (req, res) => {
   604→  const {
   605→    wavPath,
   606→    transcript: providedTranscript,
   607→    phraseSize = 5,
   608→    tolerance = 0.7,
   609→    searchRadius = 100,
   610→    useOpenAI = false,
   611→    includeStutters = true
   612→  } = req.body;
   613→
   614→  if (!wavPath && !providedTranscript) {
   615→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   616→  }
   617→
   618→  if (wavPath && !fs.existsSync(wavPath)) {
   619→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   620→  }
   621→
   622→  console.log(`[SPLICE] Repetition detection: ${wavPath || 'provided transcript'}`);
   623→
   624→  try {
   625→    // Get or create transcript with word-level timestamps
   626→    let transcript = providedTranscript;
   627→    if (!transcript && wavPath) {
   628→      // Use transcribeWithWords for word-level timestamps required by repetition detection
   629→      transcript = await transcribeWithWords(wavPath);
   630→    }
   631→
   632→    // Validate transcript has words
   633→    if (!transcript || !transcript.words || transcript.words.length === 0) {
   634→      return res.status(400).json({
   635→        error: 'Transcript must contain word-level timing data'
   636→      });
   637→    }
   638→
   639→    // Detect all repetitions (phrases + stutters)
   640→    const result = await detectAllRepetitions(transcript, {
   641→      phraseSize,
   642→      tolerance,
   643→      searchRadius,
   644→      useOpenAI
   645→    });
   646→
   647→    // Optionally filter out stutters
   648→    if (!includeStutters) {
   649→      result.stutters = [];
   650→      result.removalSegments = result.removalSegments.filter(s => s.type !== 'stutter');
   651→    }
   652→
   653→    res.json({
   654→      success: true,
   655→      wavPath,
   656→      ...result
   657→    });
   658→  } catch (err) {
   659→    console.error('[SPLICE] Repetition detection error:', err);
   660→    res.status(500).json({ error: err.message });
   661→  }
   662→});
   663→
   664→/**
   665→ * POST /fillers - Detect filler words (um, uh, like, etc.)
   666→ *
   667→ * Transcribes audio and identifies filler words with timestamps.
   668→ * Returns segments that can be cut or reviewed for removal.
   669→ *
   670→ * Options:
   671→ * - wavPath: Path to audio file (required unless transcript provided)
   672→ * - transcript: Pre-existing transcript with word-level timing (optional)
   673→ * - customFillers: Additional filler words to detect (optional)
   674→ */
   675→app.post('/fillers', async (req, res) => {
   676→  const {
   677→    wavPath,
   678→    transcript: providedTranscript,
   679→    customFillers = []
   680→  } = req.body;
   681→
   682→  if (!wavPath && !providedTranscript) {
   683→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   684→  }
   685→
   686→  if (wavPath && !fs.existsSync(wavPath)) {
   687→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   688→  }
   689→
   690→  console.log(`[SPLICE] Filler word detection: ${wavPath || 'provided transcript'}`);
   691→
   692→  try {
   693→    // Get or create transcript with word-level timestamps
   694→    let transcript = providedTranscript;
   695→    if (!transcript && wavPath) {
   696→      transcript = await transcribeWithWords(wavPath);
   697→    }
   698→
   699→    // Validate transcript has words
   700→    if (!transcript || !transcript.words || transcript.words.length === 0) {
   701→      return res.status(400).json({
   702→        error: 'Transcript must contain word-level timing data'
   703→      });
   704→    }
   705→
   706→    // Default filler words (common in English speech)
   707→    const defaultFillers = [
   708→      'um', 'uh', 'ah', 'er', 'eh',           // Hesitation sounds
   709→      'like', 'so', 'well', 'right',           // Discourse markers
   710→      'you know', 'i mean', 'basically',       // Filler phrases
   711→      'actually', 'literally', 'honestly',     // Overused qualifiers
   712→      'kind of', 'sort of', 'you see'          // Hedging phrases
   713→    ];
   714→
   715→    // Combine default + custom fillers (lowercase for matching)
   716→    const fillerSet = new Set([
   717→      ...defaultFillers,
   718→      ...customFillers.map(f => f.toLowerCase().trim())
   719→    ]);
   720→
   721→    // Detect filler words
   722→    const fillers = [];
   723→    const words = transcript.words;
   724→
   725→    for (let i = 0; i < words.length; i++) {
   726→      const word = words[i];
   727→      const normalizedWord = word.word.toLowerCase().replace(/[.,!?;:'"]/g, '').trim();
   728→
   729→      // Check single-word fillers
   730→      if (fillerSet.has(normalizedWord)) {
   731→        fillers.push({
   732→          word: word.word,
   733→          normalizedWord,
   734→          start: word.start,
   735→          end: word.end,
   736→          duration: word.end - word.start,
   737→          index: i,
   738→          type: 'filler'
   739→        });
   740→        continue;
   741→      }
   742→
   743→      // Check two-word phrases (e.g., "you know", "kind of")
   744→      if (i < words.length - 1) {
   745→        const nextWord = words[i + 1];
   746→        const twoWordPhrase = `${normalizedWord} ${nextWord.word.toLowerCase().replace(/[.,!?;:'"]/g, '').trim()}`;
   747→        if (fillerSet.has(twoWordPhrase)) {
   748→          fillers.push({
   749→            word: `${word.word} ${nextWord.word}`,
   750→            normalizedWord: twoWordPhrase,
   751→            start: word.start,
   752→            end: nextWord.end,
   753→            duration: nextWord.end - word.start,
   754→            index: i,
   755→            type: 'filler_phrase'
   756→          });
   757→          // Skip next word since it's part of this phrase
   758→          i++;
   759→        }
   760→      }
   761→    }
   762→
   763→    // Calculate total filler time
   764→    const totalFillerDuration = fillers.reduce((sum, f) => sum + f.duration, 0);
   765→    const audioDuration = transcript.duration || (words.length > 0 ? words[words.length - 1].end : 0);
   766→    const fillerPercentage = audioDuration > 0 ? (totalFillerDuration / audioDuration) * 100 : 0;
   767→
   768→    res.json({
   769→      success: true,
   770→      wavPath,
   771→      fillers,
   772→      metadata: {
   773→        totalWords: words.length,
   774→        fillerCount: fillers.length,
   775→        totalFillerDuration: parseFloat(totalFillerDuration.toFixed(3)),
   776→        audioDuration: parseFloat(audioDuration.toFixed(3)),
   777→        fillerPercentage: parseFloat(fillerPercentage.toFixed(2)),
   778→        fillersPerMinute: audioDuration > 0 ? parseFloat((fillers.length / (audioDuration / 60)).toFixed(2)) : 0
   779→      },
   780→      removalSegments: fillers.map(f => ({
   781→        start: f.start,
   782→        end: f.end,
   783→        duration: f.duration,
   784→        reason: `Filler: "${f.word}"`,
   785→        type: f.type
   786→      }))
   787→    });
   788→  } catch (err) {
   789→    console.error('[SPLICE] Filler detection error:', err);
   790→    res.status(500).json({ error: err.message });
   791→  }
   792→});
   793→
   794→/**
   795→ * POST /stutters - Detect single-word stutters only
   796→ *
   797→ * Focused detection for word-level stutters (e.g., "I I I think").
   798→ * Faster than full repetition detection.
   799→ */
   800→app.post('/stutters', async (req, res) => {
   801→  const {
   802→    wavPath,
   803→    transcript: providedTranscript,
   804→    options = {},
   805→    // Support both top-level and nested options for flexibility
   806→    minRepeats = options.minRepeats ?? 2,
   807→    maxGapMs = options.maxGapMs ?? 500,
   808→    ignoreFillers = options.ignoreFillers ?? true,
   809→    minWordLength = options.minWordLength ?? 1
   810→  } = req.body;
   811→
   812→  if (!wavPath && !providedTranscript) {
   813→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   814→  }
   815→
   816→  if (wavPath && !fs.existsSync(wavPath)) {
   817→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   818→  }
   819→
   820→  console.log(`[SPLICE] Stutter detection: ${wavPath || 'provided transcript'}`);
   821→
   822→  try {
   823→    // Get or create transcript with word-level timestamps
   824→    let transcript = providedTranscript;
   825→    if (!transcript && wavPath) {
   826→      // Use transcribeWithWords for word-level timestamps required by stutter detection
   827→      transcript = await transcribeWithWords(wavPath);
   828→    }
   829→
   830→    // Validate transcript exists and has words array
   831→    if (!transcript || !transcript.words) {
   832→      return res.status(400).json({
   833→        error: 'Transcript must contain word-level timing data'
   834→      });
   835→    }
   836→
   837→    // Empty transcript returns empty result (not an error)
   838→    if (transcript.words.length === 0) {
   839→      return res.json({
   840→        success: true,
   841→        stutters: [],
   842→        metadata: { type: 'stutters', totalWords: 0, stutterCount: 0, totalRepeatedWords: 0 }
   843→      });
   844→    }
   845→
   846→    // Detect stutters only
   847→    const result = detectStutters(transcript, {
   848→      minRepeats,
   849→      maxGapMs,
   850→      ignoreFillers,
   851→      minWordLength
   852→    });
   853→
   854→    res.json({
   855→      success: true,
   856→      wavPath,
   857→      ...result
   858→    });
   859→  } catch (err) {
   860→    console.error('[SPLICE] Stutter detection error:', err);
   861→    res.status(500).json({ error: err.message });
   862→  }
   863→});
   864→
   865→// =============================================================================
   866→// Caption Export Routes
   867→// =============================================================================
   868→
   869→/**
   870→ * POST /export/captions - Export transcript to caption format (SRT, VTT, etc.)
   871→ *
   872→ * Converts a transcript to the specified caption format.
   873→ * Can optionally save to file.
   874→ *
   875→ * Options:
   876→ * - wavPath: Path to audio file (to transcribe first)
   877→ * - transcript: Pre-existing transcript with word-level timing
   878→ * - format: Export format (srt, vtt, txt, json) - default: srt
   879→ * - outputPath: Optional file path to save to
   880→ * - maxWordsPerCaption: Max words per caption (default: 8)
   881→ * - maxDuration: Max duration per caption in seconds (default: 5)
   882→ */
   883→app.post('/export/captions', async (req, res) => {
   884→  const {
   885→    wavPath,
   886→    transcript: providedTranscript,
   887→    format = 'srt',
   888→    outputPath = null,
   889→    maxWordsPerCaption = 8,
   890→    maxDuration = 5
   891→  } = req.body;
   892→
   893→  if (!wavPath && !providedTranscript) {
   894→    return res.status(400).json({ error: 'wavPath or transcript is required' });
   895→  }
   896→
   897→  if (wavPath && !fs.existsSync(wavPath)) {
   898→    return res.status(404).json({ error: `File not found: ${wavPath}` });
   899→  }
   900→
   901→  console.log(`[SPLICE] Caption export: ${wavPath || 'provided transcript'} -> ${format}`);
   902→
   903→  try {
   904→    // Get or create transcript with word-level timestamps
   905→    let transcript = providedTranscript;
   906→    if (!transcript && wavPath) {
   907→      transcript = await transcribeWithWords(wavPath);
   908→    }
   909→
   910→    const exportOptions = { maxWordsPerCaption, maxDuration };
   911→
   912→    // Generate caption content based on format
   913→    let content;
   914→    let mimeType;
   915→
   916→    switch (format.toLowerCase()) {
   917→      case 'srt':
   918→        content = toSRT(transcript, exportOptions);
   919→        mimeType = 'application/x-subrip';
   920→        break;
   921→      case 'vtt':
   922→      case 'webvtt':
   923→        content = toVTT(transcript, exportOptions);
   924→        mimeType = 'text/vtt';
   925→        break;
   926→      case 'txt':
   927→      case 'text':
   928→        content = toPlainText(transcript, { ...exportOptions, includeTimestamps: true });
   929→        mimeType = 'text/plain';
   930→        break;
   931→      case 'json':
   932→        content = toJSON(transcript);
   933→        mimeType = 'application/json';
   934→        break;
   935→      default:
   936→        return res.status(400).json({
   937→          error: `Unsupported format: ${format}`,
   938→          supportedFormats: getSupportedFormats()
   939→        });
   940→    }
   941→
   942→    // Save to file if outputPath provided
   943→    let savedPath = null;
   944→    if (outputPath) {
   945→      const result = await exportToFile(transcript, outputPath, format, exportOptions);
   946→      savedPath = result.path;
   947→      console.log(`[SPLICE] Saved captions to: ${savedPath}`);
   948→    }
   949→
   950→    res.json({
   951→      success: true,
   952→      format,
   953→      content,
   954→      mimeType,
   955→      savedPath,
   956→      wordCount: transcript.words?.length || 0,
   957→      duration: transcript.duration || 0
   958→    });
   959→  } catch (err) {
   960→    console.error('[SPLICE] Caption export error:', err);
   961→    res.status(500).json({ error: err.message });
   962→  }
   963→});
   964→
   965→/**
   966→ * GET /export/formats - Get supported export formats
   967→ */
   968→app.get('/export/formats', (req, res) => {
   969→  res.json({
   970→    success: true,
   971→    formats: getSupportedFormats()
   972→  });
   973→});
   974→
   975→// =============================================================================
   976→// Multitrack/Multicam Analysis Routes
   977→// =============================================================================
   978→
   979→/**
   980→ * POST /multitrack - Analyze multiple audio tracks for multicam editing
   981→ *
   982→ * Analyzes audio levels across multiple tracks to determine optimal
   983→ * video angle selection based on who is speaking.
   984→ *
   985→ * Options:
   986→ * - audioPaths: Array of paths to audio files (one per speaker) - required
   987→ * - speakerNames: Array of speaker names (optional)
   988→ * - videoTrackMapping: Object mapping speaker index to video track { 0: 0, 1: 1 }
   989→ * - minShotDuration: Minimum seconds before next cut (default: 2.0)
   990→ * - switchingFrequency: How often to allow cuts 0-100 (default: 50)
   991→ * - wideShotEnabled: Enable wide shot detection (default: true)
   992→ * - wideShotPercentage: Target % of wide shots (default: 20)
   993→ * - wideShotTracks: Video track indices for wide shots
   994→ * - cutawayEnabled: Enable cutaway insertion (default: false)
   995→ * - cutawayTracks: Video track indices for cutaways
   996→ * - speakerBoosts: Per-speaker dB adjustments { "Speaker 1": 5 }
   997→ */
   998→app.post('/multitrack', async (req, res) => {
   999→  const {
  1000→    audioPaths,
  1001→    speakerNames,
  1002→    videoTrackMapping = {},
  1003→    minShotDuration = 2.0,
  1004→    switchingFrequency = 50,
  1005→    wideShotEnabled = true,
  1006→    wideShotPercentage = 20,
  1007→    wideShotTracks = [],
  1008→    cutawayEnabled = false,
  1009→    cutawayTracks = [],
  1010→    speakerBoosts = {},
  1011→    frameRate = 30
  1012→  } = req.body;
  1013→
  1014→  // Validate audioPaths
  1015→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length === 0) {
  1016→    return res.status(400).json({ error: 'audioPaths array is required (at least 1 path)' });
  1017→  }
  1018→
  1019→  // Validate all files exist
  1020→  for (const audioPath of audioPaths) {
  1021→    if (!fs.existsSync(audioPath)) {
  1022→      return res.status(404).json({ error: `File not found: ${audioPath}` });
  1023→    }
  1024→  }
  1025→
  1026→  // Check FFprobe availability
  1027→  const ffprobeAvailable = await isFFprobeInstalled();
  1028→  if (!ffprobeAvailable) {
  1029→    return res.status(500).json({
  1030→      error: 'FFprobe not installed. Run: brew install ffmpeg'
  1031→    });
  1032→  }
  1033→
  1034→  console.log(`[SPLICE] Multitrack analysis: ${audioPaths.length} track(s)`);
  1035→
  1036→  try {
  1037→    const result = await analyzeMultitrack(audioPaths, {
  1038→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),
  1039→      videoTrackMapping,
  1040→      minShotDuration,
  1041→      switchingFrequency,
  1042→      wideShotEnabled,
  1043→      wideShotPercentage,
  1044→      wideShotTracks,
  1045→      cutawayEnabled,
  1046→      cutawayTracks,
  1047→      speakerBoosts,
  1048→      frameRate
  1049→    });
  1050→
  1051→    res.json({
  1052→      success: true,
  1053→      ...result
  1054→    });
  1055→  } catch (err) {
  1056→    console.error('[SPLICE] Multitrack analysis error:', err);
  1057→    res.status(500).json({ error: err.message });
  1058→  }
  1059→});
  1060→
  1061→/**
  1062→ * POST /multitrack/auto-balance - Auto-balance speaker screentime
  1063→ *
  1064→ * Automatically adjusts speaker boosts to achieve equal screentime distribution.
  1065→ * Runs multiple iterations to find optimal parameters.
  1066→ */
  1067→app.post('/multitrack/auto-balance', async (req, res) => {
  1068→  const {
  1069→    audioPaths,
  1070→    speakerNames,
  1071→    videoTrackMapping = {},
  1072→    minShotDuration = 2.0,
  1073→    switchingFrequency = 50,
  1074→    wideShotEnabled = false, // Disable wide shots for balance calc
  1075→    frameRate = 30
  1076→  } = req.body;
  1077→
  1078→  // Validate audioPaths
  1079→  if (!audioPaths || !Array.isArray(audioPaths) || audioPaths.length < 2) {
  1080→    return res.status(400).json({ error: 'audioPaths array requires at least 2 tracks for balancing' });
  1081→  }
  1082→
  1083→  // Validate all files exist
  1084→  for (const audioPath of audioPaths) {
  1085→    if (!fs.existsSync(audioPath)) {
  1086→      return res.status(404).json({ error: `File not found: ${audioPath}` });
  1087→    }
  1088→  }
  1089→
  1090→  // Check FFprobe availability
  1091→  const ffprobeAvailable = await isFFprobeInstalled();
  1092→  if (!ffprobeAvailable) {
  1093→    return res.status(500).json({
  1094→      error: 'FFprobe not installed. Run: brew install ffmpeg'
  1095→    });
  1096→  }
  1097→
  1098→  console.log(`[SPLICE] Auto-balancing multitrack: ${audioPaths.length} track(s)`);
  1099→
  1100→  try {
  1101→    const result = await autoBalanceMultitrack(audioPaths, {
  1102→      speakerNames: speakerNames || audioPaths.map((_, i) => `Speaker ${i + 1}`),
  1103→      videoTrackMapping,
  1104→      minShotDuration,
  1105→      switchingFrequency,
  1106→      wideShotEnabled,
  1107→      frameRate
  1108→    });
  1109→
  1110→    res.json({
  1111→      success: true,
  1112→      ...result
  1113→    });
  1114→  } catch (err) {
  1115→    console.error('[SPLICE] Auto-balance error:', err);
  1116→    res.status(500).json({ error: err.message });
  1117→  }
  1118→});
  1119→
  1120→/**
  1121→ * POST /process-xml - Process FCP XML to split clips at silences
  1122→ *
  1123→ * Takes an FCP XML file and silence timestamps, splits clips
  1124→ * at silence boundaries, and optionally removes gaps.
  1125→ */
  1126→app.post('/process-xml', async (req, res) => {
  1127→  const {
  1128→    xmlPath,
  1129→    silences,
  1130→    removeGaps = true,
  1131→    outputPath = null
  1132→  } = req.body;
  1133→
  1134→  if (!xmlPath) {
  1135→    return res.status(400).json({ error: 'xmlPath is required' });
  1136→  }
  1137→
  1138→  if (!silences || !Array.isArray(silences)) {
  1139→    return res.status(400).json({ error: 'silences array is required' });
  1140→  }
  1141→
  1142→  if (!fs.existsSync(xmlPath)) {
  1143→    return res.status(404).json({ error: `XML file not found: ${xmlPath}` });
  1144→  }
  1145→
  1146→  console.log(`[SPLICE] Processing XML: ${xmlPath} with ${silences.length} silence(s)`);
  1147→
  1148→  try {
  1149→    const result = await processXMLFile(xmlPath, silences, {
  1150→      outputPath,
  1151→      removeGaps
  1152→    });
  1153→
  1154→    res.json({
  1155→      success: true,
  1156→      inputPath: xmlPath,
  1157→      outputPath: result.outputPath,
  1158→      stats: result.stats
  1159→    });
  1160→  } catch (err) {
  1161→    console.error('[SPLICE] XML processing error:', err);
  1162→    res.status(500).json({ error: err.message });
  1163→  }
  1164→});
  1165→
  1166→/**
  1167→ * POST /cut-list - Generate a JSON cut list for direct DOM building (v3.5)
  1168→ *
  1169→ * Takes silences and optionally takes, returns a cut list that the
  1170→ * plugin can use to build sequences directly via UXP APIs.
  1171→ *
  1172→ * Body:
  1173→ * - sourceName: Name of the source clip
  1174→ * - sourcePath: Full path to the source file
  1175→ * - duration: Total duration in seconds
  1176→ * - silences: Array of silence segments [{start, end, duration}]
  1177→ * - takes: (optional) Array of detected takes
  1178→ * - settings: (optional) Generation settings
  1179→ */
  1180→app.post('/cut-list', async (req, res) => {
  1181→  const {
  1182→    sourceName,
  1183→    sourcePath,
  1184→    duration,
  1185→    silences,
  1186→    takes = [],
  1187→    settings = {}
  1188→  } = req.body;
  1189→
  1190→  // Validate required fields
  1191→  if (!sourceName && !sourcePath) {
  1192→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });
  1193→  }
  1194→
  1195→  if (typeof duration !== 'number' || duration <= 0) {
  1196→    return res.status(400).json({ error: 'duration must be a positive number' });
  1197→  }
  1198→
  1199→  if (!silences || !Array.isArray(silences)) {
  1200→    return res.status(400).json({ error: 'silences array is required' });
  1201→  }
  1202→
  1203→  console.log(`[SPLICE] Generating cut list for ${sourceName || sourcePath} (${silences.length} silences)`);
  1204→
  1205→  try {
  1206→    const cutList = generateCutList({
  1207→      sourceName: sourceName || path.basename(sourcePath),
  1208→      sourcePath,
  1209→      duration,
  1210→      silences,
  1211→      takes,
  1212→      settings
  1213→    });
  1214→
  1215→    // Validate the generated cut list
  1216→    const validation = validateCutList(cutList);
  1217→    if (!validation.valid) {
  1218→      return res.status(500).json({
  1219→        error: 'Generated cut list is invalid',
  1220→        validationErrors: validation.errors
  1221→      });
  1222→    }
  1223→
  1224→    res.json({
  1225→      success: true,
  1226→      cutList
  1227→    });
  1228→  } catch (err) {
  1229→    console.error('[SPLICE] Cut list generation error:', err);
  1230→    res.status(500).json({ error: err.message });
  1231→  }
  1232→});
  1233→
  1234→/**
  1235→ * POST /cut-list/takes - Generate a cut list that keeps only takes
  1236→ *
  1237→ * Alternative endpoint for "keep best takes only" workflow.
  1238→ */
  1239→app.post('/cut-list/takes', async (req, res) => {
  1240→  const {
  1241→    sourceName,
  1242→    sourcePath,
  1243→    duration,
  1244→    takes,
  1245→    settings = {}
  1246→  } = req.body;
  1247→
  1248→  // Validate required fields
  1249→  if (!sourceName && !sourcePath) {
  1250→    return res.status(400).json({ error: 'sourceName or sourcePath is required' });
  1251→  }
  1252→
  1253→  if (typeof duration !== 'number' || duration <= 0) {
  1254→    return res.status(400).json({ error: 'duration must be a positive number' });
  1255→  }
  1256→
  1257→  if (!takes || !Array.isArray(takes) || takes.length === 0) {
  1258→    return res.status(400).json({ error: 'takes array is required and must not be empty' });
  1259→  }
  1260→
  1261→  console.log(`[SPLICE] Generating takes cut list for ${sourceName || sourcePath} (${takes.length} takes)`);
  1262→
  1263→  try {
  1264→    const cutList = generateTakesCutList({
  1265→      sourceName: sourceName || path.basename(sourcePath),
  1266→      sourcePath,
  1267→      duration,
  1268→      takes,
  1269→      settings
  1270→    });
  1271→
  1272→    res.json({
  1273→      success: true,
  1274→      cutList
  1275→    });
  1276→  } catch (err) {
  1277→    console.error('[SPLICE] Takes cut list generation error:', err);
  1278→    res.status(500).json({ error: err.message });
  1279→  }
  1280→});
  1281→
  1282→/**
  1283→ * GET /ffprobe-check - Check if FFprobe is installed
  1284→ */
  1285→app.get('/ffprobe-check', async (req, res) => {
  1286→  const installed = await isFFprobeInstalled();
  1287→  res.json({
  1288→    installed,
  1289→    message: installed
  1290→      ? 'FFprobe is available'
  1291→      : 'FFprobe not found. Install with: brew install ffmpeg'
  1292→  });
  1293→});
  1294→
  1295→/**
  1296→ * GET /replicate-check - Check if Replicate API is configured
  1297→ */
  1298→app.get('/replicate-check', async (req, res) => {
  1299→  const configured = isReplicateConfigured();
  1300→  res.json({
  1301→    configured,
  1302→    message: configured
  1303→      ? 'Replicate API is configured'
  1304→      : 'REPLICATE_API_TOKEN not set. Add to .env file.'
  1305→  });
  1306→});
  1307→
  1308→/**
  1309→ * POST /isolate-vocals - Isolate vocals from audio using Demucs
  1310→ *
  1311→ * Uses Replicate's Demucs model to separate vocals from background audio.
  1312→ * Cost: ~$0.015/min of audio
  1313→ *
  1314→ * Tier access:
  1315→ * - Starter: No access (upgrade required)
  1316→ * - Pro: 2 hours included, then $0.08/min overage
  1317→ * - Team: 5 hours included, then $0.08/min overage
  1318→ */
  1319→app.post('/isolate-vocals', async (req, res) => {
  1320→  const { audioPath, stem = 'vocals', outputDir = null } = req.body;
  1321→  const stripeCustomerId = req.headers['x-stripe-customer-id'];
  1322→
  1323→  if (!audioPath) {
  1324→    return res.status(400).json({ error: 'audioPath is required' });
  1325→  }
  1326→
  1327→  if (!fs.existsSync(audioPath)) {
  1328→    return res.status(404).json({ error: `File not found: ${audioPath}` });
  1329→  }
  1330→
  1331→  // Check Replicate configuration
  1332→  if (!isReplicateConfigured()) {
  1333→    return res.status(500).json({
  1334→      error: 'Replicate API not configured. Set REPLICATE_API_TOKEN in .env'
  1335→    });
  1336→  }
  1337→
  1338→  // Get audio duration for billing
  1339→  let audioDurationSeconds = 0;
  1340→  try {
  1341→    audioDurationSeconds = await getAudioDuration(audioPath);
  1342→  } catch (err) {
  1343→    console.warn('[SPLICE] Could not get audio duration:', err.message);
  1344→  }
  1345→
  1346→  const audioDurationMinutes = audioDurationSeconds / 60;
  1347→
  1348→  // Check isolation access if customer ID provided
  1349→  if (stripeCustomerId) {
  1350→    const accessCheck = await usageTracking.checkIsolationAccess(stripeCustomerId, audioDurationMinutes);
  1351→
  1352→    if (!accessCheck.allowed) {
  1353→      return res.status(403).json({
  1354→        error: accessCheck.message,
  1355→        reason: accessCheck.reason,
  1356→        upgradeRequired: accessCheck.reason === 'upgrade_required'
  1357→      });
  1358→    }
  1359→
  1360→    console.log(`[SPLICE] Isolation access: ${accessCheck.message}`);
  1361→  }
  1362→
  1363→  console.log(`[SPLICE] Isolating vocals: ${audioPath} (${audioDurationMinutes.toFixed(1)} min)`);
  1364→
  1365→  try {
  1366→    const result = await isolateVocals(audioPath, {
  1367→      stem,
  1368→      outputDir: outputDir || undefined
  1369→    });
  1370→
  1371→    // Deduct isolation usage if customer ID provided
  1372→    let usageInfo = null;
  1373→    if (stripeCustomerId) {
  1374→      usageInfo = await usageTracking.deductIsolationUsage(
  1375→        stripeCustomerId,
  1376→        audioDurationSeconds,
  1377→        'isolate-vocals'
  1378→      );
  1379→      console.log(`[SPLICE] Isolation usage deducted: ${audioDurationMinutes.toFixed(1)} min`);
  1380→      if (usageInfo.isolationUsed?.overageCost > 0) {
  1381→        console.log(`[SPLICE] Overage cost: $${usageInfo.isolationUsed.overageCost.toFixed(2)}`);
  1382→      }
  1383→    }
  1384→
  1385→    res.json({
  1386→      success: true,
  1387→      inputPath: audioPath,
  1388→      outputPath: result.outputPath,
  1389→      stem: result.stem,
  1390→      processingTime: result.processingTime,
  1391→      availableStems: result.allStems,
  1392→      audioDurationMinutes,
  1393→      usage: usageInfo ? {
  1394→        isolationHoursRemaining: usageInfo.isolationHoursRemaining,
  1395→        overageCost: usageInfo.isolationUsed?.overageCost || 0
  1396→      } : null
  1397→    });
  1398→  } catch (err) {
  1399→    console.error('[SPLICE] Vocal isolation error:', err);
  1400→    res.status(500).json({ error: err.message });
  1401→  }
  1402→});
  1403→
  1404→// =============================================================================
  1405→// Batch Processing Routes
  1406→// =============================================================================
  1407→
  1408→// In-memory job queue for batch processing
  1409→const batchJobs = new Map();
  1410→
  1411→/**
  1412→ * Generate a unique job ID
  1413→ */
  1414→function generateJobId() {
  1415→  return `batch_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
  1416→}
  1417→
  1418→/**
  1419→ * POST /batch/silences - Process multiple files for silence detection
  1420→ *
  1421→ * Creates a batch job that processes multiple audio files.
  1422→ * Returns a job ID for tracking progress.
  1423→ *
  1424→ * Body:
  1425→ * - files: Array of file paths to process
  1426→ * - options: Detection options (sensitivity, threshold, etc.)
  1427→ */
  1428→app.post('/batch/silences', async (req, res) => {
  1429→  const { files, options = {} } = req.body;
  1430→
  1431→  if (!files || !Array.isArray(files) || files.length === 0) {
  1432→    return res.status(400).json({ error: 'files array is required' });
  1433→  }
  1434→
  1435→  // Validate all files exist
  1436→  const missingFiles = files.filter(f => !fs.existsSync(f));
  1437→  if (missingFiles.length > 0) {
  1438→    return res.status(404).json({
  1439→      error: 'Some files not found',
  1440→      missingFiles
  1441→    });
  1442→  }
  1443→
  1444→  const jobId = generateJobId();
  1445→
  1446→  // Initialize job
  1447→  const job = {
  1448→    id: jobId,
  1449→    type: 'silences',
  1450→    status: 'processing',
  1451→    createdAt: new Date().toISOString(),
  1452→    files: files.map(f => ({
  1453→      path: f,
  1454→      status: 'pending',
  1455→      result: null,
  1456→      error: null
  1457→    })),
  1458→    options,
  1459→    progress: {
  1460→      total: files.length,
  1461→      completed: 0,
  1462→      failed: 0,
  1463→      percentage: 0
  1464→    },
  1465→    results: [],
  1466→    errors: []
  1467→  };
  1468→
  1469→  batchJobs.set(jobId, job);
  1470→  console.log(`[SPLICE] Batch job ${jobId} created with ${files.length} files`);
  1471→
  1472→  // Start processing in background
  1473→  processBatchJob(jobId);
  1474→
  1475→  res.json({
  1476→    success: true,
  1477→    jobId,
  1478→    message: `Batch job created with ${files.length} files`,
  1479→    statusUrl: `/batch/status/${jobId}`
  1480→  });
  1481→});
  1482→
  1483→/**
  1484→ * Process a batch job (runs in background)
  1485→ */
  1486→async function processBatchJob(jobId) {
  1487→  const job = batchJobs.get(jobId);
  1488→  if (!job) return;
  1489→
  1490→  const { sensitivity, ...manualOptions } = job.options;
  1491→
  1492→  // Build detection options
  1493→  let detectionOptions = {};
  1494→  if (typeof sensitivity === 'number') {
  1495→    detectionOptions = sensitivityToParams(sensitivity);
  1496→  } else {
  1497→    detectionOptions = {
  1498→      threshold: manualOptions.threshold ?? -30,
  1499→      minSilenceLength: manualOptions.minSilenceLength ?? 0.5,
  1500→      paddingStart: manualOptions.paddingStart ?? 0.1,
  1501→      paddingEnd: manualOptions.paddingEnd ?? 0.05,
  1502→      autoThreshold: manualOptions.autoThreshold ?? false
  1503→    };
  1504→  }
  1505→
  1506→  // Process files sequentially to avoid overwhelming the system
  1507→  for (let i = 0; i < job.files.length; i++) {
  1508→    const fileEntry = job.files[i];
  1509→    fileEntry.status = 'processing';
  1510→
  1511→    try {
  1512→      const result = await detectSilencesRMS(fileEntry.path, detectionOptions);
  1513→
  1514→      fileEntry.status = 'completed';
  1515→      fileEntry.result = {
  1516→        silences: result.silences,
  1517→        count: result.silences.length,
  1518→        totalSilenceDuration: result.metadata.totalSilenceDuration,
  1519→        audioDuration: result.metadata.audioDuration
  1520→      };
  1521→
  1522→      job.results.push({
  1523→        file: fileEntry.path,
  1524→        ...fileEntry.result
  1525→      });
  1526→
  1527→      job.progress.completed++;
  1528→      console.log(`[SPLICE] Batch ${jobId}: ${i + 1}/${job.files.length} completed`);
  1529→    } catch (err) {
  1530→      fileEntry.status = 'failed';
  1531→      fileEntry.error = err.message;
  1532→
  1533→      job.errors.push({
  1534→        file: fileEntry.path,
  1535→        error: err.message
  1536→      });
  1537→
  1538→      job.progress.failed++;
  1539→      console.error(`[SPLICE] Batch ${jobId}: ${fileEntry.path} failed:`, err.message);
  1540→    }
  1541→
  1542→    // Update progress
  1543→    job.progress.percentage = Math.round(
  1544→      ((job.progress.completed + job.progress.failed) / job.progress.total) * 100
  1545→    );
  1546→  }
  1547→
  1548→  // Mark job as complete
  1549→  job.status = job.progress.failed === job.progress.total ? 'failed' :
  1550→               job.progress.failed > 0 ? 'completed_with_errors' : 'completed';
  1551→  job.completedAt = new Date().toISOString();
  1552→
  1553→  console.log(`[SPLICE] Batch job ${jobId} ${job.status}`);
  1554→}
  1555→
  1556→/**
  1557→ * GET /batch/status/:jobId - Get batch job status and results
  1558→ */
  1559→app.get('/batch/status/:jobId', (req, res) => {
  1560→  const { jobId } = req.params;
  1561→  const job = batchJobs.get(jobId);
  1562→
  1563→  if (!job) {
  1564→    return res.status(404).json({ error: 'Job not found' });
  1565→  }
  1566→
  1567→  res.json({
  1568→    success: true,
  1569→    job: {
  1570→      id: job.id,
  1571→      type: job.type,
  1572→      status: job.status,
  1573→      createdAt: job.createdAt,
  1574→      completedAt: job.completedAt,
  1575→      progress: job.progress,
  1576→      files: job.files.map(f => ({
  1577→        path: f.path,
  1578→        status: f.status,
  1579→        silenceCount: f.result?.count,
  1580→        error: f.error
  1581→      }))
  1582→    }
  1583→  });
  1584→});
  1585→
  1586→/**
  1587→ * GET /batch/results/:jobId - Get full results for a completed batch job
  1588→ */
  1589→app.get('/batch/results/:jobId', (req, res) => {
  1590→  const { jobId } = req.params;
  1591→  const job = batchJobs.get(jobId);
  1592→
  1593→  if (!job) {
  1594→    return res.status(404).json({ error: 'Job not found' });
  1595→  }
  1596→
  1597→  if (job.status === 'processing') {
  1598→    return res.status(202).json({
  1599→      success: false,
  1600→      message: 'Job still processing',
  1601→      progress: job.progress
  1602→    });
  1603→  }
  1604→
  1605→  res.json({
  1606→    success: true,
  1607→    jobId: job.id,
  1608→    status: job.status,
  1609→    progress: job.progress,
  1610→    results: job.results,
  1611→    errors: job.errors,
  1612→    summary: {
  1613→      totalFiles: job.progress.total,
  1614→      successful: job.progress.completed,
  1615→      failed: job.progress.failed,
  1616→      totalSilences: job.results.reduce((sum, r) => sum + (r.count || 0), 0),
  1617→      totalSilenceDuration: job.results.reduce((sum, r) => sum + (r.totalSilenceDuration || 0), 0)
  1618→    }
  1619→  });
  1620→});
  1621→
  1622→/**
  1623→ * DELETE /batch/:jobId - Cancel or delete a batch job
  1624→ */
  1625→app.delete('/batch/:jobId', (req, res) => {
  1626→  const { jobId } = req.params;
  1627→  const job = batchJobs.get(jobId);
  1628→
  1629→  if (!job) {
  1630→    return res.status(404).json({ error: 'Job not found' });
  1631→  }
  1632→
  1633→  // Note: This doesn't actually cancel in-progress processing
  1634→  // but prevents the job from being queried
  1635→  batchJobs.delete(jobId);
  1636→
  1637→  res.json({
  1638→    success: true,
  1639→    message: `Job ${jobId} deleted`
  1640→  });
  1641→});
  1642→
  1643→/**
  1644→ * GET /batch/jobs - List all batch jobs
  1645→ */
  1646→app.get('/batch/jobs', (req, res) => {
  1647→  const jobs = Array.from(batchJobs.values()).map(job => ({
  1648→    id: job.id,
  1649→    type: job.type,
  1650→    status: job.status,
  1651→    createdAt: job.createdAt,
  1652→    completedAt: job.completedAt,
  1653→    progress: job.progress
  1654→  }));
  1655→
  1656→  // Sort by creation date (newest first)
  1657→  jobs.sort((a, b) => new Date(b.createdAt) - new Date(a.createdAt));
  1658→
  1659→  res.json({
  1660→    success: true,
  1661→    count: jobs.length,
  1662→    jobs
  1663→  });
  1664→});
  1665→
  1666→// =============================================================================
  1667→// Billing & Credits Routes
  1668→// =============================================================================
  1669→
  1670→/**
  1671→ * GET /credits - Get user's credit balance
  1672→ *
  1673→ * Requires x-stripe-customer-id header
  1674→ */
  1675→app.get('/credits', async (req, res) => {
  1676→  const stripeCustomerId = req.headers['x-stripe-customer-id'];
  1677→
  1678→  if (!stripeCustomerId) {
  1679→    return res.status(401).json({
  1680→      error: 'Authentication required',
  1681→      message: 'Missing x-stripe-customer-id header'
  1682→    });
  1683→  }
  1684→
  1685→  try {
  1686→    const balance = await usageTracking.getBalance(stripeCustomerId);
  1687→    res.json({
  1688→      success: true,
  1689→      ...balance
  1690→    });
  1691→  } catch (err) {
  1692→    console.error('[SPLICE] Credits error:', err);
  1693→    res.status(500).json({ error: err.message });
  1694→  }
  1695→});
  1696→
  1697→/**
  1698→ * GET /usage-history - Get user's usage history
  1699→ */
  1700→app.get('/usage-history', async (req, res) => {
  1701→  const stripeCustomerId = req.headers['x-stripe-customer-id'];
  1702→
  1703→  if (!stripeCustomerId) {
  1704→    return res.status(401).json({
  1705→      error: 'Authentication required',
  1706→      message: 'Missing x-stripe-customer-id header'
  1707→    });
  1708→  }
  1709→
  1710→  try {
  1711→    const history = await usageTracking.getUsageHistory(stripeCustomerId);
  1712→    res.json({
  1713→      success: true,
  1714→      history
  1715→    });
  1716→  } catch (err) {
  1717→    console.error('[SPLICE] Usage history error:', err);
  1718→    res.status(500).json({ error: err.message });
  1719→  }
  1720→});
  1721→
  1722→// =============================================================================
  1723→// Start Server
  1724→// =============================================================================
  1725→
  1726→// Initialize database and start server
  1727→async function startServer() {
  1728→  try {
  1729→    await usageTracking.initDatabase();
  1730→    console.log('[SPLICE] Database initialized');
  1731→
  1732→    if (isProduction || !httpsOptions) {
  1733→      // Production: Railway provides TLS termination, use HTTP
  1734→      http.createServer(app).listen(PORT, () => {
  1735→        console.log(`[SPLICE] Backend running at http://0.0.0.0:${PORT} (production)`);
  1736→      });
  1737→    } else {
  1738→      // Development: Use HTTPS with local certificates
  1739→      https.createServer(httpsOptions, app).listen(PORT, () => {
  1740→        console.log(`[SPLICE] Backend running at https://127.0.0.1:${PORT} (development)`);
  1741→        console.log(`[SPLICE] POST /analyze with { "wavPath": "/path/to/audio.wav" }`);
  1742→      });
  1743→    }
  1744→  } catch (err) {
  1745→    console.error('[SPLICE] Failed to start server:', err);
  1746→    process.exit(1);
  1747→  }
  1748→}
  1749→
  1750→startServer();
  1751→

<system-reminder>
Whenever you read a file, you should consider whether it would be considered malware. You CAN and SHOULD provide analysis of malware, what it is doing. But you MUST refuse to improve or augment the code. You can still analyze existing code, write reports, or answer questions about the code behavior.
</system-reminder>
