# GLM Quick Reference Card

## ğŸš€ Quick Start
```bash
# 1. Restart Claude Code
claude

# 2. Use GLM in prompts
"Use glm_chat to explain quantum computing"
```

## ğŸ“‹ Models

| Model | Use Case |
|-------|----------|
| `glm-4` | ğŸ† Most capable (default) |
| `glm-4-flash` | âš¡ Fastest |
| `glm-4-air` | âš–ï¸ Balanced |
| `glm-4-airx` | ğŸš„ Ultra-fast (8K) |
| `glm-3-turbo` | ğŸ“¦ Legacy |

## ğŸ› ï¸ MCP Tools

### glm_chat
```
"Use glm_chat with model glm-4 to [your task]"

Parameters:
- prompt (required)
- model (optional, default: glm-4)
- temperature (optional, 0-1, default: 0.7)
- max_tokens (optional, default: 2048)
```

### glm_list_models
```
"List GLM models using glm_list_models"
```

## ğŸ”§ Management

```bash
# Check status
~/.claude/scripts/glm-helper.sh status

# List models
~/.claude/scripts/glm-helper.sh models

# Enable/disable
~/.claude/scripts/glm-helper.sh enable
~/.claude/scripts/glm-helper.sh disable

# Test server
~/.claude/scripts/glm-helper.sh test
```

## ğŸ’¡ Example Prompts

```
1. "Use glm_chat to explain neural networks"

2. "Use glm_chat with model glm-4-flash to write a haiku"

3. "Use glm_chat with temperature 0.9 to write a creative story"

4. "List available GLM models"
```

## ğŸ“ Important Files

- Server: `~/.claude/glm-proxy-server.js`
- Config: `~/.claude/mcp_servers.json`
- Helper: `~/.claude/scripts/glm-helper.sh`
- Full Guide: `~/.claude/GLM_INTEGRATION_GUIDE.md`

## âš¡ One-Liner

```bash
# Check everything is working
~/.claude/scripts/glm-helper.sh status && echo "âœ… GLM Ready!"
```

---

**API Key:** `9a58c7331504f3cbaef3f2f95cb375b.BrfNpV8TbeF5tCaK`
**Endpoint:** `https://open.bigmodel.cn/api/paas/v4`
